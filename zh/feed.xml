<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes â€“ ç”Ÿäº§çº§åˆ«çš„å®¹å™¨ç¼–æŽ’ç³»ç»Ÿ</title><link>https://kubernetes.io/zh/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/zh/</link></image><atom:link href="https://kubernetes.io/zh/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;!--
**Authors:** [Kubernetes 1.18 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 å‘å¸ƒå›¢é˜Ÿ&lt;/a>&lt;/p>
&lt;!--
We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.
-->
&lt;p>æˆ‘ä»¬å¾ˆé«˜å…´å®£å¸ƒ Kubernetes 1.18 ç‰ˆæœ¬çš„äº¤ä»˜ï¼Œè¿™æ˜¯æˆ‘ä»¬ 2020 å¹´çš„ç¬¬ä¸€ç‰ˆï¼ Kubernetes 1.18 åŒ…å« 38 ä¸ªå¢žå¼ºåŠŸèƒ½ï¼š15 é¡¹å¢žå¼ºåŠŸèƒ½å·²è½¬ä¸ºç¨³å®šç‰ˆï¼Œ11 é¡¹å¢žå¼ºåŠŸèƒ½å¤„äºŽ beta é˜¶æ®µï¼Œ12 é¡¹å¢žå¼ºåŠŸèƒ½å¤„äºŽ alpha é˜¶æ®µã€‚&lt;/p>
&lt;!--
Kubernetes 1.18 is a "fit and finish" release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
-->
&lt;p>Kubernetes 1.18 æ˜¯ä¸€ä¸ªè¿‘ä¹Ž â€œå®Œç¾Žâ€ çš„ç‰ˆæœ¬ã€‚ ä¸ºäº†æ”¹å–„ beta å’Œç¨³å®šçš„ç‰¹æ€§ï¼Œå·²è¿›è¡Œäº†å¤§é‡å·¥ä½œï¼Œä»¥ç¡®ä¿ç”¨æˆ·èŽ·å¾—æ›´å¥½çš„ä½“éªŒã€‚ æˆ‘ä»¬åœ¨å¢žå¼ºçŽ°æœ‰åŠŸèƒ½çš„åŒæ—¶ä¹Ÿå¢žåŠ äº†ä»¤äººå…´å¥‹çš„æ–°ç‰¹æ€§ï¼Œè¿™äº›æœ‰æœ›è¿›ä¸€æ­¥å¢žå¼ºç”¨æˆ·ä½“éªŒã€‚&lt;/p>
&lt;!--
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.
-->
&lt;p>å¯¹ alphaï¼Œbeta å’Œç¨³å®šç‰ˆè¿›è¡Œå‡ ä¹ŽåŒç­‰ç¨‹åº¦çš„å¢žå¼ºæ˜¯ä¸€é¡¹ä¼Ÿå¤§çš„æˆå°±ã€‚ å®ƒå±•çŽ°äº†ç¤¾åŒºåœ¨æé«˜ Kubernetes çš„å¯é æ€§ä»¥åŠç»§ç»­æ‰©å±•å…¶çŽ°æœ‰åŠŸèƒ½æ–¹é¢æ‰€åšçš„å·¨å¤§åŠªåŠ›ã€‚&lt;/p>
&lt;!--
## Major Themes
-->
&lt;h2 id="ä¸»è¦å†…å®¹">ä¸»è¦å†…å®¹&lt;/h2>
&lt;!--
### Kubernetes Topology Manager Moves to Beta - Align Up!
-->
&lt;h3 id="kubernetes-æ‹“æ‰‘ç®¡ç†å™¨-topology-manager-è¿›å…¥-beta-é˜¶æ®µ-å¯¹é½">Kubernetes æ‹“æ‰‘ç®¡ç†å™¨ï¼ˆTopology Managerï¼‰è¿›å…¥ Beta é˜¶æ®µ - å¯¹é½ï¼&lt;/h3>
&lt;!--
A beta feature of Kubernetes in release 1.18, the [Topology Manager feature](https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md) enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.
-->
&lt;p>Kubernetes åœ¨ 1.18 ç‰ˆä¸­çš„ Beta é˜¶æ®µåŠŸèƒ½ &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">æ‹“æ‰‘ç®¡ç†å™¨ç‰¹æ€§&lt;/a> å¯ç”¨ CPU å’Œè®¾å¤‡ï¼ˆä¾‹å¦‚ SR-IOV VFï¼‰çš„ NUMA å¯¹é½ï¼Œè¿™å°†ä½¿æ‚¨çš„å·¥ä½œè´Ÿè½½åœ¨é’ˆå¯¹ä½Žå»¶è¿Ÿè€Œä¼˜åŒ–çš„çŽ¯å¢ƒä¸­è¿è¡Œã€‚åœ¨å¼•å…¥æ‹“æ‰‘ç®¡ç†å™¨ä¹‹å‰ï¼ŒCPU å’Œè®¾å¤‡ç®¡ç†å™¨å°†åšå‡ºå½¼æ­¤ç‹¬ç«‹çš„èµ„æºåˆ†é…å†³ç­–ã€‚ è¿™å¯èƒ½ä¼šå¯¼è‡´åœ¨å¤šå¤„ç†å™¨ç³»ç»Ÿä¸Šéžé¢„æœŸçš„èµ„æºåˆ†é…ç»“æžœï¼Œä»Žè€Œå¯¼è‡´å¯¹å»¶è¿Ÿæ•æ„Ÿçš„åº”ç”¨ç¨‹åºçš„æ€§èƒ½ä¸‹é™ã€‚&lt;/p>
&lt;!--
### Serverside Apply Introduces Beta 2
-->
&lt;h3 id="serverside-apply-æŽ¨å‡ºbeta-2">Serverside Apply æŽ¨å‡ºBeta 2&lt;/h3>
&lt;!--
Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.
-->
&lt;p>Serverside Apply åœ¨1.16 ä¸­è¿›å…¥ Beta é˜¶æ®µï¼Œä½†çŽ°åœ¨åœ¨ 1.18 ä¸­è¿›å…¥äº†ç¬¬äºŒä¸ª Beta é˜¶æ®µã€‚ è¿™ä¸ªæ–°ç‰ˆæœ¬å°†è·Ÿè¸ªå’Œç®¡ç†æ‰€æœ‰æ–° Kubernetes å¯¹è±¡çš„å­—æ®µæ›´æ”¹ï¼Œä»Žè€Œä½¿æ‚¨çŸ¥é“ä»€ä¹ˆæ›´æ”¹äº†èµ„æºä»¥åŠä½•æ—¶å‘ç”Ÿäº†æ›´æ”¹ã€‚&lt;/p>
&lt;!--
### Extending Ingress with and replacing a deprecated annotation with IngressClass
-->
&lt;h3 id="ä½¿ç”¨-ingressclass-æ‰©å±•-ingress-å¹¶ç”¨-ingressclass-æ›¿æ¢å·²å¼ƒç”¨çš„æ³¨é‡Š">ä½¿ç”¨ IngressClass æ‰©å±• Ingress å¹¶ç”¨ IngressClass æ›¿æ¢å·²å¼ƒç”¨çš„æ³¨é‡Š&lt;/h3>
&lt;!--
In Kubernetes 1.18, there are two significant additions to Ingress: A new `pathType` field and a new `IngressClass` resource. The `pathType` field allows specifying how paths should be matched. In addition to the default `ImplementationSpecific` type, there are new `Exact` and `Prefix` path types.
-->
&lt;p>åœ¨ Kubernetes 1.18 ä¸­ï¼ŒIngress æœ‰ä¸¤ä¸ªé‡è¦çš„è¡¥å……ï¼šä¸€ä¸ªæ–°çš„ &lt;code>pathType&lt;/code> å­—æ®µå’Œä¸€ä¸ªæ–°çš„ &lt;code>IngressClass&lt;/code> èµ„æºã€‚&lt;code>pathType&lt;/code> å­—æ®µå…è®¸æŒ‡å®šè·¯å¾„çš„åŒ¹é…æ–¹å¼ã€‚ é™¤äº†é»˜è®¤çš„&lt;code>ImplementationSpecific&lt;/code>ç±»åž‹å¤–ï¼Œè¿˜æœ‰æ–°çš„ &lt;code>Exact&lt;/code>å’Œ&lt;code>Prefix&lt;/code> è·¯å¾„ç±»åž‹ã€‚&lt;/p>
&lt;!--
The `IngressClass` resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new `ingressClassName` field on Ingresses. This new resource and field replace the deprecated `kubernetes.io/ingress.class` annotation.
-->
&lt;p>&lt;code>IngressClass&lt;/code> èµ„æºç”¨äºŽæè¿° Kubernetes é›†ç¾¤ä¸­ Ingress çš„ç±»åž‹ã€‚ Ingress å¯¹è±¡å¯ä»¥é€šè¿‡åœ¨Ingress èµ„æºç±»åž‹ä¸Šä½¿ç”¨æ–°çš„&lt;code>ingressClassName&lt;/code> å­—æ®µæ¥æŒ‡å®šä¸Žå®ƒä»¬å…³è”çš„ç±»ã€‚ è¿™ä¸ªæ–°çš„èµ„æºå’Œå­—æ®µæ›¿æ¢äº†ä¸å†å»ºè®®ä½¿ç”¨çš„ &lt;code>kubernetes.io/ingress.class&lt;/code> æ³¨è§£ã€‚&lt;/p>
&lt;!--
### SIG-CLI introduces kubectl alpha debug
-->
&lt;h3 id="sig-cli-å¼•å…¥äº†-kubectl-alpha-debug">SIG-CLI å¼•å…¥äº† kubectl alpha debug&lt;/h3>
&lt;!--
SIG-CLI was debating the need for a debug utility for quite some time already. With the development of [ephemeral containers](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/), it became more obvious how we can support developers with tooling built on top of `kubectl exec`. The addition of the [`kubectl alpha debug` command](https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md) (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.
-->
&lt;p>SIG-CLI ä¸€ç›´åœ¨äº‰è®ºç€è°ƒè¯•å·¥å…·çš„å¿…è¦æ€§ã€‚éšç€ &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ä¸´æ—¶å®¹å™¨&lt;/a> çš„å‘å±•ï¼Œæˆ‘ä»¬å¦‚ä½•ä½¿ç”¨åŸºäºŽ &lt;code>kubectl exec&lt;/code> çš„å·¥å…·æ¥æ”¯æŒå¼€å‘äººå‘˜çš„å¿…è¦æ€§å˜å¾—è¶Šæ¥è¶Šæ˜Žæ˜¾ã€‚ &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> å‘½ä»¤&lt;/a> çš„å¢žåŠ ï¼Œï¼ˆç”±äºŽæ˜¯ alpha é˜¶æ®µï¼Œéžå¸¸æ¬¢è¿Žæ‚¨åé¦ˆæ„è§ï¼‰ï¼Œä½¿å¼€å‘äººå‘˜å¯ä»¥è½»æ¾åœ°åœ¨é›†ç¾¤ä¸­è°ƒè¯• Podã€‚æˆ‘ä»¬è®¤ä¸ºè¿™ä¸ªåŠŸèƒ½çš„ä»·å€¼éžå¸¸é«˜ã€‚ æ­¤å‘½ä»¤å…è®¸åˆ›å»ºä¸€ä¸ªä¸´æ—¶å®¹å™¨ï¼Œè¯¥å®¹å™¨åœ¨è¦å°è¯•æ£€æŸ¥çš„ Pod æ—è¾¹è¿è¡Œï¼Œå¹¶ä¸”è¿˜é™„åŠ åˆ°æŽ§åˆ¶å°ä»¥è¿›è¡Œäº¤äº’å¼æ•…éšœæŽ’é™¤ã€‚&lt;/p>
&lt;!--
### Introducing Windows CSI support alpha for Kubernetes
-->
&lt;h3 id="ä¸º-kubernetes-å¼•å…¥-windows-csi-æ”¯æŒ-alpha">ä¸º Kubernetes å¼•å…¥ Windows CSI æ”¯æŒï¼ˆAlphaï¼‰&lt;/h3>
&lt;!--
The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.
-->
&lt;p>ç”¨äºŽ Windows çš„ CSI ä»£ç†çš„ Alpha ç‰ˆæœ¬éš Kubernetes 1.18 ä¸€èµ·å‘å¸ƒã€‚ CSI ä»£ç†é€šè¿‡å…è®¸Windows ä¸­çš„å®¹å™¨æ‰§è¡Œç‰¹æƒå­˜å‚¨æ“ä½œæ¥å¯ç”¨ Windows ä¸Šçš„ CSI é©±åŠ¨ç¨‹åºã€‚&lt;/p>
&lt;!--
## Other Updates
-->
&lt;h2 id="å…¶å®ƒæ›´æ–°">å…¶å®ƒæ›´æ–°&lt;/h2>
&lt;!--
### Graduated to Stable ðŸ’¯
-->
&lt;h3 id="æ¯•ä¸šè½¬ä¸ºç¨³å®šç‰ˆ">æ¯•ä¸šè½¬ä¸ºç¨³å®šç‰ˆ&lt;/h3>
&lt;!--
- [Taint Based Eviction](https://github.com/kubernetes/enhancements/issues/166)
- [`kubectl diff`](https://github.com/kubernetes/enhancements/issues/491)
- [CSI Block storage support](https://github.com/kubernetes/enhancements/issues/565)
- [API Server dry run](https://github.com/kubernetes/enhancements/issues/576)
- [Pass Pod information in CSI calls](https://github.com/kubernetes/enhancements/issues/603)
- [Support Out-of-Tree vSphere Cloud Provider](https://github.com/kubernetes/enhancements/issues/670)
- [Support GMSA for Windows workloads](https://github.com/kubernetes/enhancements/issues/689)
- [Skip attach for non-attachable CSI volumes](https://github.com/kubernetes/enhancements/issues/770)
- [PVC cloning](https://github.com/kubernetes/enhancements/issues/989)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
- [AppProtocol for Services and Endpoints](https://github.com/kubernetes/enhancements/issues/1507)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
- [Node-local DNS cache](https://github.com/kubernetes/enhancements/issues/1024)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">åŸºäºŽæ±¡ç‚¹çš„é€å‡ºæ“ä½œ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI å—å­˜å‚¨æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API æœåŠ¡å™¨ dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">åœ¨ CSI è°ƒç”¨ä¸­ä¼ é€’ Pod ä¿¡æ¯&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">æ”¯æŒæ ‘å¤– vSphere äº‘é©±åŠ¨&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">å¯¹ Windows è´Ÿè½½æ”¯æŒ GMSA&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">å¯¹ä¸å¯æŒ‚è½½çš„CSIå·è·³è¿‡æŒ‚è½½&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC å…‹éš†&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">ç§»åŠ¨ kubectl åŒ…ä»£ç åˆ° staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">Windows çš„ RunAsUserName&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">æœåŠ¡å’Œç«¯ç‚¹çš„ AppProtocol&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">æ‰©å±• Hugepage ç‰¹æ€§&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Major Changes
-->
&lt;h3 id="ä¸»è¦å˜åŒ–">ä¸»è¦å˜åŒ–&lt;/h3>
&lt;!--
- [EndpointSlice API](https://github.com/kubernetes/enhancements/issues/752)
- [Moving kubectl package code to staging](https://github.com/kubernetes/enhancements/issues/1020)
- [CertificateSigningRequest API](https://github.com/kubernetes/enhancements/issues/1513)
- [Extending Hugepage Feature](https://github.com/kubernetes/enhancements/issues/1539)
- [client-go signature refactor to standardize options and context handling](https://github.com/kubernetes/enhancements/issues/1601)
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go çš„è°ƒç”¨è§„èŒƒé‡æž„æ¥æ ‡å‡†åŒ–é€‰é¡¹å’Œç®¡ç†ä¸Šä¸‹æ–‡&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
### Release Notes
-->
&lt;h3 id="å‘å¸ƒè¯´æ˜Ž">å‘å¸ƒè¯´æ˜Ž&lt;/h3>
&lt;!--
Check out the full details of the Kubernetes 1.18 release in our [release notes](https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md).
-->
&lt;p>åœ¨æˆ‘ä»¬çš„ &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">å‘å¸ƒæ–‡æ¡£&lt;/a>ä¸­æŸ¥çœ‹ Kubernetes 1.18 å‘è¡Œç‰ˆçš„å®Œæ•´è¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Availability
-->
&lt;h3 id="ä¸‹è½½å®‰è£…">ä¸‹è½½å®‰è£…&lt;/h3>
&lt;!--
Kubernetes 1.18 is available for download on [GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/) or run local Kubernetes clusters using Docker container â€œnodesâ€ with [kind](https://kind.sigs.k8s.io/). You can also easily install 1.18 using [kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;p>Kubernetes 1.18 å¯ä»¥åœ¨ &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a> ä¸Šä¸‹è½½ã€‚ è¦å¼€å§‹ä½¿ç”¨Kubernetesï¼Œè¯·æŸ¥çœ‹è¿™äº› &lt;a href="https://kubernetes.io/docs/tutorials/">äº¤äº’æ•™ç¨‹&lt;/a> æˆ–é€šè¿‡&lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> ä½¿ç”¨ Docker å®¹å™¨è¿è¡Œæœ¬åœ° kubernetes é›†ç¾¤ã€‚æ‚¨è¿˜å¯ä»¥ä½¿ç”¨&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>è½»æ¾å®‰è£… 1.18ã€‚&lt;/p>
&lt;!--
### Release Team
-->
&lt;h3 id="å‘å¸ƒå›¢é˜Ÿ">å‘å¸ƒå›¢é˜Ÿ&lt;/h3>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md) led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>é€šè¿‡æ•°ç™¾ä½è´¡çŒ®äº†æŠ€æœ¯å’ŒéžæŠ€æœ¯å†…å®¹çš„ä¸ªäººçš„åŠªåŠ›ï¼Œä½¿æœ¬æ¬¡å‘è¡Œæˆä¸ºå¯èƒ½ã€‚ ç‰¹åˆ«æ„Ÿè°¢ç”± Searchable AI çš„ç½‘ç«™å¯é æ€§å·¥ç¨‹å¸ˆ Jorge Alarcon Ochoa é¢†å¯¼çš„&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">å‘å¸ƒå›¢é˜Ÿ&lt;/a>ã€‚ 34 ä½å‘å¸ƒå›¢é˜Ÿæˆå‘˜åè°ƒäº†å‘å¸ƒçš„å„ä¸ªæ–¹é¢ï¼Œä»Žæ–‡æ¡£åˆ°æµ‹è¯•ã€éªŒè¯å’ŒåŠŸèƒ½å®Œæ•´æ€§ã€‚&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [40,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 3,000 people.
-->
&lt;p>éšç€ Kubernetes ç¤¾åŒºçš„å‘å±•å£®å¤§ï¼Œæˆ‘ä»¬çš„å‘å¸ƒè¿‡ç¨‹å¾ˆå¥½åœ°å±•ç¤ºäº†å¼€æºè½¯ä»¶å¼€å‘ä¸­çš„åä½œã€‚ Kubernetes ç»§ç»­å¿«é€ŸèŽ·å–æ–°ç”¨æˆ·ã€‚ è¿™ç§å¢žé•¿åˆ›é€ äº†ä¸€ä¸ªç§¯æžçš„åé¦ˆå›žè·¯ï¼Œå…¶ä¸­æœ‰æ›´å¤šçš„è´¡çŒ®è€…æäº¤äº†ä»£ç ï¼Œä»Žè€Œåˆ›å»ºäº†æ›´åŠ æ´»è·ƒçš„ç”Ÿæ€ç³»ç»Ÿã€‚ è¿„ä»Šä¸ºæ­¢ï¼ŒKubernetes å·²æœ‰ &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 ç‹¬ç«‹è´¡çŒ®è€…&lt;/a> å’Œä¸€ä¸ªè¶…è¿‡3000äººçš„æ´»è·ƒç¤¾åŒºã€‚&lt;/p>
&lt;!--
### Release Logo
-->
&lt;h3 id="å‘å¸ƒ-logo">å‘å¸ƒ logo&lt;/h3>
&lt;!--
![Kubernetes 1.18 Release Logo](/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 å‘å¸ƒå›¾æ ‡">&lt;/p>
&lt;!--
#### Why the LHC?
-->
&lt;h4 id="ä¸ºä»€ä¹ˆæ˜¯-lhc">ä¸ºä»€ä¹ˆæ˜¯ LHC&lt;/h4>
&lt;!--
The LHC is the worldâ€™s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations â€“ all to work towards the same goal of improving cloud computing in all aspects! "A Bit Quarky" as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.
-->
&lt;p>LHC æ˜¯ä¸–ç•Œä¸Šæœ€å¤§ï¼ŒåŠŸèƒ½æœ€å¼ºå¤§çš„ç²’å­åŠ é€Ÿå™¨ã€‚ï¼Œæ˜¯æ¥è‡ªä¸–ç•Œå„åœ°æˆåƒä¸Šä¸‡ç§‘å­¦å®¶åˆä½œçš„ç»“æžœã€‚æ‰€æœ‰è¿™äº›åˆä½œéƒ½æ˜¯ä¸ºäº†ä¿ƒè¿›ç§‘å­¦çš„å‘å±•ã€‚ä»¥ç±»ä¼¼çš„æ–¹å¼ï¼ŒKubernetes å·²ç»æˆä¸ºä¸€ä¸ªèšé›†äº†æ¥è‡ªæ•°ç™¾ä¸ªç»„ç»‡çš„æ•°åƒåè´¡çŒ®è€…â€“æ‰€æœ‰äººéƒ½æœç€åœ¨å„ä¸ªæ–¹é¢æ”¹å–„äº‘è®¡ç®—çš„ç›¸åŒç›®æ ‡åŠªåŠ›çš„é¡¹ç›®ï¼ å‘å¸ƒåç§°â€œ A Bit Quarkyâ€ çš„æ„æ€æ˜¯æé†’æˆ‘ä»¬ï¼Œéžå¸¸è§„çš„æƒ³æ³•å¯ä»¥å¸¦æ¥å·¨å¤§çš„å˜åŒ–ï¼Œå¯¹å¼€æ”¾æ€§ä¿æŒå¼€æ”¾æ€åº¦å°†æœ‰åŠ©äºŽæˆ‘ä»¬è¿›è¡Œåˆ›æ–°ã€‚&lt;/p>
&lt;!--
#### About the designer
-->
&lt;h4 id="å…³äºŽè®¾è®¡è€…">å…³äºŽè®¾è®¡è€…&lt;/h4>
&lt;!--
Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: https://marulango.com
-->
&lt;p>Maru Lango æ˜¯ç›®å‰å±…ä½åœ¨å¢¨è¥¿å“¥åŸŽçš„è®¾è®¡å¸ˆã€‚å¥¹çš„ä¸“é•¿æ˜¯äº§å“è®¾è®¡ï¼Œå¥¹è¿˜å–œæ¬¢ä½¿ç”¨ CSS + JS è¿›è¡Œå“ç‰Œã€æ’å›¾å’Œè§†è§‰å®žéªŒï¼Œä¸ºæŠ€æœ¯å’Œè®¾è®¡ç¤¾åŒºçš„å¤šæ ·æ€§åšè´¡çŒ®ã€‚æ‚¨å¯èƒ½ä¼šåœ¨å¤§å¤šæ•°ç¤¾äº¤åª’ä½“ä¸Šä»¥ @marulango çš„èº«ä»½æ‰¾åˆ°å¥¹ï¼Œæˆ–æŸ¥çœ‹å¥¹çš„ç½‘ç«™ï¼š &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;!--
### User Highlights
-->
&lt;h3 id="é«˜å…‰ç”¨æˆ·">é«˜å…‰ç”¨æˆ·&lt;/h3>
&lt;!--
- Ericsson is using Kubernetes and other cloud native technology to deliver a [highly demanding 5G network](https://www.cncf.io/case-study/ericsson/) that resulted in up to 90 percent CI/CD savings.
- Zendesk is using Kubernetes to [run around 70% of its existing applications](https://www.cncf.io/case-study/zendesk/). Itâ€™s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.
- LifeMiles has [reduced infrastructure spending by 50%](https://www.cncf.io/case-study/lifemiles/) because of its move to Kubernetes. It has also allowed them to double its available resource capacity.
-->
&lt;ul>
&lt;li>çˆ±ç«‹ä¿¡æ­£åœ¨ä½¿ç”¨ Kubernetes å’Œå…¶ä»–äº‘åŽŸç”ŸæŠ€æœ¯æ¥äº¤ä»˜&lt;a href="https://www.cncf.io/case-study/ericsson/">é«˜æ ‡å‡†çš„ 5G ç½‘ç»œ&lt;/a>ï¼Œè¿™å¯ä»¥åœ¨ CI/CD ä¸ŠèŠ‚çœå¤šè¾¾ 90ï¼… çš„æ”¯å‡ºã€‚&lt;/li>
&lt;li>Zendesk æ­£åœ¨ä½¿ç”¨ Kubernetes &lt;a href="https://www.cncf.io/case-study/zendesk/">è¿è¡Œå…¶çŽ°æœ‰åº”ç”¨ç¨‹åºçš„çº¦ 70ï¼…&lt;/a>ã€‚å®ƒè¿˜æ­£åœ¨ä½¿æ‰€æž„å»ºçš„æ‰€æœ‰æ–°åº”ç”¨éƒ½å¯ä»¥åœ¨ Kubernetes ä¸Šè¿è¡Œï¼Œä»Žè€ŒèŠ‚çœæ—¶é—´ã€æé«˜çµæ´»æ€§å¹¶åŠ å¿«å…¶åº”ç”¨ç¨‹åºå¼€å‘çš„é€Ÿåº¦ã€‚&lt;/li>
&lt;li>LifeMiles å› è¿ç§»åˆ° Kubernetes è€Œ&lt;a href="https://www.cncf.io/case-study/lifemiles/">é™ä½Žäº† 50% çš„åŸºç¡€è®¾æ–½å¼€æ”¯&lt;/a>ã€‚Kubernetes è¿˜ä½¿ä»–ä»¬å¯ä»¥å°†å…¶å¯ç”¨èµ„æºå®¹é‡å¢žåŠ ä¸€å€ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Ecosystem Updates
-->
&lt;h3 id="ç”Ÿæ€ç³»ç»Ÿæ›´æ–°">ç”Ÿæ€ç³»ç»Ÿæ›´æ–°&lt;/h3>
&lt;!--
- The CNCF published the results of its [annual survey](https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/) showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.
- The â€œIntroduction to Kubernetesâ€ course hosted by the CNCF [surpassed 100,000 registrations](https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/).
-->
&lt;ul>
&lt;li>CNCFå‘å¸ƒäº†&lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">å¹´åº¦è°ƒæŸ¥&lt;/a> çš„ç»“æžœï¼Œè¡¨æ˜Ž Kubernetes åœ¨ç”Ÿäº§ä¸­çš„ä½¿ç”¨æ­£åœ¨é£žé€Ÿå¢žé•¿ã€‚è°ƒæŸ¥å‘çŽ°ï¼Œæœ‰78ï¼…çš„å—è®¿è€…åœ¨ç”Ÿäº§ä¸­ä½¿ç”¨Kubernetesï¼Œè€ŒåŽ»å¹´è¿™ä¸€æ¯”ä¾‹ä¸º 58ï¼…ã€‚&lt;/li>
&lt;li>CNCF ä¸¾åŠžçš„ â€œKuberneteså…¥é—¨â€ è¯¾ç¨‹æœ‰&lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">è¶…è¿‡ 100,000 äººæ³¨å†Œ&lt;/a>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Project Velocity
-->
&lt;h3 id="é¡¹ç›®é€Ÿåº¦">é¡¹ç›®é€Ÿåº¦&lt;/h3>
&lt;!--
The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. [K8s DevStats](https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1) illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.
-->
&lt;p>CNCF ç»§ç»­å®Œå–„ DevStatsã€‚è¿™æ˜¯ä¸€ä¸ªé›„å¿ƒå‹ƒå‹ƒçš„é¡¹ç›®ï¼Œæ—¨åœ¨å¯¹é¡¹ç›®ä¸­çš„æ— æ•°è´¡çŒ®æ•°æ®è¿›è¡Œå¯è§†åŒ–å±•ç¤ºã€‚&lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> å±•ç¤ºäº†ä¸»è¦å…¬å¸è´¡çŒ®è€…çš„è´¡çŒ®ç»†ç›®ï¼Œä»¥åŠä¸€ç³»åˆ—ä»¤äººå°è±¡æ·±åˆ»çš„é¢„å®šä¹‰çš„æŠ¥å‘Šï¼Œæ¶‰åŠä»Žè´¡çŒ®è€…ä¸ªäººçš„å„æ–¹é¢åˆ° PR ç”Ÿå‘½å‘¨æœŸçš„å„ä¸ªæ–¹é¢ã€‚&lt;/p>
&lt;!--
This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. [Check out DevStats](https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;var-period=m&amp;var-repogroup_name=All) to learn more about the overall velocity of the Kubernetes project and community.
-->
&lt;p>åœ¨è¿‡åŽ»çš„ä¸€ä¸ªå­£åº¦ä¸­ï¼Œ641 å®¶ä¸åŒçš„å…¬å¸å’Œè¶…è¿‡ 6,409 ä¸ªä¸ªäººä¸º Kubernetes ä½œå‡ºè´¡çŒ®ã€‚ &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">æŸ¥çœ‹ DevStats&lt;/a> ä»¥äº†è§£æœ‰å…³ Kubernetes é¡¹ç›®å’Œç¤¾åŒºå‘å±•é€Ÿåº¦çš„ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Event Update
-->
&lt;h3 id="æ´»åŠ¨ä¿¡æ¯">æ´»åŠ¨ä¿¡æ¯&lt;/h3>
&lt;!--
Kubecon + CloudNativeCon EU 2020 is being pushed back â€“ for the more most up-to-date information, please check the [Novel Coronavirus Update page](https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/).
-->
&lt;p>Kubecon + CloudNativeCon EU 2020 å·²ç»æŽ¨è¿Ÿ - æœ‰å…³æœ€æ–°ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹&lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">æ–°åž‹è‚ºç‚Žå‘å¸ƒé¡µé¢&lt;/a>ã€‚&lt;/p>
&lt;!--
### Upcoming Release Webinar
-->
&lt;h3 id="å³å°†åˆ°æ¥çš„å‘å¸ƒçš„çº¿ä¸Šä¼šè®®">å³å°†åˆ°æ¥çš„å‘å¸ƒçš„çº¿ä¸Šä¼šè®®&lt;/h3>
&lt;!--
Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: https://www.cncf.io/webinars/kubernetes-1-18/.
-->
&lt;p>åœ¨ 2020 å¹´ 4 æœˆ 23 æ—¥ï¼Œå’Œ Kubernetes 1.18 ç‰ˆæœ¬å›¢é˜Ÿä¸€èµ·äº†è§£æ­¤ç‰ˆæœ¬çš„ä¸»è¦åŠŸèƒ½ï¼ŒåŒ…æ‹¬ kubectl debugã€æ‹“æ‰‘ç®¡ç†å™¨ã€Ingress æ¯•ä¸šä¸º V1 ç‰ˆæœ¬ä»¥åŠ client-goã€‚ åœ¨æ­¤å¤„æ³¨å†Œï¼šhttps://www.cncf.io/webinars/kubernetes-1-18/ ã€‚&lt;/p>
&lt;!--
### Get Involved
-->
&lt;h3 id="å¦‚ä½•å‚ä¸Ž">å¦‚ä½•å‚ä¸Ž&lt;/h3>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;p>å‚ä¸Ž Kubernetes çš„æœ€ç®€å•æ–¹æ³•æ˜¯åŠ å…¥ä¼—å¤šä¸Žæ‚¨çš„å…´è¶£ç›¸å…³çš„ &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a> (SIGs) ä¹‹ä¸€ã€‚ æ‚¨æœ‰ä»€ä¹ˆæƒ³å‘ Kubernetes ç¤¾åŒºå‘å¸ƒçš„å†…å®¹å—ï¼Ÿ å‚ä¸Žæˆ‘ä»¬çš„æ¯å‘¨ &lt;a href="https://github.com/kubernetes/community/tree/master/communication">ç¤¾åŒºä¼šè®®&lt;/a>ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ¸ é“åˆ†äº«æ‚¨çš„å£°éŸ³ã€‚ æ„Ÿè°¢æ‚¨ä¸€ç›´ä»¥æ¥çš„åé¦ˆå’Œæ”¯æŒã€‚&lt;/p>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
- Read more about whatâ€™s happening with Kubernetes on the [blog](https://kubernetes.io/blog/)
- Learn more about the [Kubernetes Release Team](https://github.com/kubernetes/sig-release/tree/master/release-team)
-->
&lt;ul>
&lt;li>åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ä»¬ &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>ï¼Œäº†è§£æœ€æ–°åŠ¨æ€&lt;/li>
&lt;li>åœ¨ &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a> ä¸Šå‚ä¸Žç¤¾åŒºè®¨è®º&lt;/li>
&lt;li>åŠ å…¥ &lt;a href="http://slack.k8s.io/">Slack&lt;/a> ä¸Šçš„ç¤¾åŒº&lt;/li>
&lt;li>åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>æé—®ï¼ˆæˆ–å›žç­”ï¼‰&lt;/li>
&lt;li>åˆ†äº«æ‚¨çš„ Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/li>
&lt;li>é€šè¿‡ &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>äº†è§£æ›´å¤šå…³äºŽ Kubernetes çš„æ–°é²œäº‹&lt;/li>
&lt;li>äº†è§£æ›´å¤šå…³äºŽ &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes å‘å¸ƒå›¢é˜Ÿ&lt;/a> çš„ä¿¡æ¯&lt;/li>
&lt;/ul></description></item><item><title>Blog: åŸºäºŽ MIPS æž¶æž„çš„ Kubernetes æ–¹æ¡ˆ</title><link>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;!--
layout: blog
title: "Kubernetes on MIPS"
date: 2020-01-15
slug: Kubernetes-on-MIPS
-->
&lt;!--
**Authors:** TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> çŸ³å…‰é“¶ï¼Œå°¹ä¸œè¶…ï¼Œå±•æœ›ï¼Œæ±Ÿç‡•ï¼Œè”¡å«å«ï¼Œé«˜ä¼ é›†ï¼Œå­™æ€æ¸…ï¼ˆæµªæ½®ï¼‰&lt;/p>
&lt;!--
## Background
-->
&lt;h2 id="èƒŒæ™¯">èƒŒæ™¯&lt;/h2>
&lt;!--
[MIPS](https://en.wikipedia.org/wiki/MIPS_architecture) (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.
-->
&lt;p>&lt;a href="https://zh.wikipedia.org/wiki/MIPS%E6%9E%B6%E6%A7%8B">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) æ˜¯ä¸€ç§é‡‡å–ç²¾ç®€æŒ‡ä»¤é›†ï¼ˆRISCï¼‰çš„å¤„ç†å™¨æž¶æž„ (ISA)ï¼Œå‡ºçŽ°äºŽ 1981 å¹´ï¼Œç”± MIPS ç§‘æŠ€å…¬å¸å¼€å‘ã€‚å¦‚ä»Š MIPS æž¶æž„è¢«å¹¿æ³›åº”ç”¨äºŽè®¸å¤šç”µå­äº§å“ä¸Šã€‚&lt;/p>
&lt;!--
[Kubernetes](https://kubernetes.io) has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.
-->
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> å®˜æ–¹ç›®å‰æ”¯æŒä¼—å¤š CPU æž¶æž„è¯¸å¦‚ x86, arm/arm64, ppc64le, s390x ç­‰ã€‚ç„¶è€Œç›®å‰è¿˜ä¸æ”¯æŒ MIPS æž¶æž„ï¼Œå§‹ç»ˆæ˜¯ä¸€ä¸ªé—æ†¾ã€‚éšç€äº‘åŽŸç”ŸæŠ€æœ¯çš„å¹¿æ³›åº”ç”¨ï¼ŒMIPS æž¶æž„ä¸‹çš„ç”¨æˆ·å§‹ç»ˆå¯¹ Kubernetes on MIPS æœ‰ç€è¿«åˆ‡çš„éœ€æ±‚ã€‚&lt;/p>
&lt;!--
## Achievements
-->
&lt;h2 id="æˆæžœ">æˆæžœ&lt;/h2>
&lt;!--
For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.
-->
&lt;p>å¤šå¹´æ¥ï¼Œä¸ºäº†ä¸°å¯Œå¼€æºç¤¾åŒºçš„ç”Ÿæ€ï¼Œæˆ‘ä»¬ä¸€ç›´è‡´åŠ›äºŽåœ¨ MIPS æž¶æž„ä¸‹é€‚é… Kubernetesã€‚éšç€ MIPS CPU çš„ä¸æ–­è¿­ä»£ä¼˜åŒ–å’Œæ€§èƒ½çš„æå‡ï¼Œæˆ‘ä»¬åœ¨ mips64el å¹³å°ä¸Šå–å¾—äº†ä¸€äº›çªç ´æ€§çš„è¿›å±•ã€‚&lt;/p>
&lt;!--
Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.
-->
&lt;p>å¤šå¹´æ¥ï¼Œæˆ‘ä»¬ä¸€ç›´ç§¯æžæŠ•å…¥ Kubernetes ç¤¾åŒºï¼Œåœ¨ Kubernetes æŠ€æœ¯åº”ç”¨å’Œä¼˜åŒ–æ–¹é¢å…·å¤‡äº†ä¸°å¯Œçš„ç»éªŒã€‚æœ€è¿‘ï¼Œæˆ‘ä»¬åœ¨ç ”å‘è¿‡ç¨‹ä¸­å°è¯•å°† Kubernetes é€‚é…åˆ° MIPS æž¶æž„å¹³å°ï¼Œå¹¶å–å¾—äº†é˜¶æ®µæ€§æˆæžœã€‚æˆåŠŸå®Œæˆäº† Kubernetes ä»¥åŠç›¸å…³ç»„ä»¶çš„è¿ç§»é€‚é…ï¼Œä¸ä»…æ­å»ºå‡ºç¨³å®šé«˜å¯ç”¨çš„ MIPS é›†ç¾¤ï¼ŒåŒæ—¶å®Œæˆäº† Kubernetes v1.16.2 ç‰ˆæœ¬çš„ä¸€è‡´æ€§æµ‹è¯•ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;!--
_Figure 1 Kubernetes on MIPS_
-->
&lt;p>&lt;em>å›¾ä¸€ Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;!--
## K8S-MIPS component build
-->
&lt;h2 id="k8s-mips-ç»„ä»¶æž„å»º">K8S-MIPS ç»„ä»¶æž„å»º&lt;/h2>
&lt;!--
Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:
-->
&lt;p>å‡ ä¹Žæ‰€æœ‰çš„ Kubernetes ç›¸å…³çš„äº‘åŽŸç”Ÿç»„ä»¶éƒ½æ²¡æœ‰æä¾› MIPS ç‰ˆæœ¬çš„å®‰è£…åŒ…æˆ–é•œåƒï¼Œåœ¨ MIPS å¹³å°ä¸Šéƒ¨ç½² Kubernetes çš„å‰ææ˜¯è‡ªè¡Œç¼–è¯‘æž„å»ºå‡ºå…¨éƒ¨æ‰€éœ€ç»„ä»¶ã€‚è¿™äº›ç»„ä»¶ä¸»è¦åŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;!--
Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.
-->
&lt;p>å¾—ç›ŠäºŽ Golang ä¼˜ç§€çš„è®¾è®¡ä»¥åŠå¯¹äºŽ MIPS å¹³å°çš„è‰¯å¥½æ”¯æŒï¼Œæžå¤§åœ°ç®€åŒ–äº†ä¸Šè¿°äº‘åŽŸç”Ÿç»„ä»¶çš„ç¼–è¯‘è¿‡ç¨‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ mips64el å¹³å°ç¼–è¯‘å‡ºäº†æœ€æ–°ç¨³å®šçš„ golang, ç„¶åŽé€šè¿‡æºç æž„å»ºçš„æ–¹å¼ç¼–è¯‘å®Œæˆäº†ä¸Šè¿°å¤§éƒ¨åˆ†ç»„ä»¶ã€‚&lt;/p>
&lt;!--
During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.
-->
&lt;p>åœ¨ç¼–è¯‘è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä¸å¯é¿å…åœ°é‡åˆ°äº†å¾ˆå¤šå¹³å°å…¼å®¹æ€§çš„é—®é¢˜ï¼Œæ¯”å¦‚å…³äºŽ golang ç³»ç»Ÿè°ƒç”¨ (syscall) çš„å…¼å®¹æ€§é—®é¢˜, syscall.Stat_t 32 ä½ ä¸Ž 64 ä½ç±»åž‹è½¬æ¢ï¼ŒEpollEvent ä¿®æ­£ä½ç¼ºå¤±ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.
-->
&lt;p>æž„å»º K8S-MIPS ç»„ä»¶ä¸»è¦ä½¿ç”¨äº†äº¤å‰ç¼–è¯‘æŠ€æœ¯ã€‚æž„å»ºè¿‡ç¨‹åŒ…æ‹¬é›†æˆ QEMU å·¥å…·æ¥å®žçŽ° MIPS CPU æŒ‡ä»¤çš„è½¬æ¢ã€‚åŒæ—¶ä¿®æ”¹ Kubernetes å’Œ E2E é•œåƒçš„æž„å»ºè„šæœ¬ï¼Œæž„å»ºäº† Hyperkube å’Œ MIPS æž¶æž„çš„ E2E æµ‹è¯•é•œåƒã€‚&lt;/p>
&lt;!--
After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.
-->
&lt;p>æˆåŠŸæž„å»ºå‡ºä»¥ä¸Šç»„ä»¶åŽï¼Œæˆ‘ä»¬ä½¿ç”¨å·¥å…·å®Œæˆ Kubernetes é›†ç¾¤çš„æ­å»ºï¼Œæ¯”å¦‚ kubesprayã€kubeadm ç­‰ã€‚&lt;/p>
&lt;!--
| Name | Version | MIPS Repository |
|--------------------------------|---------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| golang on MIPS | 1.12.5 | - |
| docker-ce on MIPS | 18.09.8 | - |
| metrics-server for CKE on MIPS | 0.3.2 | `registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2` |
| etcd for CKE on MIPS | 3.2.26 | `registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26` |
| pause for CKE on MIPS | 3.1 | `registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1` |
| hyperkube for CKE on MIPS | 1.14.3 | `registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3` |
| coredns for CKE on MIPS | 1.6.5 | `registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5` |
| calico for CKE on MIPS | 3.8.0 | `registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0` `registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0` |
-->
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>åç§°&lt;/th>
&lt;th>ç‰ˆæœ¬&lt;/th>
&lt;th>MIPS é•œåƒä»“åº“&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ golang&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ docker-ce&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º metrics-server&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º etcd&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º pause&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º hyperkube&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º coredns&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>MIPS ç‰ˆæœ¬ CKE æž„å»º calico&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;!--
**Note**: CKE is a Kubernetes-based cloud container engine launched by Inspur
-->
&lt;p>&lt;strong>æ³¨&lt;/strong>: CKE æ˜¯æµªæ½®æŽ¨å‡ºçš„ä¸€æ¬¾åŸºäºŽ Kubernetes çš„å®¹å™¨äº‘æœåŠ¡å¼•æ“Ž&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;!--
_Figure 2 K8S-MIPS Cluster Components_
-->
&lt;p>&lt;em>å›¾äºŒ K8S-MIPS é›†ç¾¤ç»„ä»¶&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;!--
_Figure 3 CPU Architecture_
-->
&lt;p>&lt;em>å›¾ä¸‰ CPU æž¶æž„&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;!--
_Figure 4 Cluster Node Information_
-->
&lt;p>&lt;em>å›¾å›› é›†ç¾¤èŠ‚ç‚¹ä¿¡æ¯&lt;/em>&lt;/p>
&lt;!--
## Run K8S Conformance Test
-->
&lt;h2 id="è¿è¡Œ-k8s-ä¸€è‡´æ€§æµ‹è¯•">è¿è¡Œ K8S ä¸€è‡´æ€§æµ‹è¯•&lt;/h2>
&lt;!--
The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes [conformance test](https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md).
-->
&lt;p>éªŒè¯ K8S-MIP é›†ç¾¤ç¨³å®šæ€§å’Œå¯ç”¨æ€§æœ€ç®€å•ç›´æŽ¥çš„æ–¹å¼æ˜¯è¿è¡Œ Kubernetes çš„ &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">ä¸€è‡´æ€§æµ‹è¯•&lt;/a>ã€‚&lt;/p>
&lt;!--
Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.
-->
&lt;p>ä¸€è‡´æ€§æµ‹è¯•æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å®¹å™¨ï¼Œç”¨äºŽå¯åŠ¨ Kubernetes ç«¯åˆ°ç«¯çš„ä¸€è‡´æ€§æµ‹è¯•ã€‚&lt;/p>
&lt;!--
Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from `kubernetes/test/images`, and the built images are at `gcr.io/kubernetes-e2e-test-images`. Since there are no MIPS images in the repository, we must first build all needed images to run the test.
-->
&lt;p>å½“æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯•æ—¶ï¼Œæµ‹è¯•ç¨‹åºä¼šå¯åŠ¨è®¸å¤š Pod è¿›è¡Œå„ç§ç«¯åˆ°ç«¯çš„è¡Œä¸ºæµ‹è¯•ï¼Œè¿™äº› Pod ä½¿ç”¨çš„é•œåƒæºç å¤§éƒ¨åˆ†æ¥è‡ªäºŽ &lt;code>kubernetes/test/images&lt;/code> ç›®å½•ä¸‹ï¼Œæž„å»ºçš„é•œåƒä½äºŽ &lt;code>gcr.io/kubernetes-e2e-test-images/&lt;/code>ã€‚ç”±äºŽé•œåƒä»“åº“ä¸­ç›®å‰å¹¶ä¸å­˜åœ¨ MIPS æž¶æž„çš„é•œåƒï¼Œæˆ‘ä»¬è¦æƒ³è¿è¡Œ E2E æµ‹è¯•ï¼Œå¿…é¡»é¦–å…ˆæž„å»ºå‡ºæµ‹è¯•æ‰€éœ€çš„å…¨éƒ¨é•œåƒã€‚&lt;/p>
&lt;!--
### Build needed images for test
-->
&lt;h3 id="æž„å»ºæµ‹è¯•æ‰€éœ€é•œåƒ">æž„å»ºæµ‹è¯•æ‰€éœ€é•œåƒ&lt;/h3>
&lt;!--
The first step is to find all needed images for the test. We can run `sonobuoy images-p e2e` command to list all images, or we can find those images in [/test/utils/image/manifest.go](https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go). Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.
-->
&lt;p>ç¬¬ä¸€æ­¥æ˜¯æ‰¾åˆ°æµ‹è¯•æ‰€éœ€çš„æ‰€æœ‰é•œåƒã€‚æˆ‘ä»¬å¯ä»¥æ‰§è¡Œ &lt;code>sonobuoy images-p e2e&lt;/code> å‘½ä»¤æ¥åˆ—å‡ºæ‰€æœ‰é•œåƒï¼Œæˆ–è€…æˆ‘ä»¬å¯ä»¥åœ¨ &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a> ä¸­æ‰¾åˆ°è¿™äº›é•œåƒã€‚å°½ç®¡ Kubernetes å®˜æ–¹æä¾›äº†å®Œæ•´çš„ Makefile å’Œ shell è„šæœ¬ï¼Œä¸ºæž„å»ºæµ‹è¯•æ˜ åƒæä¾›äº†å‘½ä»¤ï¼Œä½†æ˜¯ä»ç„¶æœ‰è®¸å¤šä¸Žä½“ç³»ç»“æž„ç›¸å…³çš„é—®é¢˜æœªèƒ½è§£å†³ï¼Œæ¯”å¦‚åŸºç¡€æ˜ åƒå’Œä¾èµ–åŒ…çš„ä¸å…¼å®¹é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•é€šè¿‡ç›´æŽ¥æ‰§è¡Œè¿™äº›æž„å»ºå‘½ä»¤æ¥åˆ¶ä½œ mips64el æž¶æž„é•œåƒã€‚&lt;/p>
&lt;!--
Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of [alpine](https://www.alpinelinux.org/), so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.
-->
&lt;p>å¤šæ•°æµ‹è¯•é•œåƒéƒ½æ˜¯ä½¿ç”¨ golang ç¼–å†™ï¼Œç„¶åŽç¼–è¯‘å‡ºäºŒè¿›åˆ¶æ–‡ä»¶ï¼Œå¹¶åŸºäºŽç›¸åº”çš„ Dockerfile åˆ¶ä½œå‡ºé•œåƒã€‚è¿™äº›é•œåƒå¯¹æˆ‘ä»¬æ¥è¯´å¯ä»¥è½»æ¾åœ°åˆ¶ä½œå‡ºæ¥ã€‚ä½†æ˜¯éœ€è¦æ³¨æ„ä¸€ç‚¹ï¼šæµ‹è¯•é•œåƒé»˜è®¤ä½¿ç”¨çš„åŸºç¡€é•œåƒå¤§å¤šæ˜¯ alpine, ç›®å‰ &lt;a href="https://www.alpinelinux.org/">Alpine&lt;/a> å®˜æ–¹å¹¶ä¸æ”¯æŒ mips64el æž¶æž„ï¼Œæˆ‘ä»¬æš‚æ—¶æœªèƒ½è‡ªå·±åˆ¶ä½œå‡º mips64el ç‰ˆæœ¬çš„ alpine ç¡€é•œåƒï¼Œåªèƒ½å°†åŸºç¡€é•œåƒæ›¿æ¢ä¸ºæˆ‘ä»¬ç›®å‰å·²æœ‰çš„ mips64el åŸºç¡€é•œåƒï¼Œæ¯”å¦‚ debian-stretch,fedora, ubuntu ç­‰ã€‚æ›¿æ¢åŸºç¡€é•œåƒçš„åŒæ—¶ä¹Ÿéœ€è¦æ›¿æ¢å®‰è£…ä¾èµ–åŒ…çš„å‘½ä»¤ï¼Œç”šè‡³ä¾èµ–åŒ…çš„ç‰ˆæœ¬ç­‰ã€‚&lt;/p>
&lt;!--
Some images are not in `kubernetes/test/images`, such as `gcr.io/google-samples/gb-frontend:v6`. There is no clear documentation explaining where these images are locaated, though we found the source code in repository [github.com/GoogleCloudPlatform/kubernetes-engine-samples](https://github.com/GoogleCloudPlatform/kubernetes-engine-samples). We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as `php:5-apache`, `redis`, and `perl`.
-->
&lt;p>æœ‰äº›æµ‹è¯•æ‰€éœ€é•œåƒçš„æºç å¹¶ä¸åœ¨ &lt;code>kubernetes/test/images&lt;/code> ä¸‹,æ¯”å¦‚ &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code> ç­‰ï¼Œæ²¡æœ‰æ˜Žç¡®çš„æ–‡æ¡£è¯´æ˜Žè¿™ç±»é•œåƒæ¥è‡ªäºŽä½•æ–¹ï¼Œæœ€ç»ˆè¿˜æ˜¯åœ¨ &lt;a href="github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a> è¿™ä¸ªä»“åº“æ‰¾åˆ°äº†åŽŸå§‹çš„é•œåƒæºä»£ç ã€‚ä½†æ˜¯å¾ˆå¿«æˆ‘ä»¬é‡åˆ°äº†æ–°çš„é—®é¢˜ï¼Œä¸ºäº†åˆ¶ä½œè¿™äº›é•œåƒï¼Œè¿˜è¦åˆ¶ä½œå®ƒä¾èµ–çš„åŸºç¡€é•œåƒï¼Œç”šè‡³åŸºç¡€é•œåƒçš„åŸºç¡€é•œåƒï¼Œæ¯”å¦‚ &lt;code>php:5-apache&lt;/code>ã€&lt;code>redis&lt;/code>ã€&lt;code>perl&lt;/code> ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is `imagePullPolicy: ifNotPresent`.
-->
&lt;p>ç»è¿‡æ¼«é•¿åºžæ‚çš„çš„é•œåƒé‡åˆ¶å·¥ä½œï¼Œæˆ‘ä»¬å®Œæˆäº†æ€»è®¡çº¦ 40 ä¸ªé•œåƒçš„åˆ¶ä½œ ï¼ŒåŒ…æ‹¬æµ‹è¯•é•œåƒä»¥åŠç›´æŽ¥å’Œé—´æŽ¥ä¾èµ–çš„åŸºç¡€é•œåƒã€‚
æœ€ç»ˆæˆ‘ä»¬å°†æ‰€æœ‰é•œåƒåœ¨é›†ç¾¤å†…å‡†å¤‡å¦¥å½“ï¼Œå¹¶ç¡®ä¿æµ‹è¯•ç”¨ä¾‹å†…æ‰€æœ‰ Pod çš„é•œåƒæ‹‰å–ç­–ç•¥è®¾ç½®ä¸º &lt;code>imagePullPolicy: ifNotPresent&lt;/code>ã€‚&lt;/p>
&lt;!--
Here are some of the images we built
-->
&lt;p>è¿™æ˜¯æˆ‘ä»¬æž„å»ºå‡ºçš„éƒ¨åˆ†é•œåƒåˆ—è¡¨ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;!--
Finally, we ran the tests and got the test result, include `e2e.log`, which showed that all test cases passed. Additionally, we submitted our test result to [k8s-conformance](https://github.com/cncf/k8s-conformance) as a [pull request](https://github.com/cncf/k8s-conformance/pull/779).
-->
&lt;p>æœ€ç»ˆæˆ‘ä»¬æ‰§è¡Œä¸€è‡´æ€§æµ‹è¯•å¹¶ä¸”å¾—åˆ°äº†æµ‹è¯•æŠ¥å‘Šï¼ŒåŒ…æ‹¬ &lt;code>e2e.log&lt;/code>ï¼Œæ˜¾ç¤ºæˆ‘ä»¬é€šè¿‡äº†å…¨éƒ¨çš„æµ‹è¯•ç”¨ä¾‹ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†æµ‹è¯•ç»“æžœä»¥ &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a> çš„å½¢å¼æäº¤ç»™äº† &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;!--
_Figure 5 Pull request for conformance test results_
-->
&lt;p>&lt;em>å›¾äº” ä¸€è‡´æ€§æµ‹è¯•ç»“æžœçš„ PR&lt;/em>&lt;/p>
&lt;!--
## What's next
-->
&lt;h2 id="åŽç»­è®¡åˆ’">åŽç»­è®¡åˆ’&lt;/h2>
&lt;!--
We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.
-->
&lt;p>æˆ‘ä»¬æ‰‹åŠ¨æž„å»ºäº† K8S-MIPS ç»„ä»¶ä»¥åŠæ‰§è¡Œäº† E2E æµ‹è¯•ï¼ŒéªŒè¯äº† Kubernetes on MIPS çš„å¯è¡Œæ€§ï¼Œæžå¤§çš„å¢žå¼ºäº†æˆ‘ä»¬å¯¹äºŽæŽ¨è¿› Kubernetes æ”¯æŒ MIPS æž¶æž„çš„ä¿¡å¿ƒã€‚&lt;/p>
&lt;!--
In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.
-->
&lt;p>åŽç»­ï¼Œæˆ‘ä»¬å°†ç§¯æžåœ°å‘ç¤¾åŒºè´¡çŒ®æˆ‘ä»¬çš„å·¥ä½œç»éªŒä»¥åŠæˆæžœï¼Œæäº¤ PR ä»¥åŠ Patch For MIPS ç­‰ï¼Œ å¸Œæœ›èƒ½å¤Ÿæœ‰æ›´å¤šçš„æ¥è‡ªç¤¾åŒºçš„åŠ›é‡åŠ å…¥è¿›æ¥ï¼Œå…±åŒæŽ¨è¿› Kubernetes for MIPS çš„è¿›ç¨‹ã€‚&lt;/p>
&lt;!--
Contribution planï¼š
-->
&lt;p>åŽç»­å¼€æºè´¡çŒ®è®¡åˆ’ï¼š&lt;/p>
&lt;!--
- contribute the source of e2e test images for MIPS
- contribute the source of hyperkube for MIPS
- contribute the source of deploy tools like kubeadm for MIPS
-->
&lt;ul>
&lt;li>è´¡çŒ®æž„å»º E2E æµ‹è¯•é•œåƒä»£ç &lt;/li>
&lt;li>è´¡çŒ®æž„å»º MIPS ç‰ˆæœ¬ hyperkube ä»£ç &lt;/li>
&lt;li>è´¡çŒ®æž„å»º MIPS ç‰ˆæœ¬ kubeadm ç­‰é›†ç¾¤éƒ¨ç½²å·¥å…·&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Kubernetes 1.17ï¼šç¨³å®š</title><link>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</link><pubDate>Mon, 09 Dec 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/12/09/kubernetes-1-17-release-announcement/</guid><description>
&lt;!-- ---
layout: blog
title: "Kubernetes 1.17: Stability"
date: 2019-12-09T13:00:00-08:00
slug: kubernetes-1-17-release-announcement
--- -->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">Kubernetes 1.17å‘å¸ƒå›¢é˜Ÿ&lt;/a>&lt;/p>
&lt;!--
**Authors:** [Kubernetes 1.17 Release Team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md)
-->
&lt;p>æˆ‘ä»¬é«˜å…´çš„å®£å¸ƒKubernetes 1.17ç‰ˆæœ¬çš„äº¤ä»˜ï¼Œå®ƒæ˜¯æˆ‘ä»¬2019å¹´çš„ç¬¬å››ä¸ªä¹Ÿæ˜¯æœ€åŽä¸€ä¸ªå‘å¸ƒç‰ˆæœ¬ã€‚Kubernetes v1.17åŒ…å«22ä¸ªå¢žå¼ºåŠŸèƒ½ï¼šæœ‰14ä¸ªå¢žå¼ºå·²ç»é€æ­¥ç¨³å®š(stable)ï¼Œ4ä¸ªå¢žå¼ºåŠŸèƒ½å·²ç»è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ(beta)ï¼Œ4ä¸ªå¢žå¼ºåŠŸèƒ½åˆšåˆšè¿›å…¥å†…éƒ¨æµ‹è¯•ç‰ˆ(alpha)ã€‚&lt;/p>
&lt;!--
Weâ€™re pleased to announce the delivery of Kubernetes 1.17, our fourth and final release of 2019! Kubernetes v1.17 consists of 22 enhancements: 14 enhancements have graduated to stable, 4 enhancements are moving to beta, and 4 enhancements are entering alpha.
-->
&lt;h2 id="ä¸»è¦çš„ä¸»é¢˜">ä¸»è¦çš„ä¸»é¢˜&lt;/h2>
&lt;!--
## Major Themes
-->
&lt;h3 id="äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨">äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨&lt;/h3>
&lt;!--
### Cloud Provider Labels reach General Availability
-->
&lt;p>ä½œä¸ºå…¬å¼€æµ‹è¯•ç‰ˆç‰¹æ€§æ·»åŠ åˆ°v1.2ï¼Œv1.7ä¸­å¯ä»¥çœ‹åˆ°äº‘æä¾›å•†æ ‡ç­¾è¾¾åˆ°åŸºæœ¬å¯ç”¨ã€‚&lt;/p>
&lt;!--
Added as a beta feature way back in v1.2, v1.17 sees the general availability of cloud provider labels.
-->
&lt;h3 id="å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ">å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h3>
&lt;!--
### Volume Snapshot Moves to Beta
-->
&lt;p>åœ¨v1.7ä¸­ï¼ŒKuberneteså·å¿«ç…§ç‰¹æ€§æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚è¿™ä¸ªç‰¹æ€§æ˜¯åœ¨v1.12ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ï¼Œç¬¬äºŒä¸ªæœ‰é‡å¤§å˜åŒ–çš„å†…éƒ¨æµ‹è¯•ç‰ˆæ˜¯v1.13ã€‚&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13.
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»å…¬å¼€æµ‹è¯•ç‰ˆ">å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h2>
&lt;!--
### CSI Migration Beta
-->
&lt;p>åœ¨v1.7ä¸­ï¼ŒKubernetesæ ‘å†…å­˜å‚¨æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æŽ¥å£(CSI)çš„è¿ç§»åŸºç¡€æž¶æž„æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»æœ€åˆæ˜¯åœ¨Kubernetes v1.14ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ã€‚&lt;/p>
&lt;!--
The Kubernetes in-tree storage plugin to Container Storage Interface (CSI) migration infrastructure is now beta in Kubernetes v1.17. CSI migration was introduced as alpha in Kubernetes v1.14.
-->
&lt;h2 id="äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨-1">äº‘æœåŠ¡æä¾›å•†æ ‡ç­¾åŸºæœ¬å¯ç”¨&lt;/h2>
&lt;!--
## Cloud Provider Labels reach General Availability
-->
&lt;p>å½“èŠ‚ç‚¹å’Œå·è¢«åˆ›å»ºï¼Œä¼šåŸºäºŽåŸºç¡€äº‘æä¾›å•†çš„Kubernetesé›†ç¾¤æ‰“ä¸Šä¸€ç³»åˆ—æ ‡å‡†æ ‡ç­¾ã€‚èŠ‚ç‚¹ä¼šèŽ·å¾—ä¸€ä¸ªå®žä¾‹ç±»åž‹æ ‡ç­¾ã€‚èŠ‚ç‚¹å’Œå·éƒ½ä¼šå¾—åˆ°ä¸¤ä¸ªæè¿°èµ„æºåœ¨äº‘æä¾›å•†æ‹“æ‰‘çš„ä½ç½®æ ‡ç­¾,é€šå¸¸æ˜¯ä»¥åŒºåŸŸå’Œåœ°åŒºçš„æ–¹å¼ç»„ç»‡ã€‚&lt;/p>
&lt;!--
When nodes and volumes are created, a set of standard labels are applied based on the underlying cloud provider of the Kubernetes cluster. Nodes get a label for the instance type. Both nodes and volumes get two labels describing the location of the resource in the cloud provider topology, usually organized in zones and regions.
-->
&lt;p>Kubernetesç»„ä»¶ä½¿ç”¨æ ‡å‡†æ ‡ç­¾æ¥æ”¯æŒä¸€äº›ç‰¹æ€§ã€‚ä¾‹å¦‚ï¼Œè°ƒåº¦è€…ä¼šä¿è¯podså’Œå®ƒä»¬æ‰€å£°æ˜Žçš„å·æ”¾ç½®åœ¨ç›¸åŒçš„åŒºåŸŸï¼›å½“è°ƒåº¦éƒ¨ç½²çš„podsæ—¶ï¼Œè°ƒåº¦å™¨ä¼šä¼˜å…ˆå°†å®ƒä»¬åˆ†å¸ƒåœ¨ä¸åŒçš„åŒºåŸŸã€‚ä½ è¿˜å¯ä»¥åœ¨è‡ªå·±çš„podsæ ‡å‡†ä¸­åˆ©ç”¨æ ‡ç­¾æ¥é…ç½®ï¼Œå¦‚èŠ‚ç‚¹äº²å’Œæ€§ï¼Œä¹‹ç±»çš„äº‹ã€‚æ ‡å‡†æ ‡ç­¾ä½¿å¾—ä½ å†™çš„podè§„èŒƒåœ¨ä¸åŒçš„äº‘æä¾›å•†ä¹‹é—´æ˜¯å¯ç§»æ¤çš„ã€‚&lt;/p>
&lt;!--
Standard labels are used by Kubernetes components to support some features. For example, the scheduler would ensure that pods are placed on the same zone as the volumes they claim; and when scheduling pods belonging to a deployment, the scheduler would prioritize spreading them across zones. You can also use the labels in your pod specs to configure things as such node affinity. Standard labels allow you to write pod specs that are portable among different cloud providers.
-->
&lt;p>åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ï¼Œæ ‡ç­¾å·²ç»è¾¾åˆ°åŸºæœ¬å¯ç”¨ã€‚Kubernetesç»„ä»¶éƒ½å·²ç»æ›´æ–°ï¼Œå¯ä»¥å¡«å……åŸºæœ¬å¯ç”¨å’Œå…¬å¼€æµ‹è¯•ç‰ˆæ ‡ç­¾ï¼Œå¹¶å¯¹ä¸¤è€…åšå‡ºååº”ã€‚ç„¶è€Œï¼Œå¦‚æžœä½ çš„podè§„èŒƒæˆ–è‡ªå®šä¹‰çš„æŽ§åˆ¶å™¨æ­£åœ¨ä½¿ç”¨å…¬å¼€æµ‹è¯•ç‰ˆæ ‡ç­¾ï¼Œå¦‚èŠ‚ç‚¹äº²å’Œæ€§ï¼Œæˆ‘ä»¬å»ºè®®ä½ å¯ä»¥å°†å®ƒä»¬è¿ç§»åˆ°æ–°çš„åŸºæœ¬å¯ç”¨æ ‡ç­¾ä¸­ã€‚ä½ å¯ä»¥ä»Žå¦‚ä¸‹åœ°æ–¹æ‰¾åˆ°æ–°æ ‡ç­¾çš„æ–‡æ¡£ï¼š&lt;/p>
&lt;!--
The labels are reaching general availability in this release. Kubernetes components have been updated to populate the GA and beta labels and to react to both. However, if you are using the beta labels in your pod specs for features such as node affinity, or in your custom controllers, we recommend that you start migrating them to the new GA labels. You can find the documentation for the new labels here:
-->
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type">å®žä¾‹ç±»åž‹&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion">åœ°åŒº&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone">åŒºåŸŸ&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [node.kubernetes.io/instance-type](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#nodekubernetesioinstance-type)
- [topology.kubernetes.io/region](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesioregion)
- [topology.kubernetes.io/zone](https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#topologykubernetesiozone)
-->
&lt;h2 id="å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ-1">å·å¿«ç…§è¿›å…¥å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/h2>
&lt;!--
## Volume Snapshot Moves to Beta
-->
&lt;p>åœ¨v1.7ä¸­ï¼ŒKuberneteså·å¿«ç…§æ˜¯æ˜¯å…¬å¼€æµ‹è¯•ç‰ˆã€‚æœ€åˆæ˜¯åœ¨v1.12ä¸­ä»¥å†…éƒ¨æµ‹è¯•ç‰ˆå¼•å…¥çš„ï¼Œç¬¬äºŒä¸ªæœ‰é‡å¤§å˜åŒ–çš„å†…éƒ¨æµ‹è¯•ç‰ˆæ˜¯v1.13ã€‚è¿™ç¯‡æ–‡ç« æ€»ç»“å®ƒåœ¨å…¬å¼€ç‰ˆæœ¬ä¸­çš„å˜åŒ–ã€‚&lt;/p>
&lt;!--
The Kubernetes Volume Snapshot feature is now beta in Kubernetes v1.17. It was introduced as alpha in Kubernetes v1.12, with a second alpha with breaking changes in Kubernetes v1.13. This post summarizes the changes in the beta release.
-->
&lt;h3 id="å·å¿«ç…§æ˜¯ä»€ä¹ˆ">å·å¿«ç…§æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h3>
&lt;!-- ### What is a Volume Snapshot? -->
&lt;p>è®¸å¤šçš„å­˜å‚¨ç³»ç»Ÿ(å¦‚è°·æ­Œäº‘æŒä¹…åŒ–ç£ç›˜ï¼Œäºšé©¬é€Šå¼¹æ€§å—å­˜å‚¨å’Œè®¸å¤šçš„å†…éƒ¨å­˜å‚¨ç³»ç»Ÿ)æ”¯æŒä¸ºæŒä¹…å·åˆ›å»ºå¿«ç…§ã€‚å¿«ç…§ä»£è¡¨å·åœ¨ä¸€ä¸ªæ—¶é—´ç‚¹çš„å¤åˆ¶ã€‚å®ƒå¯ç”¨äºŽé…ç½®æ–°å·(ä½¿ç”¨å¿«ç…§æ•°æ®æå‰å¡«å……)æˆ–æ¢å¤å·åˆ°ä¸€ä¸ªä¹‹å‰çš„çŠ¶æ€(ç”¨å¿«ç…§è¡¨ç¤º)ã€‚&lt;/p>
&lt;!--
Many storage systems (like Google Cloud Persistent Disks, Amazon Elastic Block Storage, and many on-premise storage systems) provide the ability to create a â€œsnapshotâ€ of a persistent volume. A snapshot represents a point-in-time copy of a volume. A snapshot can be used either to provision a new volume (pre-populated with the snapshot data) or to restore an existing volume to a previous state (represented by the snapshot).
-->
&lt;h3 id="ä¸ºä»€ä¹ˆç»™kubernetesåŠ å…¥å·å¿«ç…§">ä¸ºä»€ä¹ˆç»™KubernetesåŠ å…¥å·å¿«ç…§ï¼Ÿ&lt;/h3>
&lt;!--
### Why add Volume Snapshots to Kubernetes?
-->
&lt;p>Kuberneteså·æ’ä»¶ç³»ç»Ÿå·²ç»æä¾›äº†åŠŸèƒ½å¼ºå¤§çš„æŠ½è±¡ç”¨äºŽè‡ªåŠ¨é…ç½®ã€é™„åŠ å’ŒæŒ‚è½½å—æ–‡ä»¶ç³»ç»Ÿã€‚&lt;/p>
&lt;!--
The Kubernetes volume plugin system already provides a powerful abstraction that automates the provisioning, attaching, and mounting of block and file storage.
-->
&lt;p>æ”¯æŒæ‰€æœ‰è¿™äº›ç‰¹æ€§æ˜¯Kubernetsè´Ÿè½½å¯ç§»æ¤çš„ç›®æ ‡ï¼šKubernetesæ—¨åœ¨åˆ†å¸ƒå¼ç³»ç»Ÿåº”ç”¨å’Œåº•å±‚é›†ç¾¤ä¹‹é—´åˆ›å»ºä¸€ä¸ªæŠ½è±¡å±‚,ä½¿å¾—åº”ç”¨å¯ä»¥ä¸æ„ŸçŸ¥å…¶è¿è¡Œé›†ç¾¤çš„å…·ä½“ä¿¡æ¯å¹¶ä¸”éƒ¨ç½²ä¹Ÿä¸éœ€ç‰¹å®šé›†ç¾¤çš„çŸ¥è¯†ã€‚&lt;/p>
&lt;!--
Underpinning all these features is the Kubernetes goal of workload portability: Kubernetes aims to create an abstraction layer between distributed systems applications and underlying clusters so that applications can be agnostic to the specifics of the cluster they run on and application deployment requires no â€œcluster specificâ€ knowledge.
-->
&lt;p>Kuberneteså­˜å‚¨ç‰¹åˆ«å…´è¶£ç»„(SIG)å°†å¿«ç…§æ“ä½œç¡®å®šä¸ºå¯¹å¾ˆå¤šæœ‰çŠ¶æ€è´Ÿè½½çš„å…³é”®åŠŸèƒ½ã€‚å¦‚æ•°æ®åº“ç®¡ç†å‘˜å¸Œæœ›åœ¨æ“ä½œæ•°æ®åº“å‰ä¿å­˜æ•°æ®åº“å·å¿«ç…§ã€‚&lt;/p>
&lt;!--
The Kubernetes Storage SIG identified snapshot operations as critical functionality for many stateful workloads. For example, a database administrator may want to snapshot a database volume before starting a database operation.
-->
&lt;p>åœ¨KubernetesæŽ¥å£ä¸­æä¾›ä¸€ç§æ ‡å‡†çš„æ–¹å¼è§¦å‘å¿«ç…§æ“ä½œï¼ŒKubernetesç”¨æˆ·å¯ä»¥å¤„ç†è¿™ç§ç”¨æˆ·åœºæ™¯ï¼Œè€Œä¸å¿…ä½¿ç”¨Kubernetes API(å¹¶æ‰‹åŠ¨æ‰§è¡Œå­˜å‚¨ç³»ç»Ÿçš„å…·ä½“æ“ä½œ)ã€‚&lt;/p>
&lt;!--
By providing a standard way to trigger snapshot operations in the Kubernetes API, Kubernetes users can now handle use cases like this without having to go around the Kubernetes API (and manually executing storage system specific operations).
-->
&lt;p>å–è€Œä»£ä¹‹çš„æ˜¯ï¼ŒKubernetesç”¨æˆ·çŽ°åœ¨è¢«æŽˆæƒä»¥ä¸Žé›†ç¾¤æ— å…³çš„æ–¹å¼å°†å¿«ç…§æ“ä½œæ”¾è¿›ä»–ä»¬çš„å·¥å…·å’Œç­–ç•¥ä¸­ï¼Œå¹¶ä¸”ç¡®ä¿¡å®ƒå°†å¯¹ä»»æ„çš„Kubernetesé›†ç¾¤æœ‰æ•ˆï¼Œè€Œä¸Žåº•å±‚å­˜å‚¨æ— å…³ã€‚&lt;/p>
&lt;!--
Instead, Kubernetes users are now empowered to incorporate snapshot operations in a cluster agnostic way into their tooling and policy with the comfort of knowing that it will work against arbitrary Kubernetes clusters regardless of the underlying storage.
-->
&lt;p>æ­¤å¤–ï¼ŒKubernetes å¿«ç…§åŽŸè¯­ä½œä¸ºåŸºç¡€æž„å»ºèƒ½åŠ›è§£é”äº†ä¸ºKuberneteså¼€å‘é«˜çº§ã€ä¼ä¸šçº§ã€å­˜å‚¨ç®¡ç†ç‰¹æ€§çš„èƒ½åŠ›:åŒ…æ‹¬åº”ç”¨æˆ–é›†ç¾¤çº§åˆ«çš„å¤‡ä»½æ–¹æ¡ˆã€‚&lt;/p>
&lt;!--
Additionally these Kubernetes snapshot primitives act as basic building blocks that unlock the ability to develop advanced, enterprise grade, storage administration features for Kubernetes: including application or cluster level backup solutions.
-->
&lt;p>ä½ å¯ä»¥é˜…è¯»æ›´å¤šå…³äºŽ&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/">å‘å¸ƒå®¹å™¨å­˜å‚¨æŽ¥å£å·å¿«ç…§å…¬å¼€æµ‹è¯•ç‰ˆ&lt;/a>&lt;/p>
&lt;!--
You can read more in the blog entry about [releasing CSI volume snapshots to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-cis-volume-snapshot-beta/).
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»å…¬æµ‹ç‰ˆ">å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»å…¬æµ‹ç‰ˆ&lt;/h2>
&lt;!--
## CSI Migration Beta
-->
&lt;h3 id="ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿ç§»å†…å»ºæ ‘æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æŽ¥å£">ä¸ºä»€ä¹ˆæˆ‘ä»¬è¿ç§»å†…å»ºæ ‘æ’ä»¶åˆ°å®¹å™¨å­˜å‚¨æŽ¥å£ï¼Ÿ&lt;/h3>
&lt;!--
### Why are we migrating in-tree plugins to CSI?
-->
&lt;p>åœ¨å®¹å™¨å­˜å‚¨æŽ¥å£ä¹‹å‰ï¼ŒKubernetesæä¾›åŠŸèƒ½å¼ºå¤§çš„å·æ’ä»¶ç³»ç»Ÿã€‚è¿™äº›å·æ’ä»¶æ˜¯æ ‘å†…çš„æ„å‘³ç€å®ƒä»¬çš„ä»£ç æ˜¯æ ¸å¿ƒKubernetesä»£ç çš„ä¸€éƒ¨åˆ†å¹¶é™„å¸¦åœ¨æ ¸å¿ƒKubernetesäºŒè¿›åˆ¶ä¸­ã€‚ç„¶è€Œï¼Œä¸ºKubernetesæ·»åŠ æ’ä»¶æ”¯æŒæ–°å·æ˜¯éžå¸¸æœ‰æŒ‘æˆ˜çš„ã€‚å¸Œæœ›åœ¨Kubernetesä¸Šä¸ºè‡ªå·±å­˜å‚¨ç³»ç»Ÿæ·»åŠ æ”¯æŒ(æˆ–ä¿®å¤çŽ°æœ‰å·æ’ä»¶çš„bug)çš„ä¾›åº”å•†è¢«è¿«ä¸ŽKuberneteså‘è¡Œè¿›ç¨‹å¯¹é½ã€‚æ­¤å¤–ï¼Œç¬¬ä¸‰æ–¹å­˜å‚¨ä»£ç åœ¨æ ¸å¿ƒKubernetesäºŒè¿›åˆ¶ä¸­ä¼šé€ æˆå¯é æ€§å’Œå®‰å…¨é—®é¢˜ï¼Œå¹¶ä¸”è¿™äº›ä»£ç å¯¹äºŽKubernetesçš„ç»´æŠ¤è€…æ¥è¯´æ˜¯éš¾ä»¥(ä¸€äº›åœºæ™¯æ˜¯ä¸å¯èƒ½)æµ‹è¯•å’Œç»´æŠ¤çš„ã€‚åœ¨Kubernetesä¸Šé‡‡ç”¨å®¹å™¨å­˜å‚¨æŽ¥å£å¯ä»¥è§£å†³å¤§éƒ¨åˆ†é—®é¢˜ã€‚&lt;/p>
&lt;!--
Prior to CSI, Kubernetes provided a powerful volume plugin system. These volume plugins were â€œin-treeâ€ meaning their code was part of the core Kubernetes code and shipped with the core Kubernetes binaries. However, adding support for new volume plugins to Kubernetes was challenging. Vendors that wanted to add support for their storage system to Kubernetes (or even fix a bug in an existing volume plugin) were forced to align with the Kubernetes release process. In addition, third-party storage code caused reliability and security issues in core Kubernetes binaries and the code was often difficult (and in some cases impossible) for Kubernetes maintainers to test and maintain. Using the Container Storage Interface in Kubernetes resolves these major issues.
-->
&lt;p>éšç€æ›´å¤šå®¹å™¨å­˜å‚¨æŽ¥å£é©±åŠ¨å˜æˆç”Ÿäº§çŽ¯å¢ƒå¯ç”¨ï¼Œæˆ‘ä»¬å¸Œæœ›æ‰€æœ‰çš„Kubernetesç”¨æˆ·ä»Žå®¹å™¨å­˜å‚¨æŽ¥å£æ¨¡åž‹ä¸­èŽ·ç›Šã€‚ç„¶è€Œï¼Œæˆ‘ä»¬ä¸å¸Œæœ›å¼ºåˆ¶ç”¨æˆ·ä»¥ç ´åçŽ°æœ‰åŸºæœ¬å¯ç”¨çš„å­˜å‚¨æŽ¥å£çš„æ–¹å¼åŽ»æ”¹å˜è´Ÿè½½å’Œé…ç½®ã€‚é“è·¯å¾ˆæ˜Žç¡®ï¼Œæˆ‘ä»¬å°†ä¸å¾—ä¸ç”¨CSIæ›¿æ¢æ ‘å†…æ’ä»¶æŽ¥å£ã€‚ä»€ä¹ˆæ˜¯å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»ï¼Ÿ&lt;/p>
&lt;!--
As more CSI Drivers were created and became production ready, we wanted all Kubernetes users to reap the benefits of the CSI model. However, we did not want to force users into making workload/configuration changes by breaking the existing generally available storage APIs. The way forward was clear - we would have to replace the backend of the â€œin-tree pluginâ€ APIs with CSI.What is CSI migration?
-->
&lt;p>åœ¨å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»ä¸Šæ‰€åšçš„åŠªåŠ›ä½¿å¾—æ›¿æ¢çŽ°æœ‰çš„æ ‘å†…å­˜å‚¨æ’ä»¶ï¼Œå¦‚&lt;code>kubernetes.io/gce-pd&lt;/code>æˆ–&lt;code>kubernetes.io/aws-ebs&lt;/code>ï¼Œä¸ºç›¸åº”çš„å®¹å™¨å­˜å‚¨æŽ¥å£é©±åŠ¨æˆä¸ºå¯èƒ½ã€‚å¦‚æžœå®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»æ­£å¸¸å·¥ä½œï¼ŒKubernetesç»ˆç«¯ç”¨æˆ·ä¸ä¼šæ³¨æ„åˆ°ä»»ä½•å·®åˆ«ã€‚è¿ç§»è¿‡åŽï¼ŒKubernetesç”¨æˆ·å¯ä»¥ç»§ç»­ä½¿ç”¨çŽ°æœ‰æŽ¥å£æ¥ä¾èµ–æ ‘å†…å­˜å‚¨æ’ä»¶çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
The CSI migration effort enables the replacement of existing in-tree storage plugins such as `kubernetes.io/gce-pd` or `kubernetes.io/aws-ebs` with a corresponding CSI driver. If CSI Migration is working properly, Kubernetes end users shouldnâ€™t notice a difference. After migration, Kubernetes users may continue to rely on all the functionality of in-tree storage plugins using the existing interface.
-->
&lt;p>å½“Kubernetesé›†ç¾¤ç®¡ç†è€…æ›´æ–°é›†ç¾¤ä½¿å¾—CSIè¿ç§»å¯ç”¨ï¼ŒçŽ°æœ‰çš„æœ‰çŠ¶æ€éƒ¨ç½²å’Œå·¥ä½œè´Ÿè½½ç…§å¸¸å·¥ä½œï¼›ç„¶è€Œï¼Œåœ¨å¹•åŽKuberneteså°†å­˜å‚¨ç®¡ç†æ“ä½œäº¤ç»™äº†(ä»¥å‰æ˜¯äº¤ç»™æ ‘å†…é©±åŠ¨)CSIé©±åŠ¨ã€‚&lt;/p>
&lt;!--
When a Kubernetes cluster administrator updates a cluster to enable CSI migration, existing stateful deployments and workloads continue to function as they always have; however, behind the scenes Kubernetes hands control of all storage management operations (previously targeting in-tree drivers) to CSI drivers.
-->
&lt;p>Kubernetesç»„éžå¸¸åŠªåŠ›åœ°ä¿è¯å­˜å‚¨æŽ¥å£çš„ç¨³å®šæ€§å’Œå¹³æ»‘å‡çº§ä½“éªŒçš„æ‰¿è¯ºã€‚è¿™éœ€è¦ç»†è‡´çš„è€ƒè™‘çŽ°æœ‰ç‰¹æ€§å’Œè¡Œä¸ºæ¥ç¡®ä¿åŽå‘å…¼å®¹å’ŒæŽ¥å£ç¨³å®šæ€§ã€‚ä½ å¯ä»¥æƒ³åƒæˆåœ¨åŠ é€Ÿè¡Œé©¶çš„ç›´çº¿ä¸Šç»™èµ›è½¦æ¢è½®èƒŽã€‚&lt;/p>
&lt;!--
The Kubernetes team has worked hard to ensure the stability of storage APIs and for the promise of a smooth upgrade experience. This involves meticulous accounting of all existing features and behaviors to ensure backwards compatibility and API stability. You can think of it like changing the wheels on a racecar while itâ€™s speeding down the straightaway.
-->
&lt;p>ä½ å¯ä»¥åœ¨è¿™ç¯‡åšå®¢ä¸­é˜…è¯»æ›´å¤šå…³äºŽ&lt;a href="https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/">å®¹å™¨å­˜å‚¨æŽ¥å£è¿ç§»æˆä¸ºå…¬å¼€æµ‹è¯•ç‰ˆ&lt;/a>.&lt;/p>
&lt;!--
You can read more in the blog entry about [CSI migration going to beta](https://kubernetes.io/blog/2019/12/09/kubernetes-1-17-feature-csi-migration-beta/). -->
&lt;h2 id="å…¶å®ƒæ›´æ–°">å…¶å®ƒæ›´æ–°&lt;/h2>
&lt;!--
## Other Updates
-->
&lt;h3 id="ç¨³å®š">ç¨³å®šðŸ’¯&lt;/h3>
&lt;!--
### Graduated to Stable ðŸ’¯
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/382">æŒ‰æ¡ä»¶æ±¡æŸ“èŠ‚ç‚¹&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/495">å¯é…ç½®çš„Podè¿›ç¨‹å…±äº«å‘½åç©ºé—´&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/548">é‡‡ç”¨kube-schedulerè°ƒåº¦DaemonSet Pods&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/554">åŠ¨æ€å·æœ€å¤§å€¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/557">Kuberneteså®¹å™¨å­˜å‚¨æŽ¥å£æ”¯æŒæ‹“æ‰‘&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/559">åœ¨SubPathæŒ‚è½½æä¾›çŽ¯å¢ƒå˜é‡æ‰©å±•&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/575">ä¸ºCustom Resourcesæä¾›é»˜è®¤å€¼&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/589">ä»Žé¢‘ç¹çš„Kubletå¿ƒè·³åˆ°ç§Ÿçº¦æŽ¥å£&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/714">æ‹†åˆ†Kubernetesæµ‹è¯•Tarball&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/956">æ·»åŠ Watchä¹¦ç­¾æ”¯æŒ&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/960">è¡Œä¸ºé©±åŠ¨ä¸€è‡´æ€§æµ‹è¯•&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/980">æœåŠ¡è´Ÿè½½å‡è¡¡ç»ˆç»“ä¿æŠ¤&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1152">é¿å…æ¯ä¸€ä¸ªWatcherç‹¬ç«‹åºåˆ—åŒ–ç›¸åŒçš„å¯¹è±¡&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Taint Node by Condition](https://github.com/kubernetes/enhancements/issues/382)
- [Configurable Pod Process Namespace Sharing](https://github.com/kubernetes/enhancements/issues/495)
- [Schedule DaemonSet Pods by kube-scheduler](https://github.com/kubernetes/enhancements/issues/548)
- [Dynamic Maximum Volume Count](https://github.com/kubernetes/enhancements/issues/554)
- [Kubernetes CSI Topology Support](https://github.com/kubernetes/enhancements/issues/557)
- [Provide Environment Variables Expansion in SubPath Mount](https://github.com/kubernetes/enhancements/issues/559)
- [Defaulting of Custom Resources](https://github.com/kubernetes/enhancements/issues/575)
- [Move Frequent Kubelet Heartbeats To Lease Api](https://github.com/kubernetes/enhancements/issues/589)
- [Break Apart The Kubernetes Test Tarball](https://github.com/kubernetes/enhancements/issues/714)
- [Add Watch Bookmarks Support](https://github.com/kubernetes/enhancements/issues/956)
- [Behavior-Driven Conformance Testing](https://github.com/kubernetes/enhancements/issues/960)
- [Finalizer Protection For Service Loadbalancers](https://github.com/kubernetes/enhancements/issues/980)
- [Avoid Serializing The Same Object Independently For Every Watcher](https://github.com/kubernetes/enhancements/issues/1152)
-->
&lt;h3 id="ä¸»è¦å˜åŒ–">ä¸»è¦å˜åŒ–&lt;/h3>
&lt;!--
### Major Changes
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/563">æ·»åŠ IPv4/IPv6åŒæ ˆæ”¯æŒ&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Add IPv4/IPv6 Dual Stack Support](https://github.com/kubernetes/enhancements/issues/563)
-->
&lt;h3 id="å…¶å®ƒæ˜¾è‘—ç‰¹æ€§">å…¶å®ƒæ˜¾è‘—ç‰¹æ€§&lt;/h3>
&lt;!--
### Other Notable Features
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/536">æ‹“æ‰‘æ„ŸçŸ¥è·¯ç”±æœåŠ¡(å†…éƒ¨æµ‹è¯•ç‰ˆ)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">ä¸ºWindowsæ·»åŠ RunAsUserName&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- [Topology Aware Routing of Services (Alpha)](https://github.com/kubernetes/enhancements/issues/536)
- [RunAsUserName for Windows](https://github.com/kubernetes/enhancements/issues/1043)
-->
&lt;h3 id="å¯ç”¨æ€§">å¯ç”¨æ€§&lt;/h3>
&lt;!--
### Availability
-->
&lt;p>Kubernetes 1.17 å¯ä»¥&lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0">åœ¨GitHubä¸‹è½½&lt;/a>ã€‚å¼€å§‹ä½¿ç”¨Kubernetesï¼Œçœ‹çœ‹è¿™äº›&lt;a href="https://kubernetes.io/docs/tutorials/">äº¤äº’æ•™å­¦&lt;/a>ã€‚ä½ å¯ä»¥éžå¸¸å®¹æ˜“ä½¿ç”¨&lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>å®‰è£…1.17ã€‚&lt;/p>
&lt;!--
Kubernetes 1.17 is available for [download on GitHub](https://github.com/kubernetes/kubernetes/releases/tag/v1.17.0). To get started with Kubernetes, check out these [interactive tutorials](https://kubernetes.io/docs/tutorials/). You can also easily install 1.17 using
[kubeadm](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/).
-->
&lt;h3 id="å‘å¸ƒå›¢é˜Ÿ">å‘å¸ƒå›¢é˜Ÿ&lt;/h3>
&lt;!--
### Release Team
-->
&lt;p>æ­£æ˜¯å› ä¸ºæœ‰ä¸Šåƒäººå‚ä¸ŽæŠ€æœ¯æˆ–éžæŠ€æœ¯å†…å®¹çš„è´¡çŒ®æ‰ä½¿è¿™ä¸ªç‰ˆæœ¬æˆä¸ºå¯èƒ½ã€‚ç‰¹åˆ«æ„Ÿè°¢ç”±Guinevere Saengeré¢†å¯¼çš„&lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md">å‘å¸ƒå›¢é˜Ÿ&lt;/a>ã€‚å‘å¸ƒå›¢é˜Ÿçš„35åæˆå‘˜åœ¨å‘å¸ƒç‰ˆæœ¬çš„å¤šæ–¹é¢è¿›è¡Œäº†åè°ƒï¼Œä»Žæ–‡æ¡£åˆ°æµ‹è¯•ï¼Œæ ¡éªŒå’Œç‰¹æ€§çš„å®Œå–„ã€‚&lt;/p>
&lt;!--
This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the [release team](https://github.com/kubernetes/sig-release/blob/master/releases/release-1.17/release_team.md) led by Guinevere Saenger. The 35 individuals on the release team coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.
-->
&lt;p>éšç€Kubernetesç¤¾åŒºçš„æˆé•¿ï¼Œæˆ‘ä»¬çš„å‘å¸ƒæµç¨‹æ˜¯åœ¨å¼€æºè½¯ä»¶åä½œæ–¹é¢æƒŠäººçš„ç¤ºä¾‹ã€‚Kuberneteså¿«é€Ÿå¹¶æŒç»­èŽ·å¾—æ–°ç”¨æˆ·ã€‚è¿™ä¸€æˆé•¿äº§ç”Ÿäº†è‰¯æ€§çš„åé¦ˆå¾ªçŽ¯ï¼Œæ›´å¤šçš„è´¡çŒ®è€…è´¡çŒ®ä»£ç åˆ›é€ äº†æ›´åŠ æ´»è·ƒçš„ç”Ÿæ€ã€‚Kuberneteså·²ç»æœ‰è¶…è¿‡&lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">39000ä½è´¡çŒ®è€…&lt;/a>å’Œä¸€ä¸ªè¶…è¿‡66000äººçš„æ´»è·ƒç¤¾åŒºã€‚&lt;/p>
&lt;!--
As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over [39,000 individual contributors](https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1) to date and an active community of more than 66,000 people.
-->
&lt;h3 id="ç½‘ç»œç ”è®¨ä¼š">ç½‘ç»œç ”è®¨ä¼š&lt;/h3>
&lt;!--
### Webinar
-->
&lt;p>2020å¹´1æœˆ7å·ï¼ŒåŠ å…¥Kubernetes 1.17å‘å¸ƒå›¢é˜Ÿï¼Œå­¦ä¹ å…³äºŽè¿™æ¬¡å‘å¸ƒçš„ä¸»è¦ç‰¹æ€§ã€‚&lt;a href="https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A">è¿™é‡Œ&lt;/a>æ³¨å†Œã€‚&lt;/p>
&lt;!--
Join members of the Kubernetes 1.17 release team on Jan 7th, 2020 to learn about the major features in this release. Register [here](https://zoom.us/webinar/register/9315759188139/WN_kPOZA_6RTjeGdXTG7YFO3A).
-->
&lt;h3 id="å‚ä¸Žå…¶ä¸­">å‚ä¸Žå…¶ä¸­&lt;/h3>
&lt;!--
### Get Involved
-->
&lt;p>æœ€ç®€å•çš„å‚ä¸ŽKubernetesçš„æ–¹å¼æ˜¯åŠ å…¥å…¶ä¸­ä¸€ä¸ªä¸Žä½ å…´è¶£ç›¸åŒçš„&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£ç»„&lt;/a>ï¼ˆSIGs)ã€‚æœ‰ä»€ä¹ˆæƒ³è¦å¹¿æ’­åˆ°Kubernetesç¤¾åŒºå—ï¼Ÿé€šè¿‡å¦‚ä¸‹çš„é¢‘é“ï¼Œåœ¨æ¯å‘¨çš„&lt;a href="https://github.com/kubernetes/community/tree/master/communication">ç¤¾åŒºä¼šè®®&lt;/a>åˆ†äº«ä½ çš„å£°éŸ³ã€‚æ„Ÿè°¢ä½ çš„è´¡çŒ®å’Œæ”¯æŒã€‚&lt;/p>
&lt;!--
The simplest way to get involved with Kubernetes is by joining one of the many [Special Interest Groups](https://github.com/kubernetes/community/blob/master/sig-list.md) (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly [community meeting](https://github.com/kubernetes/community/tree/master/communication), and through the channels below. Thank you for your continued feedback and support.
-->
&lt;ul>
&lt;li>åœ¨Twitterä¸Šå…³æ³¨æˆ‘ä»¬&lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>èŽ·å–æœ€æ–°çš„æ›´æ–°&lt;/li>
&lt;li>åœ¨&lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>å‚ä¸Žç¤¾åŒºçš„è®¨è®º&lt;/li>
&lt;li>åœ¨&lt;a href="http://slack.k8s.io/">Slack&lt;/a>åŠ å…¥ç¤¾åŒº&lt;/li>
&lt;li>åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>å‘å¸ƒé—®é¢˜(æˆ–å›žç­”é—®é¢˜)&lt;/li>
&lt;li>åˆ†äº«ä½ çš„Kubernetes&lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
- Join the community discussion on [Discuss](https://discuss.kubernetes.io/)
- Join the community on [Slack](http://slack.k8s.io/)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Share your Kubernetes [story](https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform)
--></description></item><item><title>Blog: ä½¿ç”¨ Java å¼€å‘ä¸€ä¸ª Kubernetes controller</title><link>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/develop-a-kubernetes-controller-in-java/</guid><description>
&lt;!--
---
layout: blog
title: "Develop a Kubernetes controller in Java"
date: 2019-11-26
slug: Develop-A-Kubernetes-Controller-in-Java
---
-->
&lt;!--
**Authors:** Min Kim (Ant Financial), Tony Ado (Ant Financial)
-->
&lt;p>&lt;strong>ä½œè€…:&lt;/strong> Min Kim (èš‚èšé‡‘æœ), Tony Ado (èš‚èšé‡‘æœ)&lt;/p>
&lt;!--
The official [Kubernetes Java SDK](https://github.com/kubernetes-client/java) project
recently released their latest work on providing the Java Kubernetes developers
a handy Kubernetes controller-builder SDK which is helpful for easily developing
advanced workloads or systems.
-->
&lt;p>&lt;a href="https://github.com/kubernetes-client/java">Kubernetes Java SDK&lt;/a> å®˜æ–¹é¡¹ç›®æœ€è¿‘å‘å¸ƒäº†ä»–ä»¬çš„æœ€æ–°å·¥ä½œï¼Œä¸º Java Kubernetes å¼€å‘äººå‘˜æä¾›ä¸€ä¸ªä¾¿æ·çš„ Kubernetes æŽ§åˆ¶å™¨-æž„å»ºå™¨ SDKï¼Œå®ƒæœ‰åŠ©äºŽè½»æ¾å¼€å‘é«˜çº§å·¥ä½œè´Ÿè½½æˆ–ç³»ç»Ÿã€‚&lt;/p>
&lt;!--
## Overall
Java is no doubt one of the most popular programming languages in the world but
it's been difficult for a period time for those non-Golang developers to build up
their customized controller/operator due to the lack of library resources in the
community. In the world of Golang, there're already some excellent controller
frameworks, for example, [controller runtime](https://github.com/kubernetes-sigs/controller-runtime),
[operator SDK](https://github.com/operator-framework/operator-sdk). These
existing Golang frameworks are relying on the various utilities from the
[Kubernetes Golang SDK](https://github.com/kubernetes/client-go) proven to
be stable over years. Driven by the emerging need of further integration into
the platform of Kubernetes, we not only ported many essential toolings from the Golang
SDK into the kubernetes Java SDK including informers, work-queues, leader-elections,
etc. but also developed a controller-builder SDK which wires up everything into
a runnable controller without hiccups.
-->
&lt;h2 id="ç»¼è¿°">ç»¼è¿°&lt;/h2>
&lt;p>Java æ— ç–‘æ˜¯ä¸–ç•Œä¸Šæœ€æµè¡Œçš„ç¼–ç¨‹è¯­è¨€ä¹‹ä¸€ï¼Œä½†ç”±äºŽç¤¾åŒºä¸­ç¼ºå°‘åº“èµ„æºï¼Œä¸€æ®µæ—¶é—´ä»¥æ¥ï¼Œé‚£äº›éž Golang å¼€å‘äººå‘˜å¾ˆéš¾æž„å»ºä»–ä»¬å®šåˆ¶çš„ controller/operatorã€‚åœ¨ Golang çš„ä¸–ç•Œé‡Œï¼Œå·²ç»æœ‰ä¸€äº›å¾ˆå¥½çš„ controller æ¡†æž¶äº†ï¼Œä¾‹å¦‚ï¼Œ&lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller runtime&lt;/a>ï¼Œ&lt;a href="https://github.com/operator-framework/operator-sdk">operator SDK&lt;/a>ã€‚è¿™äº›çŽ°æœ‰çš„ Golang æ¡†æž¶ä¾èµ–äºŽ &lt;a href="https://github.com/kubernetes/client-go">Kubernetes Golang SDK&lt;/a> æä¾›çš„å„ç§å®žç”¨å·¥å…·ï¼Œè¿™äº›å·¥å…·ç»è¿‡å¤šå¹´è¯æ˜Žæ˜¯ç¨³å®šçš„ã€‚å—è¿›ä¸€æ­¥é›†æˆåˆ° Kubernetes å¹³å°çš„éœ€æ±‚é©±åŠ¨ï¼Œæˆ‘ä»¬ä¸ä»…å°† Golang SDK ä¸­çš„è®¸å¤šåŸºæœ¬å·¥å…·ç§»æ¤åˆ° kubernetes Java SDK ä¸­ï¼ŒåŒ…æ‹¬ informersã€work-queuesã€leader-elections ç­‰ï¼Œä¹Ÿå¼€å‘äº†ä¸€ä¸ªæŽ§åˆ¶å™¨æž„å»º SDKï¼Œå®ƒå¯ä»¥å°†æ‰€æœ‰ä¸œè¥¿è¿žæŽ¥åˆ°ä¸€ä¸ªå¯è¿è¡Œçš„æŽ§åˆ¶å™¨ä¸­ï¼Œè€Œä¸ä¼šäº§ç”Ÿä»»ä½•é—®é¢˜ã€‚&lt;/p>
&lt;!--
## Backgrounds
Why use Java to implement Kubernetes tooling? You might pick Java for:
- __Integrating legacy enterprise Java systems__: Many companies have their legacy
systems or frameworks written in Java in favor of stability. We are not able to
move everything to Golang easily.
- __More open-source community resources__: Java is mature and has accumulated abundant open-source
libraries over decades, even though Golang is getting more and more fancy and
popular for developers. Additionally, nowadays developers are able to develop
their aggregated-apiservers over SQL-storage and Java has way better support on SQLs.
-->
&lt;h2 id="èƒŒæ™¯">èƒŒæ™¯&lt;/h2>
&lt;p>ä¸ºä»€ä¹ˆè¦ä½¿ç”¨ Java å®žçŽ° kubernetes å·¥å…·ï¼Ÿé€‰æ‹© Java çš„åŽŸå› å¯èƒ½æ˜¯ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>&lt;strong>é›†æˆé—ç•™çš„ä¼ä¸šçº§ Java ç³»ç»Ÿ&lt;/strong>ï¼šè®¸å¤šå…¬å¸çš„é—ç•™ç³»ç»Ÿæˆ–æ¡†æž¶éƒ½æ˜¯ç”¨ Java ç¼–å†™çš„ï¼Œç”¨ä»¥æ”¯æŒç¨³å®šæ€§ã€‚æˆ‘ä»¬ä¸èƒ½è½»æ˜“æŠŠæ‰€æœ‰ä¸œè¥¿æ¬åˆ° Golangã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;strong>æ›´å¤šå¼€æºç¤¾åŒºçš„èµ„æº&lt;/strong>ï¼šJava æ˜¯æˆç†Ÿçš„ï¼Œå¹¶ä¸”åœ¨è¿‡åŽ»å‡ åå¹´ä¸­ç´¯è®¡äº†ä¸°å¯Œçš„å¼€æºåº“ï¼Œå°½ç®¡ Golang å¯¹äºŽå¼€å‘äººå‘˜æ¥è¯´è¶Šæ¥è¶Šå…·æœ‰å¸å¼•åŠ›ï¼Œè¶Šæ¥è¶Šæµè¡Œã€‚æ­¤å¤–ï¼ŒçŽ°åœ¨å¼€å‘äººå‘˜èƒ½å¤Ÿåœ¨ SQL å­˜å‚¨ä¸Šå¼€å‘ä»–ä»¬çš„èšåˆ-apiserverï¼Œè€Œ Java åœ¨ SQL ä¸Šæœ‰æ›´å¥½çš„æ”¯æŒã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## How to use?
Take maven project as example, adding the following dependencies into your dependencies:
-->
&lt;h2 id="å¦‚ä½•åŽ»ä½¿ç”¨">å¦‚ä½•åŽ»ä½¿ç”¨&lt;/h2>
&lt;p>ä»¥ maven é¡¹ç›®ä¸ºä¾‹ï¼Œå°†ä»¥ä¸‹ä¾èµ–é¡¹æ·»åŠ åˆ°æ‚¨çš„ä¾èµ–ä¸­ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-xml" data-lang="xml">&lt;span style="color:#008000;font-weight:bold">&amp;lt;dependency&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;groupId&amp;gt;&lt;/span>io.kubernetes&lt;span style="color:#008000;font-weight:bold">&amp;lt;/groupId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;artifactId&amp;gt;&lt;/span>client-java-extended&lt;span style="color:#008000;font-weight:bold">&amp;lt;/artifactId&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;version&amp;gt;&lt;/span>6.0.1&lt;span style="color:#008000;font-weight:bold">&amp;lt;/version&amp;gt;&lt;/span>
&lt;span style="color:#008000;font-weight:bold">&amp;lt;/dependency&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Then we can make use of the provided builder libraries to write your own controller.
For example, the following one is a simple controller prints out node information
on watch notification, see complete example [here](https://github.com/kubernetes-client/java/blob/master/examples/src/main/java/io/kubernetes/client/examples/ControllerExample.java):
-->
&lt;p>ç„¶åŽæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æä¾›çš„ç”Ÿæˆå™¨åº“æ¥ç¼–å†™è‡ªå·±çš„æŽ§åˆ¶å™¨ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„æŽ§åˆ¶ï¼Œå®ƒæ‰“å°å‡ºå…³äºŽç›‘è§†é€šçŸ¥çš„èŠ‚ç‚¹ä¿¡æ¯ï¼Œè¯·çœ‹å®Œæ•´çš„ä¾‹å­ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-java" data-lang="java">&lt;span style="color:#666">...&lt;/span>
Reconciler reconciler &lt;span style="color:#666">=&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Reconciler&lt;span style="color:#666">()&lt;/span> &lt;span style="color:#666">{&lt;/span>
&lt;span style="color:#a2f">@Override&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">public&lt;/span> Result &lt;span style="color:#00a000">reconcile&lt;/span>&lt;span style="color:#666">(&lt;/span>Request request&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">{&lt;/span>
V1Node node &lt;span style="color:#666">=&lt;/span> nodeLister&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">get&lt;/span>&lt;span style="color:#666">(&lt;/span>request&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
System&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">out&lt;/span>&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">println&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;triggered reconciling &amp;#34;&lt;/span> &lt;span style="color:#666">+&lt;/span> node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">getMetadata&lt;/span>&lt;span style="color:#666">().&lt;/span>&lt;span style="color:#b44">getName&lt;/span>&lt;span style="color:#666">());&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">return&lt;/span> &lt;span style="color:#a2f;font-weight:bold">new&lt;/span> Result&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#666">);&lt;/span>
&lt;span style="color:#666">}&lt;/span>
&lt;span style="color:#666">};&lt;/span>
Controller controller &lt;span style="color:#666">=&lt;/span>
ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">defaultBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>informerFactory&lt;span style="color:#666">)&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">watch&lt;/span>&lt;span style="color:#666">(&lt;/span>
&lt;span style="color:#666">(&lt;/span>workQueue&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#666">-&amp;gt;&lt;/span> ControllerBuilder&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">controllerWatchBuilder&lt;/span>&lt;span style="color:#666">(&lt;/span>V1Node&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">class&lt;/span>&lt;span style="color:#666">,&lt;/span> workQueue&lt;span style="color:#666">).&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">())&lt;/span>
&lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReconciler&lt;/span>&lt;span style="color:#666">(&lt;/span>nodeReconciler&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// required, set the actual reconciler
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withName&lt;/span>&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#b44">&amp;#34;node-printing-controller&amp;#34;&lt;/span>&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set name for controller for logging, thread-tracing
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withWorkerCount&lt;/span>&lt;span style="color:#666">(&lt;/span>4&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, set worker thread count
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">withReadyFunc&lt;/span>&lt;span style="color:#666">(&lt;/span> nodeInformer&lt;span style="color:#666">::&lt;/span>hasSynced&lt;span style="color:#666">)&lt;/span> &lt;span style="color:#080;font-style:italic">// optional, only starts controller when the cache has synced up
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#666">.&lt;/span>&lt;span style="color:#b44">build&lt;/span>&lt;span style="color:#666">();&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you notice, the new Java controller framework learnt a lot from the design of
[controller-runtime](https://github.com/kubernetes-sigs/controller-runtime) which
successfully encapsulates the complex components inside controller into several
clean interfaces. With the help of Java Generics, we even move on a bit and simply
the encapsulation in a better way.
-->
&lt;p>å¦‚æžœæ‚¨ç•™æ„ï¼Œæ–°çš„ Java æŽ§åˆ¶å™¨æ¡†æž¶å¾ˆå¤šåœ°æ–¹å€Ÿé‰´äºŽ &lt;a href="https://github.com/kubernetes-sigs/controller-runtime">controller-runtime&lt;/a> çš„è®¾è®¡ï¼Œå®ƒæˆåŠŸåœ°å°†æŽ§åˆ¶å™¨å†…éƒ¨çš„å¤æ‚ç»„ä»¶å°è£…åˆ°å‡ ä¸ªå¹²å‡€çš„æŽ¥å£ä¸­ã€‚åœ¨ Java æ³›åž‹çš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬ç”šè‡³æ›´è¿›ä¸€æ­¥ï¼Œä»¥æ›´å¥½çš„æ–¹å¼ç®€åŒ–äº†å°è£…ã€‚&lt;/p>
&lt;!--
As for more advanced usage, we can wrap multiple controllers into a controller-manager
or a leader-electing controller which helps deploying in HA setup. In a word, we can
basically find most of the equivalence implementations here from Golang SDK and
more advanced features are under active development by us.
-->
&lt;p>æˆ‘ä»¬å¯ä»¥å°†å¤šä¸ªæŽ§åˆ¶å™¨å°è£…åˆ°ä¸€ä¸ª controller-manager æˆ– leader-electing controller ä¸­ï¼Œè¿™æœ‰åŠ©äºŽåœ¨ HA è®¾ç½®ä¸­è¿›è¡Œéƒ¨ç½²ã€‚&lt;/p>
&lt;!--
## Future steps
The community behind the official Kubernetes Java SDK project will be focusing on
providing more useful utilities for developers who hope to program cloud native
Java applications to extend Kubernetes. If you are interested in more details,
please look at our repo [kubernetes-client/java](https://github.com/kubernetes-client/java).
Feel free to share also your feedback with us, through Issues or [Slack](http://kubernetes.slack.com/messages/kubernetes-client/).
-->
&lt;h2 id="æœªæ¥è®¡åˆ’">æœªæ¥è®¡åˆ’&lt;/h2>
&lt;p>Kubernetes Java SDK é¡¹ç›®èƒŒåŽçš„ç¤¾åŒºå°†ä¸“æ³¨äºŽä¸ºå¸Œæœ›ç¼–å†™äº‘åŽŸç”Ÿ Java åº”ç”¨ç¨‹åºæ¥æ‰©å±• Kubernetes çš„å¼€å‘äººå‘˜æä¾›æ›´æœ‰ç”¨çš„å®žç”¨ç¨‹åºã€‚å¦‚æžœæ‚¨å¯¹æ›´è¯¦ç»†çš„ä¿¡æ¯æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„ä»“åº“ &lt;a href="https://github.com/kubernetes-client/java">kubernetes-client/java&lt;/a>ã€‚è¯·é€šè¿‡é—®é¢˜æˆ– &lt;a href="http://kubernetes.slack.com/messages/kubernetes-client/">Slack&lt;/a> ä¸Žæˆ‘ä»¬åˆ†äº«æ‚¨çš„åé¦ˆã€‚&lt;/p></description></item><item><title>Blog: ä½¿ç”¨ Microk8s åœ¨ Linux ä¸Šæœ¬åœ°è¿è¡Œ Kubernetes</title><link>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</link><pubDate>Tue, 26 Nov 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/11/26/%E4%BD%BF%E7%94%A8-microk8s-%E5%9C%A8-linux-%E4%B8%8A%E6%9C%AC%E5%9C%B0%E8%BF%90%E8%A1%8C-kubernetes/</guid><description>
&lt;!--
---
title: 'Running Kubernetes locally on Linux with Microk8s'
date: 2019-11-26
---
-->
&lt;!--
**Authors**: [Ihor Dvoretskyi](https://twitter.com/idvoretskyi), Developer Advocate, Cloud Native Computing Foundation; [Carmine Rimi](https://twitter.com/carminerimi)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>ï¼Œå¼€å‘æ”¯æŒè€…ï¼Œäº‘åŽŸç”Ÿè®¡ç®—åŸºé‡‘ä¼šï¼›&lt;a href="https://twitter.com/carminerimi">Carmine Rimi&lt;/a>&lt;/p>
&lt;!--
This article, the second in a [series](/blog/2019/03/28/running-kubernetes-locally-on-linux-with-minikube-now-with-kubernetes-1.14-support/) about local deployment options on Linux, and covers [MicroK8s](https://microk8s.io/). Microk8s is the click-and-run solution for deploying a Kubernetes cluster locally, originally developed by Canonical, the publisher of Ubuntu.
-->
&lt;p>æœ¬æ–‡æ˜¯å…³äºŽ Linux ä¸Šçš„æœ¬åœ°éƒ¨ç½²é€‰é¡¹&lt;a href="https://twitter.com/idvoretskyi">ç³»åˆ—&lt;/a>çš„ç¬¬äºŒç¯‡ï¼Œæ¶µç›–äº† &lt;a href="https://microk8s.io/">MicroK8s&lt;/a>ã€‚Microk8s æ˜¯æœ¬åœ°éƒ¨ç½² Kubernetes é›†ç¾¤çš„ 'click-and-run' æ–¹æ¡ˆï¼Œæœ€åˆç”± Ubuntu çš„å‘å¸ƒè€… Canonical å¼€å‘ã€‚&lt;/p>
&lt;!--
While Minikube usually spins up a local virtual machine (VM) for the Kubernetes cluster, MicroK8s doesnâ€™t require a VM. It uses [snap](https://snapcraft.io/) packages, an application packaging and isolation technology.
-->
&lt;p>è™½ç„¶ Minikube é€šå¸¸ä¸º Kubernetes é›†ç¾¤åˆ›å»ºä¸€ä¸ªæœ¬åœ°è™šæ‹Ÿæœºï¼ˆVMï¼‰ï¼Œä½†æ˜¯ MicroK8s ä¸éœ€è¦ VMã€‚å®ƒä½¿ç”¨&lt;a href="https://snapcraft.io/">snap&lt;/a> åŒ…ï¼Œè¿™æ˜¯ä¸€ç§åº”ç”¨ç¨‹åºæ‰“åŒ…å’Œéš”ç¦»æŠ€æœ¯ã€‚&lt;/p>
&lt;!--
This difference has its pros and cons. Here weâ€™ll discuss a few of the interesting differences, and comparing the benefits of a VM based approach with the benefits of a non-VM approach. One of the first factors is cross-platform portability. While a Minikube VM is portable across operating systems - it supports not only Linux, but Windows, macOS, and even FreeBSD - Microk8s requires Linux, and only on those distributions [that support snaps](https://snapcraft.io/docs/installing-snapd). Most popular Linux distributions are supported.
-->
&lt;p>è¿™ç§å·®å¼‚æœ‰å…¶ä¼˜ç‚¹å’Œç¼ºç‚¹ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æœ‰è¶£çš„åŒºåˆ«ï¼Œå¹¶ä¸”åŸºäºŽ VM çš„æ–¹æ³•å’Œéž VM æ–¹æ³•çš„å¥½å¤„ã€‚ç¬¬ä¸€ä¸ªå› ç´ æ˜¯è·¨å¹³å°çš„ç§»æ¤æ€§ã€‚è™½ç„¶ Minikube VM å¯ä»¥è·¨æ“ä½œç³»ç»Ÿç§»æ¤â€”â€”å®ƒä¸ä»…æ”¯æŒ Linuxï¼Œè¿˜æ”¯æŒ Windowsã€macOSã€ç”šè‡³ FreeBSDï¼Œä½† Microk9s éœ€è¦ Linuxï¼Œè€Œä¸”åªåœ¨&lt;a href="https://snapcraft.io/docs/installing-snapd">é‚£äº›æ”¯æŒ snaps&lt;/a> çš„å‘è¡Œç‰ˆä¸Šã€‚æ”¯æŒå¤§å¤šæ•°æµè¡Œçš„ Linux å‘è¡Œç‰ˆã€‚&lt;/p>
&lt;!--
Another factor to consider is resource consumption. While a VM appliance gives you greater portability, it does mean youâ€™ll consume more resources to run the VM, primarily because the VM ships a complete operating system, and runs on top of a hypervisor. Youâ€™ll consume more disk space when the VM is dormant. Youâ€™ll consume more RAM and CPU while it is running. Since Microk8s doesnâ€™t require spinning up a virtual machine youâ€™ll have more resources to run your workloads and other applications. Given its smaller footprint, MicroK8s is ideal for IoT devices - you can even use it on a Raspberry Pi device!
-->
&lt;p>å¦ä¸€ä¸ªè€ƒè™‘åˆ°çš„å› ç´ æ˜¯èµ„æºæ¶ˆè€—ã€‚è™½ç„¶ VM è®¾å¤‡ä¸ºæ‚¨æä¾›äº†æ›´å¥½çš„å¯ç§»æ¤æ€§ï¼Œä½†å®ƒç¡®å®žæ„å‘³ç€æ‚¨å°†æ¶ˆè€—æ›´å¤šèµ„æºæ¥è¿è¡Œ VMï¼Œè¿™ä¸»è¦æ˜¯å› ä¸º VM æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„æ“ä½œç³»ç»Ÿï¼Œå¹¶ä¸”è¿è¡Œåœ¨ç®¡ç†ç¨‹åºä¹‹ä¸Šã€‚å½“ VM å¤„äºŽä¼‘çœ æ—¶ä½ å°†æ¶ˆè€—æ›´å¤šçš„ç£ç›˜ç©ºé—´ã€‚å½“å®ƒè¿è¡Œæ—¶ï¼Œä½ å°†ä¼šæ¶ˆè€—æ›´å¤šçš„ RAM å’Œ CPUã€‚å› ä¸º MIcrok8s ä¸éœ€è¦åˆ›å»ºè™šæ‹Ÿæœºï¼Œä½ å°†ä¼šæœ‰æ›´å¤šçš„èµ„æºåŽ»è¿è¡Œä½ çš„å·¥ä½œè´Ÿè½½å’Œå…¶ä»–è®¾å¤‡ã€‚è€ƒè™‘åˆ°æ‰€å ç”¨çš„ç©ºé—´æ›´å°ï¼ŒMIcroK8s æ˜¯ç‰©è”ç½‘è®¾å¤‡çš„ç†æƒ³é€‰æ‹©-ä½ ç”šè‡³å¯ä»¥åœ¨ Paspberry Pi å’Œè®¾å¤‡ä¸Šä½¿ç”¨å®ƒï¼&lt;/p>
&lt;!--
Finally, the projects appear to follow a different release cadence and strategy. MicroK8s, and snaps in general provide [channels](https://snapcraft.io/docs/channels) that allow you to consume beta and release candidate versions of new releases of Kubernetes, as well as the previous stable release. Microk8s generally releases the stable release of upstream Kubernetes almost immediately.
-->
&lt;p>æœ€åŽï¼Œé¡¹ç›®ä¼¼ä¹Žéµå¾ªäº†ä¸åŒçš„å‘å¸ƒèŠ‚å¥å’Œç­–ç•¥ã€‚Microk8s å’Œ snaps é€šå¸¸æä¾›&lt;a href="https://snapcraft.io/docs/channels">æ¸ é“&lt;/a>å…è®¸ä½ ä½¿ç”¨æµ‹è¯•ç‰ˆå’Œå‘å¸ƒ KUbernetes æ–°ç‰ˆæœ¬çš„å€™é€‰ç‰ˆæœ¬ï¼ŒåŒæ ·ä¹Ÿæä¾›å…ˆå‰ç¨³å®šç‰ˆæœ¬ã€‚Microk8s é€šå¸¸å‡ ä¹Žç«‹åˆ»å‘å¸ƒ Kubernetes ä¸Šæ¸¸çš„ç¨³å®šç‰ˆæœ¬ã€‚&lt;/p>
&lt;!--
But wait, thereâ€™s more! Minikube and MicroK8s both started as single-node clusters. Essentially, they allow you to create a Kubernetes cluster with a single worker node. That is about to change - thereâ€™s an early alpha release of MicroK8s that includes clustering. With this capability, you can create Kubernetes clusters with as many worker nodes as you wish. This is effectively an un-opinionated option for creating a cluster - the developer must create the network connectivity between the nodes, as well as integrate with other infrastructure that may be required, like an external load-balancer. In summary, MicroK8s offers a quick and easy way to turn a handful of computers or VMs into a multi-node Kubernetes cluster. Weâ€™ll write more about this kind of architecture in a future article.
-->
&lt;p>ä½†æ˜¯ç­‰ç­‰ï¼Œè¿˜æœ‰æ›´å¤šï¼Minikube å’Œ Microk8s éƒ½æ˜¯ä½œä¸ºå•èŠ‚ç‚¹é›†ç¾¤å¯åŠ¨çš„ã€‚æœ¬è´¨ä¸Šæ¥è¯´ï¼Œå®ƒä»¬å…è®¸ä½ ç”¨å•ä¸ªå·¥ä½œèŠ‚ç‚¹åˆ›å»º Kubernetes é›†ç¾¤ã€‚è¿™ç§æƒ…å†µå³å°†æ”¹å˜ - MicroK8s æ—©æœŸçš„ alpha ç‰ˆæœ¬åŒ…æ‹¬é›†ç¾¤ã€‚æœ‰äº†è¿™ä¸ªèƒ½åŠ›ï¼Œä½ å¯ä»¥åˆ›å»ºæ­£å¦‚ä½ å¸Œæœ›å¤šçš„å·¥ä½œèŠ‚ç‚¹çš„ KUbernetes é›†ç¾¤ã€‚å¯¹äºŽåˆ›å»ºé›†ç¾¤æ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªæ²¡æœ‰ä¸»è§çš„é€‰é¡¹ - å¼€å‘è€…åœ¨èŠ‚ç‚¹ä¹‹é—´åˆ›å»ºç½‘ç»œè¿žæŽ¥å’Œé›†æˆäº†å…¶ä»–æ‰€éœ€è¦çš„åŸºç¡€è®¾æ–½ï¼Œæ¯”å¦‚ä¸€ä¸ªå¤–éƒ¨çš„è´Ÿè½½å‡è¡¡ã€‚æ€»çš„æ¥è¯´ï¼ŒMicroK8s æä¾›äº†ä¸€ç§å¿«é€Ÿç®€æ˜“çš„æ–¹æ³•ï¼Œä½¿å¾—å°‘é‡çš„è®¡ç®—æœºå’Œè™šæ‹Ÿæœºå˜æˆä¸€ä¸ªå¤šèŠ‚ç‚¹çš„ Kubernetes é›†ç¾¤ã€‚ä»¥åŽæˆ‘ä»¬å°†æ’°å†™æ›´å¤šè¿™ç§ä½“ç³»ç»“æž„çš„æ–‡ç« ã€‚&lt;/p>
&lt;!--
## Disclaimer
This is not an official guide to MicroK8s. You may find detailed information on running and using MicroK8s on it's official [webpage](https://microk8s.io/docs/), where different use cases, operating systems, environments, etc. are covered. Instead, the purpose of this post is to provide clear and easy guidelines for running MicroK8s on Linux.
-->
&lt;h2 id="å…è´£å£°æ˜Ž">å…è´£å£°æ˜Ž&lt;/h2>
&lt;p>è¿™ä¸æ˜¯ MicroK8s å®˜æ–¹ä»‹ç»æ–‡æ¡£ã€‚ä½ å¯ä»¥åœ¨å®ƒçš„å®˜æ–¹&lt;a href="https://microk8s.io/docs/">ç½‘é¡µ&lt;/a>æŸ¥è¯¢è¿è¡Œå’Œä½¿ç”¨ MicroK8s çš„è¯¦æƒ…ä¿¡æ¯ï¼Œå…¶ä¸­è¦†ç›–äº†ä¸åŒçš„ç”¨ä¾‹ï¼Œæ“ä½œç³»ç»Ÿï¼ŒçŽ¯å¢ƒç­‰ã€‚ç›¸åï¼Œè¿™ç¯‡æ–‡ç« çš„æ„å›¾æ˜¯æä¾›åœ¨ Linux ä¸Šè¿è¡Œ MicroK8s æ¸…æ™°æ˜“æ‡‚çš„æŒ‡å—ã€‚&lt;/p>
&lt;!--
## Prerequisites
A Linux distribution that [supports snaps](https://snapcraft.io/docs/installing-snapd), is required. In this guide, weâ€™ll use Ubuntu 18.04 LTS, it supports snaps out-of-the-box.
If you are interested in running Microk8s on Windows or Mac, you should check out [Multipass](https://multipass.run) to stand up a quick Ubuntu VM as the official way to run virtual Ubuntu on your system.
-->
&lt;h2 id="å‰ææ¡ä»¶">å‰ææ¡ä»¶&lt;/h2>
&lt;p>ä¸€ä¸ª&lt;a href="https://snapcraft.io/docs/installing-snapd">æ”¯æŒ snaps&lt;/a> çš„ Linux å‘è¡Œç‰ˆæ˜¯è¢«éœ€è¦çš„ã€‚è¿™ç¯‡æŒ‡å—ï¼Œæˆ‘ä»¬å°†ä¼šç”¨æ”¯æŒ snaps ä¸”å³å¼€å³ç”¨çš„ Ubuntu 18.04 LTSã€‚å¦‚æžœä½ å¯¹è¿è¡Œåœ¨ Windows æˆ–è€… Mac ä¸Šçš„ MicroK8s æ„Ÿå…´è¶£ï¼Œä½ åº”è¯¥æ£€æŸ¥&lt;a href="https://multipass.run">å¤šé€šé“&lt;/a>ï¼Œå®‰è£…ä¸€ä¸ªå¿«é€Ÿçš„ Ubuntu VMï¼Œä½œä¸ºåœ¨ä½ çš„ç³»ç»Ÿä¸Šè¿è¡Œè™šæ‹Ÿæœº Ubuntu çš„å®˜æ–¹æ–¹å¼ã€‚&lt;/p>
&lt;!--
## MicroK8s installation
MicroK8s installation is straightforward:
-->
&lt;h2 id="microk8s-å®‰è£…">MicroK8s å®‰è£…&lt;/h2>
&lt;p>ç®€æ´çš„ MicroK8s å®‰è£…ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap install microk8s --classic
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/001-install.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
The command above installs a local single-node Kubernetes cluster in seconds. Once the command execution is finished, your Kubernetes cluster is up and running.
&lt;p>You may verify the MicroK8s status with the following command:
--&amp;gt;
ä»¥ä¸Šçš„å‘½ä»¤å°†ä¼šåœ¨å‡ ç§’å†…å®‰è£…ä¸€ä¸ªæœ¬åœ°å•èŠ‚ç‚¹çš„ Kubernetes é›†ç¾¤ã€‚ä¸€æ—¦å‘½ä»¤æ‰§è¡Œç»“æŸï¼Œä½ çš„ Kubernetes é›†ç¾¤å°†ä¼šå¯åŠ¨å¹¶è¿è¡Œã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo microk8s.status
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/002-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Using microk8s
Using MicroK8s is as straightforward as installing it. MicroK8s itself includes a `kubectl` binary, which can be accessed by running the `microk8s.kubectl` command. As an example:
-->
&lt;h2 id="ä½¿ç”¨-microk8s">ä½¿ç”¨ microk8s&lt;/h2>
&lt;p>ä½¿ç”¨ MicrosK8s å°±åƒå’Œå®‰è£…å®ƒä¸€æ ·ä¾¿æ·ã€‚MicroK8s æœ¬èº«åŒ…æ‹¬ä¸€ä¸ª &lt;code>kubectl&lt;/code> åº“ï¼Œè¯¥åº“å¯ä»¥é€šè¿‡æ‰§è¡Œ &lt;code>microk8s.kubectl&lt;/code> å‘½ä»¤åŽ»è®¿é—®ã€‚ä¾‹å¦‚ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">microk8s.kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/003-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
While using the prefix `microk8s.kubectl` allows for a parallel install of another system-wide kubectl without impact, you can easily get rid of it by using the `snap alias` command:
-->
&lt;p>å½“ä½¿ç”¨å‰ç¼€ &lt;code>microk8s.kubectl&lt;/code> æ—¶ï¼Œå…è®¸åœ¨æ²¡æœ‰å½±å“çš„æƒ…å†µä¸‹å¹¶è¡Œåœ°å®‰è£…å¦ä¸€ä¸ªç³»ç»Ÿçº§çš„ kubectlï¼Œä½ å¯ä»¥ä¾¿æ·åœ°ä½¿ç”¨ &lt;code>snap alias&lt;/code> å‘½ä»¤æ‘†è„±å®ƒï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap &lt;span style="color:#a2f">alias&lt;/span> microk8s.kubectl kubectl
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
This will allow you to simply use `kubectl` after. You can revert this change using the `snap unalias` command.
-->
&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/004-alias.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;p>è¿™å°†å…è®¸ä½ ä»¥åŽä¾¿æ·åœ°ä½¿ç”¨ &lt;code>kubectl&lt;/code>ï¼Œä½ å¯ä»¥ç”¨ &lt;code>snap unalias&lt;/code>å‘½ä»¤æ¢å¤è¿™ä¸ªæ”¹å˜ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/005-nodes.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## MicroK8s addons
One of the biggest benefits of using Microk8s is the fact that it also supports various add-ons and extensions. What is even more important is they are shipped out of the box, the user just has to enable them.
The full list of extensions can be checked by running the `microk8s.status` command:
-->
&lt;h2 id="microk8s-æ’ä»¶">MicroK8s æ’ä»¶&lt;/h2>
&lt;p>ä½¿ç”¨ MIcroK8s å…¶ä¸­æœ€å¤§çš„å¥½å¤„ä¹‹ä¸€äº‹å®žä¸Šæ˜¯ä¹Ÿæ”¯æŒå„ç§å„æ ·çš„æ’ä»¶å’Œæ‰©å±•ã€‚æ›´é‡è¦çš„æ˜¯å®ƒä»¬æ˜¯å¼€ç®±å³ç”¨çš„ï¼Œç”¨æˆ·ä»…ä»…éœ€è¦å¯åŠ¨å®ƒä»¬ã€‚é€šè¿‡è¿è¡Œ &lt;code>microk8s.status&lt;/code> å‘½ä»¤æ£€æŸ¥å‡ºæ‰©å±•çš„å®Œæ•´åˆ—è¡¨ã€‚&lt;/p>
&lt;pre>&lt;code>sudo microk8s.status
&lt;/code>&lt;/pre>&lt;!--
As of the time of writing this article, the following add-ons are supported:
-->
&lt;p>æˆªè‡³åˆ°å†™è¿™ç¯‡æ–‡ç« ä¸ºæ­¢ï¼ŒMicroK8s å·²æ”¯æŒä»¥ä¸‹æ’ä»¶ï¼š&lt;/p>
&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/006-status.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
More add-ons are being created and contributed by the community all the time, it definitely helps to check often!
-->
&lt;p>ç¤¾åŒºåˆ›å»ºå’Œè´¡çŒ®äº†è¶Šæ¥è¶Šå¤šçš„æ’ä»¶ï¼Œç»å¸¸æ£€æŸ¥ä»–ä»¬æ˜¯ååˆ†æœ‰å¸®åŠ©çš„ã€‚&lt;/p>
&lt;!--
## Release channels
-->
&lt;h2 id="å‘å¸ƒæ¸ é“">å‘å¸ƒæ¸ é“&lt;/h2>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap info microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/010-releases.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Installing the sample application
In this tutorial weâ€™ll use NGINX as a sample application ([the official Docker Hub image](https://hub.docker.com/_/nginx)).
It will be installed as a Kubernetes deployment:
-->
&lt;h2 id="å®‰è£…ç®€å•çš„åº”ç”¨">å®‰è£…ç®€å•çš„åº”ç”¨&lt;/h2>
&lt;p>åœ¨è¿™ç¯‡æŒ‡å—ä¸­æˆ‘å°†ä¼šç”¨ NGINX ä½œä¸ºä¸€ä¸ªç¤ºä¾‹åº”ç”¨ç¨‹åºï¼ˆ&lt;a href="https://hub.docker.com/_/nginx">å®˜æ–¹ Docker Hub é•œåƒ&lt;/a>ï¼‰ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create deployment nginx --image&lt;span style="color:#666">=&lt;/span>nginx
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
To verify the installation, letâ€™s run the following:
-->
&lt;p>ä¸ºäº†æ£€æŸ¥å®‰è£…ï¼Œè®©æˆ‘ä»¬è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get deployments
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get pods
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/007-deployments.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
Also, we can retrieve the full output of all available objects within our Kubernetes cluster:
-->
&lt;p>æˆ‘ä»¬ä¹Ÿå¯ä»¥æ£€ç´¢å‡º Kubernetes é›†ç¾¤ä¸­æ‰€æœ‰å¯ç”¨å¯¹è±¡çš„å®Œæ•´è¾“å‡ºã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/008-all.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Uninstalling MicroK8s
Uninstalling your microk8s cluster is so easy as uninstalling the snap:
-->
&lt;h2 id="å¸è½½-mircrok8s">å¸è½½ MircroK8s&lt;/h2>
&lt;p>å¸è½½æ‚¨çš„ microk8s é›†ç¾¤ä¸Žå¸è½½ Snap åŒæ ·ä¾¿æ·ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">sudo snap remove microk8s
&lt;/code>&lt;/pre>&lt;/div>&lt;center>&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2019-11-05-kubernetes-with-microk8s/009-remove.png" width="600"/>
&lt;/figure>
&lt;/center>
&lt;!--
## Screencast
-->
&lt;h2 id="æˆªå±è§†é¢‘">æˆªå±è§†é¢‘&lt;/h2>
&lt;p>&lt;a href="https://asciinema.org/a/263394">&lt;img src="https://asciinema.org/a/263394.svg" alt="asciicast">&lt;/a>&lt;/p></description></item><item><title>Blog: åœ£è¿­æˆˆè´¡çŒ®è€…å³°ä¼šæ—¥ç¨‹å…¬å¸ƒï¼</title><link>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</link><pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/10/contributor-summit-san-diego-schedule/</guid><description>
&lt;!--
layout: blog
title: "Contributor Summit San Diego Schedule Announced!"
date: 2019-10-10
slug: contributor-summit-san-diego-schedule
-->
&lt;!--
Authors: Josh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)
-->
&lt;p>ä½œè€…ï¼šJosh Berkus (Red Hat), Paris Pittman (Google), Jonas Rosland (VMware)&lt;/p>
&lt;!--
tl;dr A week ago we announced that [registration is open][reg] for the contributor
summit , and we're now live with [the full Contributor Summit schedule!][schedule]
Grab your spot while tickets are still available. There is currently a waitlist
for new contributor workshop. ([Register here!][reg])
-->
&lt;p>ä¸€å‘¨å‰ï¼Œæˆ‘ä»¬å®£å¸ƒè´¡çŒ®è€…å³°ä¼š&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">å¼€æ”¾æ³¨å†Œ&lt;/a>ï¼ŒçŽ°åœ¨æˆ‘ä»¬å·²ç»å®Œæˆäº†æ•´ä¸ª&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/program/schedule/">è´¡çŒ®è€…å³°ä¼šçš„æ—¥ç¨‹å®‰æŽ’&lt;/a>ï¼è¶çŽ°åœ¨è¿˜æœ‰ç¥¨ï¼Œé©¬ä¸ŠæŠ¢å ä½ çš„ä½ç½®ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæ–°è´¡çŒ®è€…ç ”è®¨ä¼šçš„ç­‰å¾…åå•ã€‚ (&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">ç‚¹å‡»è¿™é‡Œæ³¨å†Œ!&lt;/a>)&lt;/p>
&lt;!--
There are many great sessions planned for the Contributor Summit, spread across
five rooms of current contributor content in addition to the new contributor
workshops. Since this is an upstream contributor summit and we don't often meet,
being a globally distributed team, most of these sessions are discussions or
hands-on labs, not just presentations. We want folks to learn and have a
good time meeting their OSS teammates.
-->
&lt;p>é™¤äº†æ–°è´¡çŒ®è€…ç ”è®¨ä¼šä¹‹å¤–ï¼Œè´¡çŒ®è€…å³°ä¼šè¿˜å®‰æŽ’äº†è®¸å¤šç²¾å½©çš„ä¼šè®®ï¼Œè¿™äº›ä¼šè®®åˆ†å¸ƒåœ¨å½“å‰äº”ä¸ªè´¡çŒ®è€…å†…å®¹çš„ä¼šè®®å®¤ä¸­ã€‚ç”±äºŽè¿™æ˜¯ä¸€ä¸ªä¸Šæ¸¸è´¡çŒ®è€…å³°ä¼šï¼Œå¹¶ä¸”æˆ‘ä»¬ä¸ç»å¸¸è§é¢ï¼Œæ‰€ä»¥ä½œä¸ºä¸€ä¸ªå…¨çƒåˆ†å¸ƒçš„å›¢é˜Ÿï¼Œè¿™äº›ä¼šè®®å¤§å¤šæ˜¯è®¨è®ºæˆ–åŠ¨æ‰‹å®žè·µï¼Œè€Œä¸ä»…ä»…æ˜¯æ¼”ç¤ºã€‚æˆ‘ä»¬å¸Œæœ›å¤§å®¶äº’ç›¸å­¦ä¹ ï¼Œå¹¶äºŽä»–ä»¬çš„å¼€æºä»£ç é˜Ÿå‹çŽ©çš„å¼€å¿ƒã€‚&lt;/p>
&lt;!--
Unconference tracks are returning from last year with sessions to be chosen
Monday morning. These are ideal for the latest hot topics and specific
discussions that contributors want to have. In previous years, we've covered
flaky tests, cluster lifecycle, KEPs (Kubernetes Enhancement Proposals), mentoring,
security, and more.
-->
&lt;p>åƒåŽ»å¹´ä¸€æ ·ï¼Œéžç»„ç»‡ä¼šè®®å°†é‡æ–°å¼€å§‹ï¼Œä¼šè®®å°†åœ¨å‘¨ä¸€ä¸Šåˆè¿›è¡Œé€‰æ‹©ã€‚å¯¹äºŽæœ€æ–°çš„çƒ­é—¨è¯é¢˜å’Œè´¡çŒ®è€…æƒ³è¦è¿›è¡Œçš„ç‰¹å®šè®¨è®ºï¼Œè¿™æ˜¯ç†æƒ³çš„é€‰æ‹©ã€‚åœ¨è¿‡åŽ»çš„å‡ å¹´ä¸­ï¼Œæˆ‘ä»¬æ¶µç›–äº†ä¸ç¨³å®šçš„æµ‹è¯•ï¼Œé›†ç¾¤ç”Ÿå‘½å‘¨æœŸï¼ŒKEPï¼ˆKuberneteså¢žå¼ºå»ºè®®ï¼‰ï¼ŒæŒ‡å¯¼ï¼Œå®‰å…¨æ€§ç­‰ç­‰ã€‚&lt;/p>
&lt;!--
![Unconference](/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg)
-->
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-10-10-contributor-summit-san-diego-schedule/DSCF0806.jpg" alt="éžç»„ç»‡ä¼šè®®">&lt;/p>
&lt;!--
While the schedule contains difficult decisions in every timeslot, we've picked
a few below to give you a taste of what you'll hear, see, and participate in, at
the summit:
-->
&lt;p>å°½ç®¡åœ¨æ¯ä¸ªæ—¶é—´é—´éš™æ—¥ç¨‹å®‰æŽ’éƒ½åŒ…å«å›°éš¾çš„å†³å®šï¼Œä½†æˆ‘ä»¬é€‰æ‹©äº†ä»¥ä¸‹å‡ ç‚¹ï¼Œè®©æ‚¨ä½“éªŒä¸€ä¸‹æ‚¨å°†åœ¨å³°ä¼šä¸Šå¬åˆ°ã€çœ‹åˆ°å’Œå‚ä¸Žçš„å†…å®¹ï¼š&lt;/p>
&lt;!--
* **[Vision]**: SIG-Architecture will be sharing their vision of where we're going
with Kubernetes development for the next year and beyond.
* **[Security]**: Tim Allclair and CJ Cullen will present on the current state of
Kubernetes security. In another security talk, Vallery Lancey will lead a
discussion about making our platform secure by default.
* **[Prow]**: Interested in working with Prow and contributing to Test-Infra, but
not sure where to start? Rob Keilty will help you get a Prow test environment
running on your laptop.
* **[Git]**: Staff from GitHub will be collaborating with Christoph Blecker to share
practical Git tips for Kubernetes contributors.
* **[Reviewing]**: Tim Hockin will share the secrets of becoming a great code
reviewer, and Jordan Liggitt will conduct a live API review so that you can do
one, or at least pass one.
* **[End Users]**: Several end users from the CNCF partner ecosystem, invited by
Cheryl Hung, will hold a Q&amp;A with contributors to strengthen our feedback loop.
* **[Docs]**: As always, SIG-Docs will run a three-hour contributing-to-documentation
workshop.
-->
&lt;ul>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMc">é¢„è§&lt;/a>&lt;/strong>: SIGç»„ç»‡å°†åˆ†äº«ä»–ä»¬å¯¹äºŽæ˜Žå¹´å’Œä»¥åŽKuberneteså¼€å‘å‘å±•æ–¹å‘çš„è®¤è¯†ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvMj">å®‰å…¨&lt;/a>&lt;/strong>: Tim Allclairå’ŒCJ Cullenå°†ä»‹ç»Kuberneteså®‰å…¨çš„å½“å‰æƒ…å†µã€‚åœ¨å¦ä¸€ä¸ªå®‰å…¨æ€§æ¼”è®²ä¸­ï¼ŒVallery Lanceyå°†ä¸»æŒæœ‰å…³ä½¿æˆ‘ä»¬çš„å¹³å°é»˜è®¤æƒ…å†µä¸‹å®‰å…¨çš„è®¨è®ºã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vv6Z">Prow&lt;/a>&lt;/strong>: æœ‰å…´è¶£ä¸ŽProwåˆä½œå¹¶ä¸ºTest-Infraåšè´¡çŒ®ï¼Œä½†ä¸ç¡®å®šä»Žå“ªé‡Œå¼€å§‹ï¼Ÿ Rob Keiltyå°†å¸®åŠ©æ‚¨åœ¨ç¬”è®°æœ¬ç”µè„‘ä¸Šè¿è¡ŒProwæµ‹è¯•çŽ¯å¢ƒ&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNa">Git&lt;/a>&lt;/strong>: GitHubçš„å‘˜å·¥å°†ä¸ŽChristoph Bleckeråˆä½œï¼Œä¸ºKubernetesè´¡çŒ®è€…åˆ†äº«å®žç”¨çš„GitæŠ€å·§ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VutA">å®¡é˜…&lt;/a>&lt;/strong>: è’‚å§†Â·éœé‡‘ï¼ˆTim Hockinï¼‰å°†åˆ†äº«æˆä¸ºä¸€åå‡ºè‰²çš„ä»£ç å®¡é˜…è€…çš„ç§˜å¯†ï¼Œè€Œä¹”ä¸¹Â·åˆ©å‰ç‰¹ï¼ˆJordan Liggittï¼‰å°†è¿›è¡Œå®žæ—¶APIå®¡é˜…ï¼Œä»¥ä¾¿æ‚¨å¯ä»¥è¿›è¡Œä¸€æ¬¡æˆ–è‡³å°‘äº†è§£ä¸€æ¬¡å®¡é˜…ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/VvNJ">ç»ˆç«¯ç”¨æˆ·&lt;/a>&lt;/strong>: åº”Cheryl Hungé‚€è¯·ï¼Œæ¥è‡ªCNCFåˆä½œä¼™ä¼´ç”Ÿæ€çš„æ•°ä¸ªç»ˆç«¯ç”¨æˆ·ï¼Œå°†å›žç­”è´¡çŒ®è€…çš„é—®é¢˜ï¼Œä»¥åŠ å¼ºæˆ‘ä»¬çš„åé¦ˆå¾ªçŽ¯ã€‚&lt;/li>
&lt;li>&lt;strong>&lt;a href="https://sched.co/Vux2">æ–‡æ¡£&lt;/a>&lt;/strong>: ä¸Žå¾€å¸¸ä¸€æ ·ï¼ŒSIG-Docså°†ä¸¾åŠžä¸€ä¸ªä¸ºæ—¶ä¸‰ä¸ªå°æ—¶çš„æ–‡æ¡£æ’°å†™ç ”è®¨ä¼šã€‚&lt;/li>
&lt;/ul>
&lt;!--
We're also giving out awards to contributors who distinguished themselves in 2019,
and there will be a huge Meet &amp; Greet for new contributors to find their SIG
(and for existing contributors to ask about their PRs) at the end of the day on
Monday.
-->
&lt;p>æˆ‘ä»¬è¿˜å°†å‘åœ¨2019å¹´æ°å‡ºçš„è´¡çŒ®è€…é¢å‘å¥–é¡¹ï¼Œå‘¨ä¸€æ˜ŸæœŸä¸€ç»“æŸæ—¶å°†æœ‰ä¸€ä¸ªå·¨å¤§çš„è§é¢ä¼šï¼Œä¾›æ–°çš„è´¡çŒ®è€…æ‰¾åˆ°ä»–ä»¬çš„SIGï¼ˆä»¥åŠçŽ°æœ‰çš„è´¡çŒ®è€…è¯¢é—®ä»–ä»¬çš„PRï¼‰ã€‚&lt;/p>
&lt;!--
Hope to see you all there, and [make sure you register!][reg]
-->
&lt;p>å¸Œæœ›èƒ½å¤Ÿåœ¨å³°ä¼šä¸Šè§åˆ°æ‚¨ï¼Œå¹¶ä¸”ç¡®ä¿æ‚¨å·²ç»æå‰&lt;a href="https://events19.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/register/">æ³¨å†Œ&lt;/a>ï¼&lt;/p>
&lt;!--
[San Diego team][team]
-->
&lt;p>&lt;a href="http://git.k8s.io/community/events/events-team">åœ£è¿­æˆˆå›¢é˜Ÿ&lt;/a>&lt;/p></description></item><item><title>Blog: 2019 æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æžœ</title><link>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</link><pubDate>Thu, 03 Oct 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/10/03/2019-steering-committee-election-results/</guid><description>
&lt;!--
---
layout: blog
title: "2019 Steering Committee Election Results"
date: 2019-10-03
slug: 2019-steering-committee-election-results
---
-->
&lt;!--
**Authors**: Bob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šBob Killen (University of Michigan), Jorge Castro (VMware),
Brian Grant (Google), and Ihor Dvoretskyi (CNCF)&lt;/p>
&lt;!--
The [2019 Steering Committee Election] is a landmark milestone for the
Kubernetes project. The initial bootstrap committee is graduating to emeritus
and the committee has now shrunk to its final allocation of seven seats. All
members of the Steering Committee are now fully elected by the Kubernetes
Community.
-->
&lt;p>&lt;a href="https://git.k8s.io/community/events/elections/2021">2019 æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾&lt;/a> æ˜¯ Kubernetes é¡¹ç›®çš„é‡è¦é‡Œç¨‹ç¢‘ã€‚æœ€åˆçš„è‡ªåŠ©å§”å‘˜ä¼šæ­£é€æ­¥é€€ä¼‘ï¼ŒçŽ°åœ¨è¯¥å§”å‘˜ä¼šå·²ç¼©å‡åˆ°æœ€åŽåˆ†é…çš„ 7 ä¸ªå¸­ä½ã€‚æŒ‡å¯¼å§”å‘˜ä¼šçš„æ‰€æœ‰æˆå‘˜çŽ°åœ¨éƒ½ç”± Kubernetes ç¤¾åŒºé€‰ä¸¾äº§ç”Ÿã€‚&lt;/p>
&lt;!--
Moving forward elections will elect either 3 or 4 people to the committee for
two-year terms.
-->
&lt;p>æŽ¥ä¸‹æ¥çš„é€‰ä¸¾å°†é€‰å‡º 3 åˆ° 4 åå§”å‘˜ï¼Œä»»æœŸä¸¤å¹´ã€‚&lt;/p>
&lt;!--
## **Results**
The Kubernetes Steering Committee Election is now complete and the following
candidates came ahead to secure two-year terms that start immediately
(in alphabetical order by GitHub handle):
-->
&lt;h2 id="é€‰ä¸¾ç»“æžœ">é€‰ä¸¾ç»“æžœ&lt;/h2>
&lt;p>Kubernetes æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾çŽ°å·²å®Œæˆï¼Œä»¥ä¸‹å€™é€‰äººæå‰èŽ·å¾—ç«‹å³å¼€å§‹çš„ä¸¤å¹´ä»»æœŸ (æŒ‰ GitHub handle çš„å­—æ¯é¡ºåºæŽ’åˆ—) ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), Loodse&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>)&lt;/strong>, &lt;strong>Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;!--
They join Aaron Crickenberger ([@spiffxp]), Google; Davanum Srinivas ([@dims]),
VMware; and Timothy St. Clair ([@timothysc]), VMware, to round out the committee.
The seats held by Aaron, Davanum, and Timothy will be up for election around
this time next year.
-->
&lt;p>ä»–ä»¬åŠ å…¥äº† Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>)ï¼Œ Googleï¼›Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>)ï¼ŒVMware; and Timothy St. Clair (&lt;a href="https://github.com/timothysc">@timothysc&lt;/a>), VMwareï¼Œä½¿å¾—å§”å‘˜ä¼šæ›´åœ†æ»¡ã€‚Aaronã€Davanum å’Œ Timothy å æ®çš„è¿™äº›å¸­ä½å°†ä¼šåœ¨æ˜Žå¹´çš„è¿™ä¸ªæ—¶å€™è¿›è¡Œé€‰ä¸¾ã€‚&lt;/p>
&lt;!--
## Big Thanks!
* Thanks to the initial bootstrap committee for establishing the initial
project governance and overseeing a multi-year transition period:
-->
&lt;h2 id="è¯šæŒšçš„æ„Ÿè°¢">è¯šæŒšçš„æ„Ÿè°¢ï¼&lt;/h2>
&lt;ul>
&lt;li>æ„Ÿè°¢æœ€åˆçš„å¼•å¯¼å§”å‘˜ä¼šåˆ›ç«‹äº†æœ€åˆé¡¹ç›®çš„ç®¡ç†å¹¶ç›‘ç£äº†å¤šå¹´çš„è¿‡æ¸¡æœŸï¼š
&lt;ul>
&lt;li>Joe Beda (&lt;a href="https://github.com/jbeda">@jbeda&lt;/a>), VMware&lt;/li>
&lt;li>Brendan Burns (&lt;a href="https://github.com/brendandburns">@brendandburns&lt;/a>), Microsoft&lt;/li>
&lt;li>Clayton Coleman (&lt;a href="https://github.com/smarterclayton">@smarterclayton&lt;/a>), Red Hat&lt;/li>
&lt;li>Brian Grant (&lt;a href="https://github.com/bgrant0607">@bgrant0607&lt;/a>), Google&lt;/li>
&lt;li>Tim Hockin (&lt;a href="https://github.com/thockin">@thockin&lt;/a>), Google&lt;/li>
&lt;li>Sarah Novotny (&lt;a href="https://github.com/sarahnovotny">@sarahnovotny&lt;/a>), Microsoft&lt;/li>
&lt;li>Brandon Philips (&lt;a href="https://github.com/philips">@philips&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* And also thanks to the other Emeritus Steering Committee Members. Your
prior service is appreciated by the community:
-->
&lt;ul>
&lt;li>åŒæ ·æ„Ÿè°¢å…¶ä»–çš„å·²é€€ä¼‘æŒ‡å¯¼å§”å‘˜ä¼šæˆå‘˜ã€‚ç¤¾åŒºå¯¹ä½ ä»¬å…ˆå‰çš„æœåŠ¡è¡¨ç¤ºèµžèµï¼š
&lt;ul>
&lt;li>Quinton Hoole (&lt;a href="https://github.com/quinton-hoole">@quinton-hoole&lt;/a>), Huawei&lt;/li>
&lt;li>Michelle Noorali (&lt;a href="https://github.com/michelleN">@michelleN&lt;/a>), Microsoft&lt;/li>
&lt;li>Phillip Wittrock (&lt;a href="https://github.com/pwittrock">@pwittrock&lt;/a>), Google&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Thanks to the candidates that came forward to run for election. May we always
have a strong set of people who want to push the community forward like yours
in every election.
* Thanks to all 377 voters who cast a ballot.
* And last but not leastâ€¦Thanks to Cornell University for hosting [CIVS]!
-->
&lt;ul>
&lt;li>æ„Ÿè°¢å‚é€‰çš„å€™é€‰äººã€‚ æ„¿åœ¨æ¯æ¬¡é€‰ä¸¾ä¸­ï¼Œæˆ‘ä»¬éƒ½èƒ½æ‹¥æœ‰ä¸€ç¾¤åƒæ‚¨ä¸€æ ·æŽ¨åŠ¨ç¤¾åŒºå‘å‰å‘å±•çš„äººã€‚&lt;/li>
&lt;li>æ„Ÿè°¢æ‰€æœ‰æŠ•ç¥¨çš„377ä½é€‰æ°‘ã€‚&lt;/li>
&lt;li>æœ€åŽï¼Œæ„Ÿè°¢åº·å¥ˆå°”å¤§å­¦ä¸¾åŠžçš„ &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>!&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
You can follow along with Steering Committee [backlog items] and weigh in by
filing an issue or creating a PR against their [repo]. They meet bi-weekly on
[Wednesdays at 8pm UTC] and regularly attend Meet Our Contributors. They can
also be contacted at their public mailing list [steering@kubernetes.io].
Steering Committee Meetings:
-->
&lt;h2 id="å‚ä¸ŽæŒ‡å¯¼å§”å‘˜ä¼š">å‚ä¸ŽæŒ‡å¯¼å§”å‘˜ä¼š&lt;/h2>
&lt;p>ä½ å¯ä»¥è·Ÿè¿›æŒ‡å¯¼å§”å‘˜ä¼šçš„ &lt;a href="https://github.com/kubernetes/steering/projects/1">ä»£åŠžäº‹é¡¹&lt;/a>ï¼Œé€šè¿‡æå‡ºé—®é¢˜æˆ–è€…å‘ &lt;a href="https://github.com/kubernetes/steering">ä»“åº“&lt;/a> æäº¤ä¸€ä¸ª pr ã€‚ä»–ä»¬æ¯ä¸¤å‘¨ä¸€æ¬¡ï¼Œåœ¨ &lt;a href="https://github.com/kubernetes/steering">UTC æ—¶é—´å‘¨ä¸‰æ™šä¸Š 8 ç‚¹&lt;/a> ä¼šé¢ï¼Œå¹¶å®šæœŸä¸Žæˆ‘ä»¬çš„è´¡çŒ®è€…è§é¢ã€‚ä¹Ÿå¯ä»¥é€šè¿‡ä»–ä»¬çš„å…¬å…±é‚®ä»¶åˆ—è¡¨ &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a> è”ç³»ä»–ä»¬ã€‚&lt;/p>
&lt;p>æŒ‡å¯¼å§”å‘˜ä¼šä¼šè®®ï¼š&lt;/p>
&lt;!--
* [YouTube Playlist]
-->
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube æ’­æ”¾åˆ—è¡¨&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: San Diego è´¡çŒ®è€…å³°ä¼šå¼€æ”¾æ³¨å†Œï¼</title><link>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</link><pubDate>Tue, 24 Sep 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/09/24/san-diego-contributor-summit/</guid><description>
&lt;!--
---
layout: blog
title: "Contributor Summit San Diego Registration Open!"
date: 2019-09-24
slug: san-diego-contributor-summit
---
--->
&lt;!--
**Authors: Paris Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)**
--->
&lt;p>&lt;strong>ä½œè€…ï¼šParis Pittman (Google), Jeffrey Sica (Red Hat), Jonas Rosland (VMware)&lt;/strong>&lt;/p>
&lt;!--
[Contributor Summit San Diego 2019 Event Page]
Registration is now open and in record time, weâ€™ve hit capacity for the
*new contributor workshop* session of the event! Waitlist is now available.
--->
&lt;p>&lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">2019 San Diego è´¡çŒ®è€…å³°ä¼šæ´»åŠ¨é¡µé¢&lt;/a>
æ³¨å†Œå·²ç»å¼€æ”¾ï¼Œå¹¶ä¸”åœ¨åˆ›çºªå½•çš„æ—¶é—´å†…ï¼Œ&lt;em>æ–°è´¡çŒ®è€…ç ”è®¨ä¼š&lt;/em> æ´»åŠ¨å·²æ»¡å‘˜ï¼å€™è¡¥åå•å·²ç»å¼€æ”¾ã€‚&lt;/p>
&lt;!--
**Sunday, November 17**
Evening Contributor Celebration:
[QuartYard]*
Address: 1301 Market Street, San Diego, CA 92101
Time: 6:00PM - 9:00PM
--->
&lt;p>&lt;strong>11æœˆ17æ—¥ï¼Œæ˜ŸæœŸæ—¥&lt;/strong>&lt;br>
æ™šé—´è´¡çŒ®è€…åº†å…¸ï¼š&lt;br>
&lt;a href="https://quartyardsd.com/">QuartYard&lt;/a>*&lt;br>
åœ°å€: 1301 Market Street, San Diego, CA 92101&lt;br>
æ—¶é—´: ä¸‹åˆ6:00 - ä¸‹åˆ9:00&lt;/p>
&lt;!--
**Monday, November 18**
All Day Contributor Summit:
[Marriott Marquis San Diego Marina]
Address: 333 W Harbor Dr, San Diego, CA 92101
Time: 9:00AM - 5:00PM
--->
&lt;p>&lt;strong>11æœˆ18æ—¥ï¼Œæ˜ŸæœŸä¸€&lt;/strong>&lt;br>
å…¨å¤©è´¡çŒ®è€…å³°ä¼šï¼š&lt;br>
&lt;a href="https://www.marriott.com/hotels/travel/sandt-marriott-marquis-san-diego-marina/?scid=bb1a189a-fec3-4d19-a255-54ba596febe2">Marriott Marquis San Diego Marina&lt;/a>&lt;br>
åœ°å€: 333 W Harbor Dr, San Diego, CA 92101&lt;br>
æ—¶é—´: ä¸Šåˆ9:00 - ä¸‹åˆ5:00&lt;/p>
&lt;!--
While the Kubernetes project is only five years old, weâ€™re already going into our
9th Contributor Summit this November in San Diego before KubeCon + CloudNativeCon.
The rapid increase is thanks to adding European and Asian Contributor Summits to
the North American events weâ€™ve done previously. We will continue to run Contributor
Summits across the globe, as it is important that our contributor base grows in
all forms of diversity.
--->
&lt;p>è™½ç„¶ Kubernetes é¡¹ç›®åªæœ‰äº”å¹´çš„åŽ†å²ï¼Œä½†æ˜¯åœ¨ KubeCon + CloudNativeCon ä¹‹å‰ï¼Œä»Šå¹´11æœˆåœ¨åœ£è¿­æˆˆæ—¶æˆ‘ä»¬ä¸¾åŠžçš„ç¬¬ä¹å±Šè´¡çŒ®è€…å³°ä¼šäº†ã€‚å¿«é€Ÿå¢žé•¿çš„åŽŸå› æ˜¯åœ¨æˆ‘ä»¬ä¹‹å‰æ‰€åšçš„åŒ—ç¾Žæ´»åŠ¨ä¸­å¢žåŠ äº†æ¬§æ´²å’Œäºšæ´²è´¡çŒ®è€…å³°ä¼šã€‚æˆ‘ä»¬å°†ç»§ç»­åœ¨å…¨çƒä¸¾åŠžè´¡çŒ®è€…å³°ä¼šï¼Œå› ä¸ºé‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬çš„è´¡çŒ®è€…è¦ä»¥å„ç§å½¢å¼çš„å¤šæ ·æ€§åœ°æˆé•¿ã€‚&lt;/p>
&lt;!--
Kubernetes has a large distributed remote contributing team, from [individuals and
organizations] all over the world. The Contributor Summits give the community three
chances a year to get together, work on community topics, and have hallway track
time. The upcoming San Diego summit is expected to bring over 450 attendees, and
will contain multiple tracks with something for everyone. The focus will be around
contributor growth and sustainability. We're going to stop here with capacity for
future summits; we want this event to offer value to individuals and the project.
We've heard from past summit attendee feedback that getting work done, learning,
and meeting folks face to face is a priority. By capping attendance and offering
the contributor gatherings in more locations, it will help us achieve those goals.
--->
&lt;p>Kubernetes æ‹¥æœ‰ä¸€ä¸ªåºžå¤§çš„åˆ†å¸ƒå¼è¿œç¨‹è´¡çŒ®å›¢é˜Ÿï¼Œç”±æ¥è‡ªä¸–ç•Œå„åœ°çš„ &lt;a href="https://k8s.devstats.cncf.io/d/8/company-statistics-by-repository-group?orgId=1&amp;amp;var-period=y&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All&amp;amp;var-companies=All">ä¸ªäººå’Œç»„ç»‡&lt;/a> ç»„æˆã€‚è´¡çŒ®è€…å³°ä¼šæ¯å¹´ä¸ºç¤¾åŒºæä¾›ä¸‰æ¬¡èšä¼šçš„æœºä¼šï¼Œå›´ç»•ç¤¾åŒºä¸»é¢˜å¼€å±•å·¥ä½œï¼Œå¹¶æœ‰äº’ç›¸äº†è§£çš„æ—¶é—´ã€‚å³å°†ä¸¾è¡Œçš„ San Diego å³°ä¼šé¢„è®¡å°†å¸å¼• 450 å¤šåä¸Žä¼šè€…ï¼Œå¹¶å°†åŒ…å«å¤šä¸ªæ–¹å‘ï¼Œé€‚åˆæ‰€æœ‰äººã€‚é‡ç‚¹å°†å›´ç»•è´¡çŒ®è€…çš„å¢žé•¿å’Œå¯æŒç»­æ€§ã€‚æˆ‘ä»¬å°†åœ¨è¿™é‡Œåœç•™ï¼Œä¸ºä¸¾è¡Œæœªæ¥å³°ä¼šåšå‡†å¤‡ï¼›æˆ‘ä»¬å¸Œæœ›è¿™æ¬¡æ´»åŠ¨ä¸ºä¸ªäººå’Œé¡¹ç›®æä¾›ä»·å€¼ã€‚æˆ‘ä»¬å·²ç»ä»Žå³°ä¼šä¸Žä¼šè€…çš„åé¦ˆä¸­å¾—çŸ¥ï¼Œå®Œæˆå·¥ä½œã€å­¦ä¹ å’Œä¸Žäººä»¬é¢å¯¹é¢äº¤æµæ˜¯å½“åŠ¡ä¹‹æ€¥ã€‚é€šè¿‡é™åˆ¶å‚åŠ äººæ•°å¹¶åœ¨æ›´å¤šåœ°æ–¹æä¾›è´¡çŒ®è€…èšä¼šï¼Œå°†æœ‰åŠ©äºŽæˆ‘ä»¬å®žçŽ°è¿™äº›ç›®æ ‡ã€‚&lt;/p>
&lt;!--
This summit is unique as weâ€™ve taken big moves on sustaining ourselves, the
contributor experience events team. Taking a page from the release teamâ€™s playbook,
we have added additional core team and shadow roles making it a natural mentoring
(watching+doing) relationship. The shadows are expected to fill another role at
one of the three events in 2020, and core team members to take the lead.
In preparation for this team, weâ€™ve open sourced our [rolebooks, guidelines,
best practices] and opened up our [meetings] and [project board]. Our team makes up
many parts of the Kubernetes project and takes care of making sure all voices
are represented.
--->
&lt;p>è¿™æ¬¡å³°ä¼šæ˜¯ç‹¬ä¸€æ— äºŒçš„ï¼Œå› ä¸ºæˆ‘ä»¬åœ¨è´¡çŒ®è€…ä½“éªŒæ´»åŠ¨å›¢é˜Ÿçš„åšæŒè‡ªæˆ‘æ–¹é¢å·²é‡‡å–äº†é‡å¤§ä¸¾æŽªã€‚ä»Žå‘è¡Œå›¢é˜Ÿçš„æ‰‹å†Œä¸­æ‘˜å½•ï¼Œæˆ‘ä»¬æ·»åŠ äº†å…¶ä»–æ ¸å¿ƒå›¢é˜Ÿå’Œè·Ÿéšå­¦ä¹ è§’è‰²ï¼Œä½¿å…¶æˆä¸ºå¤©ç„¶çš„è¾…å¯¼å…³ç³»ï¼ˆåŒ…æ‹¬ç›‘ç£å’Œå®žæ–½ï¼‰ã€‚é¢„è®¡è·Ÿéšå­¦ä¹ è€…å°†åœ¨ 2020 å¹´çš„ä¸‰é¡¹æ´»åŠ¨ä¸­æ‰®æ¼”å¦ä¸€ä¸ªè§’è‰²ï¼Œæ ¸å¿ƒå›¢é˜Ÿæˆå‘˜å°†å¸¦å¤´å®Œæˆã€‚ä¸ºè¿™ä¸ªå›¢é˜Ÿåšå‡†å¤‡ï¼Œæˆ‘ä»¬å¼€æºäº† &lt;a href="https://github.com/kubernetes/community/tree/master/events/events-team">æŠ€æœ¯æ‰‹å†Œï¼ŒæŒ‡å—ï¼Œæœ€ä½³åšæ³•&lt;/a>ï¼Œå¹¶å¼€æ”¾äº† &lt;a href="https://docs.google.com/document/d/1oLXv5_rM4f645jlXym_Vd7AUq7x6DV-O87E6tcW1sjU/edit?usp=sharing">ä¼šè®®&lt;/a> å’Œ &lt;a href="https://github.com/orgs/kubernetes/projects/21">é¡¹ç›®å§”å‘˜ä¼š&lt;/a>ã€‚æˆ‘ä»¬çš„å›¢é˜Ÿç»„æˆäº† Kubernetes é¡¹ç›®çš„è®¸å¤šéƒ¨åˆ†ï¼Œå¹¶ç¡®ä¿æ‰€æœ‰å£°éŸ³éƒ½å¾—åˆ°ä½“çŽ°ã€‚&lt;/p>
&lt;!--
Are you at KubeCon + CloudNativeCon but canâ€™t make it to the summit? Check out
the [SIG Intro and Deep Dive sessions] during KubeCon + CloudNativeCon to
participate in Q&amp;A and hear whatâ€™s up with each Special interest Group (SIG).
Weâ€™ll also record all of Contributor Summitâ€™s presentation sessions, take notes
in discussions, and share it back with you, after the event is complete.
--->
&lt;p>æ‚¨æ˜¯å¦å·²ç»åœ¨ KubeCon + CloudNativeCon ä¸Šï¼Œä½†æ— æ³•å‚ä¸Žä¼šè®®ï¼Ÿ åœ¨ KubeCon + CloudNativeCon æœŸé—´æŸ¥çœ‹ &lt;a href="https://kccncna19.sched.com/overview/type/Maintainer+Track+Sessions?iframe=yes">SIGå…¥é—¨å’Œæ·±æ½œè¯¾ç¨‹&lt;/a> å‚ä¸Žé—®ç­”ï¼Œå¹¶å¬å–æ¯ä¸ªç‰¹æ®Šå…´è¶£å°ç»„ï¼ˆSIGï¼‰çš„æœ€æ–°æ¶ˆæ¯ã€‚æ´»åŠ¨ç»“æŸåŽï¼Œæˆ‘ä»¬è¿˜å°†è®°å½•æ‰€æœ‰è´¡çŒ®è€…å³°ä¼šçš„è¯¾é¢˜ï¼Œåœ¨è®¨è®ºä¸­åšç¬”è®°ï¼Œå¹¶ä¸Žæ‚¨åˆ†äº«ã€‚&lt;/p>
&lt;!--
We hope to see you all at Kubernetes Contributor Summit San Diego, make sure you
head over and [register right now]! This event will sell out - hereâ€™s your warning.
:smiley:
--->
&lt;p>æˆ‘ä»¬å¸Œæœ›èƒ½åœ¨ San Diego çš„ Kubernetes è´¡çŒ®è€…å³°ä¼šä¸Šä¸Žå¤§å®¶è§é¢ï¼Œç¡®ä¿æ‚¨ç›´æŽ¥è¿›å…¥å¹¶ç‚¹å‡» &lt;a href="https://events.linuxfoundation.org/events/kubernetes-contributor-summit-north-america-2019/">ç«‹å³æ³¨å†Œ&lt;/a>ï¼ æ­¤æ´»åŠ¨å°†å…³é—­ - ç‰¹æ­¤æé†’ã€‚ ï¼šç¬‘è„¸ï¼š&lt;/p>
&lt;!--
Check out past blogs on [persona building around our events] and the [Barcelona summit story].
![Group Picture in 2018](/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG)
--->
&lt;p>æŸ¥çœ‹å¾€æœŸåšå®¢æœ‰å…³ &lt;a href="https://kubernetes.io/blog/2019/03/20/a-look-back-and-whats-in-store-for-kubernetes-contributor-summits/">å›´ç»•æˆ‘ä»¬çš„æ´»åŠ¨æž„å»ºè§’è‰²&lt;/a> å’Œ &lt;a href="https://kubernetes.io/blog/2019/06/25/recap-of-kubernetes-contributor-summit-barcelona-2019/">å·´å¡žç½—é‚£å³°ä¼šæ•…äº‹&lt;/a>ã€‚&lt;/p>
&lt;p>ï¼&lt;a href="https://kubernetes.io/images/blog/2019-09-24-san-diego-contributor-summit/IMG_2588.JPG">2018å¹´é›†ä½“ç…§&lt;/a>&lt;/p>
&lt;!--
*=QuartYard has a huge stage! Want to perform something in front of your contributor peers? Reach out to us! community@kubernetes.io
--->
&lt;p>*=QuartYard æœ‰ä¸€ä¸ªå·¨å¤§çš„èˆžå°ï¼æƒ³è¦åœ¨æ‚¨çš„è´¡çŒ®è€…åŒè¡Œé¢å‰åšç‚¹ä»€ä¹ˆï¼ŸåŠ å…¥æˆ‘ä»¬å§ï¼ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a>&lt;/p></description></item><item><title>Blog: OPA Gatekeeperï¼šKubernetes çš„ç­–ç•¥å’Œç®¡ç†</title><link>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</link><pubDate>Tue, 06 Aug 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/08/06/opa-gatekeeper-policy-and-governance-for-kubernetes/</guid><description>
&lt;!--
---
layout: blog
title: "OPA Gatekeeper: Policy and Governance for Kubernetes"
date: 2019-08-06
slug: OPA-Gatekeeper-Policy-and-Governance-for-Kubernetes
---
--->
&lt;!--
**Authors:** Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)
--->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong> Rita Zhang (Microsoft), Max Smythe (Google), Craig Hooper (Commonwealth Bank AU), Tim Hinrichs (Styra), Lachie Evenson (Microsoft), Torin Sandall (Styra)&lt;/p>
&lt;!--
The [Open Policy Agent Gatekeeper](https://github.com/open-policy-agent/gatekeeper) project can be leveraged to help enforce policies and strengthen governance in your Kubernetes environment. In this post, we will walk through the goals, history, and current state of the project.
--->
&lt;p>å¯ä»¥ä»Žé¡¹ç›® &lt;a href="https://github.com/open-policy-agent/gatekeeper">Open Policy Agent Gatekeeper&lt;/a> ä¸­èŽ·å¾—å¸®åŠ©ï¼Œåœ¨ Kubernetes çŽ¯å¢ƒä¸‹å®žæ–½ç­–ç•¥å¹¶åŠ å¼ºæ²»ç†ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€æ­¥ä»‹ç»è¯¥é¡¹ç›®çš„ç›®æ ‡ï¼ŒåŽ†å²å’Œå½“å‰çŠ¶æ€ã€‚&lt;/p>
&lt;!--
The following recordings from the Kubecon EU 2019 sessions are a great starting place in working with Gatekeeper:
* [Intro: Open Policy Agent Gatekeeper](https://youtu.be/Yup1FUc2Qn0)
* [Deep Dive: Open Policy Agent](https://youtu.be/n94_FNhuzy4)
--->
&lt;p>ä»¥ä¸‹æ˜¯ Kubecon EU 2019 ä¼šè®®çš„å½•éŸ³ï¼Œå¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°å¼€å±•ä¸Ž Gatekeeper åˆä½œï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/Yup1FUc2Qn0">ç®€ä»‹ï¼šå¼€æ”¾ç­–ç•¥ä»£ç† Gatekeeper&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/n94_FNhuzy4">æ·±å…¥ç ”ç©¶ï¼šå¼€æ”¾ç­–ç•¥ä»£ç†&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Motivations
If your organization has been operating Kubernetes, you probably have been looking for ways to control what end-users can do on the cluster and ways to ensure that clusters are in compliance with company policies. These policies may be there to meet governance and legal requirements or to enforce best practices and organizational conventions. With Kubernetes, how do you ensure compliance without sacrificing development agility and operational independence?
--->
&lt;h2 id="å‡ºå‘ç‚¹">å‡ºå‘ç‚¹&lt;/h2>
&lt;p>å¦‚æžœæ‚¨æ‰€åœ¨çš„ç»„ç»‡ä¸€ç›´åœ¨ä½¿ç”¨ Kubernetesï¼Œæ‚¨å¯èƒ½ä¸€ç›´åœ¨å¯»æ‰¾å¦‚ä½•æŽ§åˆ¶ç»ˆç«¯ç”¨æˆ·åœ¨é›†ç¾¤ä¸Šçš„è¡Œä¸ºï¼Œä»¥åŠå¦‚ä½•ç¡®ä¿é›†ç¾¤ç¬¦åˆå…¬å¸æ”¿ç­–ã€‚è¿™äº›ç­–ç•¥å¯èƒ½éœ€è¦æ»¡è¶³ç®¡ç†å’Œæ³•å¾‹è¦æ±‚ï¼Œæˆ–è€…ç¬¦åˆæœ€ä½³æ‰§è¡Œæ–¹æ³•å’Œç»„ç»‡æƒ¯ä¾‹ã€‚ä½¿ç”¨ Kubernetesï¼Œå¦‚ä½•åœ¨ä¸ç‰ºç‰²å¼€å‘æ•æ·æ€§å’Œè¿è¥ç‹¬ç«‹æ€§çš„å‰æä¸‹ç¡®ä¿åˆè§„æ€§ï¼Ÿ&lt;/p>
&lt;!--
For example, you can enforce policies like:
* All images must be from approved repositories
* All ingress hostnames must be globally unique
* All pods must have resource limits
* All namespaces must have a label that lists a point-of-contact
--->
&lt;p>ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹ç­–ç•¥ï¼š&lt;/p>
&lt;ul>
&lt;li>æ‰€æœ‰é•œåƒå¿…é¡»æ¥è‡ªèŽ·å¾—æ‰¹å‡†çš„å­˜å‚¨åº“&lt;/li>
&lt;li>æ‰€æœ‰å…¥å£ä¸»æœºåå¿…é¡»æ˜¯å…¨å±€å”¯ä¸€çš„&lt;/li>
&lt;li>æ‰€æœ‰ Pod å¿…é¡»æœ‰èµ„æºé™åˆ¶&lt;/li>
&lt;li>æ‰€æœ‰å‘½åç©ºé—´éƒ½å¿…é¡»å…·æœ‰åˆ—å‡ºè”ç³»çš„æ ‡ç­¾&lt;/li>
&lt;/ul>
&lt;!--
Kubernetes allows decoupling policy decisions from the API server by means of [admission controller webhooks](https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/) to intercept admission requests before they are persisted as objects in Kubernetes. [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) was created to enable users to customize admission control via configuration, not code and to bring awareness of the clusterâ€™s state, not just the single object under evaluation at admission time. Gatekeeper is a customizable admission webhook for Kubernetes that enforces policies executed by the [Open Policy Agent (OPA)](https://www.openpolicyagent.org), a policy engine for Cloud Native environments hosted by CNCF.
--->
&lt;p>åœ¨æŽ¥æ”¶è¯·æ±‚è¢«æŒä¹…åŒ–ä¸º Kubernetes ä¸­çš„å¯¹è±¡ä¹‹å‰ï¼ŒKubernetes å…è®¸é€šè¿‡ &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">admission controller webhooks&lt;/a> å°†ç­–ç•¥å†³ç­–ä¸Ž API æœåŠ¡å™¨åˆ†ç¦»ï¼Œä»Žè€Œæ‹¦æˆªè¿™äº›è¯·æ±‚ã€‚&lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> åˆ›å»ºçš„ç›®çš„æ˜¯ä½¿ç”¨æˆ·èƒ½å¤Ÿé€šè¿‡é…ç½®ï¼ˆè€Œä¸æ˜¯ä»£ç ï¼‰è‡ªå®šä¹‰æŽ§åˆ¶è®¸å¯ï¼Œå¹¶ä½¿ç”¨æˆ·äº†è§£ç¾¤é›†çš„çŠ¶æ€ï¼Œè€Œä¸ä»…ä»…æ˜¯é’ˆå¯¹è¯„ä¼°çŠ¶æ€çš„å•ä¸ªå¯¹è±¡ï¼Œåœ¨è¿™äº›å¯¹è±¡å‡†è®¸åŠ å…¥çš„æ—¶å€™ã€‚Gatekeeper æ˜¯ Kubernetes çš„ä¸€ä¸ªå¯å®šåˆ¶çš„è®¸å¯ webhook ï¼Œå®ƒç”± &lt;a href="https://www.openpolicyagent.org">Open Policy Agent (OPA)&lt;/a> å¼ºåˆ¶æ‰§è¡Œï¼Œ OPA æ˜¯ Cloud Native çŽ¯å¢ƒä¸‹çš„ç­–ç•¥å¼•æ“Žï¼Œç”± CNCF ä¸»åŠžã€‚&lt;/p>
&lt;!--
## Evolution
Before we dive into the current state of Gatekeeper, letâ€™s take a look at how the Gatekeeper project has evolved.
--->
&lt;h2 id="å‘å±•">å‘å±•&lt;/h2>
&lt;p>åœ¨æ·±å…¥äº†è§£ Gatekeeper çš„å½“å‰æƒ…å†µä¹‹å‰ï¼Œè®©æˆ‘ä»¬çœ‹ä¸€ä¸‹ Gatekeeper é¡¹ç›®æ˜¯å¦‚ä½•å‘å±•çš„ã€‚&lt;/p>
&lt;!--
* Gatekeeper v1.0 - Uses OPA as the admission controller with the kube-mgmt sidecar enforcing configmap-based policies. It provides validating and mutating admission control. Donated by Styra.
* Gatekeeper v2.0 - Uses Kubernetes policy controller as the admission controller with OPA and kube-mgmt sidecars enforcing configmap-based policies. It provides validating and mutating admission control and audit functionality. Donated by Microsoft.
* Gatekeeper v3.0 - The admission controller is integrated with the [OPA Constraint Framework](https://github.com/open-policy-agent/frameworks/tree/master/constraint) to enforce CRD-based policies and allow declaratively configured policies to be reliably shareable. Built with kubebuilder, it provides validating and, eventually, mutating (to be implemented) admission control and audit functionality. This enables the creation of policy templates for [Rego](https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/) policies, creation of policies as CRDs, and storage of audit results on policy CRDs. This project is a collaboration between Google, Microsoft, Red Hat, and Styra.
--->
&lt;ul>
&lt;li>Gatekeeper v1.0 - ä½¿ç”¨ OPA ä½œä¸ºå¸¦æœ‰ kube-mgmt sidecar çš„è®¸å¯æŽ§åˆ¶å™¨ï¼Œç”¨æ¥å¼ºåˆ¶æ‰§è¡ŒåŸºäºŽ configmap çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å®žçŽ°äº†éªŒè¯å’Œè½¬æ¢è®¸å¯æŽ§åˆ¶ã€‚è´¡çŒ®æ–¹ï¼šStyra&lt;/li>
&lt;li>Gatekeeper v2.0 - ä½¿ç”¨ Kubernetes ç­–ç•¥æŽ§åˆ¶å™¨ä½œä¸ºè®¸å¯æŽ§åˆ¶å™¨ï¼ŒOPA å’Œ kube-mgmt sidecar å®žæ–½åŸºäºŽ configmap çš„ç­–ç•¥ã€‚è¿™ç§æ–¹æ³•å®žçŽ°äº†éªŒè¯å’Œè½¬æ¢å‡†å…¥æŽ§åˆ¶å’Œå®¡æ ¸åŠŸèƒ½ã€‚è´¡çŒ®æ–¹ï¼šMicrosoft&lt;/li>
&lt;li>Gatekeeper v3.0 - å‡†å…¥æŽ§åˆ¶å™¨ä¸Ž &lt;a href="https://github.com/open-policy-agent/frameworks/tree/master/constraint">OPA Constraint Framework&lt;/a> é›†æˆåœ¨ä¸€èµ·ï¼Œç”¨æ¥å®žæ–½åŸºäºŽ CRD çš„ç­–ç•¥ï¼Œå¹¶å¯ä»¥å¯é åœ°å…±äº«å·²å®Œæˆå£°æ˜Žé…ç½®çš„ç­–ç•¥ã€‚ä½¿ç”¨ kubebuilder è¿›è¡Œæž„å»ºï¼Œå®žçŽ°äº†éªŒè¯ä»¥åŠæœ€ç»ˆè½¬æ¢ï¼ˆå¾…å®Œæˆï¼‰ä¸ºè®¸å¯æŽ§åˆ¶å’Œå®¡æ ¸åŠŸèƒ½ã€‚è¿™æ ·å°±å¯ä»¥ä¸º &lt;a href="https://www.openpolicyagent.org/docs/latest/how-do-i-write-policies/">Rego&lt;/a> ç­–ç•¥åˆ›å»ºç­–ç•¥æ¨¡æ¿ï¼Œå°†ç­–ç•¥åˆ›å»ºä¸º CRD å¹¶å­˜å‚¨å®¡æ ¸ç»“æžœåˆ°ç­–ç•¥ CRD ä¸Šã€‚è¯¥é¡¹ç›®æ˜¯ Googleï¼ŒMicrosoftï¼ŒRed Hat å’Œ Styra åˆä½œå®Œæˆçš„ã€‚&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2019-08-06-opa-gatekeeper/v3.png" alt="">&lt;/p>
&lt;!--
## Gatekeeper v3.0 Features
Now letâ€™s take a closer look at the current state of Gatekeeper and how you can leverage all the latest features. Consider an organization that wants to ensure all objects in a cluster have departmental information provided as part of the objectâ€™s labels. How can you do this with Gatekeeper?
--->
&lt;h2 id="gatekeeper-v3-0-çš„åŠŸèƒ½">Gatekeeper v3.0 çš„åŠŸèƒ½&lt;/h2>
&lt;p>çŽ°åœ¨æˆ‘ä»¬è¯¦ç»†çœ‹ä¸€ä¸‹ Gatekeeper å½“å‰çš„çŠ¶æ€ï¼Œä»¥åŠå¦‚ä½•åˆ©ç”¨æ‰€æœ‰æœ€æ–°çš„åŠŸèƒ½ã€‚å‡è®¾ä¸€ä¸ªç»„ç»‡å¸Œæœ›ç¡®ä¿é›†ç¾¤ä¸­çš„æ‰€æœ‰å¯¹è±¡éƒ½æœ‰ department ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯æ˜¯å¯¹è±¡æ ‡ç­¾çš„ä¸€éƒ¨åˆ†ã€‚å¦‚ä½•åˆ©ç”¨ Gatekeeper å®Œæˆè¿™é¡¹éœ€æ±‚ï¼Ÿ&lt;/p>
&lt;!--
### Validating Admission Control
Once all the Gatekeeper components have been [installed](https://github.com/open-policy-agent/gatekeeper) in your cluster, the API server will trigger the Gatekeeper admission webhook to process the admission request whenever a resource in the cluster is created, updated, or deleted.
During the validation process, Gatekeeper acts as a bridge between the API server and OPA. The API server will enforce all policies executed by OPA.
--->
&lt;h3 id="éªŒè¯è®¸å¯æŽ§åˆ¶">éªŒè¯è®¸å¯æŽ§åˆ¶&lt;/h3>
&lt;p>åœ¨é›†ç¾¤ä¸­æ‰€æœ‰ Gatekeeper ç»„ä»¶éƒ½ &lt;a href="https://github.com/open-policy-agent/gatekeeper">å®‰è£…&lt;/a> å®Œæˆä¹‹åŽï¼Œåªè¦é›†ç¾¤ä¸­çš„èµ„æºè¿›è¡Œåˆ›å»ºã€æ›´æ–°æˆ–åˆ é™¤ï¼ŒAPI æœåŠ¡å™¨å°†è§¦å‘ Gatekeeper å‡†å…¥ webhook æ¥å¤„ç†å‡†å…¥è¯·æ±‚ã€‚&lt;/p>
&lt;p>åœ¨éªŒè¯è¿‡ç¨‹ä¸­ï¼ŒGatekeeper å……å½“ API æœåŠ¡å™¨å’Œ OPA ä¹‹é—´çš„æ¡¥æ¢ã€‚API æœåŠ¡å™¨å°†å¼ºåˆ¶å®žæ–½ OPA æ‰§è¡Œçš„æ‰€æœ‰ç­–ç•¥ã€‚&lt;/p>
&lt;!--
### Policies and Constraints
With the integration of the OPA Constraint Framework, a Constraint is a declaration that its author wants a system to meet a given set of requirements. Each Constraint is written with Rego, a declarative query language used by OPA to enumerate instances of data that violate the expected state of the system. All Constraints are evaluated as a logical AND. If one Constraint is not satisfied, then the whole request is rejected.
--->
&lt;h3 id="ç­–ç•¥ä¸Ž-constraint">ç­–ç•¥ä¸Ž Constraint&lt;/h3>
&lt;p>ç»“åˆ OPA Constraint Frameworkï¼ŒConstraint æ˜¯ä¸€ä¸ªå£°æ˜Žï¼Œè¡¨ç¤ºä½œè€…å¸Œæœ›ç³»ç»Ÿæ»¡è¶³ç»™å®šçš„ä¸€ç³»åˆ—è¦æ±‚ã€‚Constraint éƒ½ä½¿ç”¨ Rego ç¼–å†™ï¼ŒRego æ˜¯å£°æ˜Žæ€§æŸ¥è¯¢è¯­è¨€ï¼ŒOPA ç”¨ Rego æ¥æžšä¸¾è¿èƒŒç³»ç»Ÿé¢„æœŸçŠ¶æ€çš„æ•°æ®å®žä¾‹ã€‚æ‰€æœ‰ Constraint éƒ½éµå¾ªé€»è¾‘ ANDã€‚å‡ä½¿æœ‰ä¸€ä¸ª Constraint ä¸æ»¡è¶³ï¼Œé‚£ä¹ˆæ•´ä¸ªè¯·æ±‚éƒ½å°†è¢«æ‹’ç»ã€‚&lt;/p>
&lt;!--
Before defining a Constraint, you need to create a Constraint Template that allows people to declare new Constraints. Each template describes both the Rego logic that enforces the Constraint and the schema for the Constraint, which includes the schema of the CRD and the parameters that can be passed into a Constraint, much like arguments to a function.
For example, here is a Constraint template CRD that requires certain labels to be present on an arbitrary object.
--->
&lt;p>åœ¨å®šä¹‰ Constraint ä¹‹å‰ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª Constraint Templateï¼Œå…è®¸å¤§å®¶å£°æ˜Žæ–°çš„ Constraintã€‚æ¯ä¸ªæ¨¡æ¿éƒ½æè¿°äº†å¼ºåˆ¶æ‰§è¡Œ Constraint çš„ Rego é€»è¾‘å’Œ Constraint çš„æ¨¡å¼ï¼Œå…¶ä¸­åŒ…æ‹¬ CRD çš„æ¨¡å¼å’Œä¼ é€’åˆ° enforces ä¸­çš„å‚æ•°ï¼Œå°±åƒå‡½æ•°çš„å‚æ•°ä¸€æ ·ã€‚&lt;/p>
&lt;p>ä¾‹å¦‚ï¼Œä»¥ä¸‹æ˜¯ä¸€ä¸ª Constraint æ¨¡æ¿ CRDï¼Œå®ƒçš„è¯·æ±‚æ˜¯åœ¨ä»»æ„å¯¹è±¡ä¸Šæ˜¾ç¤ºæŸäº›æ ‡ç­¾ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>templates.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ConstraintTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">crd&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">listKind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabelsList&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">plural&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">singular&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8srequiredlabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">validation&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Schema for the `parameters` field&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">openAPIV3Schema&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">properties&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>array&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">items&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>string&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">target&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>admission.k8s.gatekeeper.sh&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">rego&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44;font-style:italic">|
&lt;/span>&lt;span style="color:#b44;font-style:italic"> package k8srequiredlabels&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>deny[{&lt;span style="color:#a2f;font-weight:bold">&amp;#34;msg&amp;#34;: msg, &amp;#34;details&amp;#34;: {&amp;#34;missing_labels&amp;#34;: &lt;/span>missing}}]&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>provided&lt;span style="color:#bbb"> &lt;/span>:=&lt;span style="color:#bbb"> &lt;/span>{label&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#bbb"> &lt;/span>input.review.object.metadata.labels[label]}&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>required&lt;span style="color:#bbb"> &lt;/span>:=&lt;span style="color:#bbb"> &lt;/span>{label&lt;span style="color:#bbb"> &lt;/span>|&lt;span style="color:#bbb"> &lt;/span>label&lt;span style="color:#bbb"> &lt;/span>:=&lt;span style="color:#bbb"> &lt;/span>input.parameters.labels[_]}&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>missing&lt;span style="color:#bbb"> &lt;/span>:=&lt;span style="color:#bbb"> &lt;/span>required&lt;span style="color:#bbb"> &lt;/span>- provided&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>count(missing)&lt;span style="color:#bbb"> &lt;/span>&amp;gt;&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>msg&lt;span style="color:#bbb"> &lt;/span>:=&lt;span style="color:#bbb"> &lt;/span>sprintf(&lt;span style="color:#b44">&amp;#34;you must provide labels: %v&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>[missing])&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>}&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Once a Constraint template has been deployed in the cluster, an admin can now create individual Constraint CRDs as defined by the Constraint template. For example, here is a Constraint CRD that requires the label `hr` to be present on all namespaces.
--->
&lt;p>åœ¨é›†ç¾¤ä¸­éƒ¨ç½²äº† Constraint æ¨¡æ¿åŽï¼Œç®¡ç†å‘˜çŽ°åœ¨å¯ä»¥åˆ›å»ºç”± Constraint æ¨¡æ¿å®šä¹‰çš„å•ä¸ª Constraint CRDã€‚ä¾‹å¦‚ï¼Œè¿™é‡Œä»¥ä¸‹æ˜¯ä¸€ä¸ª Constraint CRDï¼Œè¦æ±‚æ ‡ç­¾ &lt;code>hr&lt;/code> å‡ºçŽ°åœ¨æ‰€æœ‰å‘½åç©ºé—´ä¸Šã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
Similarly, another Constraint CRD that requires the label `finance` to be present on all namespaces can easily be created from the same Constraint template.
--->
&lt;p>ç±»ä¼¼åœ°ï¼Œå¯ä»¥ä»ŽåŒä¸€ä¸ª Constraint æ¨¡æ¿è½»æ¾åœ°åˆ›å»ºå¦ä¸€ä¸ª Constraint CRDï¼Œè¯¥ Constraint CRD è¦æ±‚æ‰€æœ‰å‘½åç©ºé—´ä¸Šéƒ½æœ‰ &lt;code>finance&lt;/code> æ ‡ç­¾ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-finance&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;finance&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
As you can see, with the Constraint framework, we can reliably share Regos via the Constraint templates, define the scope of enforcement with the match field, and provide user-defined parameters to the Constraints to create customized behavior for each Constraint.
--->
&lt;p>å¦‚æ‚¨æ‰€è§ï¼Œä½¿ç”¨ Constraint frameworkï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡ Constraint æ¨¡æ¿å¯é åœ°å…±äº« regoï¼Œä½¿ç”¨åŒ¹é…å­—æ®µå®šä¹‰æ‰§è¡ŒèŒƒå›´ï¼Œå¹¶ä¸º Constraint æä¾›ç”¨æˆ·å®šä¹‰çš„å‚æ•°ï¼Œä»Žè€Œä¸ºæ¯ä¸ª Constraint åˆ›å»ºè‡ªå®šä¹‰è¡Œä¸ºã€‚&lt;/p>
&lt;!--
### Audit
The audit functionality enables periodic evaluations of replicated resources against the Constraints enforced in the cluster to detect pre-existing misconfigurations. Gatekeeper stores audit results as `violations` listed in the `status` field of the relevant Constraint. --->
&lt;h3 id="å®¡æ ¸">å®¡æ ¸&lt;/h3>
&lt;p>æ ¹æ®ç¾¤é›†ä¸­å¼ºåˆ¶æ‰§è¡Œçš„ Constraintï¼Œå®¡æ ¸åŠŸèƒ½å¯å®šæœŸè¯„ä¼°å¤åˆ¶çš„èµ„æºï¼Œå¹¶æ£€æµ‹å…ˆå‰å­˜åœ¨çš„é”™è¯¯é…ç½®ã€‚Gatekeeper å°†å®¡æ ¸ç»“æžœå­˜å‚¨ä¸º &lt;code>violations&lt;/code>ï¼Œåœ¨ç›¸å…³ Constraint çš„ &lt;code>status&lt;/code> å­—æ®µä¸­åˆ—å‡ºã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>constraints.gatekeeper.sh/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>K8sRequiredLabels&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ns-must-have-hr&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">match&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">apiGroups&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kinds&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">auditTimestamp&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;2019-08-06T01:46:13Z&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">byPod&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">enforced&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">id&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gatekeeper-controller-manager&lt;span style="color:#666">-0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">violations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">enforcementAction&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>deny&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Namespace&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">message: &amp;#39;you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: default
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: gatekeeper-system
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> name: kube-public
&lt;/span>&lt;span style="color:#b44"> - enforcementAction: deny
&lt;/span>&lt;span style="color:#b44"> kind: Namespace
&lt;/span>&lt;span style="color:#b44"> message: &amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">you must provide labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>{&lt;span style="color:#b44">&amp;#34;hr&amp;#34;&lt;/span>}&amp;#39;&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Data Replication
Audit requires replication of Kubernetes resources into OPA before they can be evaluated against the enforced Constraints. Data replication is also required by Constraints that need access to objects in the cluster other than the object under evaluation. For example, a Constraint that enforces uniqueness of ingress hostname must have access to all other ingresses in the cluster.
--->
&lt;h3 id="æ•°æ®å¤åˆ¶">æ•°æ®å¤åˆ¶&lt;/h3>
&lt;p>å®¡æ ¸è¦æ±‚å°† Kubernetes å¤åˆ¶åˆ° OPA ä¸­ï¼Œç„¶åŽæ‰èƒ½æ ¹æ®å¼ºåˆ¶çš„ Constraint å¯¹å…¶è¿›è¡Œè¯„ä¼°ã€‚æ•°æ®å¤åˆ¶åŒæ ·ä¹Ÿéœ€è¦ Constraintï¼Œè¿™äº› Constraint éœ€è¦è®¿é—®é›†ç¾¤ä¸­é™¤è¯„ä¼°å¯¹è±¡ä¹‹å¤–çš„å¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ª Constraint è¦å¼ºåˆ¶ç¡®å®šå…¥å£ä¸»æœºåçš„å”¯ä¸€æ€§ï¼Œå°±å¿…é¡»æœ‰æƒè®¿é—®é›†ç¾¤ä¸­çš„æ‰€æœ‰å…¶ä»–å…¥å£ã€‚&lt;/p>
&lt;!--
To configure Kubernetes data to be replicated, create a sync config resource with the resources to be replicated into OPA. For example, the below configuration replicates all namespace and pod resources to OPA.
--->
&lt;p>å¯¹ Kubernetes æ•°æ®è¿›è¡Œå¤åˆ¶ï¼Œè¯·ä½¿ç”¨å¤åˆ¶åˆ° OPA ä¸­çš„èµ„æºåˆ›å»º sync config èµ„æºã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„é…ç½®å°†æ‰€æœ‰å‘½åç©ºé—´å’Œ Pod èµ„æºå¤åˆ¶åˆ° OPAã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config.gatekeeper.sh/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;gatekeeper-system&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">sync&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">syncOnly&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Namespace&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Pod&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## Planned for Future
The community behind the Gatekeeper project will be focusing on providing mutating admission control to support mutation scenarios (for example: annotate objects automatically with departmental information when creating a new resource), support external data to inject context external to the cluster into the admission decisions, support dry run to see impact of a policy on existing resources in the cluster before enforcing it, and more audit functionalities.
--->
&lt;h2 id="æœªæ¥è®¡åˆ’">æœªæ¥è®¡åˆ’&lt;/h2>
&lt;p>Gatekeeper é¡¹ç›®èƒŒåŽçš„ç¤¾åŒºå°†ä¸“æ³¨äºŽæä¾›è½¬æ¢è®¸å¯æŽ§åˆ¶ï¼Œå¯ä»¥ç”¨æ¥æ”¯æŒè½¬æ¢æ–¹æ¡ˆï¼ˆä¾‹å¦‚ï¼šåœ¨åˆ›å»ºæ–°èµ„æºæ—¶ä½¿ç”¨ department ä¿¡æ¯è‡ªåŠ¨æ³¨é‡Šå¯¹è±¡ï¼‰ï¼Œæ”¯æŒå¤–éƒ¨æ•°æ®ä»¥å°†é›†ç¾¤å¤–éƒ¨çŽ¯å¢ƒåŠ å…¥åˆ°è®¸å¯å†³ç­–ä¸­ï¼Œæ”¯æŒè¯•è¿è¡Œä»¥ä¾¿åœ¨æ‰§è¡Œç­–ç•¥ä¹‹å‰äº†è§£ç­–ç•¥å¯¹é›†ç¾¤ä¸­çŽ°æœ‰èµ„æºçš„å½±å“ï¼Œè¿˜æœ‰æ›´å¤šçš„å®¡æ ¸åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
If you are interested in learning more about the project, check out the [Gatekeeper](https://github.com/open-policy-agent/gatekeeper) repo. If you are interested in helping define the direction of Gatekeeper, join the [#kubernetes-policy](https://openpolicyagent.slack.com/messages/CDTN970AX) channel on OPA Slack, and join our [weekly meetings](https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit) to discuss development, issues, use cases, etc.
--->
&lt;p>å¦‚æžœæ‚¨æœ‰å…´è¶£äº†è§£æ›´å¤šæœ‰å…³è¯¥é¡¹ç›®çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ &lt;a href="https://github.com/open-policy-agent/gatekeeper">Gatekeeper&lt;/a> å­˜å‚¨åº“ã€‚å¦‚æžœæ‚¨æœ‰å…´è¶£å¸®åŠ©ç¡®å®š Gatekeeper çš„æ–¹å‘ï¼Œè¯·åŠ å…¥ &lt;a href="https://openpolicyagent.slack.com/messages/CDTN970AX">#kubernetes-policy&lt;/a> OPA Slack é¢‘é“ï¼Œå¹¶åŠ å…¥æˆ‘ä»¬çš„ &lt;a href="https://docs.google.com/document/d/1A1-Q-1OMw3QODs1wT6eqfLTagcGmgzAJAjJihiO3T48/edit">å‘¨ä¼š&lt;/a> ä¸€åŒè®¨è®ºå¼€å‘ã€ä»»åŠ¡ã€ç”¨ä¾‹ç­‰ã€‚&lt;/p></description></item><item><title>Blog: æ¬¢è¿Žå‚åŠ åœ¨ä¸Šæµ·ä¸¾è¡Œçš„è´¡çŒ®è€…å³°ä¼š</title><link>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 11 Jun 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/06/11/%E6%AC%A2%E8%BF%8E%E5%8F%82%E5%8A%A0%E5%9C%A8%E4%B8%8A%E6%B5%B7%E4%B8%BE%E8%A1%8C%E7%9A%84%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!-- ---
layout: blog
title: 'Join us at the Contributor Summit in Shanghai'
date: 2019-06-11
--- -->
&lt;p>&lt;strong>Author&lt;/strong>: Josh Berkus (Red Hat)&lt;/p>
&lt;!-- ![Picture of contributor panel at 2018 Shanghai contributor summit. Photo by Josh Berkus, licensed CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png) -->
&lt;p>![è´¡çŒ®è€…å°ç»„è®¨è®ºæŽ å½±ï¼Œæ‘„äºŽ 2018 å¹´ä¸Šæµ·è´¡çŒ®è€…å³°ä¼šï¼Œä½œè€… Josh Berkus, è®¸å¯è¯ CC-BY 4.0](/images/blog/2019-
06-11-contributor-summit-shanghai/panel.png)&lt;/p>
&lt;!-- For the second year, we will have [a Contributor Summit event](https://www.lfasiallc.com/events/contributors-summit-china-2019/) the day before [KubeCon China](https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/) in Shanghai. If you already contribute to Kubernetes or would like to contribute, please consider attending and [register](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/). The Summit will be held June 24th, at the Shanghai Expo Center (the same location where KubeCon will take place), and will include a Current Contributor Day as well as the New Contributor Workshop and the Documentation Sprints. -->
&lt;p>è¿žç»­ç¬¬äºŒå¹´ï¼Œæˆ‘ä»¬å°†åœ¨ &lt;a href="https://events.linuxfoundation.cn/events/kubecon-cloudnativecon-china-2019/">KubeCon China&lt;/a> ä¹‹å‰ä¸¾è¡Œä¸€å¤©çš„ &lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/">è´¡çŒ®è€…å³°ä¼š&lt;/a>ã€‚
ä¸ç®¡æ‚¨æ˜¯å¦å·²ç»æ˜¯ä¸€å Kubernetes è´¡çŒ®è€…ï¼Œè¿˜æ˜¯æƒ³è¦åŠ å…¥ç¤¾åŒºé˜Ÿä¼ï¼Œè´¡çŒ®ä¸€ä»½åŠ›é‡ï¼Œéƒ½è¯·è€ƒè™‘&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œ&lt;/a>å‚åŠ è¿™æ¬¡æ´»åŠ¨ã€‚
è¿™æ¬¡å³°ä¼šå°†äºŽå…­æœˆ 24 å·ï¼Œåœ¨ä¸Šæµ·ä¸–åšä¸­å¿ƒï¼ˆå’Œ KubeCon çš„ä¸¾åŠžåœ°ç‚¹ç›¸åŒï¼‰ä¸¾è¡Œï¼Œ
ä¸€å¤©çš„æ´»åŠ¨å°†åŒ…å«â€œçŽ°æœ‰è´¡çŒ®è€…æ´»åŠ¨â€ï¼Œä»¥åŠâ€œæ–°è´¡çŒ®è€…å·¥ä½œåŠâ€å’Œâ€œæ–‡æ¡£å°ç»„æ´»åŠ¨â€ã€‚&lt;/p>
&lt;!-- ### Current Contributor Day -->
&lt;h3 id="çŽ°æœ‰è´¡çŒ®è€…æ´»åŠ¨">çŽ°æœ‰è´¡çŒ®è€…æ´»åŠ¨&lt;/h3>
&lt;!-- After last year's Contributor Day, our team received feedback that many of our contributors in Asia and Oceania would like content for current contributors as well. As such, we have added a Current Contributor track to the schedule. -->
&lt;p>åŽ»å¹´çš„è´¡çŒ®è€…èŠ‚ä¹‹åŽï¼Œæˆ‘ä»¬çš„å›¢é˜Ÿæ”¶åˆ°äº†å¾ˆå¤šåé¦ˆæ„è§ï¼Œå¾ˆå¤šäºšæ´²å’Œå¤§æ´‹æ´²çš„è´¡çŒ®è€…ä¹Ÿæƒ³è¦é’ˆå¯¹å½“å‰è´¡çŒ®è€…çš„å³°ä¼šå†…å®¹ã€‚
æœ‰é‰´äºŽæ­¤ï¼Œæˆ‘ä»¬åœ¨ä»Šå¹´çš„å®‰æŽ’ä¸­åŠ å…¥äº†å½“å‰è´¡çŒ®è€…çš„ä¸»é¢˜ã€‚&lt;/p>
&lt;!-- While we do not yet have a full schedule up, the topics covered in the current contributor track will include: -->
&lt;p>å°½ç®¡æˆ‘ä»¬è¿˜æ²¡æœ‰ä¸€ä¸ªå®Œæ•´çš„æ—¶é—´å®‰æŽ’ï¼Œä¸‹é¢æ˜¯å½“å‰è´¡çŒ®è€…ä¸»é¢˜æ‰€ä¼šåŒ…å«çš„è¯é¢˜ï¼š&lt;/p>
&lt;!-- * How to write a KEP (Kubernetes Enhancement Proposal)
* Codebase and repository review
* Local Build &amp; Test troubleshooting session
* Guide to Non-Code Contribution opportunities
* SIG-Azure face-to-face meeting
* SIG-Scheduling face-to-face meeting
* Other SIG face-to-face meetings as we confirm them -->
&lt;ul>
&lt;li>å¦‚ä½•æ’°å†™ Kubernetes æ”¹è¿›è®®æ¡ˆ (KEP)&lt;/li>
&lt;li>ä»£ç åº“ç ”ä¹ &lt;/li>
&lt;li>æœ¬åœ°æž„å»ºä»¥åŠæµ‹è¯•è°ƒè¯•&lt;/li>
&lt;li>ä¸å†™ä»£ç çš„è´¡çŒ®æœºä¼š&lt;/li>
&lt;li>SIG-Azure é¢å¯¹é¢äº¤æµ&lt;/li>
&lt;li>SIG-Scheduling é¢å¯¹é¢äº¤æµ&lt;/li>
&lt;li>å…¶ä»–å…´è¶£å°ç»„çš„é¢å¯¹é¢æœºä¼š&lt;/li>
&lt;/ul>
&lt;!-- The schedule will be on [the Community page](https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit) once it is complete. -->
&lt;p>æ•´ä¸ªè®¡åˆ’å®‰æŽ’å°†ä¼šåœ¨å®Œå…¨ç¡®å®šä¹‹åŽï¼Œæ•´ç†æ”¾åœ¨&lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/06-contributor-summit">ç¤¾åŒºé¡µé¢&lt;/a>ä¸Šã€‚&lt;/p>
&lt;!-- If your SIG wants to have a face-to-face meeting at Kubecon Shanghai, please contact [Josh Berkus](mailto:jberkus@redhat.com). -->
&lt;p>å¦‚æžœæ‚¨çš„ SIG æƒ³è¦åœ¨ Kubecon Shanghai ä¸Šè¿›è¡Œé¢å¯¹é¢çš„äº¤æµï¼Œè¯·è”ç³» &lt;a href="mailto:jberkus@redhat.com">Josh Berkus&lt;/a>ã€‚&lt;/p>
&lt;!-- ### New Contributor Workshop -->
&lt;h3 id="æ–°è´¡çŒ®è€…å·¥ä½œåŠ">æ–°è´¡çŒ®è€…å·¥ä½œåŠ&lt;/h3>
&lt;!-- Students at [last year's New Contributor Workshop](/blog/2018/12/05/new-contributor-workshop-shanghai/) (NCW) found it to be extremely valuable, and the event helped to orient a few of the many Asian and Pacific developers looking to participate in the Kubernetes community. -->
&lt;p>å‚ä¸Žè¿‡&lt;a href="https://kubernetes.io/blog/2018/12/05/new-contributor-workshop-shanghai/">åŽ»å¹´æ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ˆNCWï¼‰&lt;/a>çš„å­¦ç”Ÿè§‰å¾—è¿™é¡¹æ´»åŠ¨éžå¸¸çš„æœ‰ä»·å€¼ï¼Œ
è¿™é¡¹æ´»åŠ¨ä¹Ÿå¸®åŠ©ã€å¼•å¯¼äº†å¾ˆå¤šäºšæ´²å’Œå¤§æ´‹æ´²çš„å¼€å‘è€…æ›´å¤šåœ°å‚ä¸Žåˆ° Kubernetes ç¤¾åŒºä¹‹ä¸­ã€‚&lt;/p>
&lt;!-- > "It's a one-stop-shop for becoming familiar with the community." said one participant. -->
&lt;blockquote>
&lt;p>â€œè¿™æ¬¡æ´»åŠ¨å¯ä»¥è®©äººä¸€æ¬¡å¿«é€Ÿç†Ÿæ‚‰ç¤¾åŒºã€‚â€å…¶ä¸­çš„ä¸€ä½å‚ä¸Žè€…æåˆ°ã€‚&lt;/p>
&lt;/blockquote>
&lt;!-- If you have not contributed to Kubernetes before, or have only done one or two things, please consider [enrolling](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) in the NCW. -->
&lt;p>å¦‚æžœæ‚¨ä¹‹å‰ä»Žæ²¡æœ‰å‚ä¸Žè¿‡ Kubernetes çš„è´¡çŒ®ï¼Œæˆ–è€…åªæ˜¯åšè¿‡ä¸€æ¬¡æˆ–ä¸¤æ¬¡è´¡çŒ®ï¼Œéƒ½è¯·è€ƒè™‘&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œå‚åŠ &lt;/a>æ–°è´¡çŒ®è€…å·¥ä½œåŠã€‚&lt;/p>
&lt;!-- > "Got to know the process from signing CLA to PR and made friends with other contributors." said another. -->
&lt;blockquote>
&lt;p>â€œç†Ÿæ‚‰äº†ä»Ž CLA åˆ° PR çš„æ•´ä¸ªæµç¨‹ï¼Œä¹Ÿè®¤è¯†ç»“äº¤äº†å¾ˆå¤šè´¡çŒ®è€…ã€‚â€å¦ä¸€ä½å¼€å‘è€…æåˆ°ã€‚&lt;/p>
&lt;/blockquote>
&lt;!-- ### Documentation Sprints -->
&lt;h3 id="æ–‡æ¡£å°ç»„æ´»åŠ¨">æ–‡æ¡£å°ç»„æ´»åŠ¨&lt;/h3>
&lt;!-- Both old and new contributors on our Docs Team will spend a day both improving our documentation and translating it into other languages. If you are interested in having better documentation, fully localized into Chinese and other languages, please [sign up](https://www.lfasiallc.com/events/contributors-summit-china-2019/register/) to help with the Doc Sprints. -->
&lt;p>æ–‡æ¡£å°ç»„çš„æ–°è€è´¡çŒ®è€…éƒ½ä¼šèšé¦–ä¸€å¤©ï¼Œè®¨è®ºå¦‚ä½•æå‡æ–‡æ¡£è´¨é‡ï¼Œä»¥åŠå°†æ–‡æ¡£ç¿»è¯‘æˆæ›´å¤šçš„è¯­è¨€ã€‚
å¦‚æžœæ‚¨å¯¹ç¿»è¯‘æ–‡æ¡£ï¼Œå°†è¿™äº›çŸ¥è¯†å’Œä¿¡æ¯ç¿»è¯‘æˆä¸­æ–‡å’Œå…¶ä»–è¯­è¨€æ„Ÿå…´è¶£çš„è¯ï¼Œè¯·åœ¨è¿™é‡Œ&lt;a href="https://www.lfasiallc.com/events/contributors-summit-china-2019/register/">æ³¨å†Œ&lt;/a>ï¼ŒæŠ¥åå‚åŠ æ–‡æ¡£å°ç»„æ´»åŠ¨ã€‚&lt;/p>
&lt;!-- ### Before you attend -->
&lt;h3 id="å‚ä¸Žä¹‹å‰">å‚ä¸Žä¹‹å‰&lt;/h3>
&lt;!-- Regardless of where you participate, everyone at the Contributor Summit should [sign the Kubernetes Contributor License Agreement](https://git.k8s.io/community/CLA.md#the-contributor-license-agreement) (CLA) before coming to the conference. You should also bring a laptop suitable for working on documentation or code development. -->
&lt;p>ä¸è®ºæ‚¨å‚ä¸Žçš„æ˜¯å“ªä¸€é¡¹æ´»åŠ¨ï¼Œæ‰€æœ‰äººéƒ½éœ€è¦åœ¨åˆ°è¾¾è´¡çŒ®è€…å³°ä¼šå‰ç­¾ç½² &lt;a href="https://git.k8s.io/community/CLA.md#the-contributor-license-agreement">Kubernetes CLA&lt;/a>ã€‚
æ‚¨ä¹ŸåŒæ—¶éœ€è¦è€ƒè™‘å¸¦ä¸€ä¸ªåˆé€‚çš„ç¬”è®°æœ¬ç”µè„‘ï¼Œå¸®åŠ©æ–‡æ¡£å†™ä½œæˆ–æ˜¯ç¼–ç¨‹å¼€å‘ã€‚&lt;/p></description></item><item><title>Blog: å¦‚ä½•å‚ä¸Ž Kubernetes æ–‡æ¡£çš„æœ¬åœ°åŒ–å·¥ä½œ</title><link>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</link><pubDate>Fri, 26 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/26/%E5%A6%82%E4%BD%95%E5%8F%82%E4%B8%8E-kubernetes-%E6%96%87%E6%A1%A3%E7%9A%84%E6%9C%AC%E5%9C%B0%E5%8C%96%E5%B7%A5%E4%BD%9C/</guid><description>
&lt;p>&lt;strong>ä½œè€…: Zach Corleissenï¼ˆLinux åŸºé‡‘ä¼šï¼‰&lt;/strong>&lt;/p>
&lt;p>åŽ»å¹´æˆ‘ä»¬å¯¹ Kubernetes ç½‘ç«™è¿›è¡Œäº†ä¼˜åŒ–ï¼ŒåŠ å…¥äº†&lt;a href="https://kubernetes.io/blog/2018/11/08/kubernetes-docs-updates-international-edition/">å¤šè¯­è¨€å†…å®¹çš„æ”¯æŒ&lt;/a>ã€‚è´¡çŒ®è€…ä»¬è¸Šè·ƒå“åº”ï¼ŒåŠ å…¥äº†å¤šç§æ–°çš„æœ¬åœ°åŒ–å†…å®¹ï¼šæˆªè‡³ 2019 å¹´ 4 æœˆï¼ŒKubernetes æ–‡æ¡£æœ‰äº† 9 ä¸ªä¸åŒè¯­è¨€çš„æœªå®Œæˆç‰ˆæœ¬ï¼Œå…¶ä¸­æœ‰ 6 ä¸ªæ˜¯ 2019 å¹´åŠ å…¥çš„ã€‚åœ¨æ¯ä¸ª Kubernetes æ–‡æ¡£é¡µé¢çš„ä¸Šæ–¹ï¼Œè¯»è€…éƒ½å¯ä»¥çœ‹åˆ°ä¸€ä¸ªè¯­è¨€é€‰æ‹©å™¨ï¼Œå…¶ä¸­åˆ—å‡ºäº†æ‰€æœ‰å¯ç”¨è¯­è¨€ã€‚&lt;/p>
&lt;p>ä¸è®ºæ˜¯å®Œæˆåº¦æœ€é«˜çš„&lt;a href="https://v1-12.docs.kubernetes.io/zh/">ä¸­æ–‡ç‰ˆ v1.12&lt;/a>ï¼Œè¿˜æ˜¯æœ€æ–°åŠ å…¥çš„&lt;a href="https://kubernetes.io/pt/">è‘¡è„ç‰™æ–‡ç‰ˆ v1.14&lt;/a>ï¼Œå„è¯­è¨€çš„æœ¬åœ°åŒ–å†…å®¹è¿˜æœªå®Œæˆï¼Œè¿™æ˜¯ä¸€ä¸ªè¿›è¡Œä¸­çš„é¡¹ç›®ã€‚å¦‚æžœè¯»è€…æœ‰å…´è¶£å¯¹çŽ°æœ‰æœ¬åœ°åŒ–å·¥ä½œæä¾›æ”¯æŒï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚&lt;/p>
&lt;h2 id="ä»€ä¹ˆæ˜¯æœ¬åœ°åŒ–">ä»€ä¹ˆæ˜¯æœ¬åœ°åŒ–&lt;/h2>
&lt;p>ç¿»è¯‘æ˜¯ä»¥è¯è¡¨æ„çš„é—®é¢˜ã€‚è€Œæœ¬åœ°åŒ–åœ¨æ­¤åŸºç¡€ä¹‹ä¸Šï¼Œè¿˜åŒ…å«äº†è¿‡ç¨‹å’Œè®¾è®¡æ–¹é¢çš„å·¥ä½œã€‚&lt;/p>
&lt;p>æœ¬åœ°åŒ–å’Œç¿»è¯‘å¾ˆåƒï¼Œä½†æ˜¯åŒ…å«æ›´å¤šå†…å®¹ã€‚é™¤äº†è¿›è¡Œç¿»è¯‘ä¹‹å¤–ï¼Œæœ¬åœ°åŒ–è¿˜è¦ä¸ºç¼–å†™å’Œå‘å¸ƒè¿‡ç¨‹çš„æ¡†æž¶è¿›è¡Œä¼˜åŒ–ã€‚ä¾‹å¦‚ï¼ŒKubernetes.io å¤šæ•°çš„ç«™ç‚¹æµè§ˆåŠŸèƒ½ï¼ˆæŒ‰é’®æ–‡å­—ï¼‰éƒ½ä¿å­˜åœ¨&lt;a href="https://github.com/kubernetes/website/tree/master/i18n">å•ç‹¬çš„æ–‡ä»¶&lt;/a>ä¹‹ä¸­ã€‚æ‰€ä»¥å¯åŠ¨æ–°æœ¬åœ°åŒ–çš„è¿‡ç¨‹ä¸­ï¼Œéœ€è¦åŒ…å«åŠ å…¥å¯¹ç‰¹å®šæ–‡ä»¶ä¸­å­—ç¬¦ä¸²è¿›è¡Œç¿»è¯‘çš„å·¥ä½œã€‚&lt;/p>
&lt;p>æœ¬åœ°åŒ–å¾ˆé‡è¦ï¼Œèƒ½å¤Ÿæœ‰æ•ˆçš„é™ä½Ž Kubernetes çš„é‡‡çº³å’Œæ”¯æŒé—¨æ§›ã€‚å¦‚æžœèƒ½ç”¨æ¯è¯­é˜…è¯» Kubernetes æ–‡æ¡£ï¼Œå°±èƒ½æ›´è½»æ¾çš„å¼€å§‹ä½¿ç”¨ Kubernetesï¼Œå¹¶å¯¹å…¶å‘å±•ä½œå‡ºè´¡çŒ®ã€‚&lt;/p>
&lt;h2 id="å¦‚ä½•å¯åŠ¨æœ¬åœ°åŒ–å·¥ä½œ">å¦‚ä½•å¯åŠ¨æœ¬åœ°åŒ–å·¥ä½œ&lt;/h2>
&lt;p>ä¸åŒè¯­è¨€çš„æœ¬åœ°åŒ–å·¥ä½œéƒ½æ˜¯å•ç‹¬çš„åŠŸèƒ½â€”â€”å’Œå…¶å®ƒ Kubernetes åŠŸèƒ½ä¸€è‡´ï¼Œè´¡çŒ®è€…ä»¬åœ¨ä¸€ä¸ª SIG ä¸­è¿›è¡Œæœ¬åœ°åŒ–å·¥ä½œï¼Œåˆ†äº«å‡ºæ¥è¿›è¡Œè¯„å®¡ï¼Œå¹¶åŠ å…¥é¡¹ç›®ã€‚&lt;/p>
&lt;p>è´¡çŒ®è€…ä»¬åœ¨å›¢é˜Ÿä¸­è¿›è¡Œå†…å®¹çš„æœ¬åœ°åŒ–å·¥ä½œã€‚å› ä¸ºè‡ªå·±ä¸èƒ½æ‰¹å‡†è‡ªå·±çš„ PRï¼Œæ‰€ä»¥ä¸€ä¸ªæœ¬åœ°åŒ–å›¢é˜Ÿè‡³å°‘åº”è¯¥æœ‰ä¸¤ä¸ªäººâ€”â€”ä¾‹å¦‚æ„å¤§åˆ©æ–‡çš„æœ¬åœ°åŒ–å›¢é˜Ÿæœ‰ä¸¤ä¸ªäººã€‚è¿™ä¸ªå›¢é˜Ÿè§„æ¨¡å¯èƒ½å¾ˆå¤§ï¼šä¸­æ–‡å›¢é˜Ÿæœ‰å‡ åä¸ªæˆå‘˜ã€‚&lt;/p>
&lt;p>æ¯ä¸ªå›¢é˜Ÿéƒ½æœ‰è‡ªå·±çš„å·¥ä½œæµã€‚æœ‰äº›å›¢é˜Ÿæ‰‹å·¥å®Œæˆæ‰€æœ‰çš„å†…å®¹ç¿»è¯‘ï¼›æœ‰äº›ä¼šä½¿ç”¨å¸¦æœ‰ç¿»è¯‘æ’ä»¶çš„ç¼–è¯‘å™¨ï¼Œå¹¶ä½¿ç”¨è¯„å®¡æœºæ¥æä¾›æ­£ç¡®æ€§çš„ä¿éšœã€‚SIG Docs ä¸“æ³¨äºŽè¾“å‡ºçš„æ ‡å‡†ï¼›è¿™å°±ç»™äº†æœ¬åœ°åŒ–å›¢é˜Ÿé‡‡ç”¨é€‚åˆè‡ªå·±å·¥ä½œæƒ…å†µçš„å·¥ä½œæµã€‚è¿™æ ·ä¸€æ¥ï¼Œå›¢é˜Ÿå¯ä»¥æ ¹æ®æœ€ä½³å®žè·µè¿›è¡Œåä½œï¼Œå¹¶ä»¥ Kubernetes çš„ç¤¾åŒºç²¾ç¥žè¿›è¡Œåˆ†äº«ã€‚&lt;/p>
&lt;h2 id="ä¸ºæœ¬åœ°åŒ–å·¥ä½œæ·»ç –åŠ ç“¦">ä¸ºæœ¬åœ°åŒ–å·¥ä½œæ·»ç –åŠ ç“¦&lt;/h2>
&lt;p>å¦‚æžœä½ æœ‰å…´è¶£ä¸º Kubernetes æ–‡æ¡£åŠ å…¥æ–°è¯­ç§çš„æœ¬åœ°åŒ–å†…å®¹ï¼Œ&lt;a href="https://kubernetes.io/docs/contribute/localization/">Kubernetes contribution guide&lt;/a> ä¸­åŒ…å«äº†è¿™æ–¹é¢çš„ç›¸å…³å†…å®¹ã€‚&lt;/p>
&lt;p>å·²ç»å¯åŠ¨çš„çš„æœ¬åœ°åŒ–å·¥ä½œåŒæ ·éœ€è¦æ”¯æŒã€‚å¦‚æžœæœ‰å…´è¶£ä¸ºçŽ°å­˜é¡¹ç›®åšå‡ºè´¡çŒ®ï¼Œå¯ä»¥åŠ å…¥æœ¬åœ°åŒ–å›¢é˜Ÿçš„ Slack é¢‘é“ï¼ŒåŽ»åšä¸ªè‡ªæˆ‘ä»‹ç»ã€‚å„å›¢é˜Ÿçš„æˆå‘˜ä¼šå¸®åŠ©ä½ å¼€å§‹å·¥ä½œã€‚&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>è¯­ç§&lt;/th>
&lt;th>Slack é¢‘é“&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>ä¸­æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CE3LNFYJ1/">#kubernetes-docs-zh&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è‹±æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/">#sig-docs&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ³•æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CG838BFT9/">#kubernetes-docs-fr&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å¾·æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH4UJ2BAL/">#kubernetes-docs-de&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å°åœ°&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">#kubernetes-docs-hi&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>å°åº¦å°¼è¥¿äºšæ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ1LUCUHM/">#kubernetes-docs-id&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ„å¤§åˆ©æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CGB1MCK7X/">#kubernetes-docs-it&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>æ—¥æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CAG2M83S8/">#kubernetes-docs-ja&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>éŸ©æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CA1MMR86S/">#kubernetes-docs-ko&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è‘¡è„ç‰™æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CJ21AS0NA/">#kubernetes-docs-pt&lt;/a>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>è¥¿ç­ç‰™æ–‡&lt;/td>
&lt;td>&lt;a href="https://kubernetes.slack.com/messages/CH7GB2E3B/">#kubernetes-docs-es&lt;/a>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h2 id="ä¸‹ä¸€æ­¥">ä¸‹ä¸€æ­¥ï¼Ÿ&lt;/h2>
&lt;p>æœ€æ–°çš„&lt;a href="https://kubernetes.slack.com/messages/CJ14B9BDJ/">å°åœ°æ–‡æœ¬åœ°åŒ–&lt;/a>å·¥ä½œæ­£åœ¨å¯åŠ¨ã€‚ä¸ºä»€ä¹ˆä¸åŠ å…¥ä½ çš„è¯­è¨€ï¼Ÿ&lt;/p>
&lt;p>èº«ä¸º SIG Docs çš„ä¸»å¸­ï¼Œæˆ‘ç”šè‡³å¸Œæœ›æœ¬åœ°åŒ–å·¥ä½œè·³å‡ºæ–‡æ¡£èŒƒç•´ï¼Œç›´æŽ¥ä¸º Kubernetes ç»„ä»¶æä¾›æœ¬åœ°åŒ–æ”¯æŒã€‚æœ‰ä»€ä¹ˆç»„ä»¶æ˜¯ä½ å¸Œæœ›æ”¯æŒä¸åŒè¯­è¨€çš„ä¹ˆï¼Ÿå¯ä»¥æäº¤ä¸€ä¸ª &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposal&lt;/a> æ¥ä¿ƒæˆè¿™ä¸€è¿›æ­¥ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.14 ç¨³å®šæ€§æ”¹è¿›ä¸­çš„è¿›ç¨‹IDé™åˆ¶</title><link>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</link><pubDate>Mon, 15 Apr 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/04/15/kubernetes-1.14-%E7%A8%B3%E5%AE%9A%E6%80%A7%E6%94%B9%E8%BF%9B%E4%B8%AD%E7%9A%84%E8%BF%9B%E7%A8%8Bid%E9%99%90%E5%88%B6/</guid><description>
&lt;!--
---
title: 'Process ID Limiting for Stability Improvements in Kubernetes 1.14'
date: 2019-04-15
---
-->
&lt;!--
**Author: Derek Carr**
Have you ever seen someone take more than their fair share of the cookies? The one person who reaches in and grabs a half dozen fresh baked chocolate chip chunk morsels and skitters off like Cookie Monster exclaiming â€œOm nom nom nom.â€
In some rare workloads, a similar occurrence was taking place inside Kubernetes clusters. With each Pod and Node, there comes a finite number of possible process IDs (PIDs) for all applications to share. While it is rare for any one process or pod to reach in and grab all the PIDs, some users were experiencing resource starvation due to this type of behavior. So in Kubernetes 1.14, we introduced an enhancement to mitigate the risk of a single pod monopolizing all of the PIDs available.
-->
&lt;p>&lt;strong>ä½œè€…: Derek Carr&lt;/strong>&lt;/p>
&lt;p>ä½ æ˜¯å¦è§è¿‡æœ‰äººæ‹¿èµ°äº†æ¯”å±žäºŽä»–ä»¬é‚£ä¸€ä»½æ›´å¤šçš„é¥¼å¹²ï¼Ÿ ä¸€ä¸ªäººèµ°è¿‡æ¥ï¼ŒæŠ“èµ·åŠæ‰“æ–°é²œçƒ¤åˆ¶çš„å¤§å—å·§å…‹åŠ›é¥¼å¹²ç„¶åŽåŒ†åŒ†ç¦»åŽ»ï¼Œå°±åƒé¥¼å¹²æ€ªå…½å¤§å–Š â€œOm nom nom nomâ€ã€‚&lt;/p>
&lt;p>åœ¨ä¸€äº›ç½•è§çš„å·¥ä½œè´Ÿè½½ä¸­ï¼ŒKubernetes é›†ç¾¤å†…éƒ¨ä¹Ÿå‘ç”Ÿäº†ç±»ä¼¼çš„æƒ…å†µã€‚æ¯ä¸ª Pod å’Œ Node éƒ½æœ‰æœ‰é™æ•°é‡çš„å¯èƒ½çš„è¿›ç¨‹ IDï¼ˆPIDï¼‰ï¼Œä¾›æ‰€æœ‰åº”ç”¨ç¨‹åºå…±äº«ã€‚å°½ç®¡å¾ˆå°‘æœ‰è¿›ç¨‹æˆ– Pod èƒ½å¤Ÿè¿›å…¥å¹¶èŽ·å–æ‰€æœ‰ PIDï¼Œä½†ç”±äºŽè¿™ç§è¡Œä¸ºï¼Œä¸€äº›ç”¨æˆ·ä¼šé‡åˆ°èµ„æºåŒ®ä¹çš„æƒ…å†µã€‚ å› æ­¤ï¼Œåœ¨ Kubernetes 1.14 ä¸­ï¼Œæˆ‘ä»¬å¼•å…¥äº†ä¸€é¡¹å¢žå¼ºåŠŸèƒ½ï¼Œä»¥é™ä½Žå•ä¸ª Pod åž„æ–­æ‰€æœ‰å¯ç”¨ PID çš„é£Žé™©ã€‚&lt;/p>
&lt;!--
## Can You Spare Some PIDs?
Here, weâ€™re talking about the greed of certain containers. Outside the ideal, runaway processes occur from time to time, particularly in clusters where testing is taking place. Thus, some wildly non-production-ready activity is happening.
In such a scenario, itâ€™s possible for something akin to a fork bomb taking place inside a node. As resources slowly erode, being taken over by some zombie-like process that continually spawns children, other legitimate workloads begin to get bumped in favor of this inflating balloon of wasted processing power. This could result in other processes on the same pod being starved of their needed PIDs. It could also lead to interesting side effects as a node could fail and a replica of that pod is scheduled to a new machine where the process repeats across your entire cluster.
-->
&lt;h2 id="ä½ èƒ½é¢„ç•™ä¸€äº›-pids-å—">ä½ èƒ½é¢„ç•™ä¸€äº› PIDs å—ï¼Ÿ&lt;/h2>
&lt;p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬è°ˆè®ºçš„æ˜¯æŸäº›å®¹å™¨çš„è´ªå©ªæ€§ã€‚ åœ¨ç†æƒ³æƒ…å†µä¹‹å¤–ï¼Œå¤±æŽ§è¿›ç¨‹æœ‰æ—¶ä¼šå‘ç”Ÿï¼Œç‰¹åˆ«æ˜¯åœ¨æµ‹è¯•é›†ç¾¤ä¸­ã€‚ å› æ­¤ï¼Œåœ¨è¿™äº›é›†ç¾¤ä¸­ä¼šå‘ç”Ÿä¸€äº›æ··ä¹±çš„éžç”Ÿäº§çŽ¯å¢ƒå‡†å¤‡å°±ç»ªçš„äº‹æƒ…ã€‚&lt;/p>
&lt;p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå¯èƒ½ä¼šåœ¨èŠ‚ç‚¹å†…éƒ¨å‘ç”Ÿç±»ä¼¼äºŽ fork ç‚¸å¼¹è€—å°½ PID çš„æ”»å‡»ã€‚éšç€èµ„æºçš„ç¼“æ…¢è…èš€ï¼Œè¢«ä¸€äº›ä¸æ–­äº§ç”Ÿå­è¿›ç¨‹çš„åƒµå°¸èˆ¬çš„è¿›ç¨‹æ‰€æŽ¥ç®¡ï¼Œå…¶ä»–æ­£å¸¸çš„å·¥ä½œè´Ÿè½½ä¼šå› ä¸ºè¿™äº›åƒæ°”çƒèˆ¬ä¸æ–­è†¨èƒ€çš„æµªè´¹çš„å¤„ç†èƒ½åŠ›è€Œå¼€å§‹å—åˆ°å†²å‡»ã€‚è¿™å¯èƒ½å¯¼è‡´åŒä¸€ Pod ä¸Šçš„å…¶ä»–è¿›ç¨‹ç¼ºå°‘æ‰€éœ€çš„ PIDã€‚è¿™ä¹Ÿå¯èƒ½å¯¼è‡´æœ‰è¶£çš„å‰¯ä½œç”¨ï¼Œå› ä¸ºèŠ‚ç‚¹å¯èƒ½ä¼šå‘ç”Ÿæ•…éšœï¼Œå¹¶ä¸”è¯¥Podçš„å‰¯æœ¬å°†å®‰æŽ’åˆ°æ–°çš„æœºå™¨ä¸Šï¼Œè‡³æ­¤ï¼Œè¯¥è¿‡ç¨‹å°†åœ¨æ•´ä¸ªç¾¤é›†ä¸­é‡å¤è¿›è¡Œã€‚&lt;/p>
&lt;!--
## Fixing the Problem
Thus, in Kubernetes 1.14, we have added a feature that allows for the configuration of a kubelet to limit the number of PIDs a given pod can consume. If that machine supports 32,768 PIDs and 100 pods, one can give each pod a budget of 300 PIDs to prevent total exhaustion of PIDs. If the admin wants to overcommit PIDs similar to cpu or memory, they may do so as well with some additional risks. Either way, no one pod can bring the whole machine down. This will generally prevent against simple fork bombs from taking over your cluster.
This change allows administrators to protect one pod from another, but does not ensure if all pods on the machine can protect the node, and the node agents themselves from falling over. Thus, weâ€™ve introduced a feature in this release in alpha form that provides isolation of PIDs from end user workloads on a pod from the node agents (kubelet, runtime, etc.). The admin is able to reserve a specific number of PIDs--similar to how one reserves CPU or memory today--and ensure they are never consumed by pods on that machine. Once that graduates from alpha, to beta, then stable in future releases of Kubernetes, weâ€™ll have protection against an easily starved Linux resource.
Get started with [Kubernetes 1.14](https://github.com/kubernetes/kubernetes/releases/tag/v1.14.0).
-->
&lt;h2 id="è§£å†³é—®é¢˜">è§£å†³é—®é¢˜&lt;/h2>
&lt;p>å› æ­¤ï¼Œåœ¨ Kubernetes 1.14 ä¸­ï¼Œæˆ‘ä»¬æ·»åŠ äº†ä¸€ä¸ªç‰¹æ€§ï¼Œå…è®¸é€šè¿‡é…ç½® kubeletï¼Œé™åˆ¶ç»™å®š Pod å¯ä»¥æ¶ˆè€—çš„ PID æ•°é‡ã€‚å¦‚æžœè¯¥æœºå™¨æ”¯æŒ 32768 ä¸ª PIDs å’Œ 100 ä¸ª Podï¼Œåˆ™å¯ä»¥ä¸ºæ¯ä¸ª Pod æä¾› 300 ä¸ª PIDs çš„é¢„ç®—ï¼Œä»¥é˜²æ­¢ PIDs å®Œå…¨è€—å°½ã€‚å¦‚æžœç®¡ç†å‘˜æƒ³è¦åƒ CPU æˆ–å†…å­˜é‚£æ ·è¿‡åº¦ä½¿ç”¨ PIDsï¼Œé‚£ä¹ˆä»–ä»¬ä¹Ÿå¯ä»¥é…ç½®è¶…é¢ä½¿ç”¨ï¼Œä½†æ˜¯è¿™æ ·ä¼šæœ‰ä¸€äº›é¢å¤–é£Žé™©ã€‚ä¸ç®¡æ€Žæ ·ï¼Œæ²¡æœ‰ä¸€ä¸ªPodèƒ½æžåæ•´ä¸ªæœºå™¨ã€‚è¿™é€šå¸¸ä¼šé˜²æ­¢ç®€å•çš„åˆ†å‰å‡½æ•°ç‚¸å¼¹æŽ¥ç®¡ä½ çš„é›†ç¾¤ã€‚&lt;/p>
&lt;p>æ­¤æ›´æ”¹å…è®¸ç®¡ç†å‘˜ä¿æŠ¤ä¸€ä¸ª Pod ä¸å—å¦ä¸€ä¸ª Pod çš„å½±å“ï¼Œä½†ä¸èƒ½ç¡®ä¿è®¡ç®—æœºä¸Šçš„æ‰€æœ‰ Pod éƒ½èƒ½ä¿æŠ¤èŠ‚ç‚¹å’ŒèŠ‚ç‚¹ä»£ç†æœ¬èº«ä¸å—å½±å“ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨è¿™ä¸ªç‰ˆæœ¬ä¸­ä»¥ Alpha çš„å½¢å¼å¼•å…¥äº†è¿™ä¸ªä¸€ä¸ªç‰¹æ€§ï¼Œå®ƒæä¾›äº† PIDs åœ¨èŠ‚ç‚¹ä»£ç†ï¼ˆ kubeletã€runtime ç­‰ï¼‰ä¸Ž Pod ä¸Šçš„æœ€ç»ˆç”¨æˆ·å·¥ä½œè´Ÿè½½çš„åˆ†ç¦»ã€‚ç®¡ç†å‘˜å¯ä»¥é¢„å®šç‰¹å®šæ•°é‡çš„ pidï¼ˆç±»ä¼¼äºŽä»Šå¤©å¦‚ä½•é¢„å®š CPU æˆ–å†…å­˜ï¼‰ï¼Œå¹¶ç¡®ä¿å®ƒä»¬ä¸ä¼šè¢«è¯¥è®¡ç®—æœºä¸Šçš„ pod æ¶ˆè€—ã€‚ä¸€æ—¦ä»Ž Alpha è¿›å…¥åˆ° Betaï¼Œç„¶åŽåœ¨å°†æ¥çš„ Kubernetes ç‰ˆæœ¬ä¸­ç¨³å®šä¸‹æ¥ï¼Œæˆ‘ä»¬å°±å¯ä»¥ä½¿ç”¨è¿™ä¸ªç‰¹æ€§é˜²æ­¢ Linux èµ„æºè€—å°½ã€‚&lt;/p>
&lt;p>å¼€å§‹ä½¿ç”¨ &lt;a href="https://github.com/Kubernetes/Kubernetes/releases/tag/v1.14.0">Kubernetes 1.14&lt;/a>ã€‚&lt;/p>
&lt;!--
## Get Involved
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Node Special Interest Group](https://github.com/kubernetes/community/tree/master/sig-node).
### About the author:
Derek Carr is Senior Principal Software Engineer at Red Hat. He is a Kubernetes contributor and member of the Kubernetes Community Steering Committee.
-->
&lt;p>##å‚ä¸Žå…¶ä¸­&lt;/p>
&lt;p>å¦‚æžœæ‚¨å¯¹æ­¤ç‰¹æ€§æœ‰åé¦ˆæˆ–æœ‰å…´è¶£å‚ä¸Žå…¶è®¾è®¡ä¸Žå¼€å‘ï¼Œè¯·åŠ å…¥[èŠ‚ç‚¹ç‰¹åˆ«å…´è¶£å°ç»„](&lt;a href="https://github.com/kubernetes/community/tree/master/sig">https://github.com/kubernetes/community/tree/master/sig&lt;/a> Node)ã€‚&lt;/p>
&lt;p>###å…³äºŽä½œè€…ï¼š
Derek Carr æ˜¯ Red Hat é«˜çº§é¦–å¸­è½¯ä»¶å·¥ç¨‹å¸ˆã€‚ä»–ä¹Ÿæ˜¯ Kubernetes çš„è´¡çŒ®è€…å’Œ Kubernetes ç¤¾åŒºæŒ‡å¯¼å§”å‘˜ä¼šçš„æˆå‘˜ã€‚&lt;/p></description></item><item><title>Blog: Raw Block Volume æ”¯æŒè¿›å…¥ Beta</title><link>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</link><pubDate>Thu, 07 Mar 2019 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2019/03/07/raw-block-volume-%E6%94%AF%E6%8C%81%E8%BF%9B%E5%85%A5-beta/</guid><description>
&lt;!--
---
title: Raw Block Volume support to Beta
date: 2019-03-07
---
--->
&lt;!--
**Authors:**
Ben Swartzlander (NetApp), Saad Ali (Google)
Kubernetes v1.13 moves raw block volume support to beta. This feature allows persistent volumes to be exposed inside containers as a block device instead of as a mounted file system.
--->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong>
Ben Swartzlander (NetApp), Saad Ali (Google)&lt;/p>
&lt;p>Kubernetes v1.13 ä¸­å¯¹åŽŸç”Ÿæ•°æ®å—å·ï¼ˆRaw Block Volumeï¼‰çš„æ”¯æŒè¿›å…¥ Beta é˜¶æ®µã€‚æ­¤åŠŸèƒ½å…è®¸å°†æŒä¹…å·ä½œä¸ºå—è®¾å¤‡è€Œä¸æ˜¯ä½œä¸ºå·²æŒ‚è½½çš„æ–‡ä»¶ç³»ç»Ÿæš´éœ²åœ¨å®¹å™¨å†…éƒ¨ã€‚&lt;/p>
&lt;!--
## What are block devices?
Block devices enable random access to data in fixed-size blocks. Hard drives, SSDs, and CD-ROMs drives are all examples of block devices.
Typically persistent storage is implemented in a layered maner with a file system (like ext4) on top of a block device (like a spinning disk or SSD). Applications then read and write files instead of operating on blocks. The operating systems take care of reading and writing files, using the specified filesystem, to the underlying device as blocks.
It's worth noting that while whole disks are block devices, so are disk partitions, and so are LUNs from a storage area network (SAN) device.
--->
&lt;h2 id="ä»€ä¹ˆæ˜¯å—è®¾å¤‡">ä»€ä¹ˆæ˜¯å—è®¾å¤‡ï¼Ÿ&lt;/h2>
&lt;p>å—è®¾å¤‡å…è®¸å¯¹å›ºå®šå¤§å°çš„å—ä¸­çš„æ•°æ®è¿›è¡Œéšæœºè®¿é—®ã€‚ç¡¬ç›˜é©±åŠ¨å™¨ã€SSD å’Œ CD-ROM é©±åŠ¨å™¨éƒ½æ˜¯å—è®¾å¤‡çš„ä¾‹å­ã€‚&lt;/p>
&lt;p>é€šå¸¸ï¼ŒæŒä¹…æ€§æ€§å­˜å‚¨æ˜¯åœ¨é€šè¿‡åœ¨å—è®¾å¤‡ï¼ˆä¾‹å¦‚ç£ç›˜æˆ– SSDï¼‰ä¹‹ä¸Šæž„é€ æ–‡ä»¶ç³»ç»Ÿï¼ˆä¾‹å¦‚ ext4ï¼‰çš„åˆ†å±‚æ–¹å¼å®žçŽ°çš„ã€‚è¿™æ ·åº”ç”¨ç¨‹åºå°±å¯ä»¥è¯»å†™æ–‡ä»¶è€Œä¸æ˜¯æ“ä½œæ•°æ®å—è¿›ã€‚æ“ä½œç³»ç»Ÿè´Ÿè´£ä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶ç³»ç»Ÿå°†æ–‡ä»¶è¯»å†™è½¬æ¢ä¸ºå¯¹åº•å±‚è®¾å¤‡çš„æ•°æ®å—è¯»å†™ã€‚&lt;/p>
&lt;p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œæ•´ä¸ªç£ç›˜éƒ½æ˜¯å—è®¾å¤‡ï¼Œç£ç›˜åˆ†åŒºä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå­˜å‚¨åŒºåŸŸç½‘ç»œï¼ˆSANï¼‰è®¾å¤‡ä¸­çš„ LUN ä¹Ÿæ˜¯ä¸€æ ·çš„ã€‚&lt;/p>
&lt;!--
## Why add raw block volumes to kubernetes?
There are some specialized applications that require direct access to a block device because, for example, the file system layer introduces unneeded overhead. The most common case is databases, which prefer to organize their data directly on the underlying storage. Raw block devices are also commonly used by any software which itself implements some kind of storage service (software defined storage systems).
--->
&lt;h2 id="ä¸ºä»€ä¹ˆè¦å°†-raw-block-volume-æ·»åŠ åˆ°-kubernetes">ä¸ºä»€ä¹ˆè¦å°† raw block volume æ·»åŠ åˆ° kubernetesï¼Ÿ&lt;/h2>
&lt;p>æœ‰äº›ç‰¹æ®Šçš„åº”ç”¨ç¨‹åºéœ€è¦ç›´æŽ¥è®¿é—®å—è®¾å¤‡ï¼ŒåŽŸå› ä¾‹å¦‚ï¼Œæ–‡ä»¶ç³»ç»Ÿå±‚ä¼šå¼•å…¥ä¸å¿…è¦çš„å¼€é”€ã€‚æœ€å¸¸è§çš„æƒ…å†µæ˜¯æ•°æ®åº“ï¼Œé€šå¸¸ä¼šç›´æŽ¥åœ¨åº•å±‚å­˜å‚¨ä¸Šç»„ç»‡æ•°æ®ã€‚åŽŸç”Ÿçš„å—è®¾å¤‡ï¼ˆRaw Block Devicesï¼‰è¿˜é€šå¸¸ç”±èƒ½è‡ªå·±å®žçŽ°æŸç§å­˜å‚¨æœåŠ¡çš„è½¯ä»¶ï¼ˆè½¯ä»¶å®šä¹‰çš„å­˜å‚¨ç³»ç»Ÿï¼‰ä½¿ç”¨ã€‚&lt;/p>
&lt;!--
From a programmer's perspective, a block device is a very large array of bytes, usually with some minimum granularity for reads and writes, often 512 bytes, but frequently 4K or larger.
As it becomes more common to run database software and storage infrastructure software inside of Kubernetes, the need for raw block device support in Kubernetes becomes more important.
--->
&lt;p>ä»Žç¨‹åºå‘˜çš„è§’åº¦æ¥çœ‹ï¼Œå—è®¾å¤‡æ˜¯ä¸€ä¸ªéžå¸¸å¤§çš„å­—èŠ‚æ•°ç»„ï¼Œå…·æœ‰æŸç§æœ€å°è¯»å†™ç²’åº¦ï¼Œé€šå¸¸ä¸º 512 ä¸ªå­—èŠ‚ï¼Œå¤§éƒ¨åˆ†æƒ…å†µä¸º 4K æˆ–æ›´å¤§ã€‚&lt;/p>
&lt;p>éšç€åœ¨ Kubernetes ä¸­è¿è¡Œæ•°æ®åº“è½¯ä»¶å’Œå­˜å‚¨åŸºç¡€æž¶æž„è½¯ä»¶å˜å¾—è¶Šæ¥è¶Šæ™®éï¼Œåœ¨ Kubernetes ä¸­æ”¯æŒåŽŸç”Ÿå—è®¾å¤‡çš„éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚&lt;/p>
&lt;!--
## Which volume plugins support raw blocks?
As of the publishing of this blog, the following in-tree volumes types support raw blocks:
--->
&lt;h2 id="å“ªäº›å·æ’ä»¶æ”¯æŒ-raw-block">å“ªäº›å·æ’ä»¶æ”¯æŒ raw blockï¼Ÿ&lt;/h2>
&lt;p>åœ¨å‘å¸ƒæ­¤åšå®¢æ—¶ï¼Œä»¥ä¸‹ in-tree å·ç±»åž‹æ”¯æŒåŽŸç”Ÿå—è®¾å¤‡ï¼š&lt;/p>
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>Cinder&lt;/li>
&lt;li>Fibre Channel&lt;/li>
&lt;li>GCE PD&lt;/li>
&lt;li>iSCSI&lt;/li>
&lt;li>Local volumes&lt;/li>
&lt;li>RBD (Ceph)&lt;/li>
&lt;li>Vsphere&lt;/li>
&lt;/ul>
&lt;!--
Out-of-tree [CSI volume drivers](https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/) may also support raw block volumes. Kubernetes CSI support for raw block volumes is currently alpha. See documentation [here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;p>Out-of-tree &lt;a href="https://kubernetes.io/blog/2019/01/15/container-storage-interface-ga/">CSI å·é©±åŠ¨ç¨‹åº&lt;/a> å¯èƒ½ä¹Ÿæ”¯æŒåŽŸç”Ÿæ•°æ®å—å·ã€‚Kubernetes CSI å¯¹åŽŸç”Ÿæ•°æ®å—å·çš„æ”¯æŒç›®å‰ä¸º alpha é˜¶æ®µã€‚å‚è€ƒ &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">è¿™ç¯‡&lt;/a> æ–‡æ¡£ã€‚&lt;/p>
&lt;!--
## Kubernetes raw block volume API
Raw block volumes share a lot in common with ordinary volumes. Both are requested by creating `PersistentVolumeClaim` objects which bind to `PersistentVolume` objects, and are attached to Pods in Kubernetes by including them in the volumes array of the `PodSpec`.
There are 2 important differences however. First, to request a raw block `PersistentVolumeClaim`, you must set `volumeMode = "Block"` in the `PersistentVolumeClaimSpec`. Leaving `volumeMode` blank is the same as specifying `volumeMode = "Filesystem"` which results in the traditional behavior. `PersistentVolumes` also have a `volumeMode` field in their `PersistentVolumeSpec`, and `"Block"` type PVCs can only bind to `"Block"` type PVs and `"Filesystem"` PVCs can only bind to `"Filesystem"` PVs.
--->
&lt;h2 id="kubernetes-raw-block-volume-çš„-api">Kubernetes raw block volume çš„ API&lt;/h2>
&lt;p>åŽŸç”Ÿæ•°æ®å—å·ä¸Žæ™®é€šå­˜å‚¨å·æœ‰å¾ˆå¤šå…±åŒç‚¹ã€‚ä¸¤è€…éƒ½é€šè¿‡åˆ›å»ºä¸Ž &lt;code>PersistentVolume&lt;/code> å¯¹è±¡ç»‘å®šçš„ &lt;code>PersistentVolumeClaim&lt;/code> å¯¹è±¡å‘èµ·è¯·æ±‚ï¼Œå¹¶é€šè¿‡å°†å®ƒä»¬åŠ å…¥åˆ° &lt;code>PodSpec&lt;/code> çš„ volumes æ•°ç»„ä¸­æ¥è¿žæŽ¥åˆ° Kubernetes ä¸­çš„ Podã€‚&lt;/p>
&lt;p>ä½†æ˜¯æœ‰ä¸¤ä¸ªé‡è¦çš„åŒºåˆ«ã€‚é¦–å…ˆï¼Œè¦è¯·æ±‚åŽŸç”Ÿæ•°æ®å—è®¾å¤‡çš„ &lt;code>PersistentVolumeClaim&lt;/code> å¿…é¡»åœ¨ &lt;code>PersistentVolumeClaimSpec&lt;/code> ä¸­è®¾ç½® &lt;code>volumeMode = &amp;quot;Block&amp;quot;&lt;/code>ã€‚&lt;code>volumeMode&lt;/code> ä¸ºç©ºæ—¶ä¸Žä¼ ç»Ÿè®¾ç½®æ–¹å¼ä¸­çš„æŒ‡å®š &lt;code>volumeMode = &amp;quot;Filesystem&amp;quot;&lt;/code> æ˜¯ä¸€æ ·çš„ã€‚&lt;code>PersistentVolumes&lt;/code> åœ¨å…¶ &lt;code>PersistentVolumeSpec&lt;/code> ä¸­ä¹Ÿæœ‰ä¸€ä¸ª &lt;code>volumeMode&lt;/code> å­—æ®µï¼Œ&lt;code>&amp;quot;Block&amp;quot;&lt;/code> ç±»åž‹çš„ PVC åªèƒ½ç»‘å®šåˆ° &lt;code>&amp;quot;Block&amp;quot;&lt;/code> ç±»åž‹çš„ PV ä¸Šï¼Œè€Œ&lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> ç±»åž‹çš„ PVC åªèƒ½ç»‘å®šåˆ° &lt;code>&amp;quot;Filesystem&amp;quot;&lt;/code> PV ä¸Šã€‚&lt;/p>
&lt;!--
Secondly, when using a raw block volume in your Pods, you must specify a `VolumeDevice` in the Container portion of the `PodSpec` rather than a `VolumeMount`. `VolumeDevices` have `devicePaths` instead of `mountPaths`, and inside the container, applications will see a device at that path instead of a mounted file system.
Applications open, read, and write to the device node inside the container just like they would interact with any block device on a system in a non-containerized or virtualized context.
--->
&lt;p>å…¶æ¬¡ï¼Œåœ¨ Pod ä¸­ä½¿ç”¨åŽŸç”Ÿæ•°æ®å—å·æ—¶ï¼Œå¿…é¡»åœ¨ &lt;code>PodSpec&lt;/code> çš„ Container éƒ¨åˆ†æŒ‡å®šä¸€ä¸ª &lt;code>VolumeDevice&lt;/code>ï¼Œè€Œä¸æ˜¯ &lt;code>VolumeMount&lt;/code>ã€‚&lt;code>VolumeDevices&lt;/code> å…·å¤‡ &lt;code>devicePaths&lt;/code> è€Œä¸æ˜¯ &lt;code>mountPaths&lt;/code>ï¼Œåœ¨å®¹å™¨ä¸­ï¼Œåº”ç”¨ç¨‹åºå°†çœ‹åˆ°ä½äºŽè¯¥è·¯å¾„çš„è®¾å¤‡ï¼Œè€Œä¸æ˜¯æŒ‚è½½äº†çš„æ–‡ä»¶ç³»ç»Ÿã€‚&lt;/p>
&lt;p>åº”ç”¨ç¨‹åºæ‰“å¼€ã€è¯»å–å’Œå†™å…¥å®¹å™¨å†…çš„è®¾å¤‡èŠ‚ç‚¹ï¼Œå°±åƒå®ƒä»¬åœ¨éžå®¹å™¨åŒ–æˆ–è™šæ‹ŸçŽ¯å¢ƒä¸­ä¸Žç³»ç»Ÿä¸Šçš„ä»»ä½•å—è®¾å¤‡äº¤äº’ä¸€æ ·ã€‚&lt;/p>
&lt;!--
## Creating a new raw block PVC
First, ensure that the provisioner associated with the storage class you choose is one that support raw blocks. Then create the PVC.
--->
&lt;h2 id="åˆ›å»ºä¸€ä¸ªæ–°çš„åŽŸç”Ÿå—è®¾å¤‡-pvc">åˆ›å»ºä¸€ä¸ªæ–°çš„åŽŸç”Ÿå—è®¾å¤‡ PVC&lt;/h2>
&lt;p>é¦–å…ˆï¼Œè¯·ç¡®ä¿ä¸Žæ‚¨é€‰æ‹©çš„å­˜å‚¨ç±»å…³è”çš„é©±åŠ¨æ”¯æŒåŽŸç”Ÿå—è®¾å¤‡ã€‚ç„¶åŽåˆ›å»º PVCã€‚&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
name: my-pvc
spec:
accessModes:
- ReadWriteMany
volumeMode: Block
storageClassName: my-sc
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
## Using a raw block PVC
When you use the PVC in a pod definition, you get to choose the device path for the block device rather than the mount path for the file system.
--->
&lt;h2 id="ä½¿ç”¨åŽŸç”Ÿå—-pvc">ä½¿ç”¨åŽŸç”Ÿå— PVC&lt;/h2>
&lt;p>åœ¨ Pod å®šä¹‰ä¸­ä½¿ç”¨ PVC æ—¶ï¼Œéœ€è¦é€‰æ‹©å—è®¾å¤‡çš„è®¾å¤‡è·¯å¾„ï¼Œè€Œä¸æ˜¯æ–‡ä»¶ç³»ç»Ÿçš„å®‰è£…è·¯å¾„ã€‚&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: my-pod
spec:
containers:
- name: my-container
image: busybox
command:
- sleep
- â€œ3600â€
volumeDevices:
- devicePath: /dev/block
name: my-volume
imagePullPolicy: IfNotPresent
volumes:
- name: my-volume
persistentVolumeClaim:
claimName: my-pvc
&lt;/code>&lt;/pre>&lt;!--
## As a storage vendor, how do I add support for raw block devices to my CSI plugin?
Raw block support for CSI plugins is still alpha, but support can be added today. The [CSI specification](https://github.com/container-storage-interface/spec/blob/master/spec.md) details how to handle requests for volume that have the `BlockVolume` capability instead of the `MountVolume` capability. CSI plugins can support both kinds of volumes, or one or the other. For more details see [documentation here](https://kubernetes-csi.github.io/docs/raw-block.html).
--->
&lt;h2 id="ä½œä¸ºå­˜å‚¨ä¾›åº”å•†-æˆ‘å¦‚ä½•åœ¨-csi-æ’ä»¶ä¸­æ·»åŠ å¯¹åŽŸç”Ÿå—è®¾å¤‡çš„æ”¯æŒ">ä½œä¸ºå­˜å‚¨ä¾›åº”å•†ï¼Œæˆ‘å¦‚ä½•åœ¨ CSI æ’ä»¶ä¸­æ·»åŠ å¯¹åŽŸç”Ÿå—è®¾å¤‡çš„æ”¯æŒï¼Ÿ&lt;/h2>
&lt;p>CSI æ’ä»¶çš„åŽŸç”Ÿå—æ”¯æŒä»ç„¶æ˜¯ alpha ç‰ˆæœ¬ï¼Œä½†æ˜¯çŽ°åœ¨å¯ä»¥æ”¹è¿›äº†ã€‚&lt;a href="https://github.com/container-storage-interface/spec/blob/master/spec.md">CSI è§„èŒƒ&lt;/a> è¯¦ç»†è¯´æ˜Žäº†å¦‚ä½•å¤„ç†å…·æœ‰ &lt;code>BlockVolume&lt;/code> èƒ½åŠ›è€Œä¸æ˜¯ &lt;code>MountVolume&lt;/code> èƒ½åŠ›çš„å·çš„è¯·æ±‚ã€‚CSI æ’ä»¶å¯ä»¥æ”¯æŒä¸¤ç§ç±»åž‹çš„å·ï¼Œä¹Ÿå¯ä»¥æ”¯æŒå…¶ä¸­ä¸€ç§æˆ–å¦ä¸€ç§ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ &lt;a href="https://kubernetes-csi.github.io/docs/raw-block.html">è¿™ä¸ªæ–‡æ¡£&lt;/a>ã€‚&lt;/p>
&lt;!--
## Issues/gotchas
Because block devices are actually devices, itâ€™s possible to do low-level actions on them from inside containers that wouldnâ€™t be possible with file system volumes. For example, block devices that are actually SCSI disks support sending SCSI commands to the device using Linux ioctls.
--->
&lt;h2 id="é—®é¢˜-é™·é˜±">é—®é¢˜/é™·é˜±&lt;/h2>
&lt;p>ç”±äºŽå—è®¾å¤‡å®žè´¨ä¸Šè¿˜æ˜¯è®¾å¤‡ï¼Œå› æ­¤å¯ä»¥ä»Žå®¹å™¨å†…éƒ¨å¯¹å…¶è¿›è¡Œåº•å±‚æ“ä½œï¼Œè€Œæ–‡ä»¶ç³»ç»Ÿçš„å·åˆ™æ— æ³•æ‰§è¡Œè¿™äº›æ“ä½œã€‚ä¾‹å¦‚ï¼Œå®žé™…ä¸Šæ˜¯å—è®¾å¤‡çš„ SCSI ç£ç›˜æ”¯æŒä½¿ç”¨ Linux ioctl å‘è®¾å¤‡å‘é€ SCSI å‘½ä»¤ã€‚&lt;/p>
&lt;!--
By default, Linux wonâ€™t allow containers to send SCSI commands to disks from inside containers though. In order to do so, you must grant the `SYS_RAWIO` capability to the container security context to allow this. See documentation [here](/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container).
Also, while Kubernetes is guaranteed to deliver a block device to the container, thereâ€™s no guarantee that itâ€™s actually a SCSI disk or any other kind of disk for that matter. The user must either ensure that the desired disk type is used with his pods, or only deploy applications that can handle a variety of block device types.
--->
&lt;p>é»˜è®¤æƒ…å†µä¸‹ï¼ŒLinux ä¸å…è®¸å®¹å™¨å°† SCSI å‘½ä»¤ä»Žå®¹å™¨å†…éƒ¨å‘é€åˆ°ç£ç›˜ã€‚ä¸ºæ­¤ï¼Œå¿…é¡»å‘å®¹å™¨å®‰å…¨å±‚çº§è®¤è¯ &lt;code>SYS_RAWIO&lt;/code> åŠŸèƒ½å®žçŽ°è¿™ç§è¡Œä¸ºã€‚è¯·å‚é˜… &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/security-context/#set-capabilities-for-a-container">è¿™ç¯‡&lt;/a> æ–‡æ¡£ã€‚&lt;/p>
&lt;p>å¦å¤–ï¼Œå°½ç®¡ Kubernetes ä¿è¯å¯ä»¥å°†å—è®¾å¤‡äº¤ä»˜åˆ°å®¹å™¨ä¸­ï¼Œä½†ä¸èƒ½ä¿è¯å®ƒå®žé™…ä¸Šæ˜¯ SCSI ç£ç›˜æˆ–ä»»ä½•å…¶ä»–ç±»åž‹çš„ç£ç›˜ã€‚ç”¨æˆ·å¿…é¡»ç¡®ä¿æ‰€éœ€çš„ç£ç›˜ç±»åž‹ä¸Ž Pod ä¸€èµ·ä½¿ç”¨ï¼Œæˆ–åªéƒ¨ç½²å¯ä»¥å¤„ç†å„ç§å—è®¾å¤‡ç±»åž‹çš„åº”ç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
## How can I learn more?
Check out additional documentation on the snapshot feature here: [Raw Block Volume Support](/docs/concepts/storage/persistent-volumes/#raw-block-volume-support)
How do I get involved?
Join the Kubernetes storage SIG and the CSI community and help us add more great features and improve existing ones like raw block storage!
--->
&lt;h2 id="å¦‚ä½•å­¦ä¹ æ›´å¤š">å¦‚ä½•å­¦ä¹ æ›´å¤šï¼Ÿ&lt;/h2>
&lt;p>åœ¨æ­¤å¤„æŸ¥çœ‹æœ‰å…³ snapshot åŠŸèƒ½çš„å…¶ä»–æ–‡æ¡£ï¼š&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#raw-block-volume-support">Raw Block Volume æ”¯æŒ&lt;/a>&lt;/p>
&lt;p>å¦‚ä½•å‚ä¸Žè¿›æ¥ï¼Ÿ&lt;/p>
&lt;p>åŠ å…¥ Kubernetes å­˜å‚¨ SIG å’Œ CSI ç¤¾åŒºï¼Œå¸®åŠ©æˆ‘ä»¬æ·»åŠ æ›´å¤šå‡ºè‰²çš„åŠŸèƒ½å¹¶æ”¹è¿›çŽ°æœ‰åŠŸèƒ½ï¼Œå°±åƒ raw block å­˜å‚¨ä¸€æ ·ï¼&lt;/p>
&lt;p>&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">https://github.com/kubernetes/community/tree/master/sig-storage&lt;/a>
&lt;a href="https://github.com/container-storage-interface/community/blob/master/README.md">https://github.com/container-storage-interface/community/blob/master/README.md&lt;/a>&lt;/p>
&lt;!--
Special thanks to all the contributors who helped add block volume support to Kubernetes including:
--->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢æ‰€æœ‰ä¸º Kubernetes å¢žåŠ  block volume æ”¯æŒçš„è´¡çŒ®è€…ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>Ben Swartzlander (&lt;a href="https://github.com/bswartz">https://github.com/bswartz&lt;/a>)&lt;/li>
&lt;li>Brad Childs (&lt;a href="https://github.com/childsb">https://github.com/childsb&lt;/a>)&lt;/li>
&lt;li>Erin Boyd (&lt;a href="https://github.com/erinboyd">https://github.com/erinboyd&lt;/a>)&lt;/li>
&lt;li>Masaki Kimura (&lt;a href="https://github.com/mkimuram">https://github.com/mkimuram&lt;/a>)&lt;/li>
&lt;li>Matthew Wong (&lt;a href="https://github.com/wongma7">https://github.com/wongma7&lt;/a>)&lt;/li>
&lt;li>Michelle Au (&lt;a href="https://github.com/msau42">https://github.com/msau42&lt;/a>)&lt;/li>
&lt;li>Mitsuhiro Tanino (&lt;a href="https://github.com/mtanino">https://github.com/mtanino&lt;/a>)&lt;/li>
&lt;li>Saad Ali (&lt;a href="https://github.com/saad-ali">https://github.com/saad-ali&lt;/a>)&lt;/li>
&lt;/ul></description></item><item><title>Blog: æ–°è´¡çŒ®è€…å·¥ä½œåŠä¸Šæµ·ç«™</title><link>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</link><pubDate>Wed, 05 Dec 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/12/05/%E6%96%B0%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B7%A5%E4%BD%9C%E5%9D%8A%E4%B8%8A%E6%B5%B7%E7%AB%99/</guid><description>
&lt;!--
---
layout: blog
title: 'New Contributor Workshop Shanghai'
date: 2018-12-05
---
-->
&lt;!--
**Authors**: Josh Berkus (Red Hat), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ZTE)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Josh Berkus (çº¢å¸½), Yang Li (The Plant), Puja Abbassi (Giant Swarm), XiangPeng Zhao (ä¸­å…´é€šè®¯)&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon Shanghai New Contributor Summit attendees. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/attendees.png"
alt="KubeCon ä¸Šæµ·ç«™æ–°è´¡çŒ®è€…å³°ä¼šä¸Žä¼šè€…ï¼Œæ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>KubeCon ä¸Šæµ·ç«™æ–°è´¡çŒ®è€…å³°ä¼šä¸Žä¼šè€…ï¼Œæ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
We recently completed our first New Contributor Summit in China, at the first KubeCon in China. It was very exciting to see all of the Chinese and Asian developers (plus a few folks from around the world) interested in becoming contributors. Over the course of a long day, they learned how, why, and where to contribute to Kubernetes, created pull requests, attended a panel of current contributors, and got their CLAs signed.
-->
&lt;p>æœ€è¿‘ï¼Œåœ¨ä¸­å›½çš„é¦–æ¬¡ KubeCon ä¸Šï¼Œæˆ‘ä»¬å®Œæˆäº†åœ¨ä¸­å›½çš„é¦–æ¬¡æ–°è´¡çŒ®è€…å³°ä¼šã€‚çœ‹åˆ°æ‰€æœ‰ä¸­å›½å’Œäºšæ´²çš„å¼€å‘è€…ï¼ˆä»¥åŠæ¥è‡ªä¸–ç•Œå„åœ°çš„ä¸€äº›äººï¼‰æœ‰å…´è¶£æˆä¸ºè´¡çŒ®è€…ï¼Œè¿™ä»¤äººéžå¸¸å…´å¥‹ã€‚åœ¨é•¿è¾¾ä¸€å¤©çš„è¯¾ç¨‹ä¸­ï¼Œä»–ä»¬äº†è§£äº†å¦‚ä½•ã€ä¸ºä»€ä¹ˆä»¥åŠåœ¨ä½•å¤„ä¸º Kubernetes ä½œå‡ºè´¡çŒ®ï¼Œåˆ›å»ºäº† PRï¼Œå‚åŠ äº†è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºï¼Œå¹¶ç­¾ç½²äº†ä»–ä»¬çš„ CLAã€‚&lt;/p>
&lt;!--
This was our second New Contributor Workshop (NCW), building on the one created and led by SIG Contributor Experience members in Copenhagen. Because of the audience, it was held in both Chinese and English, taking advantage of the superb simultaneous interpretation services the CNCF sponsored. Likewise, the NCW team included both English and Chinese-speaking members of the community: Yang Li, XiangPeng Zhao, Puja Abbassi, Noah Abrahams, Tim Pepper, Zach Corleissen, Sen Lu, and Josh Berkus. In addition to presenting and helping students, the bilingual members of the team translated all of the slides into Chinese. Fifty-one students attended.
-->
&lt;p>è¿™æ˜¯æˆ‘ä»¬çš„ç¬¬äºŒå±Šæ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ˆNCWï¼‰ï¼Œå®ƒç”±å‰ä¸€æ¬¡è´¡çŒ®è€…ä½“éªŒ SIG æˆå‘˜åˆ›å»ºå’Œé¢†å¯¼çš„å“¥æœ¬å“ˆæ ¹ç ”è®¨ä¼šå»¶ä¼¸è€Œæ¥ã€‚æ ¹æ®å—ä¼—æƒ…å†µï¼Œæœ¬æ¬¡æ´»åŠ¨é‡‡ç”¨äº†ä¸­è‹±æ–‡ä¸¤ç§è¯­è¨€ï¼Œå……åˆ†åˆ©ç”¨äº† CNCF èµžåŠ©çš„ä¸€æµçš„åŒå£°ä¼ è¯‘æœåŠ¡ã€‚åŒæ ·ï¼ŒNCW å›¢é˜Ÿç”±ç¤¾åŒºæˆå‘˜ç»„æˆï¼Œæ—¢æœ‰è¯´è‹±è¯­çš„ï¼Œä¹Ÿæœ‰è¯´æ±‰è¯­çš„ï¼šYang Liã€XiangPeng Zhaoã€Puja Abbassiã€Noah Abrahamsã€Tim Pepperã€Zach Corleissenã€Sen Lu å’Œ Josh Berkusã€‚é™¤äº†æ¼”è®²å’Œå¸®åŠ©å­¦å‘˜å¤–ï¼Œå›¢é˜Ÿçš„åŒè¯­æˆå‘˜è¿˜å°†æ‰€æœ‰å¹»ç¯ç‰‡ç¿»è¯‘æˆäº†ä¸­æ–‡ã€‚å…±æœ‰äº”åä¸€åå­¦å‘˜å‚åŠ ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams explains Kubernetes communications channels. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/noahabrahams.png"
alt="Noah Abrahams è®²è§£ Kubernetes æ²Ÿé€šæ¸ é“ã€‚æ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>Noah Abrahams è®²è§£ Kubernetes æ²Ÿé€šæ¸ é“ã€‚æ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The NCW takes participants through the stages of contributing to Kubernetes, starting from deciding where to contribute, followed by an introduction to the SIG system and our repository structure. We also have "guest speakers" from Docs and Test Infrastructure who cover contributing in those areas. We finally wind up with some hands-on exercises in filing issues and creating and approving PRs.
-->
&lt;p>NCW è®©å‚ä¸Žè€…å®Œæˆäº†ä¸º Kubernetes ä½œå‡ºè´¡çŒ®çš„å„ä¸ªé˜¶æ®µï¼Œä»Žå†³å®šåœ¨å“ªé‡Œä½œå‡ºè´¡çŒ®å¼€å§‹ï¼ŒæŽ¥ç€ä»‹ç»äº† SIG ç³»ç»Ÿå’Œæˆ‘ä»¬çš„ä»£ç ä»“åº“ç»“æž„ã€‚æˆ‘ä»¬è¿˜æœ‰æ¥è‡ªæ–‡æ¡£å’Œæµ‹è¯•åŸºç¡€è®¾æ–½é¢†åŸŸçš„ã€Œå®¢åº§è®²è€…ã€ï¼Œä»–ä»¬è´Ÿè´£è®²è§£æœ‰å…³çš„è´¡çŒ®ã€‚æœ€åŽï¼Œæˆ‘ä»¬åœ¨åˆ›å»º issueã€æäº¤å¹¶æ‰¹å‡† PR çš„å®žè·µç»ƒä¹ åŽï¼Œç»“æŸäº†å·¥ä½œåŠã€‚&lt;/p>
&lt;!--
Those hands-on exercises use a repository known as [the contributor playground](https://github.com/kubernetes-sigs/contributor-playground), created by SIG Contributor Experience as a place for new contributors to try out performing various actions on a Kubernetes repo. It has modified Prow and Tide automation, uses Owners files like in the real repositories. This lets students learn how the mechanics of contributing to our repositories work without disrupting normal development.
-->
&lt;p>è¿™äº›å®žè·µç»ƒä¹ ä½¿ç”¨ä¸€ä¸ªåä¸º&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">è´¡çŒ®è€…æ¸¸ä¹åœº&lt;/a>çš„ä»£ç ä»“åº“ï¼Œç”±è´¡çŒ®è€…ä½“éªŒ SIG åˆ›å»ºï¼Œè®©æ–°è´¡çŒ®è€…å°è¯•åœ¨ä¸€ä¸ª Kubernetes ä»“åº“ä¸­æ‰§è¡Œå„ç§æ“ä½œã€‚å®ƒä¿®æ”¹äº† Prow å’Œ Tide è‡ªåŠ¨åŒ–ï¼Œä½¿ç”¨ä¸ŽçœŸå®žä»£ç ä»“åº“ç±»ä¼¼çš„ Owners æ–‡ä»¶ã€‚è¿™å¯ä»¥è®©å­¦å‘˜äº†è§£ä¸ºæˆ‘ä»¬çš„ä»“åº“åšå‡ºè´¡çŒ®çš„æœ‰å…³æœºåˆ¶ï¼ŒåŒæ—¶åˆä¸å¦¨ç¢æ­£å¸¸çš„å¼€å‘æµç¨‹ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus"/> &lt;figcaption>
&lt;p>Yang Li talks about getting your PRs reviewed. Photo by Josh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/yangli.png"
alt="Yang Li è®²åˆ°å¦‚ä½•è®©ä½ çš„ PR é€šè¿‡è¯„å®¡ã€‚æ‘„å½±ï¼šJosh Berkus"/> &lt;figcaption>
&lt;p>Yang Li è®²åˆ°å¦‚ä½•è®©ä½ çš„ PR é€šè¿‡è¯„å®¡ã€‚æ‘„å½±ï¼šJosh Berkus&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
Both the "Great Firewall" and the language barrier prevent contributing Kubernetes from China from being straightforward. What's more, because open source business models are not mature in China, the time for employees work on open source projects is limited.
-->
&lt;p>ã€Œé˜²ç«é•¿åŸŽã€å’Œè¯­è¨€éšœç¢éƒ½ä½¿å¾—åœ¨ä¸­å›½ä¸º Kubernetes ä½œå‡ºè´¡çŒ®å˜å¾—å›°éš¾ã€‚è€Œä¸”ï¼Œä¸­å›½çš„å¼€æºå•†ä¸šæ¨¡å¼å¹¶ä¸æˆç†Ÿï¼Œå‘˜å·¥åœ¨å¼€æºé¡¹ç›®ä¸Šå·¥ä½œçš„æ—¶é—´æœ‰é™ã€‚&lt;/p>
&lt;!--
Chinese engineers are eager to participate in the development of Kubernetes, but many of them don't know where to start since Kubernetes is such a large project. With this workshop, we hope to help those who want to contribute, whether they wish to fix some bugs they encountered, improve or localize documentation, or they need to work with Kubernetes at their work. We are glad to see more and more Chinese contributors joining the community in the past few years, and we hope to see more of them in the future.
-->
&lt;p>ä¸­å›½å·¥ç¨‹å¸ˆæ¸´æœ›å‚ä¸Ž Kubernetes çš„ç ”å‘ï¼Œä½†ä»–ä»¬ä¸­çš„è®¸å¤šäººä¸çŸ¥é“ä»Žä½•å¤„å¼€å§‹ï¼Œå› ä¸º Kubernetes æ˜¯ä¸€ä¸ªå¦‚æ­¤åºžå¤§çš„é¡¹ç›®ã€‚é€šè¿‡æœ¬æ¬¡å·¥ä½œåŠï¼Œæˆ‘ä»¬å¸Œæœ›å¸®åŠ©é‚£äº›æƒ³è¦å‚ä¸Žè´¡çŒ®çš„äººï¼Œä¸è®ºä»–ä»¬å¸Œæœ›ä¿®å¤ä»–ä»¬é‡åˆ°çš„ä¸€äº›é”™è¯¯ã€æ”¹è¿›æˆ–æœ¬åœ°åŒ–æ–‡æ¡£ï¼Œæˆ–è€…ä»–ä»¬éœ€è¦åœ¨å·¥ä½œä¸­ç”¨åˆ° Kubernetesã€‚æˆ‘ä»¬å¾ˆé«˜å…´çœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„ä¸­å›½è´¡çŒ®è€…åœ¨è¿‡åŽ»å‡ å¹´é‡ŒåŠ å…¥ç¤¾åŒºï¼Œæˆ‘ä»¬ä¹Ÿå¸Œæœ›å°†æ¥å¯ä»¥çœ‹åˆ°æ›´å¤šã€‚&lt;/p>
&lt;!--
"I have been participating in the Kubernetes community for about three years," said XiangPeng Zhao. "In the community, I notice that more and more Chinese developers are showing their interest in contributing to Kubernetes. However, it's not easy to start contributing to such a project. I tried my best to help those who I met in the community, but I think there might still be some new contributors leaving the community due to not knowing where to get help when in trouble. Fortunately, the community initiated NCW at KubeCon Copenhagen and held a second one at KubeCon Shanghai. I was so excited to be invited by Josh Berkus to help organize this workshop. During the workshop, I met community friends in person, mentored attendees in the exercises, and so on. All of this was a memorable experience for me. I also learned a lot as a contributor who already has years of contributing experience. I wish I had attended such a workshop when I started contributing to Kubernetes years ago."
-->
&lt;p>ã€Œæˆ‘å·²ç»å‚ä¸Žäº† Kubernetes ç¤¾åŒºå¤§çº¦ä¸‰å¹´ã€ï¼ŒXiangPeng Zhao è¯´ï¼Œã€Œåœ¨ç¤¾åŒºï¼Œæˆ‘æ³¨æ„åˆ°è¶Šæ¥è¶Šå¤šçš„ä¸­å›½å¼€å‘è€…è¡¨çŽ°å‡ºå¯¹ Kubernetes è´¡çŒ®çš„å…´è¶£ã€‚ä½†æ˜¯ï¼Œå¼€å§‹ä¸ºè¿™æ ·ä¸€ä¸ªé¡¹ç›®åšè´¡çŒ®å¹¶ä¸å®¹æ˜“ã€‚æˆ‘å°½åŠ›å¸®åŠ©é‚£äº›æˆ‘åœ¨ç¤¾åŒºé‡åˆ°çš„äººï¼Œä½†æ˜¯ï¼Œæˆ‘è®¤ä¸ºå¯èƒ½ä»æœ‰ä¸€äº›æ–°çš„è´¡çŒ®è€…ç¦»å¼€ç¤¾åŒºï¼Œå› ä¸ºä»–ä»¬åœ¨é‡åˆ°éº»çƒ¦æ—¶ä¸çŸ¥é“ä»Žå“ªé‡ŒèŽ·å¾—å¸®åŠ©ã€‚å¹¸è¿çš„æ˜¯ï¼Œç¤¾åŒºåœ¨ KubeCon å“¥æœ¬å“ˆæ ¹ç«™å‘èµ·äº† NCWï¼Œå¹¶åœ¨ KubeCon ä¸Šæµ·ç«™ä¸¾åŠžäº†ç¬¬äºŒå±Šã€‚æˆ‘å¾ˆé«˜å…´å—åˆ° Josh Berkus çš„é‚€è¯·ï¼Œå¸®åŠ©ç»„ç»‡è¿™ä¸ªå·¥ä½œåŠã€‚åœ¨å·¥ä½œåŠæœŸé—´ï¼Œæˆ‘å½“é¢è§åˆ°äº†ç¤¾åŒºé‡Œçš„æœ‹å‹ï¼Œåœ¨ç»ƒä¹ ä¸­æŒ‡å¯¼äº†ä¸Žä¼šè€…ï¼Œç­‰ç­‰ã€‚æ‰€æœ‰è¿™äº›å¯¹æˆ‘æ¥è¯´éƒ½æ˜¯éš¾å¿˜çš„ç»åŽ†ã€‚ä½œä¸ºæœ‰ç€å¤šå¹´è´¡çŒ®è€…ç»éªŒçš„æˆ‘ï¼Œä¹Ÿå­¦ä¹ åˆ°äº†å¾ˆå¤šã€‚æˆ‘å¸Œæœ›å‡ å¹´å‰æˆ‘å¼€å§‹ä¸º Kubernetes åšè´¡çŒ®æ—¶å‚åŠ è¿‡è¿™æ ·çš„å·¥ä½œåŠã€ã€‚&lt;/p>
&lt;!--
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="Panel of contributors. Photo by Jerry Zhang"/> &lt;figcaption>
&lt;p>Panel of contributors. Photo by Jerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
-->
&lt;figure>
&lt;img src="https://kubernetes.io/images/blog/2018-12-05-new-contributor-shanghai/panel.png"
alt="è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºã€‚æ‘„å½±ï¼šJerry Zhang"/> &lt;figcaption>
&lt;p>è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºã€‚æ‘„å½±ï¼šJerry Zhang&lt;/p>
&lt;/figcaption>
&lt;/figure>
&lt;!--
The workshop ended with a panel of current contributors, featuring Lucas KÃ¤ldstrÃ¶m, Janet Kuo, Da Ma, Pengfei Ni, Zefeng Wang, and Chao Xu. The panel aimed to give both new and current contributors a look behind the scenes on the day-to-day of some of the most active contributors and maintainers, both from China and around the world. Panelists talked about where to begin your contributor's journey, but also how to interact with reviewers and maintainers. They further touched upon the main issues of contributing from China and gave attendees an outlook into exciting features they can look forward to in upcoming releases of Kubernetes.
-->
&lt;p>å·¥ä½œåŠä»¥çŽ°æœ‰è´¡çŒ®è€…åœ†æ¡Œè®¨è®ºç»“æŸï¼Œå˜‰å®¾åŒ…æ‹¬ Lucas KÃ¤ldstrÃ¶mã€Janet Kuoã€Da Maã€Pengfei Niã€Zefeng Wang å’Œ Chao Xuã€‚è¿™åœºåœ†æ¡Œè®¨è®ºæ—¨åœ¨è®©æ–°çš„å’ŒçŽ°æœ‰çš„è´¡çŒ®è€…äº†è§£ä¸€äº›æœ€æ´»è·ƒçš„è´¡çŒ®è€…å’Œç»´æŠ¤è€…çš„å¹•åŽæ—¥å¸¸å·¥ä½œï¼Œä¸è®ºä»–ä»¬æ¥è‡ªä¸­å›½è¿˜æ˜¯ä¸–ç•Œå„åœ°ã€‚å˜‰å®¾ä»¬è®¨è®ºäº†ä»Žå“ªé‡Œå¼€å§‹è´¡çŒ®è€…çš„æ—…ç¨‹ï¼Œä»¥åŠå¦‚ä½•ä¸Žè¯„å®¡è€…å’Œç»´æŠ¤è€…è¿›è¡Œäº’åŠ¨ã€‚ä»–ä»¬è¿›ä¸€æ­¥æŽ¢è®¨äº†åœ¨ä¸­å›½å‚ä¸Žè´¡çŒ®çš„ä¸»è¦é—®é¢˜ï¼Œå¹¶å‘ä¸Žä¼šè€…é¢„å‘Šäº†åœ¨ Kubernetes çš„æœªæ¥ç‰ˆæœ¬ä¸­å¯ä»¥æœŸå¾…çš„ä»¤äººå…´å¥‹çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
After the workshop, XiangPeng Zhao chatted with some attendees on WeChat and Twitter about their experiences. They were very glad to have attended the NCW and had some suggestions on improving the workshop. One attendee, Mohammad, said, "I had a great time at the workshop and learned a lot about the entire process of k8s for a contributor." Another attendee, Jie Jia, said, "The workshop was wonderful. It systematically explained how to contribute to Kubernetes. The attendee could understand the process even if s/he knew nothing about that before. For those who were already contributors, they could also learn something new. Furthermore, I could make new friends from inside or outside of China in the workshop. It was awesome!"
-->
&lt;p>å·¥ä½œåŠç»“æŸåŽï¼ŒXiangPeng Zhao å’Œä¸€äº›ä¸Žä¼šè€…å°±ä»–ä»¬çš„ç»åŽ†åœ¨å¾®ä¿¡å’Œ Twitter ä¸Šè¿›è¡Œäº†äº¤è°ˆã€‚ä»–ä»¬å¾ˆé«˜å…´å‚åŠ äº† NCWï¼Œå¹¶å°±æ”¹è¿›å·¥ä½œåŠæå‡ºäº†ä¸€äº›å»ºè®®ã€‚ä¸€ä½åå« Mohammad çš„ä¸Žä¼šè€…è¯´ï¼šã€Œæˆ‘åœ¨å·¥ä½œåŠä¸ŠçŽ©å¾—å¾ˆå¼€å¿ƒï¼Œå­¦ä¹ äº†å‚ä¸Ž k8s è´¡çŒ®çš„æ•´ä¸ªè¿‡ç¨‹ã€‚ã€å¦ä¸€ä½ä¸Žä¼šè€… Jie Jia è¯´ï¼šã€Œå·¥ä½œåŠéžå¸¸ç²¾å½©ã€‚å®ƒç³»ç»Ÿåœ°è§£é‡Šäº†å¦‚ä½•ä¸º Kubernetes åšå‡ºè´¡çŒ®ã€‚å³ä½¿å‚ä¸Žè€…ä¹‹å‰å¯¹æ­¤ä¸€æ— æ‰€çŸ¥ï¼Œä»–ï¼ˆå¥¹ï¼‰ä¹Ÿå¯ä»¥ç†è§£è¿™ä¸ªè¿‡ç¨‹ã€‚å¯¹äºŽé‚£äº›å·²ç»æ˜¯è´¡çŒ®è€…çš„äººï¼Œä»–ä»¬ä¹Ÿå¯ä»¥å­¦ä¹ åˆ°æ–°ä¸œè¥¿ã€‚æ­¤å¤–ï¼Œæˆ‘è¿˜å¯ä»¥åœ¨å·¥ä½œåŠä¸Šç»“è¯†æ¥è‡ªå›½å†…å¤–çš„æ–°æœ‹å‹ã€‚çœŸæ˜¯æ£’æžäº†ï¼ã€&lt;/p>
&lt;!--
SIG Contributor Experience will continue to run New Contributor Workshops at each upcoming KubeCon, including Seattle, Barcelona, and the return to Shanghai in June 2019. If you failed to get into one this year, register for one at a future KubeCon. And, when you meet an NCW attendee, make sure to welcome them to the community.
-->
&lt;p>è´¡çŒ®è€…ä½“éªŒ SIG å°†ç»§ç»­åœ¨æœªæ¥çš„ KubeCon ä¸Šä¸¾åŠžæ–°è´¡çŒ®è€…å·¥ä½œåŠï¼ŒåŒ…æ‹¬è¥¿é›…å›¾ç«™ã€å·´å¡žç½—é‚£ç«™ï¼Œç„¶åŽåœ¨ 2019 å¹´å…­æœˆå›žåˆ°ä¸Šæµ·ã€‚å¦‚æžœä½ ä»Šå¹´æœªèƒ½å‚åŠ ï¼Œè¯·åœ¨æœªæ¥çš„ KubeCon ä¸Šæ³¨å†Œã€‚å¹¶ä¸”ï¼Œå¦‚æžœä½ é‡åˆ°å·¥ä½œåŠçš„ä¸Žä¼šè€…ï¼Œè¯·åŠ¡å¿…æ¬¢è¿Žä»–ä»¬åŠ å…¥ç¤¾åŒºã€‚&lt;/p>
&lt;!--
Links:
-->
&lt;p>é“¾æŽ¥ï¼š&lt;/p>
&lt;!--
* English versions of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf) or [Google Docs with speaker notes](https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing)
* Chinese version of the slides: [PDF](https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf)
* [Contributor playground](https://github.com/kubernetes-sigs/contributor-playground)
-->
&lt;ul>
&lt;li>ä¸­æ–‡ç‰ˆå¹»ç¯ç‰‡ï¼š&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-cihinese-pdf">PDF&lt;/a>&lt;/li>
&lt;li>è‹±æ–‡ç‰ˆå¹»ç¯ç‰‡ï¼š&lt;a href="https://gist.github.com/jberkus/889be25c234b01761ce44eccff816380#file-kubernetes-shanghai-english-pdf">PDF&lt;/a> æˆ– &lt;a href="https://docs.google.com/presentation/d/1l5f_iAFsKg50LFq3N80KbZKUIEL_tyCaUoWPzSxColo/edit?usp=sharing">å¸¦æœ‰æ¼”è®²è€…ç¬”è®°çš„ Google Docs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/contributor-playground">è´¡çŒ®è€…æ¸¸ä¹åœº&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes æ–‡æ¡£æ›´æ–°ï¼Œå›½é™…ç‰ˆ</title><link>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</link><pubDate>Thu, 08 Nov 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/11/08/kubernetes-%E6%96%87%E6%A1%A3%E6%9B%B4%E6%96%B0%E5%9B%BD%E9%99%85%E7%89%88/</guid><description>
&lt;!--
---
layout: blog
title: 'Kubernetes Docs Updates, International Edition'
date: 2018-11-08
---
-->
&lt;!-- **Author**: Zach Corleissen (Linux Foundation) -->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šZach Corleissen ï¼ˆLinux åŸºé‡‘ä¼šï¼‰&lt;/p>
&lt;!-- As a co-chair of SIG Docs, I'm excited to share that Kubernetes docs have a fully mature workflow for localization (l10n). -->
&lt;p>ä½œä¸ºæ–‡æ¡£ç‰¹åˆ«å…´è¶£å°ç»„ï¼ˆSIG Docsï¼‰çš„è”åˆä¸»å¸­ï¼Œæˆ‘å¾ˆé«˜å…´èƒ½ä¸Žå¤§å®¶åˆ†äº« Kubernetes æ–‡æ¡£åœ¨æœ¬åœ°åŒ–ï¼ˆl10nï¼‰æ–¹é¢æ‰€æ‹¥æœ‰çš„ä¸€ä¸ªå®Œå…¨æˆç†Ÿçš„å·¥ä½œæµã€‚&lt;/p>
&lt;!-- ## Abbreviations galore -->
&lt;h2 id="ä¸°å¯Œçš„ç¼©å†™">ä¸°å¯Œçš„ç¼©å†™&lt;/h2>
&lt;!-- L10n is an abbreviation for _localization_. -->
&lt;p>L10n æ˜¯ &lt;em>localization&lt;/em> çš„ç¼©å†™ã€‚&lt;/p>
&lt;!-- I18n is an abbreviation for _internationalization_. -->
&lt;p>I18n æ˜¯ &lt;em>internationalization&lt;/em> çš„ç¼©å†™ã€‚&lt;/p>
&lt;!-- I18n is [what you do](https://www.w3.org/International/questions/qa-i18n) to make l10n easier. L10n is a fuller, more comprehensive process than translation (_t9n_). -->
&lt;p>I18n å®šä¹‰äº†&lt;a href="https://www.w3.org/International/questions/qa-i18n">åšä»€ä¹ˆ&lt;/a> èƒ½è®© l10n æ›´å®¹æ˜“ã€‚è€Œ L10n æ›´å…¨é¢ï¼Œç›¸æ¯”ç¿»è¯‘ï¼ˆ &lt;em>t9n&lt;/em> ï¼‰å…·å¤‡æ›´å®Œå–„çš„æµç¨‹ã€‚&lt;/p>
&lt;!-- ## Why localization matters -->
&lt;h2 id="ä¸ºä»€ä¹ˆæœ¬åœ°åŒ–å¾ˆé‡è¦">ä¸ºä»€ä¹ˆæœ¬åœ°åŒ–å¾ˆé‡è¦&lt;/h2>
&lt;!-- The goal of SIG Docs is to make Kubernetes easier to use for as many people as possible. -->
&lt;p>SIG Docs çš„ç›®æ ‡æ˜¯è®© Kubernetes æ›´å®¹æ˜“ä¸ºå°½å¯èƒ½å¤šçš„äººä½¿ç”¨ã€‚&lt;/p>
&lt;!-- One year ago, we looked at whether it was possible to host the output of a Chinese team working independently to translate the Kubernetes docs. After many conversations (including experts on OpenStack l10n), [much transformation](https://kubernetes.io/blog/2018/05/05/hugo-migration/), and [renewed commitment to easier localization](https://github.com/kubernetes/website/pull/10485), we realized that open source documentation is, like open source software, an ongoing exercise at the edges of what's possible. -->
&lt;p>ä¸€å¹´å‰ï¼Œæˆ‘ä»¬ç ”ç©¶äº†æ˜¯å¦æœ‰å¯èƒ½ç”±ä¸€ä¸ªç‹¬ç«‹ç¿»è¯‘ Kubernetes æ–‡æ¡£çš„ä¸­å›½å›¢é˜Ÿæ¥ä¸»æŒæ–‡æ¡£è¾“å‡ºã€‚ç»è¿‡å¤šæ¬¡äº¤è°ˆï¼ˆåŒ…æ‹¬ OpenStack l10n çš„ä¸“å®¶ï¼‰ï¼Œ&lt;a href="https://kubernetes.io/blog/2018/05/05/hugo-migration/">å¤šæ¬¡è½¬å˜&lt;/a>ï¼Œä»¥åŠ&lt;a href="https://github.com/kubernetes/website/pull/10485">é‡æ–°è‡´åŠ›äºŽæ›´è½»æ¾çš„æœ¬åœ°åŒ–&lt;/a>ï¼Œæˆ‘ä»¬æ„è¯†åˆ°ï¼Œå¼€æºæ–‡æ¡£å°±åƒå¼€æºè½¯ä»¶ä¸€æ ·ï¼Œæ˜¯åœ¨å¯èƒ½çš„è¾¹ç¼˜ä¸æ–­è¿›è¡Œå®žè·µã€‚&lt;/p>
&lt;!-- Consolidating workflows, language labels, and team-level ownership may seem like simple improvements, but these features make l10n scalable for increasing numbers of l10n teams. While SIG Docs continues to iterate improvements, we've paid off a significant amount of technical debt and streamlined l10n in a single workflow. That's great for the future as well as the present. -->
&lt;p>æ•´åˆå·¥ä½œæµç¨‹ã€è¯­è¨€æ ‡ç­¾å’Œå›¢é˜Ÿçº§æ‰€æœ‰æƒå¯èƒ½çœ‹èµ·æ¥åƒæ˜¯ååˆ†ç®€å•çš„æ”¹è¿›ï¼Œä½†æ˜¯è¿™äº›åŠŸèƒ½ä½¿ l10n å¯ä»¥æ‰©å±•åˆ°è§„æ¨¡è¶Šæ¥è¶Šå¤§çš„ l10n å›¢é˜Ÿã€‚éšç€ SIG Docs ä¸æ–­æ”¹è¿›ï¼Œæˆ‘ä»¬å·²ç»åœ¨å•ä¸€å·¥ä½œæµç¨‹ä¸­å¿è¿˜äº†å¤§é‡æŠ€æœ¯å€ºåŠ¡å¹¶ç®€åŒ–äº† l10nã€‚è¿™å¯¹æœªæ¥å’ŒçŽ°åœ¨éƒ½å¾ˆæœ‰ç›Šã€‚&lt;/p>
&lt;!-- ## Consolidated workflow -->
&lt;h2 id="æ•´åˆçš„å·¥ä½œæµç¨‹">æ•´åˆçš„å·¥ä½œæµç¨‹&lt;/h2>
&lt;!-- Localization is now consolidated in the [kubernetes/website](https://github.com/kubernetes/website) repository. We've configured the Kubernetes CI/CD system, [Prow](https://github.com/kubernetes/test-infra/tree/master/prow), to handle automatic language label assignment as well as team-level PR review and approval. -->
&lt;p>çŽ°åœ¨ï¼Œæœ¬åœ°åŒ–å·²æ•´åˆåˆ° &lt;a href="https://github.com/kubernetes/website">kubernetes/website&lt;/a> å­˜å‚¨åº“ã€‚æˆ‘ä»¬å·²ç»é…ç½®äº† Kubernetes CI/CD ç³»ç»Ÿï¼Œ&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a> æ¥å¤„ç†è‡ªåŠ¨è¯­è¨€æ ‡ç­¾åˆ†é…ä»¥åŠå›¢é˜Ÿçº§ PR å®¡æŸ¥å’Œæ‰¹å‡†ã€‚&lt;/p>
&lt;!-- ### Language labels -->
&lt;h3 id="è¯­è¨€æ ‡ç­¾">è¯­è¨€æ ‡ç­¾&lt;/h3>
&lt;!-- Prow automatically applies language labels based on file path. Thanks to SIG Docs contributor [June Yi](https://github.com/kubernetes/test-infra/pull/9835), folks can also manually assign language labels in pull request (PR) comments. For example, when left as a comment on an issue or PR, this command assigns the label `language/ko` (Korean). -->
&lt;p>Prow æ ¹æ®æ–‡ä»¶è·¯å¾„è‡ªåŠ¨æ·»åŠ è¯­è¨€æ ‡ç­¾ã€‚æ„Ÿè°¢ SIG Docs è´¡çŒ®è€… &lt;a href="https://github.com/kubernetes/test-infra/pull/9835">June Yi&lt;/a>ï¼Œä»–è®©äººä»¬è¿˜å¯ä»¥åœ¨ pull requestï¼ˆPRï¼‰æ³¨é‡Šä¸­æ‰‹åŠ¨åˆ†é…è¯­è¨€æ ‡ç­¾ã€‚ä¾‹å¦‚ï¼Œå½“ä¸º issue æˆ– PR ç•™ä¸‹ä¸‹è¿°æ³¨é‡Šæ—¶ï¼Œå°†ä¸ºä¹‹åˆ†é…æ ‡ç­¾ &lt;code>language/ko&lt;/code>ï¼ˆKoreanï¼‰ã€‚&lt;/p>
&lt;pre>&lt;code>/language ko
&lt;/code>&lt;/pre>&lt;!-- These repo labels let reviewers filter for PRs and issues by language. For example, you can now filter the k/website dashboard for [PRs with Chinese content](https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh). -->
&lt;p>è¿™äº›å­˜å‚¨åº“æ ‡ç­¾å…è®¸å®¡é˜…è€…æŒ‰è¯­è¨€è¿‡æ»¤ PR å’Œ issueã€‚ä¾‹å¦‚ï¼Œæ‚¨çŽ°åœ¨å¯ä»¥è¿‡æ»¤ kubernetes/website é¢æ¿ä¸­&lt;a href="https://github.com/kubernetes/website/pulls?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Apr+label%3Alanguage%2Fzh">å…·æœ‰ä¸­æ–‡å†…å®¹çš„ PR&lt;/a>ã€‚&lt;/p>
&lt;!-- ### Team review -->
&lt;h3 id="å›¢é˜Ÿå®¡æ ¸">å›¢é˜Ÿå®¡æ ¸&lt;/h3>
&lt;!-- L10n teams can now review and approve their own PRs. For example, review and approval permissions for English are [assigned in an OWNERS file](https://github.com/kubernetes/website/blob/master/content/en/OWNERS) in the top subfolder for English content. -->
&lt;p>L10n å›¢é˜ŸçŽ°åœ¨å¯ä»¥å®¡æŸ¥å’Œæ‰¹å‡†ä»–ä»¬è‡ªå·±çš„ PRã€‚ä¾‹å¦‚ï¼Œè‹±è¯­çš„å®¡æ ¸å’Œæ‰¹å‡†æƒé™åœ¨ä½äºŽç”¨äºŽæ˜¾ç¤ºè‹±è¯­å†…å®¹çš„é¡¶çº§å­æ–‡ä»¶å¤¹ä¸­çš„ &lt;a href="https://github.com/kubernetes/website/blob/master/content/en/OWNERS">OWNERS æ–‡ä»¶ä¸­æŒ‡å®š&lt;/a>ã€‚&lt;/p>
&lt;!-- Adding `OWNERS` files to subdirectories lets localization teams review and approve changes without requiring a rubber stamp approval from reviewers who may lack fluency. -->
&lt;p>å°† &lt;code>OWNERS&lt;/code> æ–‡ä»¶æ·»åŠ åˆ°å­ç›®å½•å¯ä»¥è®©æœ¬åœ°åŒ–å›¢é˜Ÿå®¡æŸ¥å’Œæ‰¹å‡†æ›´æ”¹ï¼Œè€Œæ— éœ€ç”±å¯èƒ½å¹¶ä¸æ“…é•¿è¯¥é—¨è¯­è¨€çš„å®¡é˜…è€…è¿›è¡Œæ‰¹å‡†ã€‚&lt;/p>
&lt;!-- ## What's next -->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ&lt;/h2>
&lt;!-- We're looking forward to the [doc sprint in Shanghai](https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required) to serve as a resource for the Chinese l10n team. -->
&lt;p>æˆ‘ä»¬æœŸå¾…ç€&lt;a href="https://kccncchina2018english.sched.com/event/HVb2/contributor-summit-doc-sprint-additional-registration-required">ä¸Šæµ·çš„ doc sprint&lt;/a> èƒ½ä½œä¸ºä¸­å›½ l10n å›¢é˜Ÿçš„èµ„æºã€‚&lt;/p>
&lt;!-- We're excited to continue supporting the Japanese and Korean l10n teams, who are making excellent progress. -->
&lt;p>æˆ‘ä»¬å¾ˆé«˜å…´ç»§ç»­æ”¯æŒæ­£åœ¨å–å¾—è‰¯å¥½è¿›å±•çš„æ—¥æœ¬å’ŒéŸ©å›½ l10n é˜Ÿä¼ã€‚&lt;/p>
&lt;!-- If you're interested in localizing Kubernetes for your own language or region, check out our [guide to localizing Kubernetes docs](https://kubernetes.io/docs/contribute/localization/) and reach out to a [SIG Docs chair](https://github.com/kubernetes/community/tree/master/sig-docs#leadership) for support. -->
&lt;p>å¦‚æžœæ‚¨æœ‰å…´è¶£å°† Kubernetes æœ¬åœ°åŒ–ä¸ºæ‚¨è‡ªå·±çš„è¯­è¨€æˆ–åœ°åŒºï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬çš„&lt;a href="https://kubernetes.io/docs/contribute/localization/">æœ¬åœ°åŒ– Kubernetes æ–‡æ¡£æŒ‡å—&lt;/a>ï¼Œå¹¶è”ç³» &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#leadership">SIG Docs ä¸»å¸­å›¢&lt;/a>èŽ·å–æ”¯æŒã€‚&lt;/p>
&lt;!-- ### Get involved with SIG Docs -->
&lt;h3 id="åŠ å…¥sig-docs">åŠ å…¥SIG Docs&lt;/h3>
&lt;!-- If you're interested in Kubernetes documentation, come to a SIG Docs [weekly meeting](https://github.com/kubernetes/community/tree/master/sig-docs#meetings), or join [#sig-docs in Kubernetes Slack](https://kubernetes.slack.com/messages/C1J0BPD2M/details/). -->
&lt;p>å¦‚æžœæ‚¨å¯¹ Kubernetes æ–‡æ¡£æ„Ÿå…´è¶£ï¼Œè¯·å‚åŠ  SIG Docs &lt;a href="https://github.com/kubernetes/community/tree/master/sig-docs#meetings">æ¯å‘¨ä¼šè®®&lt;/a>ï¼Œæˆ–åœ¨ &lt;a href="https://kubernetes.slack.com/messages/C1J0BPD2M/details/">Kubernetes Slack åŠ å…¥ #sig-docs&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 2018 å¹´åŒ—ç¾Žè´¡çŒ®è€…å³°ä¼š</title><link>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</link><pubDate>Tue, 16 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/16/kubernetes-2018-%E5%B9%B4%E5%8C%97%E7%BE%8E%E8%B4%A1%E7%8C%AE%E8%80%85%E5%B3%B0%E4%BC%9A/</guid><description>
&lt;!--
---
layout: "Blog"
title: "Kubernetes 2018 North American Contributor Summit"
date: 2018-10-16
---
-->
&lt;!--
**Authors:**
-->
&lt;p>&lt;strong>ä½œè€…ï¼š&lt;/strong>&lt;/p>
&lt;!--
[Bob Killen][bob] (University of Michigan)
[Sahdev Zala][sahdev] (IBM),
[Ihor Dvoretskyi][ihor] (CNCF)
-->
&lt;p>&lt;a href="https://twitter.com/mrbobbytables">Bob Killen&lt;/a>ï¼ˆå¯†æ­‡æ ¹å¤§å­¦ï¼‰
&lt;a href="https://twitter.com/sp_zala">Sahdev Zala&lt;/a>ï¼ˆIBMï¼‰ï¼Œ
&lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>ï¼ˆCNCFï¼‰&lt;/p>
&lt;!--
The 2018 North American Kubernetes Contributor Summit to be hosted right before
[KubeCon + CloudNativeCon][kubecon] Seattle is shaping up to be the largest yet.
-->
&lt;p>2018 å¹´åŒ—ç¾Ž Kubernetes è´¡çŒ®è€…å³°ä¼šå°†åœ¨è¥¿é›…å›¾ &lt;a href="https://events.linuxfoundation.org/events/kubecon-cloudnativecon-north-america-2018/">KubeCon + CloudNativeCon&lt;/a> ä¼šè®®ä¹‹å‰ä¸¾åŠžï¼Œè¿™å°†æ˜¯è¿„ä»Šä¸ºæ­¢è§„æ¨¡æœ€å¤§çš„ä¸€æ¬¡ç››ä¼šã€‚&lt;/p>
&lt;!--
It is an event that brings together new and current contributors alike to
connect and share face-to-face; and serves as an opportunity for existing
contributors to help shape the future of community development. For new
community members, it offers a welcoming space to learn, explore and put the
contributor workflow to practice.
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªå°†æ–°è€è´¡çŒ®è€…èšé›†åœ¨ä¸€èµ·ï¼Œé¢å¯¹é¢äº¤æµå’Œåˆ†äº«çš„æ´»åŠ¨ï¼›å¹¶ä¸ºçŽ°æœ‰çš„è´¡çŒ®è€…æä¾›ä¸€ä¸ªæœºä¼šï¼Œå¸®åŠ©å¡‘é€ ç¤¾åŒºå‘å±•çš„æœªæ¥ã€‚å®ƒä¸ºæ–°çš„ç¤¾åŒºæˆå‘˜æä¾›äº†ä¸€ä¸ªå­¦ä¹ ã€æŽ¢ç´¢å’Œå®žè·µè´¡çŒ®å·¥ä½œæµç¨‹çš„è‰¯å¥½ç©ºé—´ã€‚&lt;/p>
&lt;!--
Unlike previous Contributor Summits, the event now spans two-days with a more
relaxed â€˜hallwayâ€™ track and general Contributor get-together to be hosted from
5-8pm on Sunday December 9th at the [Garage Lounge and Gaming Hall][garage], just
a short walk away from the Convention Center. There, contributors can enjoy
billiards, bowling, trivia and more; accompanied by a variety of food and drink.
-->
&lt;p>ä¸Žä¹‹å‰çš„è´¡çŒ®è€…å³°ä¼šä¸åŒï¼Œæœ¬æ¬¡æ´»åŠ¨ä¸ºæœŸä¸¤å¤©ï¼Œæœ‰ä¸€ä¸ªæ›´ä¸ºè½»æ¾çš„è¡Œç¨‹å®‰æŽ’ï¼Œä¸€èˆ¬è´¡çŒ®è€…å°†äºŽ 12 æœˆ 9 æ—¥ï¼ˆå‘¨æ—¥ï¼‰ä¸‹åˆ 5 ç‚¹è‡³ 8 ç‚¹åœ¨è·ç¦»ä¼šè®®ä¸­å¿ƒä»…å‡ æ­¥è¿œçš„ &lt;a href="https://www.garagebilliards.com/">Garage Lounge and Gaming Hall&lt;/a> ä¸¾åŠžå³°ä¼šã€‚åœ¨é‚£é‡Œï¼Œè´¡çŒ®è€…ä¹Ÿå¯ä»¥è¿›è¡Œå°çƒã€ä¿é¾„çƒç­‰å¨±ä¹æ´»åŠ¨ï¼Œè€Œä¸”è¿˜æœ‰å„ç§é£Ÿå“å’Œé¥®æ–™ã€‚&lt;/p>
&lt;!--
Things pick up the following day, Monday the 10th with three separate tracks:
-->
&lt;p>æŽ¥ä¸‹æ¥çš„ä¸€å¤©ï¼Œä¹Ÿå°±æ˜¯ 10 å·æ˜ŸæœŸä¸€ï¼Œæœ‰ä¸‰ä¸ªç‹¬ç«‹çš„ä¼šè®®ä½ å¯ä»¥é€‰æ‹©å‚ä¸Žï¼š&lt;/p>
&lt;!--
### New Contributor Workshop:
A half day workshop aimed at getting new and first time contributors onboarded
and comfortable with working within the Kubernetes Community. Staying for the
duration is required; this is not a workshop you can drop into.
-->
&lt;h3 id="æ–°è´¡çŒ®è€…ç ”è®¨ä¼š">æ–°è´¡çŒ®è€…ç ”è®¨ä¼šï¼š&lt;/h3>
&lt;p>ä¸ºæœŸåŠå¤©çš„ç ”è®¨ä¼šæ—¨åœ¨è®©æ–°è´¡çŒ®è€…åŠ å…¥ç¤¾åŒºï¼Œå¹¶è¥é€ ä¸€ä¸ªè‰¯å¥½çš„ Kubernetes ç¤¾åŒºå·¥ä½œçŽ¯å¢ƒã€‚
è¯·åœ¨å¼€ä¼šæœŸé—´ä¿æŒåœ¨åœºï¼Œè¯¥è®¨è®ºä¼šä¸å…è®¸éšæ„è¿›å‡ºã€‚&lt;/p>
&lt;!--
### Current Contributor Track:
Reserved for those that are actively engaged with the development of the
project; the Current Contributor Track includes Talks, Workshops, Birds of a
Feather, Unconferences, Steering Committee Sessions, and more! Keep an eye on
the [schedule in GitHub][schedule] as content is frequently being updated.
-->
&lt;h3 id="å½“å‰è´¡çŒ®è€…è¿½è¸ª">å½“å‰è´¡çŒ®è€…è¿½è¸ªï¼š&lt;/h3>
&lt;p>ä¿ç•™ç»™é‚£äº›ç§¯æžå‚ä¸Žé¡¹ç›®å¼€å‘çš„è´¡çŒ®è€…ï¼›ç›®å‰çš„è´¡çŒ®è€…è¿½è¸ªåŒ…æ‹¬è®²åº§ã€ç ”è®¨ä¼šã€èšä¼šã€Unconferences ä¼šè®®ã€æŒ‡å¯¼å§”å‘˜ä¼šä¼šè®®ç­‰ç­‰!
è¯·ç•™æ„ &lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#agenda">GitHub ä¸­çš„æ—¶é—´è¡¨&lt;/a>ï¼Œå› ä¸ºå†…å®¹ç»å¸¸æ›´æ–°ã€‚&lt;/p>
&lt;!--
### Docs Sprint:
SIG-Docs will have a curated list of issues and challenges to be tackled closer
to the event date.
-->
&lt;h3 id="docs-å†²åˆº">Docs å†²åˆºï¼š&lt;/h3>
&lt;p>SIG-Docs å°†åœ¨æ´»åŠ¨æ—¥æœŸä¸´è¿‘çš„æ—¶å€™åˆ—å‡ºä¸€ä¸ªéœ€è¦å¤„ç†çš„é—®é¢˜å’ŒæŒ‘æˆ˜åˆ—è¡¨ã€‚&lt;/p>
&lt;!--
## To Register:
To register for the Contributor Summit, see the [Registration section of the
Event Details in GitHub][register]. Please note that registrations are being
reviewed. If you select the â€œCurrent Contributor Trackâ€ and are not an active
contributor, you will be asked to attend the New Contributor Workshop, or asked
to be put on a waitlist. With thousands of contributors and only 300 spots, we
need to make sure the right folks are in the room.
-->
&lt;h2 id="æ³¨å†Œ">æ³¨å†Œï¼š&lt;/h2>
&lt;p>è¦æ³¨å†Œè´¡çŒ®è€…å³°ä¼šï¼Œè¯·å‚é˜… Git Hub ä¸Šçš„&lt;a href="https://git.k8s.io/community/events/2018/12-contributor-summit#registration">æ´»åŠ¨è¯¦æƒ…æ³¨å†Œéƒ¨åˆ†&lt;/a>ã€‚è¯·æ³¨æ„æŠ¥åæ­£åœ¨å®¡æ ¸ä¸­ã€‚
å¦‚æžœæ‚¨é€‰æ‹©äº† â€œå½“å‰è´¡çŒ®è€…è¿½è¸ªâ€ï¼Œè€Œæ‚¨å´ä¸æ˜¯ä¸€ä¸ªæ´»è·ƒçš„è´¡çŒ®è€…ï¼Œæ‚¨å°†è¢«è¦æ±‚å‚åŠ æ–°è´¡çŒ®è€…ç ”è®¨ä¼šï¼Œæˆ–è€…è¢«è¦æ±‚è¿›å…¥å€™è¡¥åå•ã€‚
æˆåƒä¸Šä¸‡çš„è´¡çŒ®è€…åªæœ‰ 300 ä¸ªä½ç½®ï¼Œæˆ‘ä»¬éœ€è¦ç¡®ä¿æ­£ç¡®çš„äººè¢«å®‰æŽ’å¸­ä½ã€‚&lt;/p>
&lt;!--
If you have any questions or concerns, please donâ€™t hesitate to reach out to
the Contributor Summit Events Team at community@kubernetes.io.
-->
&lt;p>å¦‚æžœæ‚¨æœ‰ä»»ä½•é—®é¢˜æˆ–ç–‘è™‘ï¼Œè¯·éšæ—¶é€šè¿‡ &lt;a href="mailto:community@kubernetes.io">community@kubernetes.io&lt;/a> è”ç³»è´¡çŒ®è€…å³°ä¼šç»„ç»‡å›¢é˜Ÿã€‚&lt;/p>
&lt;!--
Look forward to seeing everyone there!
-->
&lt;p>æœŸå¾…åœ¨é‚£é‡Œçœ‹åˆ°æ¯ä¸ªäººï¼&lt;/p></description></item><item><title>Blog: 2018 å¹´ç£å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æžœ</title><link>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</link><pubDate>Mon, 15 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/15/2018-%E5%B9%B4%E7%9D%A3%E5%AF%BC%E5%A7%94%E5%91%98%E4%BC%9A%E9%80%89%E4%B8%BE%E7%BB%93%E6%9E%9C/</guid><description>
&lt;!--
---
layout: blog
title: '2018 Steering Committee Election Results'
date: 2018-10-15
---
-->
&lt;!-- **Authors**: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google) -->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Jorge Castro (Heptio), Ihor Dvoretskyi (CNCF), Paris Pittman (Google)&lt;/p>
&lt;!--
## Results
-->
&lt;h2 id="ç»“æžœ">ç»“æžœ&lt;/h2>
&lt;!--
The [Kubernetes Steering Committee Election](https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/) is now complete and the following candidates came ahead to secure two year terms that start immediately:
-->
&lt;p>&lt;a href="https://kubernetes.io/blog/2018/09/06/2018-steering-committee-election-cycle-kicks-off/">Kubernetes ç£å¯¼å§”å‘˜ä¼šé€‰ä¸¾&lt;/a>çŽ°å·²å®Œæˆï¼Œä»¥ä¸‹å€™é€‰äººèŽ·å¾—äº†ç«‹å³å¼€å§‹çš„ä¸¤å¹´ä»»æœŸï¼š&lt;/p>
&lt;ul>
&lt;li>Aaron Crickenberger, Google, &lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>&lt;/li>
&lt;li>Davanum Srinivas, Huawei, &lt;a href="https://github.com/dims">@dims&lt;/a>&lt;/li>
&lt;li>Tim St. Clair, Heptio, &lt;a href="https://github.com/timothysc">@timothysc&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Big Thanks!
-->
&lt;h2 id="ååˆ†æ„Ÿè°¢">ååˆ†æ„Ÿè°¢ï¼&lt;/h2>
&lt;!--
* Steering Committee Member Emeritus [Quinton Hoole](https://github.com/quinton-hoole) for his service to the community over the past year. We look forward to
* The candidates that came forward to run for election. May we always have a strong set of people who want to push community forward like yours in every election.
* All 307 voters who cast a ballot.
* And last but not least...Cornell University for hosting [CIVS](https://civs.cs.cornell.edu/)!
-->
&lt;ul>
&lt;li>ç£å¯¼å§”å‘˜ä¼šè£èª‰é€€ä¼‘æˆå‘˜ &lt;a href="https://github.com/quinton-hoole">Quinton Hoole&lt;/a>ï¼Œè¡¨æ‰¬ä»–åœ¨è¿‡åŽ»ä¸€å¹´ä¸ºç¤¾åŒºæ‰€ä½œçš„è´¡çŒ®ã€‚æˆ‘ä»¬æœŸå¾…ç€&lt;/li>
&lt;li>å‚åŠ ç«žé€‰çš„å€™é€‰äººã€‚æ„¿æˆ‘ä»¬æ°¸è¿œæ‹¥æœ‰ä¸€ç¾¤å¼ºå¤§çš„äººï¼Œä»–ä»¬å¸Œæœ›åœ¨æ¯ä¸€æ¬¡é€‰ä¸¾ä¸­éƒ½èƒ½åƒä½ ä»¬ä¸€æ ·æŽ¨åŠ¨ç¤¾åŒºå‘å‰å‘å±•ã€‚&lt;/li>
&lt;li>å…±è®¡ 307 åé€‰æ°‘å‚ä¸ŽæŠ•ç¥¨ã€‚&lt;/li>
&lt;li>æœ¬æ¬¡é€‰ä¸¾ç”±åº·å¥ˆå°”å¤§å­¦ä¸»åŠž &lt;a href="https://civs.cs.cornell.edu/">CIVS&lt;/a>ï¼&lt;/li>
&lt;/ul>
&lt;!--
## Get Involved with the Steering Committee
-->
&lt;h2 id="åŠ å…¥ç£å¯¼å§”å‘˜ä¼š">åŠ å…¥ç£å¯¼å§”å‘˜ä¼š&lt;/h2>
&lt;!--
You can follow along to Steering Committee [backlog items](https://git.k8s.io/steering/backlog.md) and weigh in by filing an issue or creating a PR against their [repo](https://github.com/kubernetes/steering). They meet bi-weekly on [Wednesdays at 8pm UTC](https://github.com/kubernetes/steering) and regularly attend Meet Our Contributors.
-->
&lt;p>ä½ å¯ä»¥å…³æ³¨ç£å¯¼å§”å‘˜ä¼šçš„&lt;a href="https://git.k8s.io/steering/backlog.md">ä»»åŠ¡æ¸…å•&lt;/a>ï¼Œå¹¶é€šè¿‡å‘ä»–ä»¬çš„&lt;a href="https://github.com/kubernetes/steering">ä»£ç ä»“åº“&lt;/a>æäº¤ issue æˆ– PR çš„æ–¹å¼æ¥å‚ä¸Žã€‚ä»–ä»¬ä¹Ÿä¼šåœ¨&lt;a href="https://github.com/kubernetes/steering">UTC æ—¶é—´æ¯å‘¨ä¸‰æ™š 8 ç‚¹&lt;/a>ä¸¾è¡Œä¼šè®®ï¼Œå¹¶å®šæœŸä¸Žæˆ‘ä»¬çš„è´¡çŒ®è€…è§é¢ã€‚&lt;/p>
&lt;!--
Steering Committee Meetings:
-->
&lt;p>ç£å¯¼å§”å‘˜ä¼šä¼šè®®ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube æ’­æ”¾åˆ—è¡¨&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Meet Our Contributors Steering AMAâ€™s:
-->
&lt;p>ä¸Žæˆ‘ä»¬çš„è´¡çŒ®è€…ä¼šé¢ï¼š&lt;/p>
&lt;!--
* [Oct 3 2018](https://youtu.be/x6Jm8p0K-IQ)
* [Sept 5 2018](https://youtu.be/UbxWV12Or58)
-->
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/x6Jm8p0K-IQ">2018 å¹´ 10 æœˆ 3 æ—¥&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://youtu.be/UbxWV12Or58">2018 å¹´ 7 æœˆ 5 æ—¥&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes ä¸­çš„æ‹“æ‰‘æ„ŸçŸ¥æ•°æ®å·ä¾›åº”</title><link>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</link><pubDate>Thu, 11 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/11/kubernetes-%E4%B8%AD%E7%9A%84%E6%8B%93%E6%89%91%E6%84%9F%E7%9F%A5%E6%95%B0%E6%8D%AE%E5%8D%B7%E4%BE%9B%E5%BA%94/</guid><description>
&lt;!--
---
layout: blog
title: 'Topology-Aware Volume Provisioning in Kubernetes'
date: 2018-10-11
---
-->
&lt;!--
**Author**: Michelle Au (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>: Michelle Auï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
-->
&lt;p>é€šè¿‡æä¾›æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€å·ä¾›åº”åŠŸèƒ½ï¼Œå…·æœ‰æŒä¹…å·çš„å¤šåŒºåŸŸé›†ç¾¤ä½“éªŒåœ¨ Kubernetes 1.12 ä¸­å¾—åˆ°äº†æ”¹è¿›ã€‚æ­¤åŠŸèƒ½ä½¿å¾— Kubernetes åœ¨åŠ¨æ€ä¾›åº”å·æ—¶èƒ½åšå‡ºæ˜Žæ™ºçš„å†³ç­–ï¼Œæ–¹æ³•æ˜¯ä»Žè°ƒåº¦å™¨èŽ·å¾—ä¸º Pod æä¾›æ•°æ®å·çš„æœ€ä½³ä½ç½®ã€‚åœ¨å¤šåŒºåŸŸé›†ç¾¤çŽ¯å¢ƒï¼Œè¿™æ„å‘³ç€æ•°æ®å·èƒ½å¤Ÿåœ¨æ»¡è¶³ä½ çš„ Pod è¿è¡Œéœ€è¦çš„åˆé€‚çš„åŒºåŸŸè¢«ä¾›åº”ï¼Œä»Žè€Œå…è®¸æ‚¨è·¨æ•…éšœåŸŸè½»æ¾éƒ¨ç½²å’Œæ‰©å±•æœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½ï¼Œä»Žè€Œæä¾›é«˜å¯ç”¨æ€§å’Œå®¹é”™èƒ½åŠ›ã€‚&lt;/p>
&lt;!--
## Previous challenges
-->
&lt;h2 id="ä»¥å‰çš„æŒ‘æˆ˜">ä»¥å‰çš„æŒ‘æˆ˜&lt;/h2>
&lt;!--
Before this feature, running stateful workloads with zonal persistent disks (such as AWS ElasticBlockStore, Azure Disk, GCE PersistentDisk) in multi-zone clusters had many challenges. Dynamic provisioning was handled independently from pod scheduling, which meant that as soon as you created a PersistentVolumeClaim (PVC), a volume would get provisioned. This meant that the provisioner had no knowledge of what pods were using the volume, and any pod constraints it had that could impact scheduling.
-->
&lt;p>åœ¨æ­¤åŠŸèƒ½è¢«æä¾›ä¹‹å‰ï¼Œåœ¨å¤šåŒºåŸŸé›†ç¾¤ä¸­ä½¿ç”¨åŒºåŸŸåŒ–çš„æŒä¹…ç£ç›˜ï¼ˆä¾‹å¦‚ AWS ElasticBlockStoreï¼ŒAzure Diskï¼ŒGCE PersistentDiskï¼‰è¿è¡Œæœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½å­˜åœ¨è®¸å¤šæŒ‘æˆ˜ã€‚åŠ¨æ€ä¾›åº”ç‹¬ç«‹äºŽ Pod è°ƒåº¦å¤„ç†ï¼Œè¿™æ„å‘³ç€åªè¦æ‚¨åˆ›å»ºäº†ä¸€ä¸ª PersistentVolumeClaimï¼ˆPVCï¼‰ï¼Œä¸€ä¸ªå·å°±ä¼šè¢«ä¾›åº”ã€‚è¿™ä¹Ÿæ„å‘³ç€ä¾›åº”è€…ä¸çŸ¥é“å“ªäº› Pod æ­£åœ¨ä½¿ç”¨è¯¥å·ï¼Œä¹Ÿä¸æ¸…æ¥šä»»ä½•å¯èƒ½å½±å“è°ƒåº¦çš„ Pod çº¦æŸã€‚&lt;/p>
&lt;!--
This resulted in unschedulable pods because volumes were provisioned in zones that:
-->
&lt;p>è¿™å¯¼è‡´äº†ä¸å¯è°ƒåº¦çš„ Podï¼Œå› ä¸ºåœ¨ä»¥ä¸‹åŒºåŸŸä¸­é…ç½®äº†å·ï¼š&lt;/p>
&lt;!--
* did not have enough CPU or memory resources to run the pod
* conflicted with node selectors, pod affinity or anti-affinity policies
* could not run the pod due to taints
-->
&lt;ul>
&lt;li>æ²¡æœ‰è¶³å¤Ÿçš„ CPU æˆ–å†…å­˜èµ„æºæ¥è¿è¡Œ Pod&lt;/li>
&lt;li>ä¸ŽèŠ‚ç‚¹é€‰æ‹©å™¨ã€Pod äº²å’Œæˆ–åäº²å’Œç­–ç•¥å†²çª&lt;/li>
&lt;li>ç”±äºŽæ±¡ç‚¹ï¼ˆtaintï¼‰ä¸èƒ½è¿è¡Œ Pod&lt;/li>
&lt;/ul>
&lt;!--
Another common issue was that a non-StatefulSet pod using multiple persistent volumes could have each volume provisioned in a different zone, again resulting in an unschedulable pod.
-->
&lt;p>å¦ä¸€ä¸ªå¸¸è§é—®é¢˜æ˜¯ï¼Œä½¿ç”¨å¤šä¸ªæŒä¹…å·çš„éžæœ‰çŠ¶æ€ Pod å¯èƒ½ä¼šåœ¨ä¸åŒçš„åŒºåŸŸä¸­é…ç½®æ¯ä¸ªå·ï¼Œä»Žè€Œå¯¼è‡´ä¸€ä¸ªä¸å¯è°ƒåº¦çš„ Podã€‚&lt;/p>
&lt;!--
Suboptimal workarounds included overprovisioning of nodes, or manual creation of volumes in the correct zones, making it difficult to dynamically deploy and scale stateful workloads.
-->
&lt;p>æ¬¡ä¼˜çš„è§£å†³æ–¹æ³•åŒ…æ‹¬èŠ‚ç‚¹è¶…é…ï¼Œæˆ–åœ¨æ­£ç¡®çš„åŒºåŸŸä¸­æ‰‹åŠ¨åˆ›å»ºå·ï¼Œä½†è¿™ä¼šé€ æˆéš¾ä»¥åŠ¨æ€éƒ¨ç½²å’Œæ‰©å±•æœ‰çŠ¶æ€å·¥ä½œè´Ÿè½½çš„é—®é¢˜ã€‚&lt;/p>
&lt;!--
The topology-aware dynamic provisioning feature addresses all of the above issues.
-->
&lt;p>æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”åŠŸèƒ½è§£å†³äº†ä¸Šè¿°æ‰€æœ‰é—®é¢˜ã€‚&lt;/p>
&lt;!--
## Supported Volume Types
-->
&lt;h2 id="æ”¯æŒçš„å·ç±»åž‹">æ”¯æŒçš„å·ç±»åž‹&lt;/h2>
&lt;!--
In 1.12, the following drivers support topology-aware dynamic provisioning:
-->
&lt;p>åœ¨ 1.12 ä¸­ï¼Œä»¥ä¸‹é©±åŠ¨ç¨‹åºæ”¯æŒæ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”ï¼š&lt;/p>
&lt;!--
* AWS EBS
* Azure Disk
* GCE PD (including Regional PD)
* CSI (alpha) - currently only the GCE PD CSI driver has implemented topology support
-->
&lt;ul>
&lt;li>AWS EBS&lt;/li>
&lt;li>Azure Disk&lt;/li>
&lt;li>GCE PD ï¼ˆåŒ…æ‹¬ Regional PDï¼‰&lt;/li>
&lt;li>CSIï¼ˆalphaï¼‰ - ç›®å‰åªæœ‰ GCE PD CSI é©±åŠ¨å®žçŽ°äº†æ‹“æ‰‘æ”¯æŒ&lt;/li>
&lt;/ul>
&lt;!--
## Design Principles
-->
&lt;h2 id="è®¾è®¡åŽŸåˆ™">è®¾è®¡åŽŸåˆ™&lt;/h2>
&lt;!--
While the initial set of supported plugins are all zonal-based, we designed this feature to adhere to the Kubernetes principle of portability across environments. Topology specification is generalized and uses a similar label-based specification like in Pod nodeSelectors and nodeAffinity. This mechanism allows you to define your own topology boundaries, such as racks in on-premise clusters, without requiring modifications to the scheduler to understand these custom topologies.
-->
&lt;p>è™½ç„¶æœ€åˆæ”¯æŒçš„æ’ä»¶é›†éƒ½æ˜¯åŸºäºŽåŒºåŸŸçš„ï¼Œä½†æˆ‘ä»¬è®¾è®¡æ­¤åŠŸèƒ½æ—¶éµå¾ª Kubernetes è·¨çŽ¯å¢ƒå¯ç§»æ¤æ€§çš„åŽŸåˆ™ã€‚
æ‹“æ‰‘è§„èŒƒæ˜¯é€šç”¨çš„ï¼Œå¹¶ä½¿ç”¨ç±»ä¼¼äºŽåŸºäºŽæ ‡ç­¾çš„è§„èŒƒï¼Œå¦‚ Pod nodeSelectors å’Œ nodeAffinityã€‚
è¯¥æœºåˆ¶å…è®¸æ‚¨å®šä¹‰è‡ªå·±çš„æ‹“æ‰‘è¾¹ç•Œï¼Œä¾‹å¦‚å†…éƒ¨éƒ¨ç½²é›†ç¾¤ä¸­çš„æœºæž¶ï¼Œè€Œæ— éœ€ä¿®æ”¹è°ƒåº¦ç¨‹åºä»¥äº†è§£è¿™äº›è‡ªå®šä¹‰æ‹“æ‰‘ã€‚&lt;/p>
&lt;!--
In addition, the topology information is abstracted away from the pod specification, so a pod does not need knowledge of the underlying storage systemâ€™s topology characteristics. This means that you can use the same pod specification across multiple clusters, environments, and storage systems.
-->
&lt;p>æ­¤å¤–ï¼Œæ‹“æ‰‘ä¿¡æ¯æ˜¯ä»Ž Pod è§„èŒƒä¸­æŠ½è±¡å‡ºæ¥çš„ï¼Œå› æ­¤ Pod ä¸éœ€è¦äº†è§£åº•å±‚å­˜å‚¨ç³»ç»Ÿçš„æ‹“æ‰‘ç‰¹å¾ã€‚
è¿™æ„å‘³ç€æ‚¨å¯ä»¥åœ¨å¤šä¸ªé›†ç¾¤ã€çŽ¯å¢ƒå’Œå­˜å‚¨ç³»ç»Ÿä¸­ä½¿ç”¨ç›¸åŒçš„ Pod è§„èŒƒã€‚&lt;/p>
&lt;!--
## Getting Started
-->
&lt;h2 id="å…¥é—¨">å…¥é—¨&lt;/h2>
&lt;!--
To enable this feature, all you need to do is to create a StorageClass with `volumeBindingMode` set to `WaitForFirstConsumer`:
-->
&lt;p>è¦å¯ç”¨æ­¤åŠŸèƒ½ï¼Œæ‚¨éœ€è¦åšçš„å°±æ˜¯åˆ›å»ºä¸€ä¸ªå°† &lt;code>volumeBindingMode&lt;/code> è®¾ç½®ä¸º &lt;code>WaitForFirstConsumer&lt;/code> çš„ StorageClassï¼š&lt;/p>
&lt;pre>&lt;code>kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
name: topology-aware-standard
provisioner: kubernetes.io/gce-pd
volumeBindingMode: WaitForFirstConsumer
parameters:
type: pd-standard
&lt;/code>&lt;/pre>&lt;!--
This new setting instructs the volume provisioner to not create a volume immediately, and instead, wait for a pod using an associated PVC to run through scheduling. Note that previous StorageClass `zone` and `zones` parameters do not need to be specified anymore, as pod policies now drive the decision of which zone to provision a volume in.
-->
&lt;p>è¿™ä¸ªæ–°è®¾ç½®è¡¨æ˜Žå·é…ç½®å™¨ä¸ç«‹å³åˆ›å»ºå·ï¼Œè€Œæ˜¯ç­‰å¾…ä½¿ç”¨å…³è”çš„ PVC çš„ Pod é€šè¿‡è°ƒåº¦è¿è¡Œã€‚
è¯·æ³¨æ„ï¼Œä¸å†éœ€è¦æŒ‡å®šä»¥å‰çš„ StorageClass &lt;code>zone&lt;/code> å’Œ &lt;code>zones&lt;/code> å‚æ•°ï¼Œå› ä¸ºçŽ°åœ¨åœ¨å“ªä¸ªåŒºåŸŸä¸­é…ç½®å·ç”± Pod ç­–ç•¥å†³å®šã€‚&lt;/p>
&lt;!--
Next, create a pod and PVC with this StorageClass. This sequence is the same as before, but with a different StorageClass specified in the PVC. The following is a hypothetical example, demonstrating the capabilities of the new feature by specifying many pod constraints and scheduling policies:
-->
&lt;p>æŽ¥ä¸‹æ¥ï¼Œä½¿ç”¨æ­¤ StorageClass åˆ›å»ºä¸€ä¸ª Pod å’Œ PVCã€‚
æ­¤è¿‡ç¨‹ä¸Žä¹‹å‰ç›¸åŒï¼Œä½†åœ¨ PVC ä¸­æŒ‡å®šäº†ä¸åŒçš„ StorageClassã€‚
ä»¥ä¸‹æ˜¯ä¸€ä¸ªå‡è®¾ç¤ºä¾‹ï¼Œé€šè¿‡æŒ‡å®šè®¸å¤š Pod çº¦æŸå’Œè°ƒåº¦ç­–ç•¥æ¥æ¼”ç¤ºæ–°åŠŸèƒ½ç‰¹æ€§ï¼š&lt;/p>
&lt;!--
* multiple PVCs in a pod
* nodeAffinity across a subset of zones
* pod anti-affinity on zones
-->
&lt;ul>
&lt;li>ä¸€ä¸ª Pod å¤šä¸ª PVC&lt;/li>
&lt;li>è·¨å­åŒºåŸŸçš„èŠ‚ç‚¹äº²å’Œ&lt;/li>
&lt;li>åŒä¸€åŒºåŸŸ Pod åäº²å’Œ&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>apiVersion: apps/v1
kind: StatefulSet
metadata:
name: web
spec:
serviceName: &amp;quot;nginx&amp;quot;
replicas: 2
selector:
matchLabels:
app: nginx
template:
metadata:
labels:
app: nginx
spec:
affinity:
nodeAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
nodeSelectorTerms:
- matchExpressions:
- key: failure-domain.beta.kubernetes.io/zone
operator: In
values:
- us-central1-a
- us-central1-f
podAntiAffinity:
requiredDuringSchedulingIgnoredDuringExecution:
- labelSelector:
matchExpressions:
- key: app
operator: In
values:
- nginx
topologyKey: failure-domain.beta.kubernetes.io/zone
containers:
- name: nginx
image: gcr.io/google_containers/nginx-slim:0.8
ports:
- containerPort: 80
name: web
volumeMounts:
- name: www
mountPath: /usr/share/nginx/html
- name: logs
mountPath: /logs
volumeClaimTemplates:
- metadata:
name: www
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 10Gi
- metadata:
name: logs
spec:
accessModes: [ &amp;quot;ReadWriteOnce&amp;quot; ]
storageClassName: topology-aware-standard
resources:
requests:
storage: 1Gi
&lt;/code>&lt;/pre>&lt;!--
Afterwards, you can see that the volumes were provisioned in zones according to the policies set by the pod:
-->
&lt;p>ä¹‹åŽï¼Œæ‚¨å¯ä»¥çœ‹åˆ°æ ¹æ® Pod è®¾ç½®çš„ç­–ç•¥åœ¨åŒºåŸŸä¸­é…ç½®å·ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl get pv -o=jsonpath='{range .items[*]}{.spec.claimRef.name}{&amp;quot;\t&amp;quot;}{.metadata.labels.failure\-domain\.beta\.kubernetes\.io/zone}{&amp;quot;\n&amp;quot;}{end}'
www-web-0 us-central1-f
logs-web-0 us-central1-f
www-web-1 us-central1-a
logs-web-1 us-central1-a
&lt;/code>&lt;/pre>&lt;!--
## How can I learn more?
-->
&lt;h2 id="æˆ‘æ€Žæ ·æ‰èƒ½äº†è§£æ›´å¤š">æˆ‘æ€Žæ ·æ‰èƒ½äº†è§£æ›´å¤šï¼Ÿ&lt;/h2>
&lt;!--
Official documentation on the topology-aware dynamic provisioning feature is available here:https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode
-->
&lt;p>æœ‰å…³æ‹“æ‰‘æ„ŸçŸ¥åŠ¨æ€ä¾›åº”åŠŸèƒ½çš„å®˜æ–¹æ–‡æ¡£å¯åœ¨æ­¤å¤„èŽ·å–ï¼šhttps://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode&lt;/p>
&lt;!--
Documentation for CSI drivers is available at https://kubernetes-csi.github.io/docs/
-->
&lt;p>æœ‰å…³ CSI é©±åŠ¨ç¨‹åºçš„æ–‡æ¡£ï¼Œè¯·è®¿é—®ï¼šhttps://kubernetes-csi.github.io/docs/&lt;/p>
&lt;!--
## Whatâ€™s next?
-->
&lt;h2 id="ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆ">ä¸‹ä¸€æ­¥æ˜¯ä»€ä¹ˆï¼Ÿ&lt;/h2>
&lt;!--
We are actively working on improving this feature to support:
-->
&lt;p>æˆ‘ä»¬æ­£ç§¯æžè‡´åŠ›äºŽæ”¹è¿›æ­¤åŠŸèƒ½ä»¥æ”¯æŒï¼š&lt;/p>
&lt;!--
* more volume types, including dynamic provisioning for local volumes
* dynamic volume attachable count and capacity limits per node
-->
&lt;ul>
&lt;li>æ›´å¤šå·ç±»åž‹ï¼ŒåŒ…æ‹¬æœ¬åœ°å·çš„åŠ¨æ€ä¾›åº”&lt;/li>
&lt;li>åŠ¨æ€å®¹é‡å¯é™„åŠ è®¡æ•°å’Œæ¯ä¸ªèŠ‚ç‚¹çš„å®¹é‡é™åˆ¶&lt;/li>
&lt;/ul>
&lt;!--
## How do I get involved?
-->
&lt;h2 id="æˆ‘å¦‚ä½•å‚ä¸Ž">æˆ‘å¦‚ä½•å‚ä¸Žï¼Ÿ&lt;/h2>
&lt;!--
If you have feedback for this feature or are interested in getting involved with the design and development, join the [Kubernetes Storage Special-Interest-Group](https://github.com/kubernetes/community/tree/master/sig-storage) (SIG). Weâ€™re rapidly growing and always welcome new contributors.
-->
&lt;p>å¦‚æžœæ‚¨å¯¹æ­¤åŠŸèƒ½æœ‰åé¦ˆæ„è§æˆ–æœ‰å…´è¶£å‚ä¸Žè®¾è®¡å’Œå¼€å‘ï¼Œè¯·åŠ å…¥ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes å­˜å‚¨ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a>ï¼ˆSIGï¼‰ã€‚æˆ‘ä»¬æ­£åœ¨å¿«é€Ÿæˆé•¿ï¼Œå¹¶å§‹ç»ˆæ¬¢è¿Žæ–°çš„è´¡çŒ®è€…ã€‚&lt;/p>
&lt;!--
Special thanks to all the contributors that helped bring this feature to beta, including Cheng Xing ([verult](https://github.com/verult)), Chuqiang Li ([lichuqiang](https://github.com/lichuqiang)), David Zhu ([davidz627](https://github.com/davidz627)), Deep Debroy ([ddebroy](https://github.com/ddebroy)), Jan Å afrÃ¡nek ([jsafrane](https://github.com/jsafrane)), Jordan Liggitt ([liggitt](https://github.com/liggitt)), Michelle Au ([msau42](https://github.com/msau42)), Pengfei Ni ([feiskyer](https://github.com/feiskyer)), Saad Ali ([saad-ali](https://github.com/saad-ali)), Tim Hockin ([thockin](https://github.com/thockin)), and Yecheng Fu ([cofyc](https://github.com/cofyc)).
-->
&lt;p>ç‰¹åˆ«æ„Ÿè°¢å¸®åŠ©æŽ¨å‡ºæ­¤åŠŸèƒ½çš„æ‰€æœ‰è´¡çŒ®è€…ï¼ŒåŒ…æ‹¬ Cheng Xing (&lt;a href="https://github.com/verult">verult&lt;/a>)ã€Chuqiang Li (&lt;a href="https://github.com/lichuqiang">lichuqiang&lt;/a>)ã€David Zhu (&lt;a href="https://github.com/davidz627">davidz627&lt;/a>)ã€Deep Debroy (&lt;a href="https://github.com/ddebroy">ddebroy&lt;/a>)ã€Jan Å afrÃ¡nek (&lt;a href="https://github.com/jsafrane">jsafrane&lt;/a>)ã€Jordan Liggitt (&lt;a href="https://github.com/liggitt">liggitt&lt;/a>)ã€Michelle Au (&lt;a href="https://github.com/msau42">msau42&lt;/a>)ã€Pengfei Ni (&lt;a href="https://github.com/feiskyer">feiskyer&lt;/a>)ã€Saad Ali (&lt;a href="https://github.com/saad-ali">saad-ali&lt;/a>)ã€Tim Hockin (&lt;a href="https://github.com/thockin">thockin&lt;/a>)ï¼Œä»¥åŠ Yecheng Fu (&lt;a href="https://github.com/cofyc">cofyc&lt;/a>)ã€‚&lt;/p></description></item><item><title>Blog: KubeDirectorï¼šåœ¨ Kubernetes ä¸Šè¿è¡Œå¤æ‚çŠ¶æ€åº”ç”¨ç¨‹åºçš„ç®€å•æ–¹æ³•</title><link>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</link><pubDate>Wed, 03 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/03/kubedirector%E5%9C%A8-kubernetes-%E4%B8%8A%E8%BF%90%E8%A1%8C%E5%A4%8D%E6%9D%82%E7%8A%B6%E6%80%81%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%AE%80%E5%8D%95%E6%96%B9%E6%B3%95/</guid><description>
&lt;!--
layout: blog
title: 'KubeDirector: The easy way to run complex stateful applications on Kubernetes'
date: 2018-10-03
-->
&lt;!--
**Author**: Thomas Phelan (BlueData)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šThomas Phelanï¼ˆBlueDataï¼‰&lt;/p>
&lt;!--
KubeDirector is an open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
-->
&lt;p>KubeDirector æ˜¯ä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæ—¨åœ¨ç®€åŒ–åœ¨ Kubernetes ä¸Šè¿è¡Œå¤æ‚çš„æœ‰çŠ¶æ€æ‰©å±•åº”ç”¨ç¨‹åºé›†ç¾¤ã€‚KubeDirector ä½¿ç”¨è‡ªå®šä¹‰èµ„æºå®šä¹‰ï¼ˆCRDï¼‰
æ¡†æž¶æž„å»ºï¼Œå¹¶åˆ©ç”¨äº†æœ¬åœ° Kubernetes API æ‰©å±•å’Œè®¾è®¡å“²å­¦ã€‚è¿™æ”¯æŒä¸Ž Kubernetes ç”¨æˆ·/èµ„æº ç®¡ç†ä»¥åŠçŽ°æœ‰å®¢æˆ·ç«¯å’Œå·¥å…·çš„é€æ˜Žé›†æˆã€‚&lt;/p>
&lt;!--
We recently [introduced the KubeDirector project](https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/), as part of a broader open source Kubernetes initiative we call BlueK8s. Iâ€™m happy to announce that the pre-alpha
code for [KubeDirector](https://github.com/bluek8s/kubedirector/) is now available. And in this blog post, Iâ€™ll show how it works.
-->
&lt;p>æˆ‘ä»¬æœ€è¿‘&lt;a href="https://medium.com/@thomas_phelan/operation-stateful-introducing-bluek8s-and-kubernetes-director-aa204952f619/">ä»‹ç»äº† KubeDirector é¡¹ç›®&lt;/a>ï¼Œä½œä¸ºæˆ‘ä»¬ç§°ä¸º BlueK8s çš„æ›´å¹¿æ³›çš„ Kubernetes å¼€æºé¡¹ç›®çš„ä¸€éƒ¨åˆ†ã€‚æˆ‘å¾ˆé«˜å…´åœ°å®£å¸ƒ &lt;a href="https://github.com/bluek8s/kubedirector/">KubeDirector&lt;/a> çš„
pre-alpha ä»£ç çŽ°åœ¨å·²ç»å¯ç”¨ã€‚åœ¨è¿™ç¯‡åšå®¢æ–‡ç« ä¸­ï¼Œæˆ‘å°†å±•ç¤ºå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚&lt;/p>
&lt;!--
KubeDirector provides the following capabilities:
-->
&lt;p>KubeDirector æä¾›ä»¥ä¸‹åŠŸèƒ½ï¼š&lt;/p>
&lt;!--
* The ability to run non-cloud native stateful applications on Kubernetes without modifying the code. In other words, itâ€™s not necessary to decompose these existing applications to fit a microservices design pattern.
* Native support for preserving application-specific configuration and state.
* An application-agnostic deployment pattern, minimizing the time to onboard new stateful applications to Kubernetes.
-->
&lt;ul>
&lt;li>æ— éœ€ä¿®æ”¹ä»£ç å³å¯åœ¨ Kubernetes ä¸Šè¿è¡Œéžäº‘åŽŸç”Ÿæœ‰çŠ¶æ€åº”ç”¨ç¨‹åºã€‚æ¢å¥è¯è¯´ï¼Œä¸éœ€è¦åˆ†è§£è¿™äº›çŽ°æœ‰çš„åº”ç”¨ç¨‹åºæ¥é€‚åº”å¾®æœåŠ¡è®¾è®¡æ¨¡å¼ã€‚&lt;/li>
&lt;li>æœ¬æœºæ”¯æŒä¿å­˜ç‰¹å®šäºŽåº”ç”¨ç¨‹åºçš„é…ç½®å’ŒçŠ¶æ€ã€‚&lt;/li>
&lt;li>ä¸Žåº”ç”¨ç¨‹åºæ— å…³çš„éƒ¨ç½²æ¨¡å¼ï¼Œæœ€å¤§é™åº¦åœ°å‡å°‘å°†æ–°çš„æœ‰çŠ¶æ€åº”ç”¨ç¨‹åºè£…è½½åˆ° Kubernetes çš„æ—¶é—´ã€‚&lt;/li>
&lt;/ul>
&lt;!--
KubeDirector enables data scientists familiar with data-intensive distributed applications such as Hadoop, Spark, Cassandra, TensorFlow, Caffe2, etc. to run these applications on Kubernetes -- with a minimal learning curve and no need to write GO code. The applications controlled by KubeDirector are defined by some basic metadata and an associated package of configuration artifacts. The application metadata is referred to as a KubeDirectorApp resource.
-->
&lt;p>KubeDirector ä½¿ç†Ÿæ‚‰æ•°æ®å¯†é›†åž‹åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºï¼ˆå¦‚ Hadoopã€Sparkã€Cassandraã€TensorFlowã€Caffe2 ç­‰ï¼‰çš„æ•°æ®ç§‘å­¦å®¶èƒ½å¤Ÿåœ¨ Kubernetes ä¸Šè¿è¡Œè¿™äº›åº”ç”¨ç¨‹åº -- åªéœ€æžå°‘çš„å­¦ä¹ æ›²çº¿ï¼Œæ— éœ€ç¼–å†™ GO ä»£ç ã€‚ç”± KubeDirector æŽ§åˆ¶çš„åº”ç”¨ç¨‹åºç”±ä¸€äº›åŸºæœ¬å…ƒæ•°æ®å’Œç›¸å…³çš„é…ç½®å·¥ä»¶åŒ…å®šä¹‰ã€‚åº”ç”¨ç¨‹åºå…ƒæ•°æ®ç§°ä¸º KubeDirectorApp èµ„æºã€‚&lt;/p>
&lt;!--
To understand the components of KubeDirector, clone the repository on [GitHub](https://github.com/bluek8s/kubedirector/) using a command similar to:
-->
&lt;p>è¦äº†è§£ KubeDirector çš„ç»„ä»¶ï¼Œè¯·ä½¿ç”¨ç±»ä¼¼äºŽä»¥ä¸‹çš„å‘½ä»¤åœ¨ &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub&lt;/a> ä¸Šå…‹éš†å­˜å‚¨åº“ï¼š&lt;/p>
&lt;pre>&lt;code>git clone http://&amp;lt;userid&amp;gt;@github.com/bluek8s/kubedirector.
&lt;/code>&lt;/pre>&lt;!--
The KubeDirectorApp definition for the Spark 2.2.1 application is located
in the file `kubedirector/deploy/example_catalog/cr-app-spark221e2.json`.
-->
&lt;p>Spark 2.2.1 åº”ç”¨ç¨‹åºçš„ KubeDirectorApp å®šä¹‰ä½äºŽæ–‡ä»¶ &lt;code>kubedirector/deploy/example_catalog/cr-app-spark221e2.json&lt;/code> ä¸­ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_catalog/cr-app-spark221e2.json
{
&amp;quot;apiVersion&amp;quot;: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;,
&amp;quot;kind&amp;quot;: &amp;quot;KubeDirectorApp&amp;quot;,
&amp;quot;metadata&amp;quot;: {
&amp;quot;name&amp;quot; : &amp;quot;spark221e2&amp;quot;
},
&amp;quot;spec&amp;quot; : {
&amp;quot;systemctlMounts&amp;quot;: true,
&amp;quot;config&amp;quot;: {
&amp;quot;node_services&amp;quot;: [
{
&amp;quot;service_ids&amp;quot;: [
&amp;quot;ssh&amp;quot;,
&amp;quot;spark&amp;quot;,
&amp;quot;spark_master&amp;quot;,
&amp;quot;spark_worker&amp;quot;
],
â€¦
&lt;/code>&lt;/pre>&lt;!--
The configuration of an application cluster is referred to as a KubeDirectorCluster resource. The
KubeDirectorCluster definition for a sample Spark 2.2.1 cluster is located in the file
`kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml`.
-->
&lt;p>åº”ç”¨ç¨‹åºé›†ç¾¤çš„é…ç½®ç§°ä¸º KubeDirectorCluster èµ„æºã€‚ç¤ºä¾‹ Spark 2.2.1 é›†ç¾¤çš„ KubeDirectorCluster å®šä¹‰ä½äºŽæ–‡ä»¶
&lt;code>kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml&lt;/code> ä¸­ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; cat kubedirector/deploy/example_clusters/cr-cluster-spark221.e1.yaml
apiVersion: &amp;quot;kubedirector.bluedata.io/v1alpha1&amp;quot;
kind: &amp;quot;KubeDirectorCluster&amp;quot;
metadata:
name: &amp;quot;spark221e2&amp;quot;
spec:
app: spark221e2
roles:
- name: controller
replicas: 1
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: worker
replicas: 2
resources:
requests:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
limits:
memory: &amp;quot;4Gi&amp;quot;
cpu: &amp;quot;2&amp;quot;
- name: jupyter
â€¦
&lt;/code>&lt;/pre>&lt;!--
## Running Spark on Kubernetes with KubeDirector
-->
&lt;h2 id="ä½¿ç”¨-kubedirector-åœ¨-kubernetes-ä¸Šè¿è¡Œ-spark">ä½¿ç”¨ KubeDirector åœ¨ Kubernetes ä¸Šè¿è¡Œ Spark&lt;/h2>
&lt;!--
With KubeDirector, itâ€™s easy to run Spark clusters on Kubernetes.
-->
&lt;p>ä½¿ç”¨ KubeDirectorï¼Œå¯ä»¥è½»æ¾åœ¨ Kubernetes ä¸Šè¿è¡Œ Spark é›†ç¾¤ã€‚&lt;/p>
&lt;!--
First, verify that Kubernetes (version 1.9 or later) is running, using the command `kubectl version`
-->
&lt;p>é¦–å…ˆï¼Œä½¿ç”¨å‘½ä»¤ &lt;code>kubectl version&lt;/code> éªŒè¯ Kubernetesï¼ˆç‰ˆæœ¬ 1.9 æˆ–æ›´é«˜ï¼‰æ˜¯å¦æ­£åœ¨è¿è¡Œ&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl version
Client Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T18:02:47Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
Server Version: version.Info{Major:&amp;quot;1&amp;quot;, Minor:&amp;quot;11&amp;quot;, GitVersion:&amp;quot;v1.11.3&amp;quot;, GitCommit:&amp;quot;a4529464e4629c21224b3d52edfe0ea91b072862&amp;quot;, GitTreeState:&amp;quot;clean&amp;quot;, BuildDate:&amp;quot;2018-09-09T17:53:03Z&amp;quot;, GoVersion:&amp;quot;go1.10.3&amp;quot;, Compiler:&amp;quot;gc&amp;quot;, Platform:&amp;quot;linux/amd64&amp;quot;}
&lt;/code>&lt;/pre>&lt;!--
Deploy the KubeDirector service and the example KubeDirectorApp resource definitions with the commands:
-->
&lt;p>ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤éƒ¨ç½² KubeDirector æœåŠ¡å’Œç¤ºä¾‹ KubeDirectorApp èµ„æºå®šä¹‰ï¼š&lt;/p>
&lt;pre>&lt;code>cd kubedirector
make deploy
&lt;/code>&lt;/pre>&lt;!--
These will start the KubeDirector pod:
-->
&lt;p>è¿™äº›å°†å¯åŠ¨ KubeDirector podï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-qd9hb 1/1 Running 0 1m
&lt;/code>&lt;/pre>&lt;!--
List the installed KubeDirector applications with `kubectl get KubeDirectorApp`
-->
&lt;p>&lt;code>kubectl get KubeDirectorApp&lt;/code> åˆ—å‡ºä¸­å·²å®‰è£…çš„ KubeDirector åº”ç”¨ç¨‹åº&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get KubeDirectorApp
NAME AGE
cassandra311 30m
spark211up 30m
spark221e2 30m
&lt;/code>&lt;/pre>&lt;!--
Now you can launch a Spark 2.2.1 cluster using the example KubeDirectorCluster file and the
`kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml` command.
Verify that the Spark cluster has been started:
-->
&lt;p>çŽ°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ç¤ºä¾‹ KubeDirectorCluster æ–‡ä»¶å’Œ &lt;code>kubectl create -f deploy/example_clusters/cr-cluster-spark211up.yaml&lt;/code> å‘½ä»¤
å¯åŠ¨ Spark 2.2.1 é›†ç¾¤ã€‚éªŒè¯ Spark é›†ç¾¤å·²ç»å¯åŠ¨:&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
kubedirector-58cf59869-djdwl 1/1 Running 0 19m
spark221e2-controller-zbg4d-0 1/1 Running 0 23m
spark221e2-jupyter-2km7q-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-0 1/1 Running 0 23m
spark221e2-worker-4gzbz-1 1/1 Running 0 23m
&lt;/code>&lt;/pre>&lt;!--
The running services now include the Spark services:
-->
&lt;p>çŽ°åœ¨è¿è¡Œçš„æœåŠ¡åŒ…æ‹¬ Spark æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 21s
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 20s
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 20s
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 20s
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 20s
&lt;/code>&lt;/pre>&lt;!--
Pointing the browser at port 31533 connects to the Spark Master UI:
-->
&lt;p>å°†æµè§ˆå™¨æŒ‡å‘ç«¯å£ 31533 è¿žæŽ¥åˆ° Spark ä¸»èŠ‚ç‚¹ UIï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-10-03-kubedirector/kubedirector.png" alt="kubedirector">&lt;/p>
&lt;!--
Thatâ€™s all there is to it!
In fact, in the example above we also deployed a Jupyter notebook along with the Spark cluster.
-->
&lt;p>å°±æ˜¯è¿™æ ·!
äº‹å®žä¸Šï¼Œåœ¨ä¸Šé¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬è¿˜éƒ¨ç½²äº†ä¸€ä¸ª Jupyter notebook å’Œ Spark é›†ç¾¤ã€‚&lt;/p>
&lt;!--
To start another application (e.g. Cassandra), just specify another KubeDirectorApp file:
-->
&lt;p>è¦å¯åŠ¨å¦ä¸€ä¸ªåº”ç”¨ç¨‹åºï¼ˆä¾‹å¦‚ Cassandraï¼‰ï¼Œåªéœ€æŒ‡å®šå¦ä¸€ä¸ª KubeDirectorApp æ–‡ä»¶ï¼š&lt;/p>
&lt;pre>&lt;code>kubectl create -f deploy/example_clusters/cr-cluster-cassandra311.yaml
&lt;/code>&lt;/pre>&lt;!--
See the running Cassandra cluster:
-->
&lt;p>æŸ¥çœ‹æ­£åœ¨è¿è¡Œçš„ Cassandra é›†ç¾¤ï¼š&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get pods
NAME READY STATUS RESTARTS AGE
cassandra311-seed-v24r6-0 1/1 Running 0 1m
cassandra311-seed-v24r6-1 1/1 Running 0 1m
cassandra311-worker-rqrhl-0 1/1 Running 0 1m
cassandra311-worker-rqrhl-1 1/1 Running 0 1m
kubedirector-58cf59869-djdwl 1/1 Running 0 1d
spark221e2-controller-tq8d6-0 1/1 Running 0 22m
spark221e2-jupyter-6989v-0 1/1 Running 0 22m
spark221e2-worker-d9892-0 1/1 Running 0 22m
spark221e2-worker-d9892-1 1/1 Running 0 22m
&lt;/code>&lt;/pre>&lt;!--
Now you have a Spark cluster (with a Jupyter notebook) and a Cassandra cluster running on Kubernetes.
Use `kubectl get service` to see the set of services.
-->
&lt;p>çŽ°åœ¨ï¼Œæ‚¨æœ‰ä¸€ä¸ª Spark é›†ç¾¤ï¼ˆå¸¦æœ‰ Jupyter notebook ï¼‰å’Œä¸€ä¸ªè¿è¡Œåœ¨ Kubernetes ä¸Šçš„ Cassandra é›†ç¾¤ã€‚
ä½¿ç”¨ &lt;code>kubectl get service&lt;/code> æŸ¥çœ‹æœåŠ¡é›†ã€‚&lt;/p>
&lt;pre>&lt;code>~&amp;gt; kubectl get service
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
kubedirector ClusterIP 10.98.234.194 &amp;lt;none&amp;gt; 60000/TCP 1d
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 1d
svc-cassandra311-seed-v24r6-0 NodePort 10.96.94.204 &amp;lt;none&amp;gt; 22:31131/TCP,9042:30739/TCP 3m
svc-cassandra311-seed-v24r6-1 NodePort 10.106.144.52 &amp;lt;none&amp;gt; 22:30373/TCP,9042:32662/TCP 3m
svc-cassandra311-vhh29 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 3m
svc-cassandra311-worker-rqrhl-0 NodePort 10.109.61.194 &amp;lt;none&amp;gt; 22:31832/TCP,9042:31962/TCP 3m
svc-cassandra311-worker-rqrhl-1 NodePort 10.97.147.131 &amp;lt;none&amp;gt; 22:31454/TCP,9042:31170/TCP 3m
svc-spark221e2-5tg48 ClusterIP None &amp;lt;none&amp;gt; 8888/TCP 24m
svc-spark221e2-controller-tq8d6-0 NodePort 10.104.181.123 &amp;lt;none&amp;gt; 22:30534/TCP,8080:31533/TCP,7077:32506/TCP,8081:32099/TCP 24m
svc-spark221e2-jupyter-6989v-0 NodePort 10.105.227.249 &amp;lt;none&amp;gt; 22:30632/TCP,8888:30355/TCP 24m
svc-spark221e2-worker-d9892-0 NodePort 10.107.131.165 &amp;lt;none&amp;gt; 22:30358/TCP,8081:32144/TCP 24m
svc-spark221e2-worker-d9892-1 NodePort 10.110.88.221 &amp;lt;none&amp;gt; 22:30294/TCP,8081:31436/TCP 24m
&lt;/code>&lt;/pre>&lt;!--
## Get Involved
-->
&lt;h2 id="å‚ä¸Žå…¶ä¸­">å‚ä¸Žå…¶ä¸­&lt;/h2>
&lt;!--
KubeDirector is a fully open source, Apache v2 licensed, project â€“ the first of multiple open source projects within a broader initiative we call BlueK8s.
The pre-alpha code for KubeDirector has just been released and we would love for you to join the growing community of developers, contributors, and adopters.
Follow [@BlueK8s](https://twitter.com/BlueK8s/) on Twitter and get involved through these channels:
-->
&lt;p>KubeDirector æ˜¯ä¸€ä¸ªå®Œå…¨å¼€æ”¾æºç çš„ Apache v2 æŽˆæƒé¡¹ç›® â€“ åœ¨æˆ‘ä»¬ç§°ä¸º BlueK8s çš„æ›´å¹¿æ³›çš„è®¡åˆ’ä¸­ï¼Œå®ƒæ˜¯å¤šä¸ªå¼€æ”¾æºç é¡¹ç›®ä¸­çš„ç¬¬ä¸€ä¸ªã€‚
KubeDirector çš„ pre-alpha ä»£ç åˆšåˆšå‘å¸ƒï¼Œæˆ‘ä»¬å¸Œæœ›æ‚¨åŠ å…¥åˆ°ä¸æ–­å¢žé•¿çš„å¼€å‘äººå‘˜ã€è´¡çŒ®è€…å’Œä½¿ç”¨è€…ç¤¾åŒºã€‚
åœ¨ Twitter ä¸Šå…³æ³¨ &lt;a href="https://twitter.com/BlueK8s/">@BlueK8s&lt;/a>ï¼Œå¹¶é€šè¿‡ä»¥ä¸‹æ¸ é“å‚ä¸Ž:&lt;/p>
&lt;!--
* KubeDirector [chat room on Slack](https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/)
* KubeDirector [GitHub repo](https://github.com/bluek8s/kubedirector/)
-->
&lt;ul>
&lt;li>KubeDirector &lt;a href="https://join.slack.com/t/bluek8s/shared_invite/enQtNDUwMzkwODY5OTM4LTRhYmRmZmE4YzY3OGUzMjA1NDg0MDVhNDQ2MGNkYjRhM2RlMDNjMTI1NDQyMjAzZGVlMDFkNThkNGFjZGZjMGY/">Slack èŠå¤©å®¤&lt;/a>&lt;/li>
&lt;li>KubeDirector &lt;a href="https://github.com/bluek8s/kubedirector/">GitHub ä»“åº“&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: åœ¨ Kubernetes ä¸Šå¯¹ gRPC æœåŠ¡å™¨è¿›è¡Œå¥åº·æ£€æŸ¥</title><link>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</link><pubDate>Mon, 01 Oct 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/10/01/%E5%9C%A8-kubernetes-%E4%B8%8A%E5%AF%B9-grpc-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%BF%9B%E8%A1%8C%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5/</guid><description>
&lt;!--
---
layout: blog
title: 'Health checking gRPC servers on Kubernetes'
date: 2018-10-01
---
--->
&lt;!--
**Author**: [Ahmet Alp Balkan](https://twitter.com/ahmetb) (Google)
--->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼š &lt;a href="https://twitter.com/ahmetb">Ahmet Alp Balkan&lt;/a> (Google)&lt;/p>
&lt;!--
[gRPC](https://grpc.io) is on its way to becoming the lingua franca for
communication between cloud-native microservices. If you are deploying gRPC
applications to Kubernetes today, you may be wondering about the best way to
configure health checks. In this article, we will talk about
[grpc-health-probe](https://github.com/grpc-ecosystem/grpc-health-probe/), a
Kubernetes-native way to health check gRPC apps.
--->
&lt;p>&lt;a href="https://grpc.io">gRPC&lt;/a> å°†æˆä¸ºæœ¬åœ°äº‘å¾®æœåŠ¡é—´è¿›è¡Œé€šä¿¡çš„é€šç”¨è¯­è¨€ã€‚å¦‚æžœæ‚¨çŽ°åœ¨å°† gRPC åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Kubernetesï¼Œæ‚¨å¯èƒ½ä¼šæƒ³è¦äº†è§£é…ç½®å¥åº·æ£€æŸ¥çš„æœ€ä½³æ–¹æ³•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç» &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc-health-probe&lt;/a>ï¼Œè¿™æ˜¯ Kubernetes åŽŸç”Ÿçš„å¥åº·æ£€æŸ¥ gRPC åº”ç”¨ç¨‹åºçš„æ–¹æ³•ã€‚&lt;/p>
&lt;!--
If you're unfamiliar, Kubernetes [health
checks](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
(liveness and readiness probes) is what's keeping your applications available
while you're sleeping. They detect unresponsive pods, mark them unhealthy, and
cause these pods to be restarted or rescheduled.
--->
&lt;p>å¦‚æžœæ‚¨ä¸ç†Ÿæ‚‰ï¼ŒKubernetesçš„ &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">å¥åº·æ£€æŸ¥&lt;/a>ï¼ˆå­˜æ´»æŽ¢é’ˆå’Œå°±ç»ªæŽ¢é’ˆï¼‰å¯ä»¥ä½¿æ‚¨çš„åº”ç”¨ç¨‹åºåœ¨ç¡çœ æ—¶ä¿æŒå¯ç”¨çŠ¶æ€ã€‚å½“æ£€æµ‹åˆ°æ²¡æœ‰å›žåº”çš„ Pod æ—¶ï¼Œä¼šå°†å…¶æ ‡è®°ä¸ºä¸å¥åº·ï¼Œå¹¶ä½¿è¿™äº› Pod é‡æ–°å¯åŠ¨æˆ–é‡æ–°å®‰æŽ’ã€‚&lt;/p>
&lt;!--
Kubernetes [does not
support](https://github.com/kubernetes/kubernetes/issues/21493) gRPC health
checks natively. This leaves the gRPC developers with the following three
approaches when they deploy to Kubernetes:
[![options for health checking grpc on kubernetes today](/images/blog/2019-09-30-health-checking-grpc/options.png)](/images/blog/2019-09-30-health-checking-grpc/options.png)
--->
&lt;p>Kubernetes åŽŸæœ¬ &lt;a href="https://github.com/kubernetes/kubernetes/issues/21493">ä¸æ”¯æŒ&lt;/a> gRPC å¥åº·æ£€æŸ¥ã€‚gRPC çš„å¼€å‘äººå‘˜åœ¨ Kubernetes ä¸­éƒ¨ç½²æ—¶å¯ä»¥é‡‡ç”¨ä»¥ä¸‹ä¸‰ç§æ–¹æ³•ï¼š&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png">&lt;img src="https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/options.png" alt="å½“å‰åœ¨ kubernetes ä¸Šè¿›è¡Œ gRPC å¥åº·æ£€æŸ¥çš„é€‰é¡¹">&lt;/a>&lt;/p>
&lt;!--
1. **httpGet probe:** Cannot be natively used with gRPC. You need to refactor
your app to serve both gRPC and HTTP/1.1 protocols (on different port
numbers).
2. **tcpSocket probe:** Opening a socket to gRPC server is not meaningful,
since it cannot read the response body.
3. **exec probe:** This invokes a program in a container's ecosystem
periodically. In the case of gRPC, this means you implement a health RPC
yourself, then write and ship a client tool with your container.
Can we do better? Absolutely.
--->
&lt;ol>
&lt;li>&lt;strong>httpGet probï¼š&lt;/strong> ä¸èƒ½ä¸Ž gRPC ä¸€èµ·ä½¿ç”¨ã€‚æ‚¨éœ€è¦é‡æž„æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œå¿…é¡»åŒæ—¶æ”¯æŒ gRPC å’Œ HTTP/1.1 åè®®ï¼ˆåœ¨ä¸åŒçš„ç«¯å£å·ä¸Šï¼‰ã€‚&lt;/li>
&lt;li>&lt;strong>tcpSocket probeï¼š&lt;/strong> æ‰“å¼€ gRPC æœåŠ¡å™¨çš„ Socket æ˜¯æ²¡æœ‰æ„ä¹‰çš„ï¼Œå› ä¸ºå®ƒæ— æ³•è¯»å–å“åº”ä¸»ä½“ã€‚&lt;/li>
&lt;li>&lt;strong>exec probeï¼š&lt;/strong> å°†å®šæœŸè°ƒç”¨å®¹å™¨ç”Ÿæ€ç³»ç»Ÿä¸­çš„ç¨‹åºã€‚å¯¹äºŽ gRPCï¼Œè¿™æ„å‘³ç€æ‚¨è¦è‡ªå·±å®žçŽ°å¥åº· RPCï¼Œç„¶åŽä½¿ç”¨å®¹å™¨ç¼–å†™å¹¶äº¤ä»˜å®¢æˆ·ç«¯å·¥å…·ã€‚&lt;/li>
&lt;/ol>
&lt;p>æˆ‘ä»¬å¯ä»¥åšå¾—æ›´å¥½å—ï¼Ÿè¿™æ˜¯è‚¯å®šçš„ã€‚&lt;/p>
&lt;!--
## Introducing â€œgrpc-health-probeâ€
To standardize the "exec probe" approach mentioned above, we need:
- a **standard** health check "protocol" that can be implemented in any gRPC
server easily.
- a **standard** health check "tool" that can query the health protocol easily.
--->
&lt;h2 id="ä»‹ç»-grpc-health-probe">ä»‹ç» â€œgrpc-health-probeâ€&lt;/h2>
&lt;p>ä¸ºäº†ä½¿ä¸Šè¿° &amp;quot;exec probe&amp;quot; æ–¹æ³•æ ‡å‡†åŒ–ï¼Œæˆ‘ä»¬éœ€è¦ï¼š&lt;/p>
&lt;ul>
&lt;li>å¯ä»¥åœ¨ä»»ä½• gRPC æœåŠ¡å™¨ä¸­è½»æ¾å®žçŽ°çš„ &lt;strong>æ ‡å‡†&lt;/strong> å¥åº·æ£€æŸ¥ &amp;quot;åè®®&amp;quot; ã€‚&lt;/li>
&lt;li>ä¸€ç§ &lt;strong>æ ‡å‡†&lt;/strong> å¥åº·æ£€æŸ¥ &amp;quot;å·¥å…·&amp;quot; ï¼Œå¯ä»¥è½»æ¾æŸ¥è¯¢å¥åº·åè®®ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Thankfully, gRPC has a [standard health checking
protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md). It
can be used easily from any language. Generated code and the utilities for
setting the health status are shipped in nearly all language implementations of
gRPC.
--->
&lt;p>å¹¸è¿çš„æ˜¯ï¼ŒgRPC å…·æœ‰ &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">æ ‡å‡†çš„å¥åº·æ£€æŸ¥åè®®&lt;/a>ã€‚å¯ä»¥ç”¨ä»»ä½•è¯­è¨€è½»æ¾è°ƒç”¨å®ƒã€‚å‡ ä¹Žæ‰€æœ‰å®žçŽ° gRPC çš„è¯­è¨€éƒ½é™„å¸¦äº†ç”Ÿæˆçš„ä»£ç å’Œç”¨äºŽè®¾ç½®å¥åº·çŠ¶æ€çš„å®žç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
If you
[implement](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto)
this health check protocol in your gRPC apps, you can then use a standard/common
tool to invoke this `Check()` method to determine server status.
--->
&lt;p>å¦‚æžœæ‚¨åœ¨ gRPC åº”ç”¨ç¨‹åºä¸­ &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">å®žçŽ°&lt;/a> æ­¤å¥åº·æ£€æŸ¥åè®®ï¼Œé‚£ä¹ˆå¯ä»¥ä½¿ç”¨æ ‡å‡†æˆ–é€šç”¨å·¥å…·è°ƒç”¨ &lt;code>Check()&lt;/code> æ–¹æ³•æ¥ç¡®å®šæœåŠ¡å™¨çŠ¶æ€ã€‚&lt;/p>
&lt;!--
The next thing you need is the "standard tool", and it's the
[**grpc-health-probe**](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>æŽ¥ä¸‹æ¥æ‚¨éœ€è¦çš„æ˜¯ &amp;quot;æ ‡å‡†å·¥å…·&amp;quot; &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">&lt;strong>grpc-health-probe&lt;/strong>&lt;/a>ã€‚&lt;/p>
&lt;a href='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'>
&lt;img width="768" title='grpc-health-probe on kubernetes'
src='https://kubernetes.io/images/blog/2019-09-30-health-checking-grpc/grpc_health_probe.png'/>
&lt;/a>
&lt;!--
With this tool, you can use the same health check configuration in all your gRPC
applications. This approach requires you to:
--->
&lt;p>ä½¿ç”¨æ­¤å·¥å…·ï¼Œæ‚¨å¯ä»¥åœ¨æ‰€æœ‰ gRPC åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ç›¸åŒçš„å¥åº·æ£€æŸ¥é…ç½®ã€‚è¿™ç§æ–¹æ³•æœ‰ä»¥ä¸‹è¦æ±‚ï¼š&lt;/p>
&lt;!--
1. Find the gRPC "health" module in your favorite language and start using it
(example [Go library](https://godoc.org/github.com/grpc/grpc-go/health)).
2. Ship the
[grpc_health_probe](https://github.com/grpc-ecosystem/grpc-health-probe/)
binary in your container.
3. [Configure](https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes)
Kubernetes "exec" probe to invoke the "grpc_health_probe" tool in the
container.
--->
&lt;ol>
&lt;li>ç”¨æ‚¨å–œæ¬¢çš„è¯­è¨€æ‰¾åˆ° gRPC çš„ &amp;quot;å¥åº·&amp;quot; æ¨¡å—å¹¶å¼€å§‹ä½¿ç”¨å®ƒï¼ˆä¾‹å¦‚ &lt;a href="https://godoc.org/github.com/grpc/grpc-go/health">Go åº“&lt;/a>ï¼‰ã€‚&lt;/li>
&lt;li>å°†äºŒè¿›åˆ¶æ–‡ä»¶ &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">grpc_health_probe&lt;/a> é€åˆ°å®¹å™¨ä¸­ã€‚&lt;/li>
&lt;li>&lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/tree/1329d682b4232c102600b5e7886df8ffdcaf9e26#example-grpc-health-checking-on-kubernetes">é…ç½®&lt;/a> Kubernetes çš„ &amp;quot;exec&amp;quot; æ£€æŸ¥æ¨¡å—æ¥è°ƒç”¨å®¹å™¨ä¸­çš„ &amp;quot;grpc_health_probe&amp;quot; å·¥å…·ã€‚&lt;/li>
&lt;/ol>
&lt;!--
In this case, executing "grpc_health_probe" will call your gRPC server over
`localhost`, since they are in the same pod.
--->
&lt;p>åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‰§è¡Œ &amp;quot;grpc_health_probe&amp;quot; å°†é€šè¿‡ &lt;code>localhost&lt;/code> è°ƒç”¨æ‚¨çš„ gRPC æœåŠ¡å™¨ï¼Œå› ä¸ºå®ƒä»¬ä½äºŽåŒä¸€ä¸ªå®¹å™¨ä¸­ã€‚&lt;/p>
&lt;!--
## What's next
**grpc-health-probe** project is still in its early days and it needs your
feedback. It supports a variety of features like communicating with TLS servers
and configurable connection/RPC timeouts.
--->
&lt;h2 id="ä¸‹ä¸€æ­¥å·¥ä½œ">ä¸‹ä¸€æ­¥å·¥ä½œ&lt;/h2>
&lt;p>&lt;strong>grpc-health-probe&lt;/strong> é¡¹ç›®ä»å¤„äºŽåˆæœŸé˜¶æ®µï¼Œéœ€è¦æ‚¨çš„åé¦ˆã€‚å®ƒæ”¯æŒå¤šç§åŠŸèƒ½ï¼Œä¾‹å¦‚ä¸Ž TLS æœåŠ¡å™¨é€šä¿¡å’Œé…ç½®å»¶æ—¶è¿žæŽ¥/RPCã€‚&lt;/p>
&lt;!--
If you are running a gRPC server on Kubernetes today, try using the gRPC Health
Protocol and try the grpc-health-probe in your deployments, and [give
feedback](https://github.com/grpc-ecosystem/grpc-health-probe/).
--->
&lt;p>å¦‚æžœæ‚¨æœ€è¿‘è¦åœ¨ Kubernetes ä¸Šè¿è¡Œ gRPC æœåŠ¡å™¨ï¼Œè¯·å°è¯•ä½¿ç”¨ gRPC Health Protocolï¼Œå¹¶åœ¨æ‚¨çš„ Deployment ä¸­å°è¯• grpc-health-probeï¼Œç„¶åŽ &lt;a href="https://github.com/grpc-ecosystem/grpc-health-probe/">è¿›è¡Œåé¦ˆ&lt;/a>ã€‚&lt;/p>
&lt;!--
## Further reading
- Protocol: [GRPC Health Checking Protocol](https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md) ([health.proto](https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto))
- Documentation: [Kubernetes liveness and readiness probes](/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/)
- Article: [Advanced Kubernetes Health Check Patterns](https://ahmet.im/blog/advanced-kubernetes-health-checks/)
--->
&lt;h2 id="æ›´å¤šå†…å®¹">æ›´å¤šå†…å®¹&lt;/h2>
&lt;ul>
&lt;li>åè®®ï¼š &lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/doc/health-checking.md">GRPC Health Checking Protocol&lt;/a> (&lt;a href="https://github.com/grpc/grpc/blob/v1.15.0/src/proto/grpc/health/v1/health.proto">health.proto&lt;/a>)&lt;/li>
&lt;li>æ–‡æ¡£ï¼š &lt;a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">Kubernetes å­˜æ´»å’Œå°±ç»ªæŽ¢é’ˆ&lt;/a>&lt;/li>
&lt;li>æ–‡ç« ï¼š &lt;a href="https://ahmet.im/blog/advanced-kubernetes-health-checks/">å‡çº§ç‰ˆ Kubernetes å¥åº·æ£€æŸ¥æ¨¡å¼&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: The Machines Can Do the Work, a Story of Kubernetes Testing, CI, and Automating the Contributor Experience</title><link>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</link><pubDate>Wed, 29 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/29/the-machines-can-do-the-work-a-story-of-kubernetes-testing-ci-and-automating-the-contributor-experience/</guid><description>
&lt;hr>
&lt;h2 id="date-2019-08-29">layout: blog
title: 'æœºå™¨å¯ä»¥å®Œæˆè¿™é¡¹å·¥ä½œï¼Œä¸€ä¸ªå…³äºŽ kubernetes æµ‹è¯•ã€CI å’Œè‡ªåŠ¨åŒ–è´¡çŒ®è€…ä½“éªŒçš„æ•…äº‹'
date: 2019-08-29&lt;/h2>
&lt;!--
**Author**: Aaron Crickenberger (Google) and Benjamin Elder (Google)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šAaron Crickenbergerï¼ˆè°·æ­Œï¼‰å’Œ Benjamin Elderï¼ˆè°·æ­Œï¼‰&lt;/p>
&lt;!--
_â€œLarge projects have a lot of less exciting, yet, hard work. We value time spent automating repetitive work more highly than toil. Where that work cannot be automated, it is our culture to recognize and reward all types of contributions. However, heroism is not sustainable.â€_ - [Kubernetes Community Values](https://git.k8s.io/community/values.md#automation-over-process)
-->
&lt;p>&lt;em>â€å¤§åž‹é¡¹ç›®æœ‰å¾ˆå¤šä¸é‚£ä¹ˆä»¤äººå…´å¥‹ï¼Œä½†å´å¾ˆè¾›è‹¦çš„å·¥ä½œã€‚æ¯”èµ·è¾›è‹¦å·¥ä½œï¼Œæˆ‘ä»¬æ›´é‡è§†æŠŠæ—¶é—´èŠ±åœ¨è‡ªåŠ¨åŒ–é‡å¤æ€§å·¥ä½œä¸Šï¼Œå¦‚æžœè¿™é¡¹å·¥ä½œæ— æ³•å®žçŽ°è‡ªåŠ¨åŒ–ï¼Œæˆ‘ä»¬çš„æ–‡åŒ–å°±æ˜¯æ‰¿è®¤å¹¶å¥–åŠ±æ‰€æœ‰ç±»åž‹çš„è´¡çŒ®ã€‚ç„¶è€Œï¼Œè‹±é›„ä¸»ä¹‰æ˜¯ä¸å¯æŒç»­çš„ã€‚â€œ&lt;/em> - &lt;a href="https://git.k8s.io/community/values.md#automation-over-process">Kubernetes Community Values&lt;/a>&lt;/p>
&lt;!--
Like many open source projects, Kubernetes is hosted on GitHub. We felt the barrier to participation would be lowest if the project lived where developers already worked, using tools and processes developers already knew. Thus the project embraced the service fully: it was the basis of our workflow, our issue tracker, our documentation, our blog platform, our team structure, and more.
-->
&lt;p>åƒè®¸å¤šå¼€æºé¡¹ç›®ä¸€æ ·ï¼ŒKubernetes æ‰˜ç®¡åœ¨ GitHub ä¸Šã€‚ å¦‚æžœé¡¹ç›®ä½äºŽåœ¨å¼€å‘äººå‘˜å·²ç»å·¥ä½œçš„åœ°æ–¹ï¼Œä½¿ç”¨çš„å¼€å‘äººå‘˜å·²ç»çŸ¥é“çš„å·¥å…·å’Œæµç¨‹ï¼Œé‚£ä¹ˆå‚ä¸Žçš„éšœç¢å°†æ˜¯æœ€ä½Žçš„ã€‚ å› æ­¤ï¼Œè¯¥é¡¹ç›®å®Œå…¨æŽ¥å—äº†è¿™é¡¹æœåŠ¡ï¼šå®ƒæ˜¯æˆ‘ä»¬å·¥ä½œæµç¨‹ï¼Œé—®é¢˜è·Ÿè¸ªï¼Œæ–‡æ¡£ï¼Œåšå®¢å¹³å°ï¼Œå›¢é˜Ÿç»“æž„ç­‰çš„åŸºç¡€ã€‚&lt;/p>
&lt;!--
This strategy worked. It worked so well that the project quickly scaled past its contributorsâ€™ capacity as humans. What followed was an incredible journey of automation and innovation. We didnâ€™t just need to rebuild our airplane mid-flight without crashing, we needed to convert it into a rocketship and launch into orbit. We needed machines to do the work.
-->
&lt;p>è¿™ä¸ªç­–ç•¥å¥æ•ˆäº†ã€‚ å®ƒè¿ä½œè‰¯å¥½ï¼Œä»¥è‡³äºŽè¯¥é¡¹ç›®è¿…é€Ÿè¶…è¶Šäº†å…¶è´¡çŒ®è€…çš„äººç±»èƒ½åŠ›ã€‚ æŽ¥ä¸‹æ¥æ˜¯ä¸€æ¬¡ä»¤äººéš¾ä»¥ç½®ä¿¡çš„è‡ªåŠ¨åŒ–å’Œåˆ›æ–°ä¹‹æ—…ã€‚ æˆ‘ä»¬ä¸ä»…éœ€è¦åœ¨é£žè¡Œé€”ä¸­é‡å»ºæˆ‘ä»¬çš„é£žæœºè€Œä¸ä¼šå´©æºƒï¼Œæˆ‘ä»¬éœ€è¦å°†å…¶è½¬æ¢ä¸ºç«ç®­é£žèˆ¹å¹¶å‘å°„åˆ°è½¨é“ã€‚ æˆ‘ä»¬éœ€è¦æœºå™¨æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚&lt;/p>
&lt;!--
## The Work
-->
&lt;p>##ã€€å·¥ä½œ&lt;/p>
&lt;!--
Initially, we focused on the fact that we needed to support the sheer volume of tests mandated by a complex distributed system such as Kubernetes. Real world failure scenarios had to be exercised via end-to-end (e2e) tests to ensure proper functionality. Unfortunately, e2e tests were susceptible to flakes (random failures) and took anywhere from an hour to a day to complete.
-->
&lt;p>æœ€åˆï¼Œæˆ‘ä»¬å…³æ³¨çš„äº‹å®žæ˜¯ï¼Œæˆ‘ä»¬éœ€è¦æ”¯æŒå¤æ‚çš„åˆ†å¸ƒå¼ç³»ç»Ÿï¼ˆå¦‚ Kubernetesï¼‰æ‰€è¦æ±‚çš„å¤§é‡æµ‹è¯•ã€‚ çœŸå®žä¸–ç•Œä¸­çš„æ•…éšœåœºæ™¯å¿…é¡»é€šè¿‡ç«¯åˆ°ç«¯ï¼ˆe2eï¼‰æµ‹è¯•æ¥æ‰§è¡Œï¼Œç¡®ä¿æ­£ç¡®çš„åŠŸèƒ½ã€‚ ä¸å¹¸çš„æ˜¯ï¼Œe2e æµ‹è¯•å®¹æ˜“å—åˆ°è–„ç‰‡ï¼ˆéšæœºæ•…éšœï¼‰çš„å½±å“ï¼Œå¹¶ä¸”éœ€è¦èŠ±è´¹ä¸€ä¸ªå°æ—¶åˆ°ä¸€å¤©æ‰èƒ½å®Œæˆã€‚&lt;/p>
&lt;!--
Further experience revealed other areas where machines could do the work for us:
-->
&lt;p>è¿›ä¸€æ­¥çš„ç»éªŒæ­ç¤ºäº†æœºå™¨å¯ä»¥ä¸ºæˆ‘ä»¬å·¥ä½œçš„å…¶ä»–é¢†åŸŸï¼š&lt;/p>
&lt;!--
* PR Workflow
* Did the contributor sign our CLA?
* Did the PR pass tests?
* Is the PR mergeable?
* Did the merge commit pass tests?
* Triage
* Who should be reviewing PRs?
* Is there enough information to route an issue to the right people?
* Is an issue still relevant?
* Project Health
* What is happening in the project?
* What should we be paying attention to?
-->
&lt;ul>
&lt;li>Pull Request å·¥ä½œæµç¨‹
&lt;ul>
&lt;li>è´¡çŒ®è€…æ˜¯å¦ç­¾ç½²äº†æˆ‘ä»¬çš„ CLAï¼Ÿ&lt;/li>
&lt;li>Pull Request é€šè¿‡æµ‹è¯•å—ï¼Ÿ&lt;/li>
&lt;li>Pull Request å¯ä»¥åˆå¹¶å—ï¼Ÿ&lt;/li>
&lt;li>åˆå¹¶æäº¤æ˜¯å¦é€šè¿‡äº†æµ‹è¯•ï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>é‰´åˆ«åˆ†ç±»
&lt;ul>
&lt;li>è°åº”è¯¥å®¡æŸ¥ Pull Requestï¼Ÿ&lt;/li>
&lt;li>æ˜¯å¦æœ‰è¶³å¤Ÿçš„ä¿¡æ¯å°†é—®é¢˜å‘é€ç»™åˆé€‚çš„äººï¼Ÿ&lt;/li>
&lt;li>é—®é¢˜æ˜¯å¦ä¾æ—§å­˜åœ¨ï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>é¡¹ç›®å¥åº·
&lt;ul>
&lt;li>é¡¹ç›®ä¸­å‘ç”Ÿäº†ä»€ä¹ˆï¼Ÿ&lt;/li>
&lt;li>æˆ‘ä»¬åº”è¯¥æ³¨æ„ä»€ä¹ˆï¼Ÿ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
As we developed automation to improve our situation, we followed a few guiding principles:
-->
&lt;p>å½“æˆ‘ä»¬å¼€å‘è‡ªåŠ¨åŒ–æ¥æ”¹å–„æˆ‘ä»¬çš„æƒ…å†µæ—¶ï¼Œæˆ‘ä»¬éµå¾ªäº†ä»¥ä¸‹å‡ ä¸ªæŒ‡å¯¼åŽŸåˆ™ï¼š&lt;/p>
&lt;!--
* Follow the push/pull control loop patterns that worked well for Kubernetes
* Prefer stateless loosely coupled services that do one thing well
* Prefer empowering the entire community over empowering a few core contributors
* Eat our own dogfood and avoid reinventing wheels
-->
&lt;ul>
&lt;li>éµå¾ªé€‚ç”¨äºŽ Kubernetes çš„æŽ¨é€/æ‹‰å–æŽ§åˆ¶å¾ªçŽ¯æ¨¡å¼&lt;/li>
&lt;li>é¦–é€‰æ— çŠ¶æ€æ¾æ•£è€¦åˆæœåŠ¡&lt;/li>
&lt;li>æ›´å€¾å‘äºŽæŽˆæƒæ•´ä¸ªç¤¾åŒºæƒåˆ©ï¼Œè€Œä¸æ˜¯èµ‹äºˆå°‘æ•°æ ¸å¿ƒè´¡çŒ®è€…æƒåŠ›&lt;/li>
&lt;li>åšå¥½è‡ªå·±çš„äº‹ï¼Œè€Œä¸è¦é‡æ–°é€ è½®å­&lt;/li>
&lt;/ul>
&lt;!--
## Enter Prow
-->
&lt;h2 id="äº†è§£-prow">äº†è§£ Prow&lt;/h2>
&lt;!--
This led us to create [Prow](https://git.k8s.io/test-infra/prow) as the central component for our automation. Prow is sort of like an [If This, Then That](https://ifttt.com/) for GitHub events, with a built-in library of [commands](https://prow.k8s.io/command-help), [plugins](https://prow.k8s.io/plugins), and utilities. We built Prow on top of Kubernetes to free ourselves from worrying about resource management and scheduling, and ensure a more pleasant operational experience.
-->
&lt;p>è¿™ä¿ƒä½¿æˆ‘ä»¬åˆ›å»º &lt;a href="https://git.k8s.io/test-infra/prow">Prow&lt;/a> ä½œä¸ºæˆ‘ä»¬è‡ªåŠ¨åŒ–çš„æ ¸å¿ƒç»„ä»¶ã€‚ Prowæœ‰ç‚¹åƒ &lt;a href="https://ifttt.com/">If This, Then That&lt;/a> ç”¨äºŽ GitHub äº‹ä»¶ï¼Œ å†…ç½® &lt;a href="https://prow.k8s.io/command-help">commands&lt;/a>ï¼Œ &lt;a href="https://prow.k8s.io/plugins">plugins&lt;/a>ï¼Œ å’Œå®žç”¨ç¨‹åºã€‚ æˆ‘ä»¬åœ¨ Kubernetes ä¹‹ä¸Šå»ºç«‹äº† Prowï¼Œè®©æˆ‘ä»¬ä¸å¿…æ‹…å¿ƒèµ„æºç®¡ç†å’Œæ—¥ç¨‹å®‰æŽ’ï¼Œå¹¶ç¡®ä¿æ›´æ„‰å¿«çš„è¿è¥ä½“éªŒã€‚&lt;/p>
&lt;!--
Prow lets us do things like:
-->
&lt;p>Prow è®©æˆ‘ä»¬åšä»¥ä¸‹äº‹æƒ…ï¼š&lt;/p>
&lt;!--
* Allow our community to triage issues/PRs by commenting commands such as â€œ/priority critical-urgentâ€, â€œ/assign maryâ€ or â€œ/closeâ€
* Auto-label PRs based on how much code they change, or which files they touch
* Age out issues/PRs that have remained inactive for too long
* Auto-merge PRs that meet our PR workflow requirements
* Run CI jobs defined as [Knative Builds](https://github.com/knative/build), Kubernetes Pods, or Jenkins jobs
* Enforce org-wide and per-repo GitHub policies like [branch protection](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector) and [GitHub labels](https://github.com/kubernetes/test-infra/tree/master/label_sync)
-->
&lt;ul>
&lt;li>å…è®¸æˆ‘ä»¬çš„ç¤¾åŒºé€šè¿‡è¯„è®ºè¯¸å¦‚â€œ/priority critical-urgentâ€ï¼Œâ€œ/assign maryâ€æˆ–â€œ/closeâ€ä¹‹ç±»çš„å‘½ä»¤å¯¹ issues/Pull Requests è¿›è¡Œåˆ†ç±»&lt;/li>
&lt;li>æ ¹æ®ç”¨æˆ·æ›´æ”¹çš„ä»£ç æ•°é‡æˆ–åˆ›å»ºçš„æ–‡ä»¶è‡ªåŠ¨æ ‡è®° Pull Requests&lt;/li>
&lt;li>æ ‡å‡ºé•¿æ—¶é—´ä¿æŒä¸æ´»åŠ¨çŠ¶æ€ issues/Pull Requests&lt;/li>
&lt;li>è‡ªåŠ¨åˆå¹¶ç¬¦åˆæˆ‘ä»¬PRå·¥ä½œæµç¨‹è¦æ±‚çš„ Pull Requests&lt;/li>
&lt;li>è¿è¡Œå®šä¹‰ä¸º&lt;a href="https://github.com/knative/build">Knative Builds&lt;/a>çš„ Kubernetes Podsæˆ– Jenkins jobsçš„ CI ä½œä¸š&lt;/li>
&lt;li>å®žæ–½ç»„ç»‡èŒƒå›´å’Œé‡æž„ GitHub ä»“åº“ç­–ç•¥ï¼Œå¦‚&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/branchprotector">Knative Builds&lt;/a>å’Œ&lt;a href="https://github.com/kubernetes/test-infra/tree/master/label_sync">GitHub labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Prow was initially developed by the engineering productivity team building Google Kubernetes Engine, and is actively contributed to by multiple members of Kubernetes SIG Testing. Prow has been adopted by several other open source projects, including Istio, JetStack, Knative and OpenShift. [Getting started with Prow](https://github.com/kubernetes/test-infra/tree/master/prow#getting-started) takes a Kubernetes cluster and `kubectl apply starter.yaml` (running pods on a Kubernetes cluster).
-->
&lt;p>Prowæœ€åˆç”±æž„å»º Google Kubernetes Engine çš„å·¥ç¨‹æ•ˆçŽ‡å›¢é˜Ÿå¼€å‘ï¼Œå¹¶ç”± Kubernetes SIG Testing çš„å¤šä¸ªæˆå‘˜ç§¯æžè´¡çŒ®ã€‚ Prow å·²è¢«å…¶ä»–å‡ ä¸ªå¼€æºé¡¹ç›®é‡‡ç”¨ï¼ŒåŒ…æ‹¬ Istioï¼ŒJetStackï¼ŒKnative å’Œ OpenShiftã€‚ &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow#getting-started">Getting started with Prow&lt;/a>éœ€è¦ä¸€ä¸ª Kubernetes é›†ç¾¤å’Œ &lt;code>kubectl apply starter.yaml&lt;/code>ï¼ˆåœ¨ Kubernetes é›†ç¾¤ä¸Šè¿è¡Œ podï¼‰ã€‚&lt;/p>
&lt;!--
Once we had Prow in place, we began to hit other scaling bottlenecks, and so produced additional tooling to support testing at the scale required by Kubernetes, including:
-->
&lt;p>ä¸€æ—¦æˆ‘ä»¬å®‰è£…äº† Prowï¼Œæˆ‘ä»¬å°±å¼€å§‹é‡åˆ°å…¶ä»–çš„é—®é¢˜ï¼Œå› æ­¤éœ€è¦é¢å¤–çš„å·¥å…·ä»¥æ”¯æŒ Kubernetes æ‰€éœ€çš„è§„æ¨¡æµ‹è¯•ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
- [Boskos](https://github.com/kubernetes/test-infra/tree/master/boskos): manages job resources (such as GCP projects) in pools, checking them out for jobs and cleaning them up automatically ([with monitoring](http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1))
- [ghProxy](https://github.com/kubernetes/test-infra/tree/master/ghproxy): a reverse proxy HTTP cache optimized for use with the GitHub API, to ensure our token usage doesnâ€™t hit API limits ([with monitoring](http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;orgId=1))
- [Greenhouse](https://github.com/kubernetes/test-infra/tree/master/greenhouse): allows us to use a remote bazel cache to provide faster build and test results for PRs ([with monitoring](http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1))
- [Splice](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice): allows us to test and merge PRs in a batch, ensuring our merge velocity is not limited to our test velocity
- [Tide](https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide): allows us to merge PRs selected via GitHub queries rather than ordered in a queue, allowing for significantly higher merge velocity in tandem with splice
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/boskos">Boskos&lt;/a>: ç®¡ç†æ± ä¸­çš„ä½œä¸šèµ„æºï¼ˆä¾‹å¦‚ GCP é¡¹ç›®ï¼‰ï¼Œæ£€æŸ¥å®ƒä»¬æ˜¯å¦æœ‰å·¥ä½œå¹¶è‡ªåŠ¨æ¸…ç†å®ƒä»¬ (&lt;a href="http://velodrome.k8s.io/dashboard/db/boskos-dashboard?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/ghproxy">ghProxy&lt;/a>: ä¼˜åŒ–ç”¨äºŽ GitHub API çš„åå‘ä»£ç† HTTP ç¼“å­˜ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„ä»¤ç‰Œä½¿ç”¨ä¸ä¼šè¾¾åˆ° API é™åˆ¶ (&lt;a href="http://velodrome.k8s.io/dashboard/db/github-cache?refresh=1m&amp;amp;orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/greenhouse">Greenhouse&lt;/a>: å…è®¸æˆ‘ä»¬ä½¿ç”¨è¿œç¨‹ bazel ç¼“å­˜ä¸º Pull requests æä¾›æ›´å¿«çš„æž„å»ºå’Œæµ‹è¯•ç»“æžœ (&lt;a href="http://velodrome.k8s.io/dashboard/db/bazel-cache?orgId=1">with monitoring&lt;/a>)&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/splice">Splice&lt;/a>: å…è®¸æˆ‘ä»¬æ‰¹é‡æµ‹è¯•å’Œåˆå¹¶ Pull requestsï¼Œç¡®ä¿æˆ‘ä»¬çš„åˆå¹¶é€Ÿåº¦ä¸ä»…é™äºŽæˆ‘ä»¬çš„æµ‹è¯•é€Ÿåº¦&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow/cmd/tide">Tide&lt;/a>: å…è®¸æˆ‘ä»¬åˆå¹¶é€šè¿‡ GitHub æŸ¥è¯¢é€‰æ‹©çš„ Pull requestsï¼Œè€Œä¸æ˜¯åœ¨é˜Ÿåˆ—ä¸­æŽ’åºï¼Œå…è®¸æ˜¾ç€æ›´é«˜åˆå¹¶é€Ÿåº¦ä¸Žæ‹¼æŽ¥ä¸€èµ·&lt;/li>
&lt;/ul>
&lt;!--
## Scaling Project Health
-->
&lt;p>##ã€€å…³æ³¨é¡¹ç›®å¥åº·çŠ¶å†µ&lt;/p>
&lt;!--
With workflow automation addressed, we turned our attention to project health. We chose to use Google Cloud Storage (GCS) as our source of truth for all test data, allowing us to lean on established infrastructure, and allowed the community to contribute results. We then built a variety of tools to help individuals and the project as a whole make sense of this data, including:
-->
&lt;p>éšç€å·¥ä½œæµè‡ªåŠ¨åŒ–çš„å®žæ–½ï¼Œæˆ‘ä»¬å°†æ³¨æ„åŠ›è½¬å‘äº†é¡¹ç›®å¥åº·ã€‚æˆ‘ä»¬é€‰æ‹©ä½¿ç”¨ Google Cloud Storage (GCS)ä½œä¸ºæ‰€æœ‰æµ‹è¯•æ•°æ®çš„çœŸå®žæ¥æºï¼Œå…è®¸æˆ‘ä»¬ä¾èµ–å·²å»ºç«‹çš„åŸºç¡€è®¾æ–½ï¼Œå¹¶å…è®¸ç¤¾åŒºè´¡çŒ®ç»“æžœã€‚ç„¶åŽï¼Œæˆ‘ä»¬æž„å»ºäº†å„ç§å·¥å…·æ¥å¸®åŠ©ä¸ªäººå’Œæ•´ä¸ªé¡¹ç›®ç†è§£è¿™äº›æ•°æ®ï¼ŒåŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
* [Gubernator](https://github.com/kubernetes/test-infra/tree/master/gubernator): display the results and test history for a given PR
* [Kettle](https://github.com/kubernetes/test-infra/tree/master/kettle): transfer data from GCS to a publicly accessible bigquery dataset
* [PR dashboard](https://k8s-gubernator.appspot.com/pr): a workflow-aware dashboard that allows contributors to understand which PRs require attention and why
* [Triage](https://storage.googleapis.com/k8s-gubernator/triage/index.html): identify common failures that happen across all jobs and tests
* [Testgrid](https://k8s-testgrid.appspot.com/): display test results for a given job across all runs, summarize test results across groups of jobs
-->
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/gubernator">Gubernator&lt;/a>: æ˜¾ç¤ºç»™å®š Pull Request çš„ç»“æžœå’Œæµ‹è¯•åŽ†å²&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/test-infra/tree/master/kettle">Kettle&lt;/a>: å°†æ•°æ®ä»Ž GCS ä¼ è¾“åˆ°å¯å…¬å¼€è®¿é—®çš„ bigquery æ•°æ®é›†&lt;/li>
&lt;li>&lt;a href="https://k8s-gubernator.appspot.com/pr">PR dashboard&lt;/a>: ä¸€ä¸ªå·¥ä½œæµç¨‹è¯†åˆ«ä»ªè¡¨æ¿ï¼Œå…è®¸å‚ä¸Žè€…äº†è§£å“ªäº› Pull Request éœ€è¦æ³¨æ„ä»¥åŠä¸ºä»€ä¹ˆ&lt;/li>
&lt;li>&lt;a href="https://storage.googleapis.com/k8s-gubernator/triage/index.html">Triage&lt;/a>: è¯†åˆ«æ‰€æœ‰ä½œä¸šå’Œæµ‹è¯•ä¸­å‘ç”Ÿçš„å¸¸è§æ•…éšœ&lt;/li>
&lt;li>&lt;a href="https://k8s-testgrid.appspot.com/">Testgrid&lt;/a>: æ˜¾ç¤ºæ‰€æœ‰è¿è¡Œä¸­ç»™å®šä½œä¸šçš„æµ‹è¯•ç»“æžœï¼Œæ±‡æ€»å„ç»„ä½œä¸šçš„æµ‹è¯•ç»“æžœ&lt;/li>
&lt;/ul>
&lt;!--
We approached the Cloud Native Computing Foundation (CNCF) to develop DevStats to glean insights from our GitHub events such as:
-->
&lt;p>æˆ‘ä»¬ä¸Žäº‘è®¡ç®—æœ¬åœ°è®¡ç®—åŸºé‡‘ä¼šï¼ˆCNCFï¼‰è”ç³»ï¼Œå¼€å‘ DevStatsï¼Œä»¥ä¾¿ä»Žæˆ‘ä»¬çš„ GitHub æ´»åŠ¨ä¸­æ”¶é›†è§è§£ï¼Œä¾‹å¦‚ï¼š&lt;/p>
&lt;!--
* [Which prow commands are people most actively using](https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1)
* [PR reviews by contributor over time](https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;var-period=d7&amp;var-repo_name=All&amp;var-reviewers=All)
* [Time spent in each phase of our PR workflow](https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1)
-->
&lt;ul>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/5/bot-commands-repository-groups?orgId=1">Which prow commands are people most actively using&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/46/pr-reviews-by-contributor?orgId=1&amp;amp;var-period=d7&amp;amp;var-repo_name=All&amp;amp;var-reviewers=All">PR reviews by contributor over time&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1">Time spent in each phase of our PR workflow&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Into the Beyond
-->
&lt;h2 id="into-the-beyond">Into the Beyond&lt;/h2>
&lt;!--
Today, the Kubernetes project spans over 125 repos across five orgs. There are 31 Special Interests Groups and 10 Working Groups coordinating development within the project. In the last year the project has had [participation from over 13,800 unique developers](https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;var-period_name=Last%20year&amp;var-metric=contributions&amp;var-repogroup_name=All) on GitHub.
-->
&lt;p>ä»Šå¤©ï¼ŒKubernetes é¡¹ç›®è·¨è¶Šäº†5ä¸ªç»„ç»‡125ä¸ªä»“åº“ã€‚æœ‰31ä¸ªç‰¹æ®Šåˆ©ç›Šé›†å›¢å’Œ10ä¸ªå·¥ä½œç»„åœ¨é¡¹ç›®å†…åè°ƒå‘å±•ã€‚åœ¨è¿‡åŽ»çš„ä¸€å¹´é‡Œï¼Œè¯¥é¡¹ç›®æœ‰ &lt;a href="https://k8s.devstats.cncf.io/d/13/developer-activity-counts-by-repository-group?orgId=1&amp;amp;var-period_name=Last%20year&amp;amp;var-metric=contributions&amp;amp;var-repogroup_name=All">æ¥è‡ª13800å¤šåç‹¬ç«‹å¼€å‘äººå‘˜çš„å‚ä¸Ž&lt;/a>ã€‚&lt;/p>
&lt;!--
On any given weekday our Prow instance [runs over 10,000 CI jobs](http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;fullscreen&amp;orgId=1&amp;from=now-6M&amp;to=now); from March 2017 to March 2018 it ran 4.3 million jobs. Most of these jobs involve standing up an entire Kubernetes cluster, and exercising it using real world scenarios. They allow us to ensure all supported releases of Kubernetes work across cloud providers, container engines, and networking plugins. They make sure the latest releases of Kubernetes work with various optional features enabled, upgrade safely, meet performance requirements, and work across architectures.
-->
&lt;p>åœ¨ä»»ä½•ç»™å®šçš„å·¥ä½œæ—¥ï¼Œæˆ‘ä»¬çš„ Prow å®žä¾‹&lt;a href="http://velodrome.k8s.io/dashboard/db/bigquery-metrics?panelId=10&amp;amp;fullscreen&amp;amp;orgId=1&amp;amp;from=now-6M&amp;amp;to=now">è¿è¡Œè¶…è¿‡10,000ä¸ª CI å·¥ä½œ&lt;/a>; ä»Ž2017å¹´3æœˆåˆ°2018å¹´3æœˆï¼Œå®ƒæœ‰430ä¸‡ä¸ªå·¥ä½œå²—ä½ã€‚ è¿™äº›å·¥ä½œä¸­çš„å¤§å¤šæ•°æ¶‰åŠå»ºç«‹æ•´ä¸ª Kubernetes é›†ç¾¤ï¼Œå¹¶ä½¿ç”¨çœŸå®žåœºæ™¯æ¥å®žæ–½å®ƒã€‚ å®ƒä»¬ä½¿æˆ‘ä»¬èƒ½å¤Ÿç¡®ä¿æ‰€æœ‰å—æ”¯æŒçš„ Kubernetes ç‰ˆæœ¬è·¨äº‘æä¾›å•†ï¼Œå®¹å™¨å¼•æ“Žå’Œç½‘ç»œæ’ä»¶å·¥ä½œã€‚ ä»–ä»¬ç¡®ä¿æœ€æ–°ç‰ˆæœ¬çš„ Kubernetes èƒ½å¤Ÿå¯ç”¨å„ç§å¯é€‰åŠŸèƒ½ï¼Œå®‰å…¨å‡çº§ï¼Œæ»¡è¶³æ€§èƒ½è¦æ±‚ï¼Œå¹¶è·¨æž¶æž„å·¥ä½œã€‚&lt;/p>
&lt;!--
With todayâ€™s [announcement from CNCF](https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google) â€“ noting that Google Cloud has begun transferring ownership and management of the Kubernetes projectâ€™s cloud resources to CNCF community contributors, we are excited to embark on another journey. One that allows the project infrastructure to be owned and operated by the community of contributors, following the same open governance model that has worked for the rest of the project. Sound exciting to you? Come talk to us at #sig-testing on kubernetes.slack.com.
-->
&lt;p>ä»Šå¤©&lt;a href="https://www.cncf.io/announcement/2018/08/29/cncf-receives-9-million-cloud-credit-grant-from-google">æ¥è‡ªCNCFçš„å…¬å‘Š&lt;/a> - æ³¨æ„åˆ° Google Cloud æœ‰å¼€å§‹å°† Kubernetes é¡¹ç›®çš„äº‘èµ„æºçš„æ‰€æœ‰æƒå’Œç®¡ç†æƒè½¬è®©ç»™ CNCF ç¤¾åŒºè´¡çŒ®è€…ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿå¼€å§‹å¦ä¸€ä¸ªæ—…ç¨‹ã€‚ å…è®¸é¡¹ç›®åŸºç¡€è®¾æ–½ç”±è´¡çŒ®è€…ç¤¾åŒºæ‹¥æœ‰å’Œè¿è¥ï¼Œéµå¾ªå¯¹é¡¹ç›®å…¶ä½™éƒ¨åˆ†æœ‰æ•ˆçš„ç›¸åŒå¼€æ”¾æ²»ç†æ¨¡åž‹ã€‚ å¬èµ·æ¥ä»¤äººå…´å¥‹ã€‚ è¯·æ¥ kubernetes.slack.com ä¸Šçš„ #sig-testing on kubernetes.slack.com ä¸Žæˆ‘ä»¬è”ç³»ã€‚&lt;/p>
&lt;!--
Want to find out more? Come check out these resources:
-->
&lt;p>æƒ³äº†è§£æ›´å¤šï¼Ÿ å¿«æ¥çœ‹çœ‹è¿™äº›èµ„æºï¼š&lt;/p>
&lt;!--
* [Prow: Testing the way to Kubernetes Next](https://elder.dev/posts/prow)
* [Automation and the Kubernetes Contributor Experience](https://www.youtube.com/watch?v=BsIC7gPkH5M)
-->
&lt;ul>
&lt;li>&lt;a href="https://elder.dev/posts/prow">Prow: Testing the way to Kubernetes Next&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=BsIC7gPkH5M">Automation and the Kubernetes Contributor Experience&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: ä½¿ç”¨ CSI å’Œ Kubernetes å®žçŽ°å·çš„åŠ¨æ€æ‰©å®¹</title><link>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</link><pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/08/02/%E4%BD%BF%E7%94%A8-csi-%E5%92%8C-kubernetes-%E5%AE%9E%E7%8E%B0%E5%8D%B7%E7%9A%84%E5%8A%A8%E6%80%81%E6%89%A9%E5%AE%B9/</guid><description>
&lt;!--
---
layout: blog
title: 'Dynamically Expand Volume with CSI and Kubernetes'
date: 2018-08-02
---
-->
&lt;!--
**Author**: Orain Xiong (Co-Founder, WoquTech)
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šOrain Xiongï¼ˆè”åˆåˆ›å§‹äºº, WoquTechï¼‰&lt;/p>
&lt;!--
_There is a very powerful storage subsystem within Kubernetes itself, covering a fairly broad spectrum of use cases. Whereas, when planning to build a product-grade relational database platform with Kubernetes, we face a big challenge: coming up with storage. This article describes how to extend latest Container Storage Interface 0.2.0 and integrate with Kubernetes, and demonstrates the essential facet of dynamically expanding volume capacity._
-->
&lt;p>&lt;em>Kubernetes æœ¬èº«æœ‰ä¸€ä¸ªéžå¸¸å¼ºå¤§çš„å­˜å‚¨å­ç³»ç»Ÿï¼Œæ¶µç›–äº†ç›¸å½“å¹¿æ³›çš„ç”¨ä¾‹ã€‚è€Œå½“æˆ‘ä»¬è®¡åˆ’ä½¿ç”¨ Kubernetes æž„å»ºäº§å“çº§å…³ç³»åž‹æ•°æ®åº“å¹³å°æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªå·¨å¤§çš„æŒ‘æˆ˜ï¼šæä¾›å­˜å‚¨ã€‚æœ¬æ–‡ä»‹ç»äº†å¦‚ä½•æ‰©å±•æœ€æ–°çš„ Container Storage Interface 0.2.0 å’Œä¸Ž Kubernetes é›†æˆï¼Œå¹¶æ¼”ç¤ºäº†å·åŠ¨æ€æ‰©å®¹çš„åŸºæœ¬æ–¹é¢ã€‚&lt;/em>&lt;/p>
&lt;!--
## Introduction
-->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;!--
As we focalize our customers, especially in financial space, there is a huge upswell in the adoption of container orchestration technology.
-->
&lt;p>å½“æˆ‘ä»¬ä¸“æ³¨äºŽå®¢æˆ·æ—¶ï¼Œå°¤å…¶æ˜¯åœ¨é‡‘èžé¢†åŸŸï¼Œé‡‡ç”¨å®¹å™¨ç¼–æŽ’æŠ€æœ¯çš„æƒ…å†µå¤§å¤§å¢žåŠ ã€‚&lt;/p>
&lt;!--
They are looking forward to open source solutions to redesign already existing monolithic applications, which have been running for several years on virtualization infrastructure or bare metal.
-->
&lt;p>ä»–ä»¬æœŸå¾…ç€èƒ½ç”¨å¼€æºè§£å†³æ–¹æ¡ˆé‡æ–°è®¾è®¡å·²ç»å­˜åœ¨çš„æ•´ä½“åº”ç”¨ç¨‹åºï¼Œè¿™äº›åº”ç”¨ç¨‹åºå·²ç»åœ¨è™šæ‹ŸåŒ–åŸºç¡€æž¶æž„æˆ–è£¸æœºä¸Šè¿è¡Œäº†å‡ å¹´ã€‚&lt;/p>
&lt;!--
Considering extensibility and the extent of technical maturity, Kubernetes and Docker are at the very top of the list. But migrating monolithic applications to a distributed orchestration like Kubernetes is challenging, the relational database is critical for the migration.
-->
&lt;p>è€ƒè™‘åˆ°å¯æ‰©å±•æ€§å’ŒæŠ€æœ¯æˆç†Ÿç¨‹åº¦ï¼ŒKubernetes å’Œ Docker æŽ’åœ¨æˆ‘ä»¬é€‰æ‹©åˆ—è¡¨çš„é¦–ä½ã€‚ä½†æ˜¯å°†æ•´ä½“åº”ç”¨ç¨‹åºè¿ç§»åˆ°ç±»ä¼¼äºŽ Kubernetes ä¹‹ç±»çš„åˆ†å¸ƒå¼å®¹å™¨ç¼–æŽ’å¹³å°ä¸Šå¾ˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œå…¶ä¸­å…³ç³»æ•°æ®åº“å¯¹äºŽè¿ç§»æ¥è¯´è‡³å…³é‡è¦ã€‚&lt;/p>
&lt;!--
With respect to the relational database, we should pay attention to storage. There is a very powerful storage subsystem within Kubernetes itself. It is very useful and covers a fairly broad spectrum of use cases. When planning to run a relational database with Kubernetes in production, we face a big challenge: coming up with storage. There are still some fundamental functionalities which are left unimplemented. Specifically, dynamically expanding volume. It sounds boring but is highly required, except for actions like create and delete and mount and unmount.
-->
&lt;p>å…³äºŽå…³ç³»æ•°æ®åº“ï¼Œæˆ‘ä»¬åº”è¯¥æ³¨æ„å­˜å‚¨ã€‚Kubernetes æœ¬èº«å†…éƒ¨æœ‰ä¸€ä¸ªéžå¸¸å¼ºå¤§çš„å­˜å‚¨å­ç³»ç»Ÿã€‚å®ƒéžå¸¸æœ‰ç”¨ï¼Œæ¶µç›–äº†ç›¸å½“å¹¿æ³›çš„ç”¨ä¾‹ã€‚å½“æˆ‘ä»¬è®¡åˆ’åœ¨ç”Ÿäº§çŽ¯å¢ƒä¸­ä½¿ç”¨ Kubernetes è¿è¡Œå…³ç³»åž‹æ•°æ®åº“æ—¶ï¼Œæˆ‘ä»¬é¢ä¸´ä¸€ä¸ªå·¨å¤§æŒ‘æˆ˜ï¼šæä¾›å­˜å‚¨ã€‚ç›®å‰ï¼Œä»æœ‰ä¸€äº›åŸºæœ¬åŠŸèƒ½å°šæœªå®žçŽ°ã€‚ç‰¹åˆ«æ˜¯ï¼Œå·çš„åŠ¨æ€æ‰©å®¹ã€‚è¿™å¬èµ·æ¥å¾ˆæ— èŠï¼Œä½†åœ¨é™¤åˆ›å»ºï¼Œåˆ é™¤ï¼Œå®‰è£…å’Œå¸è½½ä¹‹ç±»çš„æ“ä½œå¤–ï¼Œå®ƒæ˜¯éžå¸¸å¿…è¦çš„ã€‚&lt;/p>
&lt;!--
Currently, expanding volume is only available with those storage provisioners:
-->
&lt;p>ç›®å‰ï¼Œæ‰©å±•å·ä»…é€‚ç”¨äºŽè¿™äº›å­˜å‚¨ä¾›åº”å•†ï¼š&lt;/p>
&lt;ul>
&lt;li>gcePersistentDisk&lt;/li>
&lt;li>awsElasticBlockStore&lt;/li>
&lt;li>OpenStack Cinder&lt;/li>
&lt;li>glusterfs&lt;/li>
&lt;li>rbd&lt;/li>
&lt;/ul>
&lt;!--
In order to enable this feature, we should set feature gate `ExpandPersistentVolumes` true and turn on the `PersistentVolumeClaimResize` admission plugin. Once `PersistentVolumeClaimResize` has been enabled, resizing will be allowed by a Storage Class whose `allowVolumeExpansion` field is set to true.
-->
&lt;p>ä¸ºäº†å¯ç”¨æ­¤åŠŸèƒ½ï¼Œæˆ‘ä»¬åº”è¯¥å°†ç‰¹æ€§å¼€å…³ &lt;code>ExpandPersistentVolumes&lt;/code> è®¾ç½®ä¸º true å¹¶æ‰“å¼€ &lt;code>PersistentVolumeClaimResize&lt;/code> å‡†å…¥æ’ä»¶ã€‚ ä¸€æ—¦å¯ç”¨äº† &lt;code>PersistentVolumeClaimResize&lt;/code>ï¼Œåˆ™å…¶å¯¹åº”çš„ &lt;code>allowVolumeExpansion&lt;/code> å­—æ®µè®¾ç½®ä¸º true çš„å­˜å‚¨ç±»å°†å…è®¸è°ƒæ•´å¤§å°ã€‚&lt;/p>
&lt;!--
Unfortunately, dynamically expanding volume through the Container Storage Interface (CSI) and Kubernetes is unavailable, even though the underlying storage providers have this feature.
-->
&lt;p>ä¸å¹¸çš„æ˜¯ï¼Œå³ä½¿åŸºç¡€å­˜å‚¨æä¾›è€…å…·æœ‰æ­¤åŠŸèƒ½ï¼Œä¹Ÿæ— æ³•é€šè¿‡å®¹å™¨å­˜å‚¨æŽ¥å£ï¼ˆCSIï¼‰å’Œ Kubernetes åŠ¨æ€æ‰©å±•å·ã€‚&lt;/p>
&lt;!--
This article will give a simplified view of CSI, followed by a walkthrough of how to introduce a new expanding volume feature on the existing CSI and Kubernetes. Finally, the article will demonstrate how to dynamically expand volume capacity.
-->
&lt;p>æœ¬æ–‡å°†ç»™å‡º CSI çš„ç®€åŒ–è§†å›¾ï¼Œç„¶åŽé€æ­¥ä»‹ç»å¦‚ä½•åœ¨çŽ°æœ‰ CSI å’Œ Kubernetes ä¸Šå¼•å…¥æ–°çš„æ‰©å±•å·åŠŸèƒ½ã€‚æœ€åŽï¼Œæœ¬æ–‡å°†æ¼”ç¤ºå¦‚ä½•åŠ¨æ€æ‰©å±•å·å®¹é‡ã€‚&lt;/p>
&lt;!--
## Container Storage Interface (CSI)
-->
&lt;h2 id="å®¹å™¨å­˜å‚¨æŽ¥å£-csi">å®¹å™¨å­˜å‚¨æŽ¥å£ï¼ˆCSIï¼‰&lt;/h2>
&lt;!--
To have a better understanding of what we're going to do, the first thing we need to know is what the Container Storage Interface is. Currently, there are still some problems for already existing storage subsystem within Kubernetes. Storage driver code is maintained in the Kubernetes core repository which is difficult to test. But beyond that, Kubernetes needs to give permissions to storage vendors to check code into the Kubernetes core repository. Ideally, that should be implemented externally.
-->
&lt;p>ä¸ºäº†æ›´å¥½åœ°äº†è§£æˆ‘ä»¬å°†è¦åšä»€ä¹ˆï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦çŸ¥é“ä»€ä¹ˆæ˜¯å®¹å™¨å­˜å‚¨æŽ¥å£ã€‚å½“å‰ï¼ŒKubernetes ä¸­å·²ç»å­˜åœ¨çš„å­˜å‚¨å­ç³»ç»Ÿä»ç„¶å­˜åœ¨ä¸€äº›é—®é¢˜ã€‚ å­˜å‚¨é©±åŠ¨ç¨‹åºä»£ç åœ¨ Kubernetes æ ¸å¿ƒå­˜å‚¨åº“ä¸­ç»´æŠ¤ï¼Œè¿™å¾ˆéš¾æµ‹è¯•ã€‚ ä½†æ˜¯é™¤æ­¤ä¹‹å¤–ï¼ŒKubernetes è¿˜éœ€è¦æŽˆäºˆå­˜å‚¨ä¾›åº”å•†è®¸å¯ï¼Œä»¥å°†ä»£ç ç­¾å…¥ Kubernetes æ ¸å¿ƒå­˜å‚¨åº“ã€‚ ç†æƒ³æƒ…å†µä¸‹ï¼Œè¿™äº›åº”åœ¨å¤–éƒ¨å®žæ–½ã€‚&lt;/p>
&lt;!--
CSI is designed to define an industry standard that will enable storage providers who enable CSI to be available across container orchestration systems that support CSI.
-->
&lt;p>CSI æ—¨åœ¨å®šä¹‰è¡Œä¸šæ ‡å‡†ï¼Œè¯¥æ ‡å‡†å°†ä½¿æ”¯æŒ CSI çš„å­˜å‚¨æä¾›å•†èƒ½å¤Ÿåœ¨æ”¯æŒ CSI çš„å®¹å™¨ç¼–æŽ’ç³»ç»Ÿä¸­ä½¿ç”¨ã€‚&lt;/p>
&lt;!--
This diagram depicts a kind of high-level Kubernetes archetypes integrated with CSI:
![csi diagram](/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png)
-->
&lt;p>è¯¥å›¾æè¿°äº†ä¸€ç§ä¸Ž CSI é›†æˆçš„é«˜çº§ Kubernetes åŽŸåž‹ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/csi-diagram.png" alt="csi diagram">&lt;/p>
&lt;!--
* Three new external components are introduced to decouple Kubernetes and Storage Provider logic
* Blue arrows present the conventional way to call against API Server
* Red arrows present gRPC to call against Volume Driver
-->
&lt;ul>
&lt;li>å¼•å…¥äº†ä¸‰ä¸ªæ–°çš„å¤–éƒ¨ç»„ä»¶ä»¥è§£è€¦ Kubernetes å’Œå­˜å‚¨æä¾›ç¨‹åºé€»è¾‘&lt;/li>
&lt;li>è“è‰²ç®­å¤´è¡¨ç¤ºé’ˆå¯¹ API æœåŠ¡å™¨è¿›è¡Œè°ƒç”¨çš„å¸¸è§„æ–¹æ³•&lt;/li>
&lt;li>çº¢è‰²ç®­å¤´æ˜¾ç¤º gRPC ä»¥é’ˆå¯¹ Volume Driver è¿›è¡Œè°ƒç”¨&lt;/li>
&lt;/ul>
&lt;!--
For more details, please visit: https://github.com/container-storage-interface/spec/blob/master/spec.md
-->
&lt;p>æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·è®¿é—®ï¼šhttps://github.com/container-storage-interface/spec/blob/master/spec.md&lt;/p>
&lt;!--
## Extend CSI and Kubernetes
-->
&lt;h2 id="æ‰©å±•-csi-å’Œ-kubernetes">æ‰©å±• CSI å’Œ Kubernetes&lt;/h2>
&lt;!--
In order to enable the feature of expanding volume atop Kubernetes, we should extend several components including CSI specification, â€œin-treeâ€ volume plugin, external-provisioner and external-attacher.
-->
&lt;p>ä¸ºäº†å®žçŽ°åœ¨ Kubernetes ä¸Šæ‰©å±•å·çš„åŠŸèƒ½ï¼Œæˆ‘ä»¬åº”è¯¥æ‰©å±•å‡ ä¸ªç»„ä»¶ï¼ŒåŒ…æ‹¬ CSI è§„èŒƒï¼Œâ€œin-treeâ€ å·æ’ä»¶ï¼Œexternal-provisioner å’Œ external-attacherã€‚&lt;/p>
&lt;!--
## Extend CSI spec
-->
&lt;h2 id="æ‰©å±•csiè§„èŒƒ">æ‰©å±•CSIè§„èŒƒ&lt;/h2>
&lt;!--
The feature of expanding volume is still undefined in latest CSI 0.2.0. The new 3 RPCs, including `RequiresFSResize` and `ControllerResizeVolume` and `NodeResizeVolume`, should be introduced.
-->
&lt;p>æœ€æ–°çš„ CSI 0.2.0 ä»æœªå®šä¹‰æ‰©å±•å·çš„åŠŸèƒ½ã€‚åº”è¯¥å¼•å…¥æ–°çš„3ä¸ª RPCï¼ŒåŒ…æ‹¬ &lt;code>RequiresFSResize&lt;/code>ï¼Œ &lt;code>ControllerResizeVolume&lt;/code> å’Œ &lt;code>NodeResizeVolume&lt;/code>ã€‚&lt;/p>
&lt;pre>&lt;code>service Controller {
rpc CreateVolume (CreateVolumeRequest)
returns (CreateVolumeResponse) {}
â€¦â€¦
rpc RequiresFSResize (RequiresFSResizeRequest)
returns (RequiresFSResizeResponse) {}
rpc ControllerResizeVolume (ControllerResizeVolumeRequest)
returns (ControllerResizeVolumeResponse) {}
}
service Node {
rpc NodeStageVolume (NodeStageVolumeRequest)
returns (NodeStageVolumeResponse) {}
â€¦â€¦
rpc NodeResizeVolume (NodeResizeVolumeRequest)
returns (NodeResizeVolumeResponse) {}
}
&lt;/code>&lt;/pre>&lt;!--
## Extend â€œIn-Treeâ€ Volume Plugin
-->
&lt;h2 id="æ‰©å±•-in-tree-å·æ’ä»¶">æ‰©å±• â€œIn-Treeâ€ å·æ’ä»¶&lt;/h2>
&lt;!--
In addition to the extend CSI specification, the `csiPluginï»¿` interface within Kubernetes should also implement `expandablePlugin`. The `csiPlugin` interface will expand `PersistentVolumeClaim` representing for `ExpanderController`.
-->
&lt;p>é™¤äº†æ‰©å±•çš„ CSI è§„èŒƒä¹‹å¤–ï¼ŒKubernetes ä¸­çš„ &lt;code>csiPlugin&lt;/code> æŽ¥å£è¿˜åº”è¯¥å®žçŽ° &lt;code>expandablePlugin&lt;/code>ã€‚&lt;code>csiPlugin&lt;/code> æŽ¥å£å°†æ‰©å±•ä»£è¡¨ &lt;code>ExpanderController&lt;/code> çš„ &lt;code>PersistentVolumeClaim&lt;/code>ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">type&lt;/span> ExpandableVolumePlugin &lt;span style="color:#a2f;font-weight:bold">interface&lt;/span> {
VolumePlugin
&lt;span style="color:#00a000">ExpandVolumeDevice&lt;/span>(spec Spec, newSize resource.Quantity, oldSize resource.Quantity) (resource.Quantity, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>)
&lt;span style="color:#00a000">RequiresFSResize&lt;/span>() &lt;span style="color:#0b0;font-weight:bold">bool&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
### Implement Volume Driver
-->
&lt;h3 id="å®žçŽ°å·é©±åŠ¨ç¨‹åº">å®žçŽ°å·é©±åŠ¨ç¨‹åº&lt;/h3>
&lt;!--
Finally, to abstract complexity of the implementation, we should hard code the separate storage provider management logic into the following functions which is well-defined in the CSI specification:
-->
&lt;p>æœ€åŽï¼Œä¸ºäº†æŠ½è±¡åŒ–å®žçŽ°çš„å¤æ‚æ€§ï¼Œæˆ‘ä»¬åº”è¯¥å°†å•ç‹¬çš„å­˜å‚¨æä¾›ç¨‹åºç®¡ç†é€»è¾‘ç¡¬ç¼–ç ä¸ºä»¥ä¸‹åŠŸèƒ½ï¼Œè¿™äº›åŠŸèƒ½åœ¨ CSI è§„èŒƒä¸­å·²æ˜Žç¡®å®šä¹‰ï¼š&lt;/p>
&lt;ul>
&lt;li>CreateVolume&lt;/li>
&lt;li>DeleteVolume&lt;/li>
&lt;li>ControllerPublishVolume&lt;/li>
&lt;li>ControllerUnpublishVolume&lt;/li>
&lt;li>ValidateVolumeCapabilities&lt;/li>
&lt;li>ListVolumes&lt;/li>
&lt;li>GetCapacity&lt;/li>
&lt;li>ControllerGetCapabilities&lt;/li>
&lt;li>RequiresFSResize&lt;/li>
&lt;li>ControllerResizeVolume&lt;/li>
&lt;/ul>
&lt;!--
## Demonstration
Letâ€™s demonstrate this feature with a concrete user case.
* Create storage class for CSI storage provisioner
-->
&lt;h2 id="å±•ç¤º">å±•ç¤º&lt;/h2>
&lt;p>è®©æˆ‘ä»¬ä»¥å…·ä½“çš„ç”¨æˆ·æ¡ˆä¾‹æ¥æ¼”ç¤ºæ­¤åŠŸèƒ½ã€‚&lt;/p>
&lt;ul>
&lt;li>ä¸º CSI å­˜å‚¨ä¾›åº”å•†åˆ›å»ºå­˜å‚¨ç±»&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">allowVolumeExpansion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">csiProvisionerSecretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>orain-test&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">csiProvisionerSecretNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfsplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">reclaimPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Delete&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeBindingMode&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Immediate&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Deploy CSI Volume Driver including storage provisioner `csi-qcfsplugin` across Kubernetes cluster
* Create PVC `qcfs-pvc` which will be dynamically provisioned by storage class `csi-qcfs`
-->
&lt;ul>
&lt;li>
&lt;p>åœ¨ Kubernetes é›†ç¾¤ä¸Šéƒ¨ç½²åŒ…æ‹¬å­˜å‚¨ä¾›åº”å•† &lt;code>csi-qcfsplugin&lt;/code> åœ¨å†…çš„ CSI å·é©±åŠ¨&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åˆ›å»º PVC &lt;code>qcfs-pvc&lt;/code>ï¼Œå®ƒå°†ç”±å­˜å‚¨ç±» &lt;code>csi-qcfs&lt;/code> åŠ¨æ€é…ç½®&lt;/p>
&lt;/li>
&lt;/ul>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>qcfs-pvc&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>....&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-qcfs&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;!--
* Create MySQL 5.7 instance to use PVC `qcfs-pvc`
* In order to mirror the exact same production-level scenario, there are actually two different types of workloads including:
* Batch insert to make MySQL consuming more file system capacity
* Surge query request
* Dynamically expand volume capacity through edit pvc `qcfs-pvc` configuration
-->
&lt;ul>
&lt;li>åˆ›å»º MySQL 5.7 å®žä¾‹ä»¥ä½¿ç”¨ PVC &lt;code>qcfs-pvc&lt;/code>&lt;/li>
&lt;li>ä¸ºäº†åæ˜ å®Œå…¨ç›¸åŒçš„ç”Ÿäº§çº§åˆ«æ–¹æ¡ˆï¼Œå®žé™…ä¸Šæœ‰ä¸¤ç§ä¸åŒç±»åž‹çš„å·¥ä½œè´Ÿè½½ï¼ŒåŒ…æ‹¬ï¼š
Â Â Â Â  * æ‰¹é‡æ’å…¥ä½¿ MySQL æ¶ˆè€—æ›´å¤šçš„æ–‡ä»¶ç³»ç»Ÿå®¹é‡
Â Â Â Â  * æµªæ¶ŒæŸ¥è¯¢è¯·æ±‚&lt;/li>
&lt;li>é€šè¿‡ç¼–è¾‘ pvc &lt;code>qcfs-pvc&lt;/code> é…ç½®åŠ¨æ€æ‰©å±•å·å®¹é‡&lt;/li>
&lt;/ul>
&lt;!--
The Prometheus and Grafana integration allows us to visualize corresponding critical metrics.
![prometheus grafana](/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png)
We notice that the middle reading shows MySQL datafile size increasing slowly during bulk inserting. At the same time, the bottom reading shows file system expanding twice in about 20 minutes, from 300 GiB to 400 GiB and then 500 GiB. Meanwhile, the upper reading shows the whole process of expanding volume immediately completes and hardly impacts MySQL QPS.
-->
&lt;p>Prometheus å’Œ Grafana çš„é›†æˆä½¿æˆ‘ä»¬å¯ä»¥å¯è§†åŒ–ç›¸åº”çš„å…³é”®æŒ‡æ ‡ã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-08-02-dynamically-expand-volume-csi/prometheus-grafana.png" alt="prometheus grafana">&lt;/p>
&lt;p>æˆ‘ä»¬æ³¨æ„åˆ°ä¸­é—´çš„è¯»æ•°æ˜¾ç¤ºåœ¨æ‰¹é‡æ’å…¥æœŸé—´ MySQL æ•°æ®æ–‡ä»¶çš„å¤§å°ç¼“æ…¢å¢žåŠ ã€‚ åŒæ—¶ï¼Œåº•éƒ¨è¯»æ•°æ˜¾ç¤ºæ–‡ä»¶ç³»ç»Ÿåœ¨å¤§çº¦20åˆ†é’Ÿå†…æ‰©å±•äº†ä¸¤æ¬¡ï¼Œä»Ž 300 GiB æ‰©å±•åˆ° 400 GiBï¼Œç„¶åŽæ‰©å±•åˆ° 500 GiBã€‚ åŒæ—¶ï¼Œä¸ŠåŠéƒ¨åˆ†æ˜¾ç¤ºï¼Œæ‰©å±•å·çš„æ•´ä¸ªè¿‡ç¨‹ç«‹å³å®Œæˆï¼Œå‡ ä¹Žä¸ä¼šå½±å“ MySQL QPSã€‚&lt;/p>
&lt;!--
## Conclusion
Regardless of whatever infrastructure applications have been running on, the database is always a critical resource. It is essential to have a more advanced storage subsystem out there to fully support database requirements. This will help drive the more broad adoption of cloud native technology.
-->
&lt;h2 id="ç»“è®º">ç»“è®º&lt;/h2>
&lt;p>ä¸ç®¡è¿è¡Œä»€ä¹ˆåŸºç¡€ç»“æž„åº”ç”¨ç¨‹åºï¼Œæ•°æ®åº“å§‹ç»ˆæ˜¯å…³é”®èµ„æºã€‚æ‹¥æœ‰æ›´é«˜çº§çš„å­˜å‚¨å­ç³»ç»Ÿä»¥å®Œå…¨æ”¯æŒæ•°æ®åº“éœ€æ±‚è‡³å…³é‡è¦ã€‚è¿™å°†æœ‰åŠ©äºŽæŽ¨åŠ¨äº‘åŽŸç”ŸæŠ€æœ¯çš„æ›´å¹¿æ³›é‡‡ç”¨ã€‚&lt;/p></description></item><item><title>Blog: ç”¨äºŽ Kubernetes é›†ç¾¤ DNS çš„ CoreDNS GA æ­£å¼å‘å¸ƒ</title><link>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</link><pubDate>Tue, 10 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/10/%E7%94%A8%E4%BA%8E-kubernetes-%E9%9B%86%E7%BE%A4-dns-%E7%9A%84-coredns-ga-%E6%AD%A3%E5%BC%8F%E5%8F%91%E5%B8%83/</guid><description>
&lt;!--
---
layout: blog
title: "CoreDNS GA for Kubernetes Cluster DNS"
date: 2018-07-10
---
--->
&lt;!--
**Author**: John Belamaric (Infoblox)
**Editorâ€™s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/) on whatâ€™s new in Kubernetes 1.11**
--->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šJohn Belamaric (Infoblox)&lt;/p>
&lt;p>**ç¼–è€…æ³¨ï¼šè¿™ç¯‡æ–‡ç« æ˜¯ &lt;a href="https://kubernetes.io/blog/2018/06/27/kubernetes-1.11-release-announcement/">ç³»åˆ—æ·±åº¦æ–‡ç« &lt;/a> ä¸­çš„ä¸€ç¯‡ï¼Œä»‹ç»äº† Kubernetes 1.11 æ–°å¢žçš„åŠŸèƒ½&lt;/p>
&lt;!--
## Introduction
In Kubernetes 1.11, [CoreDNS](https://coredns.io) has reached General Availability (GA) for DNS-based service discovery, as an alternative to the kube-dns addon. This means that CoreDNS will be offered as an option in upcoming versions of the various installation tools. In fact, the kubeadm team chose to make it the default option starting with Kubernetes 1.11.
--->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;p>åœ¨ Kubernetes 1.11 ä¸­ï¼Œ&lt;a href="https://coredns.io">CoreDNS&lt;/a> å·²ç»è¾¾åˆ°åŸºäºŽ DNS æœåŠ¡å‘çŽ°çš„ General Availability (GA)ï¼Œå¯ä»¥æ›¿ä»£ kube-dns æ’ä»¶ã€‚è¿™æ„å‘³ç€ CoreDNS ä¼šä½œä¸ºå³å°†å‘å¸ƒçš„å®‰è£…å·¥å…·çš„é€‰é¡¹ä¹‹ä¸€ä¸Šçº¿ã€‚å®žé™…ä¸Šï¼Œä»Ž Kubernetes 1.11 å¼€å§‹ï¼Œkubeadm å›¢é˜Ÿé€‰æ‹©å°†å®ƒè®¾ä¸ºé»˜è®¤é€‰é¡¹ã€‚&lt;/p>
&lt;!--
DNS-based service discovery has been part of Kubernetes for a long time with the kube-dns cluster addon. This has generally worked pretty well, but there have been some concerns around the reliability, flexibility and security of the implementation.
CoreDNS is a general-purpose, authoritative DNS server that provides a backwards-compatible, but extensible, integration with Kubernetes. It resolves the issues seen with kube-dns, and offers a number of unique features that solve a wider variety of use cases.
In this article, you will learn about the differences in the implementations of kube-dns and CoreDNS, and some of the helpful extensions offered by CoreDNS.
--->
&lt;p>å¾ˆä¹…ä»¥æ¥ï¼Œ kube-dns é›†ç¾¤æ’ä»¶ä¸€ç›´æ˜¯ Kubernetes çš„ä¸€éƒ¨åˆ†ï¼Œç”¨æ¥å®žçŽ°åŸºäºŽ DNS çš„æœåŠ¡å‘çŽ°ã€‚
é€šå¸¸ï¼Œæ­¤æ’ä»¶è¿è¡Œå¹³ç¨³ï¼Œä½†å¯¹äºŽå®žçŽ°çš„å¯é æ€§ã€çµæ´»æ€§å’Œå®‰å…¨æ€§ä»å­˜åœ¨ä¸€äº›ç–‘è™‘ã€‚&lt;/p>
&lt;p>CoreDNS æ˜¯é€šç”¨çš„ã€æƒå¨çš„ DNS æœåŠ¡å™¨ï¼Œæä¾›ä¸Ž Kubernetes å‘åŽå…¼å®¹ä½†å¯æ‰©å±•çš„é›†æˆã€‚å®ƒè§£å†³äº† kube-dns é‡åˆ°çš„é—®é¢˜ï¼Œå¹¶æä¾›äº†è®¸å¤šç‹¬ç‰¹çš„åŠŸèƒ½ï¼Œå¯ä»¥è§£å†³å„ç§ç”¨ä¾‹ã€‚&lt;/p>
&lt;p>åœ¨æœ¬æ–‡ä¸­ï¼Œæ‚¨å°†äº†è§£ kube-dns å’Œ CoreDNS çš„å®žçŽ°æœ‰ä½•å·®å¼‚ï¼Œä»¥åŠ CoreDNS æä¾›çš„ä¸€äº›éžå¸¸æœ‰ç”¨çš„æ‰©å±•ã€‚&lt;/p>
&lt;!--
## Implementation differences
In kube-dns, several containers are used within a single pod: `kubedns`, `dnsmasq`, and `sidecar`. The `kubedns`
container watches the Kubernetes API and serves DNS records based on the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md), `dnsmasq` provides caching and stub domain support, and `sidecar` provides metrics and health checks.
--->
&lt;h2 id="å®žçŽ°å·®å¼‚">å®žçŽ°å·®å¼‚&lt;/h2>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œä¸€ä¸ª Pod ä¸­ä½¿ç”¨å¤šä¸ª å®¹å™¨ï¼š&lt;code>kubedns&lt;/code>ã€&lt;code>dnsmasq&lt;/code>ã€å’Œ &lt;code>sidecar&lt;/code>ã€‚&lt;code>kubedns&lt;/code> å®¹å™¨ç›‘è§† Kubernetes API å¹¶æ ¹æ® &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNS è§„èŒƒ&lt;/a> æä¾› DNS è®°å½•ï¼Œ&lt;code>dnsmasq&lt;/code> æä¾›ç¼“å­˜å’Œå­˜æ ¹åŸŸæ”¯æŒï¼Œ&lt;code>sidecar&lt;/code> æä¾›æŒ‡æ ‡å’Œå¥åº·æ£€æŸ¥ã€‚&lt;/p>
&lt;!--
This setup leads to a few issues that have been seen over time. For one, security vulnerabilities in `dnsmasq` have led to the need
for a security-patch release of Kubernetes in the past. Additionally, because `dnsmasq` handles the stub domains,
but `kubedns` handles the External Services, you cannot use a stub domain in an external service, which is very
limiting to that functionality (see [dns#131](https://github.com/kubernetes/dns/issues/131)).
All of these functions are done in a single container in CoreDNS, which is running a process written in Go. The
different plugins that are enabled replicate (and enhance) the functionality found in kube-dns.
--->
&lt;p>éšç€æ—¶é—´çš„æŽ¨ç§»ï¼Œæ­¤è®¾ç½®ä¼šå¯¼è‡´ä¸€äº›é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œä»¥å¾€ &lt;code>dnsmasq&lt;/code> ä¸­çš„å®‰å…¨æ¼æ´žéœ€è¦é€šè¿‡å‘å¸ƒ Kubernetes çš„å®‰å…¨è¡¥ä¸æ¥è§£å†³ã€‚ä½†æ˜¯ï¼Œç”±äºŽ &lt;code>dnsmasq&lt;/code> å¤„ç†å­˜æ ¹åŸŸï¼Œè€Œ &lt;code>kubedns&lt;/code> å¤„ç†å¤–éƒ¨æœåŠ¡ï¼Œå› æ­¤æ‚¨ä¸èƒ½åœ¨å¤–éƒ¨æœåŠ¡ä¸­ä½¿ç”¨å­˜æ ¹åŸŸï¼Œå¯¼è‡´è¿™ä¸ªåŠŸèƒ½å…·æœ‰å±€é™æ€§ï¼ˆè¯·å‚é˜… &lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131&lt;/a>ï¼‰ã€‚&lt;/p>
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‰€æœ‰è¿™äº›åŠŸèƒ½éƒ½æ˜¯åœ¨ä¸€ä¸ªå®¹å™¨ä¸­å®Œæˆçš„ï¼Œè¯¥å®¹å™¨è¿è¡Œç”¨ Go ç¼–å†™çš„è¿›ç¨‹ã€‚æ‰€å¯ç”¨çš„ä¸åŒæ’ä»¶å¯å¤åˆ¶ï¼ˆå¹¶å¢žå¼ºï¼‰åœ¨ kube-dns ä¸­å­˜åœ¨çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
## Configuring CoreDNS
In kube-dns, you can [modify a ConfigMap](https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/) to change the behavior of your service discovery. This allows the addition of
features such as serving stub domains, modifying upstream nameservers, and enabling federation.
--->
&lt;h2 id="é…ç½®-coredns">é…ç½® CoreDNS&lt;/h2>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œæ‚¨å¯ä»¥ &lt;a href="https://kubernetes.io/blog/2017/04/configuring-private-dns-zones-upstream-nameservers-kubernetes/">ä¿®æ”¹ ConfigMap&lt;/a> æ¥æ›´æ”¹æœåŠ¡å‘çŽ°çš„è¡Œä¸ºã€‚ç”¨æˆ·å¯ä»¥æ·»åŠ è¯¸å¦‚ä¸ºå­˜æ ¹åŸŸæä¾›æœåŠ¡ã€ä¿®æ”¹ä¸Šæ¸¸åç§°æœåŠ¡å™¨ä»¥åŠå¯ç”¨è”ç›Ÿä¹‹ç±»çš„åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
In CoreDNS, you similarly can modify the ConfigMap for the CoreDNS [Corefile](https://coredns.io/2017/07/23/corefile-explained/) to change how service discovery
works. This Corefile configuration offers many more options than you will find in kube-dns, since it is the
primary configuration file that CoreDNS uses for configuration of all of its features, even those that are not
Kubernetes related.
When upgrading from kube-dns to CoreDNS using `kubeadm`, your existing ConfigMap will be used to generate the
customized Corefile for you, including all of the configuration for stub domains, federation, and upstream nameservers. See [Using CoreDNS for Service Discovery](/docs/tasks/administer-cluster/coredns/) for more details.
--->
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‚¨å¯ä»¥ç±»ä¼¼åœ°ä¿®æ”¹ CoreDNS &lt;a href="https://coredns.io/2017/07/23/corefile-explained/">Corefile&lt;/a> çš„ ConfigMapï¼Œä»¥æ›´æ”¹æœåŠ¡å‘çŽ°çš„å·¥ä½œæ–¹å¼ã€‚è¿™ç§ Corefile é…ç½®æä¾›äº†æ¯” kube-dns ä¸­æ›´å¤šçš„é€‰é¡¹ï¼Œå› ä¸ºå®ƒæ˜¯ CoreDNS ç”¨äºŽé…ç½®æ‰€æœ‰åŠŸèƒ½çš„ä¸»è¦é…ç½®æ–‡ä»¶ï¼Œå³ä½¿ä¸Ž Kubernetes ä¸ç›¸å…³çš„åŠŸèƒ½ä¹Ÿå¯ä»¥æ“ä½œã€‚&lt;/p>
&lt;p>ä½¿ç”¨ &lt;code>kubeadm&lt;/code> å°† kube-dns å‡çº§åˆ° CoreDNS æ—¶ï¼ŒçŽ°æœ‰çš„ ConfigMap å°†è¢«ç”¨æ¥ä¸ºæ‚¨ç”Ÿæˆè‡ªå®šä¹‰çš„ Corefileï¼ŒåŒ…æ‹¬å­˜æ ¹åŸŸã€è”ç›Ÿå’Œä¸Šæ¸¸åç§°æœåŠ¡å™¨çš„æ‰€æœ‰é…ç½®ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§
&lt;a href="https://kubernetes.io/zh/docs/tasks/administer-cluster/coredns/">ä½¿ç”¨ CoreDNS è¿›è¡ŒæœåŠ¡å‘çŽ°&lt;/a>ã€‚&lt;/p>
&lt;!--
## Bug fixes and enhancements
There are several open issues with kube-dns that are resolved in CoreDNS, either in default configuration or with some customized configurations.
--->
&lt;h2 id="é”™è¯¯ä¿®å¤å’Œå¢žå¼º">é”™è¯¯ä¿®å¤å’Œå¢žå¼º&lt;/h2>
&lt;p>åœ¨ CoreDNS ä¸­è§£å†³äº† kube-dn çš„å¤šä¸ªæœªè§£å†³é—®é¢˜ï¼Œæ— è®ºæ˜¯é»˜è®¤é…ç½®è¿˜æ˜¯æŸäº›è‡ªå®šä¹‰é…ç½®ã€‚&lt;/p>
&lt;!--
* [dns#55 - Custom DNS entries for kube-dns](https://github.com/kubernetes/dns/issues/55) may be handled using the "fallthrough" mechanism in the [kubernetes plugin](https://coredns.io/plugins/kubernetes), using the [rewrite plugin](https://coredns.io/plugins/rewrite), or simply serving a subzone with a different plugin such as the [file plugin](https://coredns.io/plugins/file).
* [dns#116 - Only one A record set for headless service with pods having single hostname](https://github.com/kubernetes/dns/issues/116). This issue is fixed without any additional configuration.
* [dns#131 - externalName not using stubDomains settings](https://github.com/kubernetes/dns/issues/131). This issue is fixed without any additional configuration.
* [dns#167 - enable skyDNS round robin A/AAAA records](https://github.com/kubernetes/dns/issues/167). The equivalent functionality can be configured using the [load balance plugin](https://coredns.io/plugins/loadbalance).
* [dns#190 - kube-dns cannot run as non-root user](https://github.com/kubernetes/dns/issues/190). This issue is solved today by using a non-default image, but it will be made the default CoreDNS behavior in a future release.
* [dns#232 - fix pod hostname to be podname for dns srv records](https://github.com/kubernetes/dns/issues/232) is an enhancement that is supported through the "endpoint_pod_names" feature described below.
--->
&lt;ul>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/55">dns#55 - kube-dns çš„è‡ªå®šä¹‰ DNS æ¡ç›®&lt;/a> å¯ä»¥ä½¿ç”¨ &lt;a href="https://coredns.io/plugins/kubernetes">kubernetes æ’ä»¶&lt;/a> ä¸­çš„ &amp;quot;fallthrough&amp;quot; æœºåˆ¶ï¼Œä½¿ç”¨ &lt;a href="https://coredns.io/plugins/rewrite">rewrite æ’ä»¶&lt;/a>ï¼Œæˆ–è€…åˆ†åŒºä½¿ç”¨ä¸åŒçš„æ’ä»¶ï¼Œä¾‹å¦‚ &lt;a href="https://coredns.io/plugins/file">file æ’ä»¶&lt;/a>ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/116">dns#116 - å¯¹å…·æœ‰ç›¸åŒä¸»æœºåçš„ã€æä¾›æ— å¤´æœåŠ¡æœåŠ¡çš„ Pod ä»…è®¾ç½®äº†ä¸€ä¸ª A è®°å½•&lt;/a>ã€‚æ— éœ€ä»»ä½•å…¶ä»–é…ç½®å³å¯è§£å†³æ­¤é—®é¢˜ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/131">dns#131 - externalName æœªä½¿ç”¨ stubDomains è®¾ç½®&lt;/a>ã€‚æ— éœ€ä»»ä½•å…¶ä»–é…ç½®å³å¯è§£å†³æ­¤é—®é¢˜ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/167">dns#167 - å…è®¸ skyDNS ä¸º A/AAAA è®°å½•æä¾›è½®æ¢&lt;/a>ã€‚å¯ä»¥ä½¿ç”¨ &lt;a href="https://coredns.io/plugins/loadbalance">è´Ÿè½½å‡è¡¡æ’ä»¶&lt;/a> é…ç½®ç­‰æ•ˆåŠŸèƒ½ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/190">dns#190 - kube-dns æ— æ³•ä»¥éž root ç”¨æˆ·èº«ä»½è¿è¡Œ&lt;/a>ã€‚ä»Šå¤©ï¼Œé€šè¿‡ä½¿ç”¨ non-default é•œåƒè§£å†³äº†æ­¤é—®é¢˜ï¼Œä½†æ˜¯åœ¨å°†æ¥çš„ç‰ˆæœ¬ä¸­ï¼Œå®ƒå°†æˆä¸ºé»˜è®¤çš„ CoreDNS è¡Œä¸ºã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>&lt;a href="https://github.com/kubernetes/dns/issues/232">dns#232 - åœ¨ dns srv è®°å½•ä¸­ä¿®å¤ pod hostname ä¸º podname&lt;/a> æ˜¯é€šè¿‡ä¸‹é¢æåˆ°çš„ &amp;quot;endpoint_pod_names&amp;quot; åŠŸèƒ½è¿›è¡Œæ”¯æŒçš„å¢žå¼ºåŠŸèƒ½ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
## Metrics
The functional behavior of the default CoreDNS configuration is the same as kube-dns. However,
one difference you need to be aware of is that the published metrics are not the same. In kube-dns,
you get separate metrics for `dnsmasq` and `kubedns` (skydns). In CoreDNS there is a completely
different set of metrics, since it is all a single process. You can find more details on these
metrics on the CoreDNS [Prometheus plugin](https://coredns.io/plugins/metrics/) page.
--->
&lt;h2 id="æŒ‡æ ‡">æŒ‡æ ‡&lt;/h2>
&lt;p>CoreDNS é»˜è®¤é…ç½®çš„åŠŸèƒ½æ€§è¡Œä¸ºä¸Ž kube-dns ç›¸åŒã€‚ä½†æ˜¯ï¼Œä½ éœ€è¦äº†è§£çš„å·®åˆ«ä¹‹ä¸€æ˜¯äºŒè€…å‘å¸ƒçš„æŒ‡æ ‡æ˜¯ä¸åŒçš„ã€‚åœ¨ kube-dns ä¸­ï¼Œæ‚¨å°†åˆ†åˆ«èŽ·å¾— &lt;code>dnsmasq&lt;/code> å’Œ &lt;code>kubedns&lt;/code>ï¼ˆskydnsï¼‰çš„åº¦é‡å€¼ã€‚åœ¨ CoreDNS ä¸­ï¼Œå­˜åœ¨ä¸€ç»„å®Œå…¨ä¸åŒçš„æŒ‡æ ‡ï¼Œå› ä¸ºå®ƒä»¬åœ¨åŒä¸€ä¸ªè¿›ç¨‹ä¸­ã€‚æ‚¨å¯ä»¥åœ¨ CoreDNS &lt;a href="https://coredns.io/plugins/metrics/">Prometheus æ’ä»¶&lt;/a> é¡µé¢ä¸Šæ‰¾åˆ°æœ‰å…³è¿™äº›æŒ‡æ ‡çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
## Some special features
The standard CoreDNS Kubernetes configuration is designed to be backwards compatible with the prior
kube-dns behavior. But with some configuration changes, CoreDNS can allow you to modify how the
DNS service discovery works in your cluster. A number of these features are intended to still be
compliant with the [Kubernetes DNS specification](https://github.com/kubernetes/dns/blob/master/docs/specification.md);
they enhance functionality but remain backward compatible. Since CoreDNS is not
*only* made for Kubernetes, but is instead a general-purpose DNS server, there are many things you
can do beyond that specification.
--->
&lt;h2 id="ä¸€äº›ç‰¹æ®ŠåŠŸèƒ½">ä¸€äº›ç‰¹æ®ŠåŠŸèƒ½&lt;/h2>
&lt;p>æ ‡å‡†çš„ CoreDNS Kubernetes é…ç½®æ—¨åœ¨ä¸Žä»¥å‰çš„ kube-dns åœ¨è¡Œä¸ºä¸Šå‘åŽå…¼å®¹ã€‚ä½†æ˜¯ï¼Œé€šè¿‡è¿›è¡Œä¸€äº›é…ç½®æ›´æ”¹ï¼ŒCoreDNS å…è®¸æ‚¨ä¿®æ”¹ DNS æœåŠ¡å‘çŽ°åœ¨ç¾¤é›†ä¸­çš„å·¥ä½œæ–¹å¼ã€‚è¿™äº›åŠŸèƒ½ä¸­çš„è®¸å¤šåŠŸèƒ½ä»è¦ç¬¦åˆ &lt;a href="https://github.com/kubernetes/dns/blob/master/docs/specification.md">Kubernetes DNSè§„èŒƒ&lt;/a>ï¼›å®ƒä»¬åœ¨å¢žå¼ºäº†åŠŸèƒ½çš„åŒæ—¶ä¿æŒå‘åŽå…¼å®¹ã€‚ç”±äºŽ CoreDNS å¹¶éž &lt;em>ä»…&lt;/em> ç”¨äºŽ Kubernetesï¼Œè€Œæ˜¯é€šç”¨çš„ DNS æœåŠ¡å™¨ï¼Œå› æ­¤æ‚¨å¯ä»¥åšå¾ˆå¤šè¶…å‡ºè¯¥è§„èŒƒçš„äº‹æƒ…ã€‚&lt;/p>
&lt;!--
### Pods verified mode
In kube-dns, pod name records are "fake". That is, any "a-b-c-d.namespace.pod.cluster.local" query will
return the IP address "a.b.c.d". In some cases, this can weaken the identity guarantees offered by TLS. So,
CoreDNS offers a "pods verified" mode, which will only return the IP address if there is a pod in the
specified namespace with that IP address.
--->
&lt;h3 id="pod-éªŒè¯æ¨¡å¼">Pod éªŒè¯æ¨¡å¼&lt;/h3>
&lt;p>åœ¨ kube-dns ä¸­ï¼ŒPod åç§°è®°å½•æ˜¯ &amp;quot;ä¼ªé€ çš„&amp;quot;ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œä»»ä½• &amp;quot;a-b-c-d.namespace.pod.cluster.local&amp;quot; æŸ¥è¯¢éƒ½å°†è¿”å›ž IP åœ°å€ &amp;quot;a.b.c.d&amp;quot;ã€‚åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œè¿™å¯èƒ½ä¼šå‰Šå¼± TLS æä¾›çš„èº«ä»½ç¡®è®¤ã€‚å› æ­¤ï¼ŒCoreDNS æä¾›äº†ä¸€ç§ &amp;quot;Pod éªŒè¯&amp;quot; çš„æ¨¡å¼ï¼Œè¯¥æ¨¡å¼ä»…åœ¨æŒ‡å®šåç§°ç©ºé—´ä¸­å­˜åœ¨å…·æœ‰è¯¥ IP åœ°å€çš„ Pod æ—¶æ‰è¿”å›ž IP åœ°å€ã€‚&lt;/p>
&lt;!--
### Endpoint names based on pod names
In kube-dns, when using a headless service, you can use an SRV request to get a list of
all endpoints for the service:
--->
&lt;h3 id="åŸºäºŽ-pod-åç§°çš„ç«¯ç‚¹åç§°">åŸºäºŽ Pod åç§°çš„ç«¯ç‚¹åç§°&lt;/h3>
&lt;p>åœ¨ kube-dns ä¸­ï¼Œä½¿ç”¨æ— å¤´æœåŠ¡æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ SRV è¯·æ±‚èŽ·å–è¯¥æœåŠ¡çš„æ‰€æœ‰ç«¯ç‚¹çš„åˆ—è¡¨ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 10 33 0 6234396237313665.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6662363165353239.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 10 33 0 6338633437303230.headless.default.svc.cluster.local.
dnstools#
&lt;/code>&lt;/pre>&lt;!--
However, the endpoint DNS names are (for practical purposes) random. In CoreDNS, by default, you get endpoint
DNS names based upon the endpoint IP address:
--->
&lt;p>ä½†æ˜¯ï¼Œç«¯ç‚¹ DNS åç§°ï¼ˆå‡ºäºŽå®žé™…ç›®çš„ï¼‰æ˜¯éšæœºçš„ã€‚åœ¨ CoreDNS ä¸­ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œæ‚¨æ‰€èŽ·å¾—çš„ç«¯ç‚¹ DNS åç§°æ˜¯åŸºäºŽç«¯ç‚¹ IP åœ°å€ç”Ÿæˆçš„ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-14.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-18.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-4.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 172-17-0-9.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
For some applications, it is desirable to have the pod name for this, rather than the pod IP
address (see for example [kubernetes#47992](https://github.com/kubernetes/kubernetes/issues/47992) and [coredns#1190](https://github.com/coredns/coredns/pull/1190)). To enable this in CoreDNS, you specify the "endpoint_pod_names" option in your Corefile, which results in this:
--->
&lt;p>å¯¹äºŽæŸäº›åº”ç”¨ç¨‹åºï¼Œä½ ä¼šå¸Œæœ›åœ¨è¿™é‡Œä½¿ç”¨ Pod åç§°ï¼Œè€Œä¸æ˜¯ Pod IP åœ°å€ï¼ˆä¾‹å¦‚ï¼Œå‚è§ &lt;a href="https://github.com/kubernetes/kubernetes/issues/47992">kubernetes#47992&lt;/a> å’Œ &lt;a href="https://github.com/coredns/coredns/pull/1190">coredns#1190&lt;/a>ï¼‰ã€‚è¦åœ¨ CoreDNS ä¸­å¯ç”¨æ­¤åŠŸèƒ½ï¼Œè¯·åœ¨ Corefile ä¸­æŒ‡å®š &amp;quot;endpoint_pod_names&amp;quot; é€‰é¡¹ï¼Œç»“æžœå¦‚ä¸‹ï¼š&lt;/p>
&lt;pre>&lt;code>dnstools# host -t srv headless
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-qv84p.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-zc8lx.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-q7lf2.headless.default.svc.cluster.local.
headless.default.svc.cluster.local has SRV record 0 25 443 headless-65bb4c479f-566rt.headless.default.svc.cluster.local.
&lt;/code>&lt;/pre>&lt;!--
### Autopath
CoreDNS also has a special feature to improve latency in DNS requests for external names. In Kubernetes, the
DNS search path for pods specifies a long list of suffixes. This enables the use of short names when requesting
services in the cluster - for example, "headless" above, rather than "headless.default.svc.cluster.local". However,
when requesting an external name - "infoblox.com", for example - several invalid DNS queries are made by the client,
requiring a roundtrip from the client to kube-dns each time (actually to `dnsmasq` and then to `kubedns`, since [negative caching is disabled](https://github.com/kubernetes/dns/issues/121)):
--->
&lt;h3 id="è‡ªåŠ¨è·¯å¾„">è‡ªåŠ¨è·¯å¾„&lt;/h3>
&lt;p>CoreDNS è¿˜å…·æœ‰ä¸€é¡¹ç‰¹æ®ŠåŠŸèƒ½ï¼Œå¯ä»¥æ”¹å–„ DNS ä¸­å¤–éƒ¨åç§°è¯·æ±‚çš„å»¶è¿Ÿã€‚åœ¨ Kubernetes ä¸­ï¼ŒPod çš„ DNS æœç´¢è·¯å¾„æŒ‡å®šäº†ä¸€é•¿ä¸²åŽç¼€ã€‚è¿™ä¸€ç‰¹ç‚¹ä½¿å¾—ä½ å¯ä»¥é’ˆå¯¹é›†ç¾¤ä¸­æœåŠ¡ä½¿ç”¨çŸ­åç§° - ä¾‹å¦‚ï¼Œä¸Šé¢çš„ &amp;quot;headless&amp;quot;ï¼Œè€Œä¸æ˜¯ &amp;quot;headless.default.svc.cluster.local&amp;quot;ã€‚ä½†æ˜¯ï¼Œå½“è¯·æ±‚ä¸€ä¸ªå¤–éƒ¨åç§°ï¼ˆä¾‹å¦‚ &amp;quot;infoblox.com&amp;quot;ï¼‰æ—¶ï¼Œå®¢æˆ·ç«¯ä¼šè¿›è¡Œå‡ ä¸ªæ— æ•ˆçš„ DNS æŸ¥è¯¢ï¼Œæ¯æ¬¡éƒ½éœ€è¦ä»Žå®¢æˆ·ç«¯åˆ° kube-dns å¾€è¿”ï¼ˆå®žé™…ä¸Šæ˜¯åˆ° &lt;code>dnsmasq&lt;/code>ï¼Œç„¶åŽåˆ° &lt;code>kubedns&lt;/code>ï¼‰ï¼Œå› ä¸º &lt;a href="https://github.com/kubernetes/dns/issues/121">ç¦ç”¨äº†è´Ÿç¼“å­˜&lt;/a>ï¼‰&lt;/p>
&lt;ul>
&lt;li>infoblox.com.default.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.svc.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.cluster.local -&amp;gt; NXDOMAIN&lt;/li>
&lt;li>infoblox.com.your-internal-domain.com -&amp;gt; NXDOMAIN&lt;/li>
&lt;/ul>
&lt;!--
* infoblox.com -> returns a valid record
--->
&lt;ul>
&lt;li>infoblox.com -&amp;gt; è¿”å›žæœ‰æ•ˆè®°å½•&lt;/li>
&lt;/ul>
&lt;!--
In CoreDNS, an optional feature called [autopath](https://coredns.io/plugins/autopath) can be enabled that will cause this search path to be followed
*in the server*. That is, CoreDNS will figure out from the source IP address which namespace the client pod is in,
and it will walk this search list until it gets a valid answer. Since the first 3 of these are resolved internally
within CoreDNS itself, it cuts out all of the back and forth between the client and server, reducing latency.
--->
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œå¯ä»¥å¯ç”¨ &lt;a href="https://coredns.io/plugins/autopath">autopath&lt;/a> çš„å¯é€‰åŠŸèƒ½ï¼Œè¯¥åŠŸèƒ½ä½¿æœç´¢è·¯å¾„åœ¨ &lt;em>æœåŠ¡å™¨ç«¯&lt;/em> éåŽ†ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼ŒCoreDNS å°†åŸºäºŽæº IP åœ°å€åˆ¤æ–­å®¢æˆ·ç«¯ Pod æ‰€åœ¨çš„å‘½åç©ºé—´ï¼Œå¹¶ä¸”éåŽ†æ­¤æœç´¢åˆ—è¡¨ï¼Œç›´åˆ°èŽ·å¾—æœ‰æ•ˆç­”æ¡ˆä¸ºæ­¢ã€‚ç”±äºŽå…¶ä¸­çš„å‰ä¸‰ä¸ªæ˜¯åœ¨ CoreDNS æœ¬èº«å†…éƒ¨è§£å†³çš„ï¼Œå› æ­¤å®ƒæ¶ˆé™¤äº†å®¢æˆ·ç«¯å’ŒæœåŠ¡å™¨ä¹‹é—´æ‰€æœ‰çš„æ¥å›žé€šä¿¡ï¼Œä»Žè€Œå‡å°‘äº†å»¶è¿Ÿã€‚&lt;/p>
&lt;!--
### A few other Kubernetes specific features
In CoreDNS, you can use standard DNS zone transfer to export the entire DNS record set. This is useful for
debugging your services as well as importing the cluster zone into other DNS servers.
You can also filter by namespaces or a label selector. This can allow you to run specific CoreDNS instances that will only server records that match the filters, exposing only a limited set of your services via DNS.
--->
&lt;h3 id="å…¶ä»–ä¸€äº›ç‰¹å®šäºŽ-kubernetes-çš„åŠŸèƒ½">å…¶ä»–ä¸€äº›ç‰¹å®šäºŽ Kubernetes çš„åŠŸèƒ½&lt;/h3>
&lt;p>åœ¨ CoreDNS ä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ ‡å‡† DNS åŒºåŸŸä¼ è¾“æ¥å¯¼å‡ºæ•´ä¸ª DNS è®°å½•é›†ã€‚è¿™å¯¹äºŽè°ƒè¯•æœåŠ¡ä»¥åŠå°†é›†ç¾¤åŒºå¯¼å…¥å…¶ä»– DNS æœåŠ¡å™¨å¾ˆæœ‰ç”¨ã€‚&lt;/p>
&lt;p>æ‚¨è¿˜å¯ä»¥æŒ‰åç§°ç©ºé—´æˆ–æ ‡ç­¾é€‰æ‹©å™¨è¿›è¡Œè¿‡æ»¤ã€‚è¿™æ ·ï¼Œæ‚¨å¯ä»¥è¿è¡Œç‰¹å®šçš„ CoreDNS å®žä¾‹ï¼Œè¯¥å®žä¾‹ä»…æœåŠ¡ä¸Žè¿‡æ»¤å™¨åŒ¹é…çš„è®°å½•ï¼Œä»Žè€Œé€šè¿‡ DNS å…¬å¼€å—é™çš„æœåŠ¡é›†ã€‚&lt;/p>
&lt;!--
## Extensibility
In addition to the features described above, CoreDNS is easily extended. It is possible to build custom versions
of CoreDNS that include your own features. For example, this ability has been used to extend CoreDNS to do recursive resolution
with the [unbound plugin](https://coredns.io/explugins/unbound), to server records directly from a database with the [pdsql plugin](https://coredns.io/explugins/pdsql), and to allow multiple CoreDNS instances to share a common level 2 cache with the [redisc plugin](https://coredns.io/explugins/redisc).
Many other interesting extensions have been added, which you will find on the [External Plugins](https://coredns.io/explugins/) page of the CoreDNS site. One that is really interesting for Kubernetes and Istio users is the [kubernetai plugin](https://coredns.io/explugins/kubernetai), which allows a single CoreDNS instance to connect to multiple Kubernetes clusters and provide service discovery across all of them.
--->
&lt;h2 id="å¯æ‰©å±•æ€§">å¯æ‰©å±•æ€§&lt;/h2>
&lt;p>é™¤äº†ä¸Šè¿°åŠŸèƒ½ä¹‹å¤–ï¼ŒCoreDNS è¿˜å¯è½»æ¾æ‰©å±•ï¼Œæž„å»ºåŒ…å«æ‚¨ç‹¬æœ‰çš„åŠŸèƒ½çš„è‡ªå®šä¹‰ç‰ˆæœ¬çš„ CoreDNSã€‚ä¾‹å¦‚ï¼Œè¿™ä¸€èƒ½åŠ›å·²è¢«ç”¨äºŽæ‰©å±• CoreDNS æ¥ä½¿ç”¨ &lt;a href="https://coredns.io/explugins/unbound">unbound æ’ä»¶&lt;/a> è¿›è¡Œé€’å½’è§£æžã€ä½¿ç”¨ &lt;a href="https://coredns.io/explugins/pdsql">pdsql æ’ä»¶&lt;/a> ç›´æŽ¥ä»Žæ•°æ®åº“æä¾›è®°å½•ï¼Œä»¥åŠä½¿ç”¨ &lt;a href="https://coredns.io/explugins/redisc">redisc æ’ä»¶&lt;/a> ä¸Žå¤šä¸ª CoreDNS å®žä¾‹å…±äº«ä¸€ä¸ªå…¬å…±çš„ 2 çº§ç¼“å­˜ã€‚&lt;/p>
&lt;p>å·²æ·»åŠ çš„è¿˜æœ‰è®¸å¤šå…¶ä»–æœ‰è¶£çš„æ‰©å±•ï¼Œæ‚¨å¯ä»¥åœ¨ CoreDNS ç«™ç‚¹çš„ &lt;a href="https://coredns.io/explugins/">å¤–éƒ¨æ’ä»¶&lt;/a> é¡µé¢ä¸Šæ‰¾åˆ°è¿™äº›æ‰©å±•ã€‚Kubernetes å’Œ Istio ç”¨æˆ·çœŸæ­£æ„Ÿå…´è¶£çš„æ˜¯ &lt;a href="https://coredns.io/explugins/kubernetai">kubernetai æ’ä»¶&lt;/a>ï¼Œå®ƒå…è®¸å•ä¸ª CoreDNS å®žä¾‹è¿žæŽ¥åˆ°å¤šä¸ª Kubernetes é›†ç¾¤å¹¶åœ¨æ‰€æœ‰é›†ç¾¤ä¸­æä¾›æœåŠ¡å‘çŽ° ã€‚&lt;/p>
&lt;!--
## What's Next?
CoreDNS is an independent project, and as such is developing many features that are not directly
related to Kubernetes. However, a number of these will have applications within Kubernetes. For example,
the upcoming integration with policy engines will allow CoreDNS to make intelligent choices about which endpoint
to return when a headless service is requested. This could be used to route traffic to a local pod, or
to a more responsive pod. Many other features are in development, and of course as an open source project, we welcome you to suggest and contribute your own features!
The features and differences described above are a few examples. There is much more you can do with CoreDNS.
You can find out more on the [CoreDNS Blog](https://coredns.io/blog).
--->
&lt;h2 id="ä¸‹ä¸€æ­¥å·¥ä½œ">ä¸‹ä¸€æ­¥å·¥ä½œ&lt;/h2>
&lt;p>CoreDNS æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„é¡¹ç›®ï¼Œè®¸å¤šä¸Ž Kubernetes ä¸ç›´æŽ¥ç›¸å…³çš„åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ã€‚ä½†æ˜¯ï¼Œå…¶ä¸­è®¸å¤šåŠŸèƒ½å°†åœ¨ Kubernetes ä¸­å…·æœ‰å¯¹åº”çš„åº”ç”¨ã€‚ä¾‹å¦‚ï¼Œä¸Žç­–ç•¥å¼•æ“Žå®Œæˆé›†æˆåŽï¼Œå½“è¯·æ±‚æ— å¤´æœåŠ¡æ—¶ï¼ŒCoreDNS èƒ½å¤Ÿæ™ºèƒ½åœ°é€‰æ‹©è¿”å›žå“ªä¸ªç«¯ç‚¹ã€‚è¿™å¯ç”¨äºŽå°†æµé‡åˆ†æµåˆ°åˆ°æœ¬åœ° Pod æˆ–å“åº”æ›´å¿«çš„ Podã€‚æ›´å¤šçš„å…¶ä»–åŠŸèƒ½æ­£åœ¨å¼€å‘ä¸­ï¼Œå½“ç„¶ä½œä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œæˆ‘ä»¬æ¬¢è¿Žæ‚¨æå‡ºå»ºè®®å¹¶è´¡çŒ®è‡ªå·±çš„åŠŸèƒ½ç‰¹æ€§ï¼&lt;/p>
&lt;p>ä¸Šè¿°ç‰¹å¾å’Œå·®å¼‚æ˜¯å‡ ä¸ªç¤ºä¾‹ã€‚CoreDNS è¿˜å¯ä»¥åšæ›´å¤šçš„äº‹æƒ…ã€‚æ‚¨å¯ä»¥åœ¨ &lt;a href="https://coredns.io/blog">CoreDNS åšå®¢&lt;/a> ä¸Šæ‰¾åˆ°æ›´å¤šä¿¡æ¯ã€‚&lt;/p>
&lt;!--
### Get involved with CoreDNS
CoreDNS is an incubated [CNCF](https:://cncf.io) project.
We're most active on Slack (and GitHub):
--->
&lt;h3 id="å‚ä¸Ž-coredns">å‚ä¸Ž CoreDNS&lt;/h3>
&lt;p>CoreDNS æ˜¯ä¸€ä¸ª &lt;a href="https:://cncf.io">CNCF&lt;/a> å­µåŒ–é¡¹ç›®ã€‚&lt;/p>
&lt;p>æˆ‘ä»¬åœ¨ Slackï¼ˆå’Œ GitHubï¼‰ä¸Šæœ€æ´»è·ƒï¼š&lt;/p>
&lt;ul>
&lt;li>Slack: #coredns on &lt;a href="https://slack.cncf.io">https://slack.cncf.io&lt;/a>&lt;/li>
&lt;li>GitHub: &lt;a href="https://github.com/coredns/coredns">https://github.com/coredns/coredns&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
More resources can be found:
--->
&lt;p>æ›´å¤šèµ„æºè¯·æµè§ˆï¼š&lt;/p>
&lt;ul>
&lt;li>Website: &lt;a href="https://coredns.io">https://coredns.io&lt;/a>&lt;/li>
&lt;li>Blog: &lt;a href="https://blog.coredns.io">https://blog.coredns.io&lt;/a>&lt;/li>
&lt;li>Twitter: &lt;a href="https://twitter.com/corednsio">@corednsio&lt;/a>&lt;/li>
&lt;li>Mailing list/group: &lt;a href="mailto:coredns-discuss@googlegroups.com">coredns-discuss@googlegroups.com&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: IPVS-Based In-Cluster Load Balancing Deep Dive</title><link>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</link><pubDate>Mon, 09 Jul 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/</guid><description>
&lt;!--
Author: Jun Du(Huawei), Haibin Xie(Huawei), Wei Liang(Huawei)
Editorâ€™s note: this post is part of a series of in-depth articles on whatâ€™s new in Kubernetes 1.11
-->
&lt;p>ä½œè€…: Jun Du(åŽä¸º), Haibin Xie(åŽä¸º), Wei Liang(åŽä¸º)&lt;/p>
&lt;p>æ³¨æ„: è¿™ç¯‡æ–‡ç« å‡ºè‡ª ç³»åˆ—æ·±åº¦æ–‡ç«  ä»‹ç» Kubernetes 1.11 çš„æ–°ç‰¹æ€§&lt;/p>
&lt;!--
Introduction
Per the Kubernetes 1.11 release blog post , we announced that IPVS-Based In-Cluster Service Load Balancing graduates to General Availability. In this blog, we will take you through a deep dive of the feature.
-->
&lt;p>ä»‹ç»&lt;/p>
&lt;p>æ ¹æ® Kubernetes 1.11 å‘å¸ƒçš„åšå®¢æ–‡ç« , æˆ‘ä»¬å®£å¸ƒåŸºäºŽ IPVS çš„é›†ç¾¤å†…éƒ¨æœåŠ¡è´Ÿè½½å‡è¡¡å·²è¾¾åˆ°ä¸€èˆ¬å¯ç”¨æ€§ã€‚ åœ¨è¿™ç¯‡åšå®¢ä¸­ï¼Œæˆ‘ä»¬å°†å¸¦æ‚¨æ·±å…¥äº†è§£è¯¥åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
What Is IPVS?
IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.
IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.
-->
&lt;p>ä»€ä¹ˆæ˜¯ IPVS ?&lt;/p>
&lt;p>IPVS (IP Virtual Server)æ˜¯åœ¨ Netfilter ä¸Šå±‚æž„å»ºçš„ï¼Œå¹¶ä½œä¸º Linux å†…æ ¸çš„ä¸€éƒ¨åˆ†ï¼Œå®žçŽ°ä¼ è¾“å±‚è´Ÿè½½å‡è¡¡ã€‚&lt;/p>
&lt;p>IPVS é›†æˆåœ¨ LVSï¼ˆLinux Virtual Serverï¼ŒLinux è™šæ‹ŸæœåŠ¡å™¨ï¼‰ä¸­ï¼Œå®ƒåœ¨ä¸»æœºä¸Šè¿è¡Œï¼Œå¹¶åœ¨ç‰©ç†æœåŠ¡å™¨é›†ç¾¤å‰ä½œä¸ºè´Ÿè½½å‡è¡¡å™¨ã€‚IPVS å¯ä»¥å°†åŸºäºŽ TCP å’Œ UDP æœåŠ¡çš„è¯·æ±‚å®šå‘åˆ°çœŸå®žæœåŠ¡å™¨ï¼Œå¹¶ä½¿çœŸå®žæœåŠ¡å™¨çš„æœåŠ¡åœ¨å•ä¸ªIPåœ°å€ä¸Šæ˜¾ç¤ºä¸ºè™šæ‹ŸæœåŠ¡ã€‚ å› æ­¤ï¼ŒIPVS è‡ªç„¶æ”¯æŒ Kubernetes æœåŠ¡ã€‚&lt;/p>
&lt;!--
Why IPVS for Kubernetes?
As Kubernetes grows in usage, the scalability of its resources becomes more and more important. In particular, the scalability of services is paramount to the adoption of Kubernetes by developers/companies running large workloads.
Kube-proxy, the building block of service routing has relied on the battle-hardened iptables to implement the core supported Service types such as ClusterIP and NodePort. However, iptables struggles to scale to tens of thousands of Services because it is designed purely for firewalling purposes and is based on in-kernel rule lists.
Even though Kubernetes already support 5000 nodes in release v1.6, the kube-proxy with iptables is actually a bottleneck to scale the cluster to 5000 nodes. One example is that with NodePort Service in a 5000-node cluster, if we have 2000 services and each services have 10 pods, this will cause at least 20000 iptable records on each worker node, and this can make the kernel pretty busy.
On the other hand, using IPVS-based in-cluster service load balancing can help a lot for such cases. IPVS is specifically designed for load balancing and uses more efficient data structures (hash tables) allowing for almost unlimited scale under the hood.
-->
&lt;p>ä¸ºä»€ä¹ˆä¸º Kubernetes é€‰æ‹© IPVS ?&lt;/p>
&lt;p>éšç€ Kubernetes çš„ä½¿ç”¨å¢žé•¿ï¼Œå…¶èµ„æºçš„å¯æ‰©å±•æ€§å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚ç‰¹åˆ«æ˜¯ï¼ŒæœåŠ¡çš„å¯æ‰©å±•æ€§å¯¹äºŽè¿è¡Œå¤§åž‹å·¥ä½œè´Ÿè½½çš„å¼€å‘äººå‘˜/å…¬å¸é‡‡ç”¨ Kubernetes è‡³å…³é‡è¦ã€‚&lt;/p>
&lt;p>Kube-proxy æ˜¯æœåŠ¡è·¯ç”±çš„æž„å»ºå—ï¼Œå®ƒä¾èµ–äºŽç»è¿‡å¼ºåŒ–æ”»å‡»çš„ iptables æ¥å®žçŽ°æ”¯æŒæ ¸å¿ƒçš„æœåŠ¡ç±»åž‹ï¼Œå¦‚ ClusterIP å’Œ NodePortã€‚ ä½†æ˜¯ï¼Œiptables éš¾ä»¥æ‰©å±•åˆ°æˆåƒä¸Šä¸‡çš„æœåŠ¡ï¼Œå› ä¸ºå®ƒçº¯ç²¹æ˜¯ä¸ºé˜²ç«å¢™è€Œè®¾è®¡çš„ï¼Œå¹¶ä¸”åŸºäºŽå†…æ ¸è§„åˆ™åˆ—è¡¨ã€‚&lt;/p>
&lt;p>å°½ç®¡ Kubernetes åœ¨ç‰ˆæœ¬v1.6ä¸­å·²ç»æ”¯æŒ5000ä¸ªèŠ‚ç‚¹ï¼Œä½†ä½¿ç”¨ iptables çš„ kube-proxy å®žé™…ä¸Šæ˜¯å°†é›†ç¾¤æ‰©å±•åˆ°5000ä¸ªèŠ‚ç‚¹çš„ç“¶é¢ˆã€‚ ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œåœ¨5000èŠ‚ç‚¹é›†ç¾¤ä¸­ä½¿ç”¨ NodePort æœåŠ¡ï¼Œå¦‚æžœæˆ‘ä»¬æœ‰2000ä¸ªæœåŠ¡å¹¶ä¸”æ¯ä¸ªæœåŠ¡æœ‰10ä¸ª podï¼Œè¿™å°†åœ¨æ¯ä¸ªå·¥ä½œèŠ‚ç‚¹ä¸Šè‡³å°‘äº§ç”Ÿ20000ä¸ª iptable è®°å½•ï¼Œè¿™å¯èƒ½ä½¿å†…æ ¸éžå¸¸ç¹å¿™ã€‚&lt;/p>
&lt;p>å¦ä¸€æ–¹é¢ï¼Œä½¿ç”¨åŸºäºŽ IPVS çš„é›†ç¾¤å†…æœåŠ¡è´Ÿè½½å‡è¡¡å¯ä»¥ä¸ºè¿™ç§æƒ…å†µæä¾›å¾ˆå¤šå¸®åŠ©ã€‚ IPVS ä¸“é—¨ç”¨äºŽè´Ÿè½½å‡è¡¡ï¼Œå¹¶ä½¿ç”¨æ›´é«˜æ•ˆçš„æ•°æ®ç»“æž„ï¼ˆå“ˆå¸Œè¡¨ï¼‰ï¼Œå…è®¸å‡ ä¹Žæ— é™çš„è§„æ¨¡æ‰©å¼ ã€‚&lt;/p>
&lt;!--
IPVS-based Kube-proxy
Parameter Changes
Parameter: --proxy-mode In addition to existing userspace and iptables modes, IPVS mode is configured via --proxy-mode=ipvs. It implicitly uses IPVS NAT mode for service port mapping.
-->
&lt;p>åŸºäºŽ IPVS çš„ Kube-proxy&lt;/p>
&lt;p>å‚æ•°æ›´æ”¹&lt;/p>
&lt;p>å‚æ•°: --proxy-mode é™¤äº†çŽ°æœ‰çš„ç”¨æˆ·ç©ºé—´å’Œ iptables æ¨¡å¼ï¼ŒIPVS æ¨¡å¼é€šè¿‡--proxy-mode = ipvs è¿›è¡Œé…ç½®ã€‚ å®ƒéšå¼ä½¿ç”¨ IPVS NAT æ¨¡å¼è¿›è¡ŒæœåŠ¡ç«¯å£æ˜ å°„ã€‚&lt;/p>
&lt;!--
Parameter: --ipvs-scheduler
A new kube-proxy parameter has been added to specify the IPVS load balancing algorithm, with the parameter being --ipvs-scheduler. If itâ€™s not configured, then round-robin (rr) is the default value.
- rr: round-robin
- lc: least connection
- dh: destination hashing
- sh: source hashing
- sed: shortest expected delay
- nq: never queue
In the future, we can implement Service specific scheduler (potentially via annotation), which has higher priority and overwrites the value.
-->
&lt;p>å‚æ•°: --ipvs-scheduler&lt;/p>
&lt;p>æ·»åŠ äº†ä¸€ä¸ªæ–°çš„ kube-proxy å‚æ•°æ¥æŒ‡å®š IPVS è´Ÿè½½å‡è¡¡ç®—æ³•ï¼Œå‚æ•°ä¸º --ipvs-schedulerã€‚ å¦‚æžœæœªé…ç½®ï¼Œåˆ™é»˜è®¤ä¸º round-robin ç®—æ³•ï¼ˆrrï¼‰ã€‚&lt;/p>
&lt;ul>
&lt;li>rr: round-robin&lt;/li>
&lt;li>lc: least connection&lt;/li>
&lt;li>dh: destination hashing&lt;/li>
&lt;li>sh: source hashing&lt;/li>
&lt;li>sed: shortest expected delay&lt;/li>
&lt;li>nq: never queue&lt;/li>
&lt;/ul>
&lt;p>å°†æ¥ï¼Œæˆ‘ä»¬å¯ä»¥å®žçŽ°ç‰¹å®šäºŽæœåŠ¡çš„è°ƒåº¦ç¨‹åºï¼ˆå¯èƒ½é€šè¿‡æ³¨é‡Šï¼‰ï¼Œè¯¥è°ƒåº¦ç¨‹åºå…·æœ‰æ›´é«˜çš„ä¼˜å…ˆçº§å¹¶è¦†ç›–è¯¥å€¼ã€‚&lt;/p>
&lt;!--
Parameter: --cleanup-ipvs Similar to the --cleanup-iptables parameter, if true, cleanup IPVS configuration and IPTables rules that are created in IPVS mode.
Parameter: --ipvs-sync-period Maximum interval of how often IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
Parameter: --ipvs-min-sync-period Minimum interval of how often the IPVS rules are refreshed (e.g. '5s', '1m'). Must be greater than 0.
-->
&lt;p>å‚æ•°: --cleanup-ipvs ç±»ä¼¼äºŽ --cleanup-iptables å‚æ•°ï¼Œå¦‚æžœä¸º trueï¼Œåˆ™æ¸…é™¤åœ¨ IPVS æ¨¡å¼ä¸‹åˆ›å»ºçš„ IPVS é…ç½®å’Œ IPTables è§„åˆ™ã€‚&lt;/p>
&lt;p>å‚æ•°: --ipvs-sync-period åˆ·æ–° IPVS è§„åˆ™çš„æœ€å¤§é—´éš”æ—¶é—´ï¼ˆä¾‹å¦‚'5s'ï¼Œ'1m'ï¼‰ã€‚ å¿…é¡»å¤§äºŽ0ã€‚&lt;/p>
&lt;p>å‚æ•°: --ipvs-min-sync-period åˆ·æ–° IPVS è§„åˆ™çš„æœ€å°é—´éš”æ—¶é—´é—´éš”ï¼ˆä¾‹å¦‚'5s'ï¼Œ'1m'ï¼‰ã€‚ å¿…é¡»å¤§äºŽ0ã€‚&lt;/p>
&lt;!--
Parameter: --ipvs-exclude-cidrs A comma-separated list of CIDR's which the IPVS proxier should not touch when cleaning up IPVS rules because IPVS proxier can't distinguish kube-proxy created IPVS rules from user original IPVS rules. If you are using IPVS proxier with your own IPVS rules in the environment, this parameter should be specified, otherwise your original rule will be cleaned.
-->
&lt;p>å‚æ•°: --ipvs-exclude-cidrs æ¸…é™¤ IPVS è§„åˆ™æ—¶ IPVS ä»£ç†ä¸åº”è§¦åŠçš„ CIDR çš„é€—å·åˆ†éš”åˆ—è¡¨ï¼Œå› ä¸º IPVS ä»£ç†æ— æ³•åŒºåˆ† kube-proxy åˆ›å»ºçš„ IPVS è§„åˆ™å’Œç”¨æˆ·åŽŸå§‹è§„åˆ™ IPVS è§„åˆ™ã€‚ å¦‚æžœæ‚¨åœ¨çŽ¯å¢ƒä¸­ä½¿ç”¨ IPVS proxier å’Œæ‚¨è‡ªå·±çš„ IPVS è§„åˆ™ï¼Œåˆ™åº”æŒ‡å®šæ­¤å‚æ•°ï¼Œå¦åˆ™å°†æ¸…é™¤åŽŸå§‹è§„åˆ™ã€‚&lt;/p>
&lt;!--
Design Considerations
IPVS Service Network Topology
When creating a ClusterIP type Service, IPVS proxier will do the following three things:
- Make sure a dummy interface exists in the node, defaults to kube-ipvs0
- Bind Service IP addresses to the dummy interface
- Create IPVS virtual servers for each Service IP address respectively
-->
&lt;p>è®¾è®¡æ³¨æ„äº‹é¡¹&lt;/p>
&lt;p>IPVS æœåŠ¡ç½‘ç»œæ‹“æ‰‘&lt;/p>
&lt;p>åˆ›å»º ClusterIP ç±»åž‹æœåŠ¡æ—¶ï¼ŒIPVS proxier å°†æ‰§è¡Œä»¥ä¸‹ä¸‰é¡¹æ“ä½œï¼š&lt;/p>
&lt;ul>
&lt;li>ç¡®ä¿èŠ‚ç‚¹ä¸­å­˜åœ¨è™šæ‹ŸæŽ¥å£ï¼Œé»˜è®¤ä¸º kube-ipvs0&lt;/li>
&lt;li>å°†æœåŠ¡ IP åœ°å€ç»‘å®šåˆ°è™šæ‹ŸæŽ¥å£&lt;/li>
&lt;li>åˆ†åˆ«ä¸ºæ¯ä¸ªæœåŠ¡ IP åœ°å€åˆ›å»º IPVS è™šæ‹ŸæœåŠ¡å™¨&lt;/li>
&lt;/ul>
&lt;!--
Here comes an example:
# kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &lt;BROADCAST,NOARP> mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-> RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-> 10.244.0.235:8080 Masq 1 0 0
-> 10.244.1.237:8080 Masq 1 0 0
-->
&lt;p>è¿™æ˜¯ä¸€ä¸ªä¾‹å­:&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
Type: ClusterIP
IP: 10.102.128.4
Port: http 3080/TCP
Endpoints: 10.244.0.235:8080,10.244.1.237:8080
Session Affinity: None
# ip addr
...
73: kube-ipvs0: &amp;lt;BROADCAST,NOARP&amp;gt; mtu 1500 qdisc noop state DOWN qlen 1000
link/ether 1a:ce:f5:5f:c1:4d brd ff:ff:ff:ff:ff:ff
inet 10.102.128.4/32 scope global kube-ipvs0
valid_lft forever preferred_lft forever
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0 0
&lt;/code>&lt;/pre>
&lt;!--
Please note that the relationship between a Kubernetes Service and IPVS virtual servers is 1:N. For example, consider a Kubernetes Service that has more than one IP address. An External IP type Service has two IP addresses - ClusterIP and External IP. Then the IPVS proxier will create 2 IPVS virtual servers - one for Cluster IP and another one for External IP. The relationship between a Kubernetes Endpoint (each IP+Port pair) and an IPVS virtual server is 1:1.
Deleting of a Kubernetes service will trigger deletion of the corresponding IPVS virtual server, IPVS real servers and its IP addresses bound to the dummy interface.
Port Mapping
There are three proxy modes in IPVS: NAT (masq), IPIP and DR. Only NAT mode supports port mapping. Kube-proxy leverages NAT mode for port mapping. The following example shows IPVS mapping Service port 3080 to Pod port 8080.
-->
&lt;p>è¯·æ³¨æ„ï¼ŒKubernetes æœåŠ¡å’Œ IPVS è™šæ‹ŸæœåŠ¡å™¨ä¹‹é—´çš„å…³ç³»æ˜¯â€œ1ï¼šNâ€ã€‚ ä¾‹å¦‚ï¼Œè€ƒè™‘å…·æœ‰å¤šä¸ª IP åœ°å€çš„ Kubernetes æœåŠ¡ã€‚ å¤–éƒ¨ IP ç±»åž‹æœåŠ¡æœ‰ä¸¤ä¸ª IP åœ°å€ - é›†ç¾¤IPå’Œå¤–éƒ¨ IPã€‚ ç„¶åŽï¼ŒIPVS ä»£ç†å°†åˆ›å»º2ä¸ª IPVS è™šæ‹ŸæœåŠ¡å™¨ - ä¸€ä¸ªç”¨äºŽé›†ç¾¤ IPï¼Œå¦ä¸€ä¸ªç”¨äºŽå¤–éƒ¨ IPã€‚ Kubernetes çš„ endpointï¼ˆæ¯ä¸ªIP +ç«¯å£å¯¹ï¼‰ä¸Ž IPVS è™šæ‹ŸæœåŠ¡å™¨ä¹‹é—´çš„å…³ç³»æ˜¯â€œ1ï¼š1â€ã€‚&lt;/p>
&lt;p>åˆ é™¤ Kubernetes æœåŠ¡å°†è§¦å‘åˆ é™¤ç›¸åº”çš„ IPVS è™šæ‹ŸæœåŠ¡å™¨ï¼ŒIPVS ç‰©ç†æœåŠ¡å™¨åŠå…¶ç»‘å®šåˆ°è™šæ‹ŸæŽ¥å£çš„ IP åœ°å€ã€‚&lt;/p>
&lt;p>ç«¯å£æ˜ å°„&lt;/p>
&lt;p>IPVS ä¸­æœ‰ä¸‰ç§ä»£ç†æ¨¡å¼ï¼šNATï¼ˆmasqï¼‰ï¼ŒIPIP å’Œ DRã€‚ åªæœ‰ NAT æ¨¡å¼æ”¯æŒç«¯å£æ˜ å°„ã€‚ Kube-proxy åˆ©ç”¨ NAT æ¨¡å¼è¿›è¡Œç«¯å£æ˜ å°„ã€‚ ä»¥ä¸‹ç¤ºä¾‹æ˜¾ç¤º IPVS æœåŠ¡ç«¯å£3080åˆ°Podç«¯å£8080çš„æ˜ å°„ã€‚&lt;/p>
&lt;pre>&lt;code>TCP 10.102.128.4:3080 rr
-&amp;gt; 10.244.0.235:8080 Masq 1 0 0
-&amp;gt; 10.244.1.237:8080 Masq 1 0
&lt;/code>&lt;/pre>
&lt;!--
Session Affinity
IPVS supports client IP session affinity (persistent connection). When a Service specifies session affinity, the IPVS proxier will set a timeout value (180min=10800s by default) in the IPVS virtual server. For example:
-->
&lt;p>ä¼šè¯å…³ç³»&lt;/p>
&lt;p>IPVS æ”¯æŒå®¢æˆ·ç«¯ IP ä¼šè¯å…³è”ï¼ˆæŒä¹…è¿žæŽ¥ï¼‰ã€‚ å½“æœåŠ¡æŒ‡å®šä¼šè¯å…³ç³»æ—¶ï¼ŒIPVS ä»£ç†å°†åœ¨ IPVS è™šæ‹ŸæœåŠ¡å™¨ä¸­è®¾ç½®è¶…æ—¶å€¼ï¼ˆé»˜è®¤ä¸º180åˆ†é’Ÿ= 10800ç§’ï¼‰ã€‚ ä¾‹å¦‚ï¼š&lt;/p>
&lt;pre>&lt;code># kubectl describe svc nginx-service
Name: nginx-service
...
IP: 10.102.128.4
Port: http 3080/TCP
Session Affinity: ClientIP
# ipvsadm -ln
IP Virtual Server version 1.2.1 (size=4096)
Prot LocalAddress:Port Scheduler Flags
-&amp;gt; RemoteAddress:Port Forward Weight ActiveConn InActConn
TCP 10.102.128.4:3080 rr persistent 10800
&lt;/code>&lt;/pre>
&lt;!--
Iptables &amp; Ipset in IPVS Proxier
IPVS is for load balancing and it can't handle other workarounds in kube-proxy, e.g. packet filtering, hairpin-masquerade tricks, SNAT, etc.
IPVS proxier leverages iptables in the above scenarios. Specifically, ipvs proxier will fall back on iptables in the following 4 scenarios:
- kube-proxy start with --masquerade-all=true
- Specify cluster CIDR in kube-proxy startup
- Support Loadbalancer type service
- Support NodePort type service
However, we don't want to create too many iptables rules. So we adopt ipset for the sake of decreasing iptables rules. The following is the table of ipset sets that IPVS proxier maintains:
-->
&lt;p>IPVS ä»£ç†ä¸­çš„ Iptables å’Œ Ipset&lt;/p>
&lt;p>IPVS ç”¨äºŽè´Ÿè½½å‡è¡¡ï¼Œå®ƒæ— æ³•å¤„ç† kube-proxy ä¸­çš„å…¶ä»–é—®é¢˜ï¼Œä¾‹å¦‚ åŒ…è¿‡æ»¤ï¼Œæ•°æ®åŒ…æ¬ºéª—ï¼ŒSNAT ç­‰&lt;/p>
&lt;p>IPVS proxier åœ¨ä¸Šè¿°åœºæ™¯ä¸­åˆ©ç”¨ iptablesã€‚ å…·ä½“æ¥è¯´ï¼Œipvs proxier å°†åœ¨ä»¥ä¸‹4ç§æƒ…å†µä¸‹ä¾èµ–äºŽ iptablesï¼š&lt;/p>
&lt;ul>
&lt;li>kube-proxy ä»¥ --masquerade-all = true å¼€å¤´&lt;/li>
&lt;li>åœ¨ kube-proxy å¯åŠ¨ä¸­æŒ‡å®šé›†ç¾¤ CIDR&lt;/li>
&lt;li>æ”¯æŒ Loadbalancer ç±»åž‹æœåŠ¡&lt;/li>
&lt;li>æ”¯æŒ NodePort ç±»åž‹çš„æœåŠ¡&lt;/li>
&lt;/ul>
&lt;p>ä½†æ˜¯ï¼Œæˆ‘ä»¬ä¸æƒ³åˆ›å»ºå¤ªå¤šçš„ iptables è§„åˆ™ã€‚ æ‰€ä»¥æˆ‘ä»¬é‡‡ç”¨ ipset æ¥å‡å°‘ iptables è§„åˆ™ã€‚ ä»¥ä¸‹æ˜¯ IPVS proxier ç»´æŠ¤çš„ ipset é›†è¡¨ï¼š&lt;/p>
&lt;!--
set name members usage
KUBE-CLUSTER-IP All Service IP + port masquerade for cases that masquerade-all=true or clusterCIDR specified
KUBE-LOOP-BACK All Service IP + port + IP masquerade for resolving hairpin issue
KUBE-EXTERNAL-IP Service External IP + port masquerade for packets to external IPs
KUBE-LOAD-BALANCER Load Balancer ingress IP + port masquerade for packets to Load Balancer type service
KUBE-LOAD-BALANCER-LOCAL Load Balancer ingress IP + port with externalTrafficPolicy=local accept packets to Load Balancer with externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW Load Balancer ingress IP + port with loadBalancerSourceRanges Drop packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-LOAD-BALANCER-SOURCE-CIDR Load Balancer ingress IP + port + source CIDR accept packets for Load Balancer type Service with loadBalancerSourceRanges specified
KUBE-NODE-PORT-TCP NodePort type Service TCP port masquerade for packets to NodePort(TCP)
KUBE-NODE-PORT-LOCAL-TCP NodePort type Service TCP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort type Service UDP port masquerade for packets to NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort type service UDP port with externalTrafficPolicy=local accept packets to NodePort Service with externalTrafficPolicy=local
-->
&lt;p>è®¾ç½®åç§° æˆå‘˜ ç”¨æ³•
KUBE-CLUSTER-IP æ‰€æœ‰æœåŠ¡ IP + ç«¯å£ masquerade-all=true æˆ– clusterCIDR æŒ‡å®šçš„æƒ…å†µä¸‹è¿›è¡Œä¼ªè£…
KUBE-LOOP-BACK æ‰€æœ‰æœåŠ¡ IP +ç«¯å£+ IP è§£å†³æ•°æ®åŒ…æ¬ºéª—é—®é¢˜
KUBE-EXTERNAL-IP æœåŠ¡å¤–éƒ¨ IP +ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆå¤–éƒ¨ IP
KUBE-LOAD-BALANCER è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆ Load Balancer ç±»åž‹çš„æœåŠ¡
KUBE-LOAD-BALANCER-LOCAL è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ ä»¥åŠ externalTrafficPolicy=local æŽ¥å—æ•°æ®åŒ…åˆ° Load Balancer externalTrafficPolicy=local
KUBE-LOAD-BALANCER-FW è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ ä»¥åŠ loadBalancerSourceRanges ä½¿ç”¨æŒ‡å®šçš„ loadBalancerSourceRanges ä¸¢å¼ƒ Load Balancerç±»åž‹Serviceçš„æ•°æ®åŒ…
KUBE-LOAD-BALANCER-SOURCE-CIDR è´Ÿè½½å‡è¡¡å™¨å…¥å£ IP +ç«¯å£ + æº CIDR æŽ¥å— Load Balancer ç±»åž‹ Service çš„æ•°æ®åŒ…ï¼Œå¹¶æŒ‡å®šloadBalancerSourceRanges
KUBE-NODE-PORT-TCP NodePort ç±»åž‹æœåŠ¡ TCP å°†æ•°æ®åŒ…ä¼ªè£…æˆ NodePortï¼ˆTCPï¼‰
KUBE-NODE-PORT-LOCAL-TCP NodePort ç±»åž‹æœåŠ¡ TCP ç«¯å£ï¼Œå¸¦æœ‰ externalTrafficPolicy=local æŽ¥å—æ•°æ®åŒ…åˆ° NodePort æœåŠ¡ ä½¿ç”¨ externalTrafficPolicy=local
KUBE-NODE-PORT-UDP NodePort ç±»åž‹æœåŠ¡ UDP ç«¯å£ å°†æ•°æ®åŒ…ä¼ªè£…æˆ NodePort(UDP)
KUBE-NODE-PORT-LOCAL-UDP NodePort ç±»åž‹æœåŠ¡ UDP ç«¯å£ ä½¿ç”¨ externalTrafficPolicy=local æŽ¥å—æ•°æ®åŒ…åˆ°NodePortæœåŠ¡ ä½¿ç”¨ externalTrafficPolicy=local&lt;/p>
&lt;!--
In general, for IPVS proxier, the number of iptables rules is static, no matter how many Services/Pods we have.
-->
&lt;p>é€šå¸¸ï¼Œå¯¹äºŽ IPVS proxierï¼Œæ— è®ºæˆ‘ä»¬æœ‰å¤šå°‘ Service/ Podï¼Œiptables è§„åˆ™çš„æ•°é‡éƒ½æ˜¯é™æ€çš„ã€‚&lt;/p>
&lt;!--
Run kube-proxy in IPVS Mode
Currently, local-up scripts, GCE scripts, and kubeadm support switching IPVS proxy mode via exporting environment variables (KUBE_PROXY_MODE=ipvs) or specifying flag (--proxy-mode=ipvs). Before running IPVS proxier, please ensure IPVS required kernel modules are already installed.
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
Finally, for Kubernetes v1.10, feature gate SupportIPVSProxyMode is set to true by default. For Kubernetes v1.11, the feature gate is entirely removed. However, you need to enable --feature-gates=SupportIPVSProxyMode=true explicitly for Kubernetes before v1.10.
-->
&lt;p>åœ¨ IPVS æ¨¡å¼ä¸‹è¿è¡Œ kube-proxy&lt;/p>
&lt;p>ç›®å‰ï¼Œæœ¬åœ°è„šæœ¬ï¼ŒGCE è„šæœ¬å’Œ kubeadm æ”¯æŒé€šè¿‡å¯¼å‡ºçŽ¯å¢ƒå˜é‡ï¼ˆKUBE_PROXY_MODE=ipvsï¼‰æˆ–æŒ‡å®šæ ‡å¿—ï¼ˆ--proxy-mode=ipvsï¼‰æ¥åˆ‡æ¢ IPVS ä»£ç†æ¨¡å¼ã€‚ åœ¨è¿è¡ŒIPVS ä»£ç†ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… IPVS æ‰€éœ€çš„å†…æ ¸æ¨¡å—ã€‚&lt;/p>
&lt;pre>&lt;code>ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
nf_conntrack_ipv4
&lt;/code>&lt;/pre>
&lt;p>æœ€åŽï¼Œå¯¹äºŽ Kubernetes v1.10ï¼Œâ€œSupportIPVSProxyModeâ€ é»˜è®¤è®¾ç½®ä¸º â€œtrueâ€ã€‚ å¯¹äºŽ Kubernetes v1.11 ï¼Œè¯¥é€‰é¡¹å·²å®Œå…¨åˆ é™¤ã€‚ ä½†æ˜¯ï¼Œæ‚¨éœ€è¦åœ¨v1.10ä¹‹å‰ä¸ºKubernetes æ˜Žç¡®å¯ç”¨ --feature-gates = SupportIPVSProxyMode = trueã€‚&lt;/p>
&lt;!--
Get Involved
The simplest way to get involved with Kubernetes is by joining one of the many Special Interest Groups (SIGs) that align with your interests. Have something youâ€™d like to broadcast to the Kubernetes community? Share your voice at our weekly community meeting, and through the channels below.
Thank you for your continued feedback and support.
Post questions (or answer questions) on Stack Overflow
Join the community portal for advocates on K8sPort
Follow us on Twitter @Kubernetesio for latest updates
Chat with the community on Slack
Share your Kubernetes story
-->
&lt;p>å‚ä¸Žå…¶ä¸­&lt;/p>
&lt;p>å‚ä¸Ž Kubernetes çš„æœ€ç®€å•æ–¹æ³•æ˜¯åŠ å…¥ä¼—å¤š&lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">ç‰¹åˆ«å…´è¶£å°ç»„&lt;/a> (SIGï¼‰ä¸­ä¸Žæ‚¨çš„å…´è¶£ä¸€è‡´çš„å°ç»„ã€‚ ä½ æœ‰ä»€ä¹ˆæƒ³è¦å‘ Kubernetes ç¤¾åŒºå¹¿æ’­çš„å—ï¼Ÿ åœ¨æˆ‘ä»¬çš„æ¯å‘¨&lt;a href="https://github.com/kubernetes/community/blob/master/communication.md#weekly-meeting">ç¤¾åŒºä¼šè®®&lt;/a>æˆ–é€šè¿‡ä»¥ä¸‹æ¸ é“åˆ†äº«æ‚¨çš„å£°éŸ³ã€‚&lt;/p>
&lt;p>æ„Ÿè°¢æ‚¨çš„æŒç»­åé¦ˆå’Œæ”¯æŒã€‚
åœ¨&lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>ä¸Šå‘å¸ƒé—®é¢˜ï¼ˆæˆ–å›žç­”é—®é¢˜ï¼‰&lt;/p>
&lt;p>åŠ å…¥&lt;a href="http://k8sport.org/">K8sPort&lt;/a>çš„å€¡å¯¼è€…ç¤¾åŒºé—¨æˆ·ç½‘ç«™&lt;/p>
&lt;p>åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ä»¬ &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a>èŽ·å–æœ€æ–°æ›´æ–°&lt;/p>
&lt;p>åœ¨&lt;a href="http://slack.k8s.io/">Slack&lt;/a>ä¸Šä¸Žç¤¾åŒºèŠå¤©&lt;/p>
&lt;p>åˆ†äº«æ‚¨çš„ Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">æ•…äº‹&lt;/a>&lt;/p></description></item><item><title>Blog: Airflowåœ¨Kubernetesä¸­çš„ä½¿ç”¨ï¼ˆç¬¬ä¸€éƒ¨åˆ†ï¼‰ï¼šä¸€ç§ä¸åŒçš„æ“ä½œå™¨</title><link>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</link><pubDate>Thu, 28 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/28/airflow%E5%9C%A8kubernetes%E4%B8%AD%E7%9A%84%E4%BD%BF%E7%94%A8%E7%AC%AC%E4%B8%80%E9%83%A8%E5%88%86%E4%B8%80%E7%A7%8D%E4%B8%8D%E5%90%8C%E7%9A%84%E6%93%8D%E4%BD%9C%E5%99%A8/</guid><description>
&lt;!--
Author: Daniel Imberman (Bloomberg LP)
-->
&lt;p>ä½œè€…: Daniel Imberman (Bloomberg LP)&lt;/p>
&lt;!--
## Introduction
As part of Bloomberg's continued commitment to developing the Kubernetes ecosystem, we are excited to announce the Kubernetes Airflow Operator; a mechanism for Apache Airflow, a popular workflow orchestration framework to natively launch arbitrary Kubernetes Pods using the Kubernetes API.
-->
&lt;h2 id="ä»‹ç»">ä»‹ç»&lt;/h2>
&lt;p>ä½œä¸ºBloomberg [ç»§ç»­è‡´åŠ›äºŽå¼€å‘Kubernetesç”Ÿæ€ç³»ç»Ÿ]çš„ä¸€éƒ¨åˆ†ï¼ˆhttps://www.techatbloomberg.com/blog/bloomberg-awarded-first-cncf-end-user-award-contributions-kubernetes/ï¼‰ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´èƒ½å¤Ÿå®£å¸ƒKubernetes Airflow Operatorçš„å‘å¸ƒ; &lt;a href="https://airflow.apache.org/">Apache Airflow&lt;/a>çš„æœºåˆ¶ï¼Œä¸€ç§æµè¡Œçš„å·¥ä½œæµç¨‹ç¼–æŽ’æ¡†æž¶ï¼Œä½¿ç”¨Kubernetes APIå¯ä»¥åœ¨æœ¬æœºå¯åŠ¨ä»»æ„çš„Kubernetes Podã€‚&lt;/p>
&lt;!--
## What Is Airflow?
Apache Airflow is one realization of the DevOps philosophy of "Configuration As Code." Airflow allows users to launch multi-step pipelines using a simple Python object DAG (Directed Acyclic Graph). You can define dependencies, programmatically construct complex workflows, and monitor scheduled jobs in an easy to read UI.
-->
&lt;h2 id="ä»€ä¹ˆæ˜¯airflow">ä»€ä¹ˆæ˜¯Airflow?&lt;/h2>
&lt;p>Apache Airflowæ˜¯DevOpsâ€œConfiguration As Codeâ€ç†å¿µçš„ä¸€ç§å®žçŽ°ã€‚ Airflowå…è®¸ç”¨æˆ·ä½¿ç”¨ç®€å•çš„Pythonå¯¹è±¡DAGï¼ˆæœ‰å‘æ— çŽ¯å›¾ï¼‰å¯åŠ¨å¤šæ­¥éª¤æµæ°´çº¿ã€‚ æ‚¨å¯ä»¥åœ¨æ˜“äºŽé˜…è¯»çš„UIä¸­å®šä¹‰ä¾èµ–å…³ç³»ï¼Œä»¥ç¼–ç¨‹æ–¹å¼æž„å»ºå¤æ‚çš„å·¥ä½œæµï¼Œå¹¶ç›‘è§†è°ƒåº¦çš„ä½œä¸šã€‚&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow_dags.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow DAGsâ€/&amp;gt;&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow UIâ€/&amp;gt;&lt;/p>
&lt;!--
## Why Airflow on Kubernetes?
Since its inception, Airflow's greatest strength has been its flexibility. Airflow offers a wide range of integrations for services ranging from Spark and HBase, to services on various cloud providers. Airflow also offers easy extensibility through its plug-in framework. However, one limitation of the project is that Airflow users are confined to the frameworks and clients that exist on the Airflow worker at the moment of execution. A single organization can have varied Airflow workflows ranging from data science pipelines to application deployments. This difference in use-case creates issues in dependency management as both teams might use vastly different libraries for their workflows.
To address this issue, we've utilized Kubernetes to allow users to launch arbitrary Kubernetes pods and configurations. Airflow users can now have full power over their run-time environments, resources, and secrets, basically turning Airflow into an "any job you want" workflow orchestrator.
-->
&lt;h2 id="ä¸ºä»€ä¹ˆåœ¨kubernetesä¸Šä½¿ç”¨airflow">ä¸ºä»€ä¹ˆåœ¨Kubernetesä¸Šä½¿ç”¨Airflow?&lt;/h2>
&lt;p>è‡ªæˆç«‹ä»¥æ¥ï¼ŒAirflowçš„æœ€å¤§ä¼˜åŠ¿åœ¨äºŽå…¶çµæ´»æ€§ã€‚ Airflowæä¾›å¹¿æ³›çš„æœåŠ¡é›†æˆï¼ŒåŒ…æ‹¬Sparkå’ŒHBaseï¼Œä»¥åŠå„ç§äº‘æä¾›å•†çš„æœåŠ¡ã€‚ Airflowè¿˜é€šè¿‡å…¶æ’ä»¶æ¡†æž¶æä¾›è½»æ¾çš„å¯æ‰©å±•æ€§ã€‚ä½†æ˜¯ï¼Œè¯¥é¡¹ç›®çš„ä¸€ä¸ªé™åˆ¶æ˜¯Airflowç”¨æˆ·ä»…é™äºŽæ‰§è¡Œæ—¶Airflowç«™ç‚¹ä¸Šå­˜åœ¨çš„æ¡†æž¶å’Œå®¢æˆ·ç«¯ã€‚å•ä¸ªç»„ç»‡å¯ä»¥æ‹¥æœ‰å„ç§Airflowå·¥ä½œæµç¨‹ï¼ŒèŒƒå›´ä»Žæ•°æ®ç§‘å­¦æµåˆ°åº”ç”¨ç¨‹åºéƒ¨ç½²ã€‚ç”¨ä¾‹ä¸­çš„è¿™ç§å·®å¼‚ä¼šåœ¨ä¾èµ–å…³ç³»ç®¡ç†ä¸­äº§ç”Ÿé—®é¢˜ï¼Œå› ä¸ºä¸¤ä¸ªå›¢é˜Ÿå¯èƒ½ä¼šåœ¨å…¶å·¥ä½œæµç¨‹ä½¿ç”¨æˆªç„¶ä¸åŒçš„åº“ã€‚&lt;/p>
&lt;p>ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬ä½¿Kuberneteså…è®¸ç”¨æˆ·å¯åŠ¨ä»»æ„Kubernetes podå’Œé…ç½®ã€‚ Airflowç”¨æˆ·çŽ°åœ¨å¯ä»¥åœ¨å…¶è¿è¡Œæ—¶çŽ¯å¢ƒï¼Œèµ„æºå’Œæœºå¯†ä¸Šæ‹¥æœ‰å…¨éƒ¨æƒé™ï¼ŒåŸºæœ¬ä¸Šå°†Airflowè½¬å˜ä¸ºâ€œæ‚¨æƒ³è¦çš„ä»»ä½•å·¥ä½œâ€å·¥ä½œæµç¨‹åè°ƒå™¨ã€‚&lt;/p>
&lt;!--
## The Kubernetes Operator
Before we move any further, we should clarify that an Operator in Airflow is a task definition. When a user creates a DAG, they would use an operator like the "SparkSubmitOperator" or the "PythonOperator" to submit/monitor a Spark job or a Python function respectively. Airflow comes with built-in operators for frameworks like Apache Spark, BigQuery, Hive, and EMR. It also offers a Plugins entrypoint that allows DevOps engineers to develop their own connectors.
Airflow users are always looking for ways to make deployments and ETL pipelines simpler to manage. Any opportunity to decouple pipeline steps, while increasing monitoring, can reduce future outages and fire-fights. The following is a list of benefits provided by the Airflow Kubernetes Operator:
-->
&lt;h2 id="kubernetesè¿è¥å•†">Kubernetesè¿è¥å•†&lt;/h2>
&lt;p>åœ¨è¿›ä¸€æ­¥è®¨è®ºä¹‹å‰ï¼Œæˆ‘ä»¬åº”è¯¥æ¾„æ¸…Airflowä¸­çš„&lt;a href="https://airflow.apache.org/concepts.html#operators">Operator&lt;/a>æ˜¯ä¸€ä¸ªä»»åŠ¡å®šä¹‰ã€‚ å½“ç”¨æˆ·åˆ›å»ºDAGæ—¶ï¼Œä»–ä»¬å°†ä½¿ç”¨åƒâ€œSparkSubmitOperatorâ€æˆ–â€œPythonOperatorâ€è¿™æ ·çš„operatoråˆ†åˆ«æäº¤/ç›‘è§†Sparkä½œä¸šæˆ–Pythonå‡½æ•°ã€‚ Airflowé™„å¸¦äº†Apache Sparkï¼ŒBigQueryï¼ŒHiveå’ŒEMRç­‰æ¡†æž¶çš„å†…ç½®è¿ç®—ç¬¦ã€‚ å®ƒè¿˜æä¾›äº†ä¸€ä¸ªæ’ä»¶å…¥å£ç‚¹ï¼Œå…è®¸DevOpså·¥ç¨‹å¸ˆå¼€å‘è‡ªå·±çš„è¿žæŽ¥å™¨ã€‚&lt;/p>
&lt;p>Airflowç”¨æˆ·ä¸€ç›´åœ¨å¯»æ‰¾æ›´æ˜“äºŽç®¡ç†éƒ¨ç½²å’ŒETLæµçš„æ–¹æ³•ã€‚ åœ¨å¢žåŠ ç›‘æŽ§çš„åŒæ—¶ï¼Œä»»ä½•è§£è€¦æµç¨‹çš„æœºä¼šéƒ½å¯ä»¥å‡å°‘æœªæ¥çš„åœæœºç­‰é—®é¢˜ã€‚ ä»¥ä¸‹æ˜¯Airflow Kubernetes Operatoræä¾›çš„å¥½å¤„ï¼š&lt;/p>
&lt;!--
* Increased flexibility for deployments:
Airflow's plugin API has always offered a significant boon to engineers wishing to test new functionalities within their DAGs. On the downside, whenever a developer wanted to create a new operator, they had to develop an entirely new plugin. Now, any task that can be run within a Docker container is accessible through the exact same operator, with no extra Airflow code to maintain.
-->
&lt;ul>
&lt;li>æé«˜éƒ¨ç½²çµæ´»æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>Airflowçš„æ’ä»¶APIä¸€ç›´ä¸ºå¸Œæœ›åœ¨å…¶DAGä¸­æµ‹è¯•æ–°åŠŸèƒ½çš„å·¥ç¨‹å¸ˆæä¾›äº†é‡è¦çš„ç¦åˆ©ã€‚ ä¸åˆ©çš„ä¸€é¢æ˜¯ï¼Œæ¯å½“å¼€å‘äººå‘˜æƒ³è¦åˆ›å»ºä¸€ä¸ªæ–°çš„operatoræ—¶ï¼Œä»–ä»¬å°±å¿…é¡»å¼€å‘ä¸€ä¸ªå…¨æ–°çš„æ’ä»¶ã€‚ çŽ°åœ¨ï¼Œä»»ä½•å¯ä»¥åœ¨Dockerå®¹å™¨ä¸­è¿è¡Œçš„ä»»åŠ¡éƒ½å¯ä»¥é€šè¿‡å®Œå…¨ç›¸åŒçš„è¿ç®—ç¬¦è®¿é—®ï¼Œè€Œæ— éœ€ç»´æŠ¤é¢å¤–çš„Airflowä»£ç ã€‚&lt;/p>
&lt;!--
* Flexibility of configurations and dependencies:
For operators that are run within static Airflow workers, dependency management can become quite difficult. If a developer wants to run one task that requires SciPy and another that requires NumPy, the developer would have to either maintain both dependencies within all Airflow workers or offload the task to an external machine (which can cause bugs if that external machine changes in an untracked manner). Custom Docker images allow users to ensure that the tasks environment, configuration, and dependencies are completely idempotent.
-->
&lt;ul>
&lt;li>é…ç½®å’Œä¾èµ–çš„çµæ´»æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>å¯¹äºŽåœ¨é™æ€Airflowå·¥ä½œç¨‹åºä¸­è¿è¡Œçš„operatorï¼Œä¾èµ–å…³ç³»ç®¡ç†å¯èƒ½å˜å¾—éžå¸¸å›°éš¾ã€‚ å¦‚æžœå¼€å‘äººå‘˜æƒ³è¦è¿è¡Œä¸€ä¸ªéœ€è¦&lt;a href="https://www.scipy.org">SciPy&lt;/a>çš„ä»»åŠ¡å’Œå¦ä¸€ä¸ªéœ€è¦&lt;a href="http://www.numpy.org">NumPy&lt;/a>çš„ä»»åŠ¡ï¼Œå¼€å‘äººå‘˜å¿…é¡»ç»´æŠ¤æ‰€æœ‰AirflowèŠ‚ç‚¹ä¸­çš„ä¾èµ–å…³ç³»æˆ–å°†ä»»åŠ¡å¸è½½åˆ°å…¶ä»–è®¡ç®—æœºï¼ˆå¦‚æžœå¤–éƒ¨è®¡ç®—æœºä»¥æœªè·Ÿè¸ªçš„æ–¹å¼æ›´æ”¹ï¼Œåˆ™å¯èƒ½å¯¼è‡´é”™è¯¯ï¼‰ã€‚ è‡ªå®šä¹‰Dockeré•œåƒå…è®¸ç”¨æˆ·ç¡®ä¿ä»»åŠ¡çŽ¯å¢ƒï¼Œé…ç½®å’Œä¾èµ–å…³ç³»å®Œå…¨æ˜¯å¹‚ç­‰çš„ã€‚&lt;/p>
&lt;!--
* Usage of kubernetes secrets for added security:
Handling sensitive data is a core responsibility of any DevOps engineer. At every opportunity, Airflow users want to isolate any API keys, database passwords, and login credentials on a strict need-to-know basis. With the Kubernetes operator, users can utilize the Kubernetes Vault technology to store all sensitive data. This means that the Airflow workers will never have access to this information, and can simply request that pods be built with only the secrets they need.
-->
&lt;ul>
&lt;li>ä½¿ç”¨kubernetes Secretä»¥å¢žåŠ å®‰å…¨æ€§ï¼š&lt;/li>
&lt;/ul>
&lt;p>å¤„ç†æ•æ„Ÿæ•°æ®æ˜¯ä»»ä½•å¼€å‘å·¥ç¨‹å¸ˆçš„æ ¸å¿ƒèŒè´£ã€‚ Airflowç”¨æˆ·æ€»æœ‰æœºä¼šåœ¨ä¸¥æ ¼æ¡æ¬¾çš„åŸºç¡€ä¸Šéš”ç¦»ä»»ä½•APIå¯†é’¥ï¼Œæ•°æ®åº“å¯†ç å’Œç™»å½•å‡­æ®ã€‚ ä½¿ç”¨Kubernetesè¿ç®—ç¬¦ï¼Œç”¨æˆ·å¯ä»¥åˆ©ç”¨Kubernetes VaultæŠ€æœ¯å­˜å‚¨æ‰€æœ‰æ•æ„Ÿæ•°æ®ã€‚ è¿™æ„å‘³ç€Airflowå·¥ä½œäººå‘˜å°†æ°¸è¿œæ— æ³•è®¿é—®æ­¤ä¿¡æ¯ï¼Œå¹¶ä¸”å¯ä»¥å®¹æ˜“åœ°è¯·æ±‚ä»…ä½¿ç”¨ä»–ä»¬éœ€è¦çš„å¯†ç ä¿¡æ¯æž„å»ºpodã€‚&lt;/p>
&lt;!--
# Architecture
The Kubernetes Operator uses the Kubernetes Python Client to generate a request that is processed by the APIServer (1). Kubernetes will then launch your pod with whatever specs you've defined (2). Images will be loaded with all the necessary environment variables, secrets and dependencies, enacting a single command. Once the job is launched, the operator only needs to monitor the health of track logs (3). Users will have the choice of gathering logs locally to the scheduler or to any distributed logging service currently in their Kubernetes cluster.
-->
&lt;p>ï¼ƒæž¶æž„&lt;/p>
&lt;p>&amp;lt;img src =â€œ/ images / blog / 2018-05-25-Airflow-Kubernetes-Operator / 2018-05-25-airflow-architecture.pngâ€width =â€œ85ï¼…â€alt =â€œAirflow Architectureâ€/&amp;gt;&lt;/p>
&lt;p>Kubernetes Operatorä½¿ç”¨&lt;a href="https://github.com/kubernetes-client/Python">Kubernetes Pythonå®¢æˆ·ç«¯&lt;/a>ç”Ÿæˆç”±APIServerå¤„ç†çš„è¯·æ±‚ï¼ˆ1ï¼‰ã€‚ ç„¶åŽï¼ŒKuberneteså°†ä½¿ç”¨æ‚¨å®šä¹‰çš„éœ€æ±‚å¯åŠ¨æ‚¨çš„podï¼ˆ2ï¼‰ã€‚æ˜ åƒæ–‡ä»¶ä¸­å°†åŠ è½½çŽ¯å¢ƒå˜é‡ï¼ŒSecretå’Œä¾èµ–é¡¹ï¼Œæ‰§è¡Œå•ä¸ªå‘½ä»¤ã€‚ ä¸€æ—¦å¯åŠ¨ä½œä¸šï¼Œoperatoråªéœ€è¦ç›‘è§†è·Ÿè¸ªæ—¥å¿—çš„çŠ¶å†µï¼ˆ3ï¼‰ã€‚ ç”¨æˆ·å¯ä»¥é€‰æ‹©å°†æ—¥å¿—æœ¬åœ°æ”¶é›†åˆ°è°ƒåº¦ç¨‹åºæˆ–å½“å‰ä½äºŽå…¶Kubernetesé›†ç¾¤ä¸­çš„ä»»ä½•åˆ†å¸ƒå¼æ—¥å¿—è®°å½•æœåŠ¡ã€‚&lt;/p>
&lt;!--
# Using the Kubernetes Operator
## A Basic Example
The following DAG is probably the simplest example we could write to show how the Kubernetes Operator works. This DAG creates two pods on Kubernetes: a Linux distro with Python and a base Ubuntu distro without it. The Python pod will run the Python request correctly, while the one without Python will report a failure to the user. If the Operator is working correctly, the passing-task pod should complete, while the failing-task pod returns a failure to the Airflow webserver.
-->
&lt;p>ï¼ƒä½¿ç”¨Kubernetes Operator&lt;/p>
&lt;p>##ä¸€ä¸ªåŸºæœ¬çš„ä¾‹å­&lt;/p>
&lt;p>ä»¥ä¸‹DAGå¯èƒ½æ˜¯æˆ‘ä»¬å¯ä»¥ç¼–å†™çš„æœ€ç®€å•çš„ç¤ºä¾‹ï¼Œä»¥æ˜¾ç¤ºKubernetes Operatorçš„å·¥ä½œåŽŸç†ã€‚ è¿™ä¸ªDAGåœ¨Kubernetesä¸Šåˆ›å»ºäº†ä¸¤ä¸ªpodï¼šä¸€ä¸ªå¸¦æœ‰Pythonçš„Linuxå‘è¡Œç‰ˆå’Œä¸€ä¸ªæ²¡æœ‰å®ƒçš„åŸºæœ¬Ubuntuå‘è¡Œç‰ˆã€‚ Python podå°†æ­£ç¡®è¿è¡ŒPythonè¯·æ±‚ï¼Œè€Œæ²¡æœ‰Pythonçš„é‚£ä¸ªå°†å‘ç”¨æˆ·æŠ¥å‘Šå¤±è´¥ã€‚ å¦‚æžœOperatoræ­£å¸¸å·¥ä½œï¼Œåˆ™åº”è¯¥å®Œæˆâ€œpassing-taskâ€podï¼Œè€Œâ€œfalling-taskâ€podåˆ™å‘Airflowç½‘ç»œæœåŠ¡å™¨è¿”å›žå¤±è´¥ã€‚&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DAG
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">datetime&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> datetime, timedelta
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.contrib.operators.kubernetes_pod_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> KubernetesPodOperator
&lt;span style="color:#a2f;font-weight:bold">from&lt;/span> &lt;span style="color:#00f;font-weight:bold">airflow.operators.dummy_operator&lt;/span> &lt;span style="color:#a2f;font-weight:bold">import&lt;/span> DummyOperator
default_args &lt;span style="color:#666">=&lt;/span> {
&lt;span style="color:#b44">&amp;#39;owner&amp;#39;&lt;/span>: &lt;span style="color:#b44">&amp;#39;airflow&amp;#39;&lt;/span>,
&lt;span style="color:#b44">&amp;#39;depends_on_past&amp;#39;&lt;/span>: False,
&lt;span style="color:#b44">&amp;#39;start_date&amp;#39;&lt;/span>: datetime&lt;span style="color:#666">.&lt;/span>utcnow(),
&lt;span style="color:#b44">&amp;#39;email&amp;#39;&lt;/span>: [&lt;span style="color:#b44">&amp;#39;airflow@example.com&amp;#39;&lt;/span>],
&lt;span style="color:#b44">&amp;#39;email_on_failure&amp;#39;&lt;/span>: False,
&lt;span style="color:#b44">&amp;#39;email_on_retry&amp;#39;&lt;/span>: False,
&lt;span style="color:#b44">&amp;#39;retries&amp;#39;&lt;/span>: &lt;span style="color:#666">1&lt;/span>,
&lt;span style="color:#b44">&amp;#39;retry_delay&amp;#39;&lt;/span>: timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>)
}
dag &lt;span style="color:#666">=&lt;/span> DAG(
&lt;span style="color:#b44">&amp;#39;kubernetes_sample&amp;#39;&lt;/span>, default_args&lt;span style="color:#666">=&lt;/span>default_args, schedule_interval&lt;span style="color:#666">=&lt;/span>timedelta(minutes&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">10&lt;/span>))
start &lt;span style="color:#666">=&lt;/span> DummyOperator(task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;run_this_first&amp;#39;&lt;/span>, dag&lt;span style="color:#666">=&lt;/span>dag)
passing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;Python:3.6&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-test&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;passing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>True,
dag&lt;span style="color:#666">=&lt;/span>dag
)
failing &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;ubuntu:1604&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
labels&lt;span style="color:#666">=&lt;/span>{&lt;span style="color:#b44">&amp;#34;foo&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;bar&amp;#34;&lt;/span>},
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>True,
dag&lt;span style="color:#666">=&lt;/span>dag
)
passing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
failing&lt;span style="color:#666">.&lt;/span>set_upstream(start)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
## But how does this relate to my workflow?
While this example only uses basic images, the magic of Docker is that this same DAG will work for any image/command pairing you want. The following is a recommended CI/CD pipeline to run production-ready code on an Airflow DAG.
### 1: PR in github
Use Travis or Jenkins to run unit and integration tests, bribe your favorite team-mate into PR'ing your code, and merge to the master branch to trigger an automated CI build.
### 2: CI/CD via Jenkins -> Docker Image
Generate your Docker images and bump release version within your Jenkins build.
### 3: Airflow launches task
Finally, update your DAGs to reflect the new release version and you should be ready to go!
-->
&lt;p>##ä½†è¿™ä¸Žæˆ‘çš„å·¥ä½œæµç¨‹æœ‰ä»€ä¹ˆå…³ç³»ï¼Ÿ&lt;/p>
&lt;p>è™½ç„¶è¿™ä¸ªä¾‹å­åªä½¿ç”¨åŸºæœ¬æ˜ åƒï¼Œä½†Dockerçš„ç¥žå¥‡ä¹‹å¤„åœ¨äºŽï¼Œè¿™ä¸ªç›¸åŒçš„DAGå¯ä»¥ç”¨äºŽæ‚¨æƒ³è¦çš„ä»»ä½•å›¾åƒ/å‘½ä»¤é…å¯¹ã€‚ ä»¥ä¸‹æ˜¯æŽ¨èçš„CI / CDç®¡é“ï¼Œç”¨äºŽåœ¨Airflow DAGä¸Šè¿è¡Œç”Ÿäº§å°±ç»ªä»£ç ã€‚&lt;/p>
&lt;h3 id="1-githubä¸­çš„pr">1ï¼šgithubä¸­çš„PR&lt;/h3>
&lt;p>ä½¿ç”¨Travisæˆ–Jenkinsè¿è¡Œå•å…ƒå’Œé›†æˆæµ‹è¯•ï¼Œè¯·æ‚¨çš„æœ‹å‹PRæ‚¨çš„ä»£ç ï¼Œå¹¶åˆå¹¶åˆ°ä¸»åˆ†æ”¯ä»¥è§¦å‘è‡ªåŠ¨CIæž„å»ºã€‚&lt;/p>
&lt;h3 id="2-ci-cdæž„å»ºjenkins-docker-image">2ï¼šCI / CDæž„å»ºJenkins - &amp;gt; Docker Image&lt;/h3>
&lt;p>&lt;a href="https://getintodevops.com/blog/building-your-first-Docker-image-with-jenkins-2-guide-for-developers">åœ¨Jenkinsæž„å»ºä¸­ç”ŸæˆDockeré•œåƒå’Œç¼“å†²ç‰ˆæœ¬&lt;/a>ã€‚&lt;/p>
&lt;h3 id="3-airflowå¯åŠ¨ä»»åŠ¡">3ï¼šAirflowå¯åŠ¨ä»»åŠ¡&lt;/h3>
&lt;p>æœ€åŽï¼Œæ›´æ–°æ‚¨çš„DAGä»¥åæ˜ æ–°ç‰ˆæœ¬ï¼Œæ‚¨åº”è¯¥å‡†å¤‡å¥½äº†ï¼&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-Python" data-lang="Python">
production_task &lt;span style="color:#666">=&lt;/span> KubernetesPodOperator(namespace&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#39;default&amp;#39;&lt;/span>,
&lt;span style="color:#080;font-style:italic"># image=&amp;#34;my-production-job:release-1.0.1&amp;#34;, &amp;lt;-- old release&lt;/span>
image&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;my-production-job:release-1.0.2&amp;#34;&lt;/span>,
cmds&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;Python&amp;#34;&lt;/span>,&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>],
arguments&lt;span style="color:#666">=&lt;/span>[&lt;span style="color:#b44">&amp;#34;print(&amp;#39;hello world&amp;#39;)&amp;#34;&lt;/span>],
name&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;fail&amp;#34;&lt;/span>,
task_id&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;failing-task&amp;#34;&lt;/span>,
get_logs&lt;span style="color:#666">=&lt;/span>True,
dag&lt;span style="color:#666">=&lt;/span>dag
)
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
# Launching a test deployment
Since the Kubernetes Operator is not yet released, we haven't released an official helm chart or operator (however both are currently in progress). However, we are including instructions for a basic deployment below and are actively looking for foolhardy beta testers to try this new feature. To try this system out please follow these steps:
## Step 1: Set your kubeconfig to point to a kubernetes cluster
## Step 2: Clone the Airflow Repo:
Run git clone https://github.com/apache/incubator-airflow.git to clone the official Airflow repo.
## Step 3: Run
To run this basic deployment, we are co-opting the integration testing script that we currently use for the Kubernetes Executor (which will be explained in the next article of this series). To launch this deployment, run these three commands:
-->
&lt;p>ï¼ƒå¯åŠ¨æµ‹è¯•éƒ¨ç½²&lt;/p>
&lt;p>ç”±äºŽKubernetesè¿è¥å•†å°šæœªå‘å¸ƒï¼Œæˆ‘ä»¬å°šæœªå‘å¸ƒå®˜æ–¹&lt;a href="https://helm.sh/">helm&lt;/a> å›¾è¡¨æˆ–operatorï¼ˆä½†ä¸¤è€…ç›®å‰éƒ½åœ¨è¿›è¡Œä¸­ï¼‰ã€‚ ä½†æ˜¯ï¼Œæˆ‘ä»¬åœ¨ä¸‹é¢åˆ—å‡ºäº†åŸºæœ¬éƒ¨ç½²çš„è¯´æ˜Žï¼Œå¹¶ä¸”æ­£åœ¨ç§¯æžå¯»æ‰¾æµ‹è¯•äººå‘˜æ¥å°è¯•è¿™ä¸€æ–°åŠŸèƒ½ã€‚ è¦è¯•ç”¨æ­¤ç³»ç»Ÿï¼Œè¯·æŒ‰ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š&lt;/p>
&lt;p>##æ­¥éª¤1ï¼šå°†kubeconfigè®¾ç½®ä¸ºæŒ‡å‘kubernetesé›†ç¾¤&lt;/p>
&lt;p>##æ­¥éª¤2ï¼šclone Airflow ä»“åº“ï¼š&lt;/p>
&lt;p>è¿è¡Œgit clone httpsï¼š// github.com / apache / incubator-airflow.gitæ¥cloneå®˜æ–¹Airflowä»“åº“ã€‚&lt;/p>
&lt;p>##æ­¥éª¤3ï¼šè¿è¡Œ&lt;/p>
&lt;p>ä¸ºäº†è¿è¡Œè¿™ä¸ªåŸºæœ¬Deploymentï¼Œæˆ‘ä»¬æ­£åœ¨é€‰æ‹©æˆ‘ä»¬ç›®å‰ç”¨äºŽKubernetes Executorçš„é›†æˆæµ‹è¯•è„šæœ¬ï¼ˆå°†åœ¨æœ¬ç³»åˆ—çš„ä¸‹ä¸€ç¯‡æ–‡ç« ä¸­å¯¹æ­¤è¿›è¡Œè§£é‡Šï¼‰ã€‚ è¦å¯åŠ¨æ­¤éƒ¨ç½²ï¼Œè¯·è¿è¡Œä»¥ä¸‹ä¸‰ä¸ªå‘½ä»¤ï¼š&lt;/p>
&lt;pre>&lt;code>
sed -ie &amp;quot;s/KubernetesExecutor/LocalExecutor/g&amp;quot; scripts/ci/kubernetes/kube/configmaps.yaml
./scripts/ci/kubernetes/Docker/build.sh
./scripts/ci/kubernetes/kube/deploy.sh
&lt;/code>&lt;/pre>&lt;!--
Before we move on, let's discuss what these commands are doing:
### sed -ie "s/KubernetesExecutor/LocalExecutor/g" scripts/ci/kubernetes/kube/configmaps.yaml
The Kubernetes Executor is another Airflow feature that allows for dynamic allocation of tasks as idempotent pods. The reason we are switching this to the LocalExecutor is simply to introduce one feature at a time. You are more then welcome to skip this step if you would like to try the Kubernetes Executor, however we will go into more detail in a future article.
### ./scripts/ci/kubernetes/Docker/build.sh
This script will tar the Airflow master source code build a Docker container based on the Airflow distribution
### ./scripts/ci/kubernetes/kube/deploy.sh
Finally, we create a full Airflow deployment on your cluster. This includes Airflow configs, a postgres backend, the webserver + scheduler, and all necessary services between. One thing to note is that the role binding supplied is a cluster-admin, so if you do not have that level of permission on the cluster, you can modify this at scripts/ci/kubernetes/kube/airflow.yaml
## Step 4: Log into your webserver
Now that your Airflow instance is running let's take a look at the UI! The UI lives in port 8080 of the Airflow pod, so simply run
-->
&lt;p>åœ¨æˆ‘ä»¬ç»§ç»­ä¹‹å‰ï¼Œè®©æˆ‘ä»¬è®¨è®ºè¿™äº›å‘½ä»¤æ­£åœ¨åšä»€ä¹ˆï¼š&lt;/p>
&lt;h3 id="sed-ie-s-kubernetesexecutor-localexecutor-g-scripts-ci-kubernetes-kube-configmaps-yaml">sed -ieâ€œs / KubernetesExecutor / LocalExecutor / gâ€scripts / ci / kubernetes / kube / configmaps.yaml&lt;/h3>
&lt;p>Kubernetes Executoræ˜¯å¦ä¸€ç§AirflowåŠŸèƒ½ï¼Œå…è®¸åŠ¨æ€åˆ†é…ä»»åŠ¡å·²è§£å†³å¹‚ç­‰podçš„é—®é¢˜ã€‚æˆ‘ä»¬å°†å…¶åˆ‡æ¢åˆ°LocalExecutorçš„åŽŸå› åªæ˜¯ä¸€æ¬¡å¼•å…¥ä¸€ä¸ªåŠŸèƒ½ã€‚å¦‚æžœæ‚¨æƒ³å°è¯•Kubernetes Executorï¼Œæ¬¢è¿Žæ‚¨è·³è¿‡æ­¤æ­¥éª¤ï¼Œä½†æˆ‘ä»¬å°†åœ¨ä»¥åŽçš„æ–‡ç« ä¸­è¯¦ç»†ä»‹ç»ã€‚&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-docker-build-sh">./scripts/ci/kubernetes/Docker/build.sh&lt;/h3>
&lt;p>æ­¤è„šæœ¬å°†å¯¹Airflowä¸»åˆ†æ”¯ä»£ç è¿›è¡Œæ‰“åŒ…ï¼Œä»¥æ ¹æ®Airflowçš„å‘è¡Œæ–‡ä»¶æž„å»ºDockerå®¹å™¨&lt;/p>
&lt;h3 id="scripts-ci-kubernetes-kube-deploy-sh">./scripts/ci/kubernetes/kube/deploy.sh&lt;/h3>
&lt;p>æœ€åŽï¼Œæˆ‘ä»¬åœ¨æ‚¨çš„ç¾¤é›†ä¸Šåˆ›å»ºå®Œæ•´çš„Airflowéƒ¨ç½²ã€‚è¿™åŒ…æ‹¬Airflowé…ç½®ï¼ŒpostgresåŽç«¯ï¼Œwebserver +è°ƒåº¦ç¨‹åºä»¥åŠä¹‹é—´çš„æ‰€æœ‰å¿…è¦æœåŠ¡ã€‚éœ€è¦æ³¨æ„çš„ä¸€ç‚¹æ˜¯ï¼Œæä¾›çš„è§’è‰²ç»‘å®šæ˜¯é›†ç¾¤ç®¡ç†å‘˜ï¼Œå› æ­¤å¦‚æžœæ‚¨æ²¡æœ‰è¯¥é›†ç¾¤çš„æƒé™çº§åˆ«ï¼Œå¯ä»¥åœ¨scripts / ci / kubernetes / kube / airflow.yamlä¸­è¿›è¡Œä¿®æ”¹ã€‚&lt;/p>
&lt;p>##æ­¥éª¤4ï¼šç™»å½•æ‚¨çš„ç½‘ç»œæœåŠ¡å™¨&lt;/p>
&lt;p>çŽ°åœ¨æ‚¨çš„Airflowå®žä¾‹æ­£åœ¨è¿è¡Œï¼Œè®©æˆ‘ä»¬æ¥çœ‹çœ‹UIï¼ç”¨æˆ·ç•Œé¢ä½äºŽAirflow podçš„8080ç«¯å£ï¼Œå› æ­¤åªéœ€è¿è¡Œå³å¯&lt;/p>
&lt;pre>&lt;code>
WEB=$(kubectl get pods -o go-template --template '{{range .items}}{{.metadata.name}}{{&amp;quot;\n&amp;quot;}}{{end}}' | grep &amp;quot;airflow&amp;quot; | head -1)
kubectl port-forward $WEB 8080:8080
&lt;/code>&lt;/pre>&lt;!--
Now the Airflow UI will exist on http://localhost:8080. To log in simply enter airflow/airflow and you should have full access to the Airflow web UI.
## Step 5: Upload a test document
To modify/add your own DAGs, you can use kubectl cp to upload local files into the DAG folder of the Airflow scheduler. Airflow will then read the new DAG and automatically upload it to its system. The following command will upload any local file into the correct directory:
-->
&lt;p>çŽ°åœ¨ï¼ŒAirflow UIå°†å­˜åœ¨äºŽhttp://localhost:8080ä¸Šã€‚ è¦ç™»å½•ï¼Œåªéœ€è¾“å…¥airflow /airflowï¼Œæ‚¨å°±å¯ä»¥å®Œå…¨è®¿é—®Airflow Web UIã€‚&lt;/p>
&lt;p>##æ­¥éª¤5ï¼šä¸Šä¼ æµ‹è¯•æ–‡æ¡£&lt;/p>
&lt;p>è¦ä¿®æ”¹/æ·»åŠ è‡ªå·±çš„DAGï¼Œå¯ä»¥ä½¿ç”¨kubectl cpå°†æœ¬åœ°æ–‡ä»¶ä¸Šä¼ åˆ°Airflowè°ƒåº¦ç¨‹åºçš„DAGæ–‡ä»¶å¤¹ä¸­ã€‚ ç„¶åŽï¼ŒAirflowå°†è¯»å–æ–°çš„DAGå¹¶è‡ªåŠ¨å°†å…¶ä¸Šä¼ åˆ°å…¶ç³»ç»Ÿã€‚ ä»¥ä¸‹å‘½ä»¤å°†ä»»ä½•æœ¬åœ°æ–‡ä»¶ä¸Šè½½åˆ°æ­£ç¡®çš„ç›®å½•ä¸­ï¼š&lt;/p>
&lt;p>kubectl cp &lt;local file> &lt;namespace>/&lt;pod>:/root/airflow/dags -c scheduler&lt;/p>
&lt;!--
## Step 6: Enjoy!
# So when will I be able to use this?
While this feature is still in the early stages, we hope to see it released for wide release in the next few months.
# Get Involved
This feature is just the beginning of multiple major efforts to improves Apache Airflow integration into Kubernetes. The Kubernetes Operator has been merged into the 1.10 release branch of Airflow (the executor in experimental mode), along with a fully k8s native scheduler called the Kubernetes Executor (article to come). These features are still in a stage where early adopters/contributors can have a huge influence on the future of these features.
For those interested in joining these efforts, I'd recommend checkint out these steps:
* Join the airflow-dev mailing list at dev@airflow.apache.org.
* File an issue in Apache Airflow JIRA
* Join our SIG-BigData meetings on Wednesdays at 10am PST.
* Reach us on slack at #sig-big-data on kubernetes.slack.com
Special thanks to the Apache Airflow and Kubernetes communities, particularly Grant Nicholas, Ben Goldberg, Anirudh Ramanathan, Fokko Dreisprong, and Bolke de Bruin, for your awesome help on these features as well as our future efforts.
-->
&lt;p>##æ­¥éª¤6ï¼šä½¿ç”¨å®ƒï¼&lt;/p>
&lt;p>#é‚£ä¹ˆæˆ‘ä»€ä¹ˆæ—¶å€™å¯ä»¥ä½¿ç”¨å®ƒï¼Ÿ&lt;/p>
&lt;p>è™½ç„¶æ­¤åŠŸèƒ½ä»å¤„äºŽæ—©æœŸé˜¶æ®µï¼Œä½†æˆ‘ä»¬å¸Œæœ›åœ¨æœªæ¥å‡ ä¸ªæœˆå†…å‘å¸ƒè¯¥åŠŸèƒ½ä»¥è¿›è¡Œå¹¿æ³›å‘å¸ƒã€‚&lt;/p>
&lt;p>#å‚ä¸Žå…¶ä¸­&lt;/p>
&lt;p>æ­¤åŠŸèƒ½åªæ˜¯å°†Apache Airflowé›†æˆåˆ°Kubernetesä¸­çš„å¤šé¡¹ä¸»è¦å·¥ä½œçš„å¼€å§‹ã€‚ Kubernetes Operatorå·²åˆå¹¶åˆ°&lt;a href="https://github.com/apache/incubator-airflow/tree/v1-10-test">Airflowçš„1.10å‘å¸ƒåˆ†æ”¯&lt;/a>ï¼ˆå®žéªŒæ¨¡å¼ä¸­çš„æ‰§è¡Œæ¨¡å—ï¼‰ï¼Œä»¥åŠå®Œæ•´çš„k8sæœ¬åœ°è°ƒåº¦ç¨‹åºç§°ä¸ºKubernetes Executorï¼ˆå³å°†å‘å¸ƒæ–‡ç« ï¼‰ã€‚è¿™äº›åŠŸèƒ½ä»å¤„äºŽæ—©æœŸé‡‡ç”¨è€…/è´¡çŒ®è€…å¯èƒ½å¯¹è¿™äº›åŠŸèƒ½çš„æœªæ¥äº§ç”Ÿå·¨å¤§å½±å“çš„é˜¶æ®µã€‚&lt;/p>
&lt;p>å¯¹äºŽæœ‰å…´è¶£åŠ å…¥è¿™äº›å·¥ä½œçš„äººï¼Œæˆ‘å»ºè®®æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š&lt;/p>
&lt;p>*åŠ å…¥airflow-devé‚®ä»¶åˆ—è¡¨dev@airflow.apache.orgã€‚&lt;/p>
&lt;p>*åœ¨[Apache Airflow JIRA]ä¸­æå‡ºé—®é¢˜ï¼ˆhttps://issues.apache.org/jira/projects/AIRFLOW/issues/ï¼‰&lt;/p>
&lt;p>*å‘¨ä¸‰ä¸Šåˆ10ç‚¹å¤ªå¹³æ´‹æ ‡å‡†æ—¶é—´åŠ å…¥æˆ‘ä»¬çš„SIG-BigDataä¼šè®®ã€‚&lt;/p>
&lt;p>*åœ¨kubernetes.slack.comä¸Šçš„ï¼ƒsig-big-dataæ‰¾åˆ°æˆ‘ä»¬ã€‚&lt;/p>
&lt;p>ç‰¹åˆ«æ„Ÿè°¢Apache Airflowå’ŒKubernetesç¤¾åŒºï¼Œç‰¹åˆ«æ˜¯Grant Nicholasï¼ŒBen Goldbergï¼ŒAnirudh Ramanathanï¼ŒFokko Dreisprongå’ŒBolke de Bruinï¼Œæ„Ÿè°¢æ‚¨å¯¹è¿™äº›åŠŸèƒ½çš„å·¨å¤§å¸®åŠ©ä»¥åŠæˆ‘ä»¬æœªæ¥çš„åŠªåŠ›ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes å†…çš„åŠ¨æ€ Ingress</title><link>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</link><pubDate>Thu, 07 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/07/dynamic-ingress-kubernetes/</guid><description>
&lt;!--
title: Dynamic Ingress in Kubernetes
date: 2018-06-07
Author: Richard Li (Datawire)
-->
&lt;p>ä½œè€…: Richard Li (Datawire)&lt;/p>
&lt;!--
Kubernetes makes it easy to deploy applications that consist of many microservices, but one of the key challenges with this type of architecture is dynamically routing ingress traffic to each of these services. One approach is Ambassador, a Kubernetes-native open source API Gateway built on the Envoy Proxy. Ambassador is designed for dynamic environment where services may come and go frequently.
Ambassador is configured using Kubernetes annotations. Annotations are used to configure specific mappings from a given Kubernetes service to a particular URL. A mapping can include a number of annotations for configuring a route. Examples include rate limiting, protocol, cross-origin request sharing, traffic shadowing, and routing rules.
-->
&lt;p>Kubernetes å¯ä»¥è½»æ¾éƒ¨ç½²ç”±è®¸å¤šå¾®æœåŠ¡ç»„æˆçš„åº”ç”¨ç¨‹åºï¼Œä½†è¿™ç§æž¶æž„çš„å…³é”®æŒ‘æˆ˜ä¹‹ä¸€æ˜¯åŠ¨æ€åœ°å°†æµé‡è·¯ç”±åˆ°è¿™äº›æœåŠ¡ä¸­çš„æ¯ä¸€ä¸ªã€‚
ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨ &lt;a href="https://www.getambassador.io">Ambassador&lt;/a>ï¼Œ
ä¸€ä¸ªåŸºäºŽ &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a> æž„å»ºçš„ Kubernetes åŽŸç”Ÿå¼€æº API ç½‘å…³ã€‚
Ambassador ä¸“ä¸ºåŠ¨æ€çŽ¯å¢ƒè€Œè®¾è®¡ï¼Œè¿™ç±»çŽ¯å¢ƒä¸­çš„æœåŠ¡å¯èƒ½è¢«é¢‘ç¹æ·»åŠ æˆ–åˆ é™¤ã€‚&lt;/p>
&lt;p>Ambassador ä½¿ç”¨ Kubernetes æ³¨è§£è¿›è¡Œé…ç½®ã€‚
æ³¨è§£ç”¨äºŽé…ç½®ä»Žç»™å®š Kubernetes æœåŠ¡åˆ°ç‰¹å®š URL çš„å…·ä½“æ˜ å°„å…³ç³»ã€‚
æ¯ä¸ªæ˜ å°„ä¸­å¯ä»¥åŒ…æ‹¬å¤šä¸ªæ³¨è§£ï¼Œç”¨äºŽé…ç½®è·¯ç”±ã€‚
æ³¨è§£çš„ä¾‹å­æœ‰é€ŸçŽ‡é™åˆ¶ã€åè®®ã€è·¨æºè¯·æ±‚å…±äº«ï¼ˆCORSï¼‰ã€æµé‡å½±å°„å’Œè·¯ç”±è§„åˆ™ç­‰ã€‚&lt;/p>
&lt;!--
## A Basic Ambassador Example
Ambassador is typically installed as a Kubernetes deployment, and is also available as a Helm chart. To configure Ambassador, create a Kubernetes service with the Ambassador annotations. Here is an example that configures Ambassador to route requests to /httpbin/ to the public httpbin.org service:
-->
&lt;h2 id="ä¸€ä¸ªç®€å•çš„-ambassador-ç¤ºä¾‹">ä¸€ä¸ªç®€å•çš„ Ambassador ç¤ºä¾‹&lt;/h2>
&lt;p>Ambassador é€šå¸¸ä½œä¸º Kubernetes Deployment æ¥å®‰è£…ï¼Œä¹Ÿå¯ä»¥ä½œä¸º Helm Chart ä½¿ç”¨ã€‚
é…ç½® Ambassador æ—¶ï¼Œè¯·ä½¿ç”¨ Ambassador æ³¨è§£åˆ›å»º Kubernetes æœåŠ¡ã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œç”¨æ¥é…ç½® Ambassadorï¼Œå°†é’ˆå¯¹ /httpbin/ çš„è¯·æ±‚è·¯ç”±åˆ°å…¬å…±çš„ httpbin.org æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Service
metadata:
name: httpbin
annotations:
getambassador.io/config: |
---
apiVersion: ambassador/v0
kind: Mapping
name: httpbin_mapping
prefix: /httpbin/
service: httpbin.org:80
host_rewrite: httpbin.org
spec:
type: ClusterIP
ports:
- port: 80
&lt;/code>&lt;/pre>&lt;!--
A mapping object is created with a prefix of /httpbin/ and a service name of httpbin.org. The host_rewrite annotation specifies that the HTTP host header should be set to httpbin.org.
-->
&lt;p>ä¾‹å­ä¸­åˆ›å»ºäº†ä¸€ä¸ª Mapping å¯¹è±¡ï¼Œå…¶ prefix è®¾ç½®ä¸º /httpbin/ï¼Œservice åç§°ä¸º httpbin.orgã€‚
å…¶ä¸­çš„ host_rewrite æ³¨è§£æŒ‡å®š HTTP çš„ host å¤´éƒ¨å­—æ®µåº”è®¾ç½®ä¸º httpbin.orgã€‚&lt;/p>
&lt;!--
## Kubeflow
Kubeflow provides a simple way to easily deploy machine learning infrastructure on Kubernetes. The Kubeflow team needed a proxy that provided a central point of authentication and routing to the wide range of services used in Kubeflow, many of which are ephemeral in nature.
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
-->
&lt;h2 id="kubeflow">Kubeflow&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubeflow/kubeflow">Kubeflow&lt;/a> æä¾›äº†ä¸€ç§ç®€å•çš„æ–¹æ³•ï¼Œç”¨äºŽåœ¨ Kubernetes ä¸Šè½»æ¾éƒ¨ç½²æœºå™¨å­¦ä¹ åŸºç¡€è®¾æ–½ã€‚
Kubeflow å›¢é˜Ÿéœ€è¦ä¸€ä¸ªä»£ç†ï¼Œä¸º Kubeflow ä¸­æ‰€ä½¿ç”¨çš„å„ç§æœåŠ¡æä¾›é›†ä¸­åŒ–çš„è®¤è¯å’Œè·¯ç”±èƒ½åŠ›ï¼›Kubeflow ä¸­è®¸å¤šæœåŠ¡æœ¬è´¨ä¸Šéƒ½æ˜¯ç”Ÿå‘½æœŸå¾ˆçŸ­çš„ã€‚&lt;/p>
&lt;center>&lt;i>Kubeflow architecture, pre-Ambassador&lt;/center>&lt;/i>
&lt;!--
## Service configuration
With Ambassador, Kubeflow can use a distributed model for configuration. Instead of a central configuration file, Ambassador allows each service to configure its route in Ambassador via Kubernetes annotations. Here is a simplified example configuration:
-->
&lt;h2 id="æœåŠ¡é…ç½®">æœåŠ¡é…ç½®&lt;/h2>
&lt;p>æœ‰äº† Ambassadorï¼ŒKubeflow å¯ä»¥ä½¿ç”¨åˆ†å¸ƒå¼æ¨¡åž‹è¿›è¡Œé…ç½®ã€‚
Ambassador ä¸ä½¿ç”¨é›†ä¸­çš„é…ç½®æ–‡ä»¶ï¼Œè€Œæ˜¯å…è®¸æ¯ä¸ªæœåŠ¡é€šè¿‡ Kubernetes æ³¨è§£åœ¨ Ambassador ä¸­é…ç½®å…¶è·¯ç”±ã€‚
ä¸‹é¢æ˜¯ä¸€ä¸ªç®€åŒ–çš„é…ç½®ç¤ºä¾‹ï¼š&lt;/p>
&lt;pre>&lt;code>---
apiVersion: ambassador/v0
kind: Mapping
name: tfserving-mapping-test-post
prefix: /models/test/
rewrite: /model/test/:predict
method: POST
service: test.kubeflow:8000
&lt;/code>&lt;/pre>&lt;!--
In this example, the â€œtestâ€ service uses Ambassador annotations to dynamically configure a route to the service, triggered only when the HTTP method is a POST, and the annotation also specifies a rewrite rule.
-->
&lt;p>ç¤ºä¾‹ä¸­ï¼Œâ€œtestâ€ æœåŠ¡ä½¿ç”¨ Ambassador æ³¨è§£æ¥ä¸ºæœåŠ¡åŠ¨æ€é…ç½®è·¯ç”±ã€‚
æ‰€é…ç½®çš„è·¯ç”±ä»…åœ¨ HTTP æ–¹æ³•æ˜¯ POST æ—¶è§¦å‘ï¼›æ³¨è§£ä¸­åŒæ—¶è¿˜ç»™å‡ºäº†ä¸€æ¡é‡å†™è§„åˆ™ã€‚&lt;/p>
&lt;!--
With Ambassador, Kubeflow manages routing easily with Kubernetes annotations. Kubeflow configures a single ingress object that directs traffic to Ambassador, then creates services with Ambassador annotations as needed to direct traffic to specific backends. For example, when deploying TensorFlow services, Kubeflow creates and and annotates a K8s service so that the model will be served at https://&lt;ingress host>/models/&lt;model name>/. Kubeflow can also use the Envoy Proxy to do the actual L7 routing. Using Ambassador, Kubeflow takes advantage of additional routing configuration like URL rewriting and method-based routing.
If youâ€™re interested in using Ambassador with Kubeflow, the standard Kubeflow install automatically installs and configures Ambassador.
If youâ€™re interested in using Ambassador as an API Gateway or Kubernetes ingress solution for your non-Kubeflow services, check out the Getting Started with Ambassador guide.
## Kubeflow and Ambassador
-->
&lt;h2 id="kubeflow-å’Œ-ambassador">Kubeflow å’Œ Ambassador&lt;/h2>
&lt;p>é€šè¿‡ Ambassadorï¼ŒKubeflow å¯ä»¥ä½¿ç”¨ Kubernetes æ³¨è§£è½»æ¾ç®¡ç†è·¯ç”±ã€‚
Kubeflow é…ç½®åŒä¸€ä¸ª Ingress å¯¹è±¡ï¼Œå°†æµé‡å®šå‘åˆ° Ambassadorï¼Œç„¶åŽæ ¹æ®éœ€è¦åˆ›å»ºå…·æœ‰ Ambassador æ³¨è§£çš„æœåŠ¡ï¼Œä»¥å°†æµé‡å®šå‘åˆ°ç‰¹å®šåŽç«¯ã€‚
ä¾‹å¦‚ï¼Œåœ¨éƒ¨ç½² TensorFlow æœåŠ¡æ—¶ï¼ŒKubeflow ä¼šåˆ›å»º Kubernetes æœåŠ¡å¹¶ä¸ºå…¶æ·»åŠ æ³¨è§£ï¼Œ
ä»¥ä¾¿ç”¨æˆ·èƒ½å¤Ÿåœ¨ &lt;code>https://&amp;lt;ingressä¸»æœº&amp;gt;/models/&amp;lt;æ¨¡åž‹åç§°&amp;gt;/&lt;/code> å¤„è®¿é—®åˆ°æ¨¡åž‹æœ¬èº«ã€‚
Kubeflow è¿˜å¯ä»¥ä½¿ç”¨ Envoy Proxy æ¥è¿›è¡Œå®žé™…çš„ L7 è·¯ç”±ã€‚
é€šè¿‡ Ambassadorï¼ŒKubeflow èƒ½å¤Ÿæ›´å……åˆ†åœ°åˆ©ç”¨ URL é‡å†™å’ŒåŸºäºŽæ–¹æ³•çš„è·¯ç”±ç­‰é¢å¤–çš„è·¯ç”±é…ç½®èƒ½åŠ›ã€‚&lt;/p>
&lt;p>å¦‚æžœæ‚¨å¯¹åœ¨ Kubeflow ä¸­ä½¿ç”¨ Ambassador æ„Ÿå…´è¶£ï¼Œæ ‡å‡†çš„ Kubeflow å®‰è£…ä¼šè‡ªåŠ¨å®‰è£…å’Œé…ç½® Ambassadorã€‚&lt;/p>
&lt;p>å¦‚æžœæ‚¨æœ‰å…´è¶£å°† Ambassador ç”¨ä½œ API ç½‘å…³æˆ– Kubernetes çš„ Ingress è§£å†³æ–¹æ¡ˆï¼Œ
è¯·å‚é˜… &lt;a href="https://www.getambassador.io/user-guide/getting-started">Ambassador å…¥é—¨æŒ‡å—&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes è¿™å››å¹´</title><link>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</link><pubDate>Wed, 06 Jun 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/06/06/kubernetes-%E8%BF%99%E5%9B%9B%E5%B9%B4/</guid><description>
&lt;!--
**Author**: Joe Beda (CTO and Founder, Heptio)
On June 6, 2014 I checked in the [first commit](https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56) of what would become the public repository for Kubernetes. Many would assume that is where the story starts. It is the beginning of history, right? But that really doesnâ€™t tell the whole story.
-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼šJoe Beda( Heptio é¦–å¸­æŠ€æœ¯å®˜å…¼åˆ›å§‹äºº)
2014 å¹´ 6 æœˆ 6 æ—¥ï¼Œæˆ‘æ£€æŸ¥äº† Kubernetes å…¬å…±ä»£ç åº“çš„&lt;a href="https://github.com/kubernetes/kubernetes/commit/2c4b3a562ce34cddc3f8218a2c4d11c7310e6d56">ç¬¬ä¸€æ¬¡ commit&lt;/a> ã€‚è®¸å¤šäººä¼šè®¤ä¸ºè¿™æ˜¯æ•…äº‹å¼€å§‹çš„åœ°æ–¹ã€‚è¿™éš¾é“ä¸æ˜¯ä¸€åˆ‡å¼€å§‹çš„åœ°æ–¹å—ï¼Ÿä½†è¿™çš„ç¡®ä¸èƒ½æŠŠæ•´ä¸ªè¿‡ç¨‹è¯´æ¸…æ¥šã€‚&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-06-06-4-years-of-k8s/k8s-first-commit.png" alt="k8s_first_commit">&lt;/p>
&lt;!--
The cast leading up to that commit was large and the success for Kubernetes since then is owed to an ever larger cast.
Kubernetes was built on ideas that had been proven out at Google over the previous ten years with Borg. And Borg, itself, owed its existence to even earlier efforts at Google and beyond.
Concretely, Kubernetes started as some prototypes from Brendan Burns combined with ongoing work from me and Craig McLuckie to better align the internal Google experience with the Google Cloud experience. Brendan, Craig, and I really wanted people to use this, so we made the case to build out this prototype as an open source project that would bring the best ideas from Borg out into the open.
After we got the nod, it was time to actually build the system. We took Brendanâ€™s prototype (in Java), rewrote it in Go, and built just enough to get the core ideas across. By this time the team had grown to include Ville Aikas, Tim Hockin, Brian Grant, Dawn Chen and Daniel Smith. Once we had something working, someone had to sign up to clean things up to get it ready for public launch. That ended up being me. Not knowing the significance at the time, I created a new repo, moved things over, and checked it in. So while I have the first public commit to the repo, there was work underway well before that.
The version of Kubernetes at that point was really just a shadow of what it was to become. The core concepts were there but it was very raw. For example, Pods were called Tasks. That was changed a day before we went public. All of this led up to the public announcement of Kubernetes on June 10th, 2014 in a keynote from Eric Brewer at the first DockerCon. You can watch that video here:
-->
&lt;p>ç¬¬ä¸€æ¬¡ commit æ¶‰åŠçš„äººå‘˜ä¼—å¤šï¼Œè‡ªé‚£ä»¥åŽ Kubernetes çš„æˆåŠŸå½’åŠŸäºŽæ›´å¤§çš„å¼€å‘è€…é˜µå®¹ã€‚
Kubernetes å»ºç«‹åœ¨è¿‡åŽ»åå¹´æ›¾ç»åœ¨ Google çš„ Borg é›†ç¾¤ç®¡ç†ç³»ç»Ÿä¸­éªŒè¯è¿‡çš„æ€è·¯ä¹‹ä¸Šã€‚è€Œ Borg æœ¬èº«ä¹Ÿæ˜¯ Google å’Œå…¶ä»–å…¬å¸æ—©æœŸåŠªåŠ›çš„ç»“æžœã€‚
å…·ä½“è€Œè¨€ï¼ŒKubernetes æœ€åˆæ˜¯ä»Ž Brendan Burns çš„ä¸€äº›åŽŸåž‹å¼€å§‹ï¼Œç»“åˆæˆ‘å’Œ Craig McLuckie æ­£åœ¨è¿›è¡Œçš„å·¥ä½œï¼Œä»¥æ›´å¥½åœ°å°† Google å†…éƒ¨å®žè·µä¸Ž Google Cloud çš„ç»éªŒç›¸ç»“åˆã€‚ Brendanï¼ŒCraig å’Œæˆ‘çœŸçš„å¸Œæœ›äººä»¬ä½¿ç”¨å®ƒï¼Œæ‰€ä»¥æˆ‘ä»¬å»ºè®®å°†è¿™ä¸ªåŽŸåž‹æž„å»ºä¸ºä¸€ä¸ªå¼€æºé¡¹ç›®ï¼Œå°† Borg çš„æœ€ä½³åˆ›æ„å¸¦ç»™å¤§å®¶ã€‚
åœ¨æˆ‘ä»¬æ‰€æœ‰äººåŒæ„åŽï¼Œå°±å¼€å§‹ç€æ‰‹æž„å»ºè¿™ä¸ªç³»ç»Ÿäº†ã€‚æˆ‘ä»¬é‡‡ç”¨äº† Brendan çš„åŽŸåž‹ï¼ˆJava è¯­è¨€ï¼‰ï¼Œç”¨ Go è¯­è¨€é‡å†™äº†å®ƒï¼Œå¹¶ä¸”ä»¥ä¸Šè¿°æ ¸å¿ƒæ€æƒ³åŽ»æž„å»ºè¯¥ç³»ç»Ÿã€‚åˆ°è¿™ä¸ªæ—¶å€™ï¼Œå›¢é˜Ÿå·²ç»æˆé•¿ä¸ºåŒ…æ‹¬ Ville Aikasï¼ŒTim Hockinï¼ŒBrian Grantï¼ŒDawn Chen å’Œ Daniel Smithã€‚ä¸€æ—¦æˆ‘ä»¬æœ‰äº†ä¸€äº›å·¥ä½œéœ€æ±‚ï¼Œæœ‰äººå¿…é¡»æ‰¿æ‹…ä¸€äº›è„±æ•çš„å·¥ä½œï¼Œä»¥ä¾¿ä¸ºå…¬å¼€å‘å¸ƒåšå¥½å‡†å¤‡ã€‚è¿™ä¸ªè§’è‰²æœ€ç»ˆç”±æˆ‘æ‰¿æ‹…ã€‚å½“æ—¶ï¼Œæˆ‘ä¸çŸ¥é“è¿™ä»¶äº‹æƒ…çš„é‡è¦æ€§ï¼Œæˆ‘åˆ›å»ºäº†ä¸€ä¸ªæ–°çš„ä»“åº“ï¼ŒæŠŠä»£ç æ¬è¿‡æ¥ï¼Œç„¶åŽè¿›è¡Œäº†æ£€æŸ¥ã€‚æ‰€ä»¥åœ¨æˆ‘ç¬¬ä¸€æ¬¡æäº¤ public commit ä¹‹å‰ï¼Œå°±æœ‰å·¥ä½œå·²ç»å¯åŠ¨äº†ã€‚
é‚£æ—¶ Kubernetes çš„ç‰ˆæœ¬åªæ˜¯çŽ°åœ¨ç‰ˆæœ¬çš„ç®€å•é›å½¢ã€‚æ ¸å¿ƒæ¦‚å¿µå·²ç»æœ‰äº†ï¼Œä½†éžå¸¸åŽŸå§‹ã€‚ä¾‹å¦‚ï¼ŒPods è¢«ç§°ä¸º Tasksï¼Œè¿™åœ¨æˆ‘ä»¬æŽ¨å¹¿å‰ä¸€å¤©å°±è¢«æ›¿æ¢ã€‚2014å¹´6æœˆ10æ—¥ Eric Brewe åœ¨ç¬¬ä¸€å±Š DockerCon ä¸Šçš„æ¼”è®²ä¸­æ­£å¼å‘å¸ƒäº† Kubernetes ã€‚æ‚¨å¯ä»¥åœ¨æ­¤å¤„è§‚çœ‹è¯¥è§†é¢‘ï¼š&lt;/p>
&lt;center>&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/YrxnVKZeqK8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen>&lt;/iframe>&lt;/center>
&lt;!--
But, however raw, that modest start was enough to pique the interest of a community that started strong and has only gotten stronger. Over the past four years Kubernetes has exceeded the expectations of all of us that were there early on. We owe the Kubernetes community a huge debt. The success the project has seen is based not just on code and technology but also the way that an amazing group of people have come together to create something special. The best expression of this is the [set of Kubernetes values](https://github.com/kubernetes/steering/blob/master/values.md) that Sarah Novotny helped curate.
Here is to another 4 years and beyond! ðŸŽ‰ðŸŽ‰ðŸŽ‰
-->
&lt;p>ä½†æ˜¯ï¼Œæ— è®ºå¤šä¹ˆåŽŸå§‹ï¼Œè¿™å°å°çš„ä¸€æ­¥è¶³ä»¥æ¿€èµ·ä¸€ä¸ªå¼€å§‹å¼ºå¤§è€Œä¸”å˜å¾—æ›´å¼ºå¤§çš„ç¤¾åŒºçš„å…´è¶£ã€‚åœ¨è¿‡åŽ»çš„å››å¹´é‡Œï¼ŒKubernetes å·²ç»è¶…å‡ºäº†æˆ‘ä»¬æ‰€æœ‰äººçš„æœŸæœ›ã€‚æˆ‘ä»¬å¯¹ Kubernetes ç¤¾åŒºçš„æ‰€æœ‰äººå‘˜è¡¨ç¤ºæ„Ÿè°¢ã€‚è¯¥é¡¹ç›®æ‰€å–å¾—çš„æˆåŠŸä¸ä»…åŸºäºŽä»£ç å’ŒæŠ€æœ¯ï¼Œè¿˜åŸºäºŽä¸€ç¾¤å‡ºè‰²çš„äººèšé›†åœ¨ä¸€èµ·æ‰€åšçš„æœ‰æ„ä¹‰çš„äº‹æƒ…ã€‚Sarah Novotny ç­–åˆ’çš„ä¸€å¥— &lt;a href="https://github.com/kubernetes/steering/blob/master/values.md">Kubernetes ä»·å€¼è§‚&lt;/a>æ˜¯ä»¥ä¸Šæœ€å¥½çš„è¡¨çŽ°å½¢å¼ã€‚
è®©æˆ‘ä»¬ä¸€èµ·æœŸå¾…ä¸‹ä¸€ä¸ª4å¹´ï¼ðŸŽ‰ðŸŽ‰ðŸŽ‰&lt;/p></description></item><item><title>Blog: Kubernetes 1 11ï¼šå‘ discuss kubernetes é—®å¥½</title><link>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</link><pubDate>Wed, 30 May 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/05/30/kubernetes-1-11%E5%90%91-discuss-kubernetes-%E9%97%AE%E5%A5%BD/</guid><description>
&lt;!--
---
title: ' Kubernetes 1 11ï¼šsay-hello-to-discuss-kubernetes '
cn-approvers:
- congfairy
layout: blog
date: 2018-05-30
---
-->
&lt;!--
Author: Jorge Castro (Heptio)
-->
&lt;p>ä½œè€…: Jorge Castro (Heptio)&lt;/p>
&lt;!--
Communication is key when it comes to engaging a community of over 35,000 people in a global and remote environment. Keeping track of everything in the Kubernetes community can be an overwhelming task. On one hand we have our official resources, like Stack Overflow, GitHub, and the mailing lists, and on the other we have more ephemeral resources like Slack, where you can hop in, chat with someone, and then go on your merry way.
-->
&lt;p>å°±ä¸€ä¸ªè¶…è¿‡ 35,000 äººçš„å…¨çƒæ€§ç¤¾åŒºè€Œè¨€ï¼Œå‚ä¸Žå…¶ä¸­æ—¶æ²Ÿé€šæ˜¯éžå¸¸å…³é”®çš„ã€‚ è·Ÿè¸ª Kubernetes ç¤¾åŒºä¸­çš„æ‰€æœ‰å†…å®¹å¯èƒ½æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚ ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æœ‰å®˜æ–¹èµ„æºï¼Œå¦‚ Stack Overflowï¼ŒGitHub å’Œé‚®ä»¶åˆ—è¡¨ï¼Œå¦ä¸€æ–¹é¢ï¼Œæˆ‘ä»¬æœ‰æ›´å¤šçž¬æ—¶æ€§çš„èµ„æºï¼Œå¦‚ Slackï¼Œä½ å¯ä»¥åŠ å…¥è¿›åŽ»ã€ä¸ŽæŸäººèŠå¤©ç„¶åŽå„èµ°å„è·¯ã€‚&lt;/p>
&lt;!--
Slack is great for casual and timely conversations and keeping up with other community members, but communication can't be easily referenced in the future. Plus it can be hard to raise your hand in a room filled with 35,000 participants and find a voice. Mailing lists are useful when trying to reach a specific group of people with a particular ask and want to keep track of responses on the thread, but can be daunting with a large amount of people. Stack Overflow and GitHub are ideal for collaborating on projects or questions that involve code and need to be searchable in the future, but certain topics like "What's your favorite CI/CD tool" or "Kubectl tips and tricks" are offtopic there.
While our current assortment of communication channels are valuable in their own rights, we found that there was still a gap between email and real time chat. Across the rest of the web, many other open source projects like Docker, Mozilla, Swift, Ghost, and Chef have had success building communities on top of Discourse, an open source discussion platform. So what if we could use this tool to bring our discussions together under a modern roof, with an open API, and perhaps not let so much of our information fade into the ether? There's only one way to find out: Welcome to discuss.kubernetes.io
-->
&lt;p>Slack éžå¸¸é€‚åˆéšæ„å’ŒåŠæ—¶çš„å¯¹è¯ï¼Œå¹¶ä¸Žå…¶ä»–ç¤¾åŒºæˆå‘˜ä¿æŒè”ç³»ï¼Œä½†æœªæ¥å¾ˆéš¾è½»æ˜“å¼•ç”¨é€šä¿¡ã€‚æ­¤å¤–ï¼Œåœ¨35,000åå‚ä¸Žè€…ä¸­æé—®å¹¶å¾—åˆ°å›žç­”å¾ˆéš¾ã€‚é‚®ä»¶åˆ—è¡¨åœ¨æœ‰é—®é¢˜å°è¯•è”ç³»ç‰¹å®šäººç¾¤å¹¶ä¸”æƒ³è¦è·Ÿè¸ªå¤§å®¶çš„å›žåº”æ—¶éžå¸¸æœ‰ç”¨ï¼Œä½†æ˜¯å¯¹äºŽå¤§é‡äººå‘˜æ¥è¯´å¯èƒ½æ˜¯éº»çƒ¦çš„ã€‚ Stack Overflow å’Œ GitHub éžå¸¸é€‚åˆåœ¨æ¶‰åŠä»£ç çš„é¡¹ç›®æˆ–é—®é¢˜ä¸Šè¿›è¡Œåä½œï¼Œå¹¶ä¸”å¦‚æžœåœ¨å°†æ¥è¦è¿›è¡Œæœç´¢ä¹Ÿå¾ˆæœ‰ç”¨ï¼Œä½†æŸäº›ä¸»é¢˜å¦‚â€œä½ æœ€å–œæ¬¢çš„ CI/CD å·¥å…·æ˜¯ä»€ä¹ˆâ€æˆ–â€œ&lt;a href="http://discuss.kubernetes.io/t/kubectl-tips-and-tricks/192">Kubectlæç¤ºå’ŒæŠ€å·§&lt;/a>â€œåœ¨é‚£é‡Œæ˜¯æ²¡æœ‰æ„ä¹‰çš„ã€‚&lt;/p>
&lt;p>è™½ç„¶æˆ‘ä»¬ç›®å‰çš„å„ç§æ²Ÿé€šæ¸ é“å¯¹ä»–ä»¬è‡ªå·±æ¥è¯´éƒ½å¾ˆæœ‰ä»·å€¼ï¼Œä½†æˆ‘ä»¬å‘çŽ°ç”µå­é‚®ä»¶å’Œå®žæ—¶èŠå¤©ä¹‹é—´ä»ç„¶å­˜åœ¨å·®è·ã€‚åœ¨ç½‘ç»œçš„å…¶ä»–éƒ¨åˆ†ï¼Œè®¸å¤šå…¶ä»–å¼€æºé¡¹ç›®ï¼Œå¦‚ Dockerã€Mozillaã€Swiftã€Ghost å’Œ Chefï¼Œå·²ç»æˆåŠŸåœ°åœ¨&lt;a href="http://www.discourse.org/features">Discourse&lt;/a>ä¹‹ä¸Šæž„å»ºç¤¾åŒºï¼Œä¸€ä¸ªå¼€æ”¾çš„è®¨è®ºå¹³å°ã€‚é‚£ä¹ˆï¼Œå¦‚æžœæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™ä¸ªå·¥å…·å°†æˆ‘ä»¬çš„è®¨è®ºç»“åˆåœ¨ä¸€ä¸ªå¹³å°ä¸‹ï¼Œä½¿ç”¨å¼€æ”¾çš„APIï¼Œæˆ–è®¸ä¹Ÿä¸ä¼šè®©æˆ‘ä»¬çš„å¤§éƒ¨åˆ†ä¿¡æ¯æ¶ˆå¤±åœ¨ç½‘ç»œä¸­å‘¢ï¼Ÿåªæœ‰ä¸€ç§æ–¹æ³•å¯ä»¥æ‰¾åˆ°ï¼šæ¬¢è¿Žæ¥åˆ°&lt;a href="http://discuss.kubernetes.io">discuss.kubernetes.io&lt;/a>&lt;/p>
&lt;!--
Right off the bat we have categories that users can browse. Checking and posting in these categories allow users to participate in things they might be interested in without having to commit to subscribing to a list. Granular notification controls allow the users to subscribe to just the category or tag they want, and allow for responding to topics via email.
Ecosystem partners and developers now have a place where they can [announce projects](https://discuss.kubernetes.io/c/announcements) that they're working on to users without wondering if it would be offtopic on an official list. We can make this place be not just about core Kubernetes, but about the hundreds of wonderful tools our community is building.
This new community forum gives people a place to go where they can discuss Kubernetes, and a sounding board for developers to make announcements of things happening around Kubernetes, all while being searchable and easily accessible to a wider audience.
Hop in and take a look. We're just getting started, so you might want to begin by [introducing yourself](https://discuss.kubernetes.io/t/introduce-yourself-here/56) and then browsing around. Apps are also available for [Android](https://play.google.com/store/apps/details?id=com.discourse&amp;hl=en_US&amp;rdid=com.discourse&amp;pli=1)and [iOS](https://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8).
-->
&lt;p>é©¬ä¸Šï¼Œæˆ‘ä»¬æœ‰ç”¨æˆ·å¯ä»¥æµè§ˆçš„ç±»åˆ«ã€‚æ£€æŸ¥å’Œå‘å¸ƒè¿™äº›ç±»åˆ«å…è®¸ç”¨æˆ·å‚ä¸Žä»–ä»¬å¯èƒ½æ„Ÿå…´è¶£çš„äº‹æƒ…ï¼Œè€Œæ— éœ€è®¢é˜…åˆ—è¡¨ã€‚ç²¾ç»†çš„é€šçŸ¥æŽ§ä»¶å…è®¸ç”¨æˆ·åªè®¢é˜…ä»–ä»¬æƒ³è¦çš„ç±»åˆ«æˆ–æ ‡ç­¾ï¼Œå¹¶å…è®¸é€šè¿‡ç”µå­é‚®ä»¶å›žå¤ä¸»é¢˜ã€‚&lt;/p>
&lt;p>ç”Ÿæ€ç³»ç»Ÿåˆä½œä¼™ä¼´å’Œå¼€å‘äººå‘˜çŽ°åœ¨æœ‰ä¸€ä¸ªåœ°æ–¹å¯ä»¥&lt;a href="http://discuss.kubernetes.io/c/announcements">å®£å¸ƒé¡¹ç›®&lt;/a>ï¼Œä»–ä»¬æ­£åœ¨ä¸ºç”¨æˆ·å·¥ä½œï¼Œè€Œä¸ä¼šæƒ³çŸ¥é“å®ƒæ˜¯å¦ä¼šåœ¨å®˜æ–¹åˆ—è¡¨ä¸­è„±ç¦»ä¸»é¢˜ã€‚æˆ‘ä»¬å¯ä»¥è®©è¿™ä¸ªåœ°æ–¹ä¸ä»…ä»…æ˜¯å…³äºŽæ ¸å¿ƒ Kubernetesï¼Œè€Œæ˜¯å…³äºŽæˆ‘ä»¬ç¤¾åŒºæ­£åœ¨å»ºè®¾çš„æ•°ç™¾ä¸ªç²¾å½©å·¥å…·ã€‚&lt;/p>
&lt;p>è¿™ä¸ªæ–°çš„ç¤¾åŒºè®ºå›ä¸ºäººä»¬æä¾›äº†ä¸€ä¸ªå¯ä»¥è®¨è®º Kubernetes çš„åœ°æ–¹ï¼Œä¹Ÿæ˜¯å¼€å‘äººå‘˜åœ¨ Kubernetes å‘¨å›´å‘å¸ƒäº‹ä»¶çš„å£°éŸ³æ¿ï¼ŒåŒæ—¶å¯ä»¥æœç´¢å¹¶ä¸”æ›´å®¹æ˜“è¢«æ›´å¹¿æ³›çš„ç”¨æˆ·è®¿é—®ã€‚&lt;/p>
&lt;p>è¿›æ¥çœ‹çœ‹ã€‚æˆ‘ä»¬åˆšåˆšå¼€å§‹ï¼Œæ‰€ä»¥ï¼Œæ‚¨å¯èƒ½å¸Œæœ›ä»Ž&lt;a href="http://discuss.kubernetes.io/t/introduce-yourself-here/56">è‡ªæˆ‘ä»‹ç»&lt;/a>å¼€å§‹ï¼Œåˆ°å¤„æµè§ˆã€‚ä¹Ÿæœ‰ &lt;a href="http://play.google.com/store/apps/details?id=com.discourse&amp;amp;hl=en_US&amp;amp;rdid=com.discourse&amp;amp;pli=1">Android&lt;/a> å’Œ &lt;a href="http://itunes.apple.com/us/app/discourse-app/id1173672076?mt=8">iOS&lt;/a> åº”ç”¨ä¸‹è½½ã€‚&lt;/p></description></item><item><title>Blog: åœ¨ Kubernetes ä¸Šå¼€å‘</title><link>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</link><pubDate>Tue, 01 May 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/05/01/developing-on-kubernetes/</guid><description>
&lt;!--
---
title: Developing on Kubernetes
date: 2018-05-01
slug: developing-on-kubernetes
---
-->
&lt;!--**Authors**:-->
&lt;p>&lt;strong>ä½œè€…&lt;/strong>ï¼š &lt;a href="https://twitter.com/mhausenblas">Michael Hausenblas&lt;/a> (Red Hat), &lt;a href="https://twitter.com/errordeveloper">Ilya Dmitrichenko&lt;/a> (Weaveworks)&lt;/p>
&lt;!--
How do you develop a Kubernetes app? That is, how do you write and test an app that is supposed to run on Kubernetes? This article focuses on the challenges, tools and methods you might want to be aware of to successfully write Kubernetes apps alone or in a team setting.
-->
&lt;p>æ‚¨å°†å¦‚ä½•å¼€å‘ä¸€ä¸ª Kubernates åº”ç”¨ï¼Ÿä¹Ÿå°±æ˜¯è¯´ï¼Œæ‚¨å¦‚ä½•ç¼–å†™å¹¶æµ‹è¯•ä¸€ä¸ªè¦åœ¨ Kubernates ä¸Šè¿è¡Œçš„åº”ç”¨ç¨‹åºï¼Ÿæœ¬æ–‡å°†é‡ç‚¹ä»‹ç»åœ¨ç‹¬è‡ªå¼€å‘æˆ–è€…å›¢é˜Ÿåä½œä¸­ï¼Œæ‚¨å¯èƒ½å¸Œæœ›äº†è§£åˆ°çš„ä¸ºäº†æˆåŠŸç¼–å†™ Kubernetes åº”ç”¨ç¨‹åºè€Œéœ€é¢ä¸´çš„æŒ‘æˆ˜ï¼Œå·¥å…·å’Œæ–¹æ³•ã€‚&lt;/p>
&lt;!--
Weâ€™re assuming you are a developer, you have a favorite programming language, editor/IDE, and a testing framework available. The overarching goal is to introduce minimal changes to your current workflow when developing the app for Kubernetes. For example, if youâ€™re a Node.js developer and are used to a hot-reload setupâ€”that is, on save in your editor the running app gets automagically updatedâ€”then dealing with containers and container images, with container registries, Kubernetes deployments, triggers, and more can not only be overwhelming but really take all the fun out if it.
-->
&lt;p>æˆ‘ä»¬å‡å®šæ‚¨æ˜¯ä¸€ä½å¼€å‘äººå‘˜ï¼Œæœ‰æ‚¨é’Ÿçˆ±çš„ç¼–ç¨‹è¯­è¨€ï¼Œç¼–è¾‘å™¨/IDEï¼ˆé›†æˆå¼€å‘çŽ¯å¢ƒï¼‰ï¼Œä»¥åŠå¯ç”¨çš„æµ‹è¯•æ¡†æž¶ã€‚åœ¨é’ˆå¯¹ Kubernates å¼€å‘åº”ç”¨æ—¶ï¼Œæœ€é‡è¦çš„ç›®æ ‡æ˜¯å‡å°‘å¯¹å½“å‰å·¥ä½œæµç¨‹çš„å½±å“ï¼Œæ”¹å˜è¶Šå°‘è¶Šå¥½ï¼Œå°½é‡åšåˆ°æœ€å°ã€‚ä¸¾ä¸ªä¾‹å­ï¼Œå¦‚æžœæ‚¨æ˜¯ Node.js å¼€å‘äººå‘˜ï¼Œä¹ æƒ¯äºŽé‚£ç§çƒ­é‡è½½çš„çŽ¯å¢ƒ - ä¹Ÿå°±æ˜¯è¯´æ‚¨åœ¨ç¼–è¾‘å™¨é‡Œä¸€åšä¿å­˜ï¼Œæ­£åœ¨è¿è¡Œçš„ç¨‹åºå°±ä¼šè‡ªåŠ¨æ›´æ–° - é‚£ä¹ˆè·Ÿå®¹å™¨ã€å®¹å™¨é•œåƒæˆ–è€…é•œåƒä»“åº“æ‰“äº¤é“ï¼Œåˆæˆ–æ˜¯è·Ÿ Kubernetes éƒ¨ç½²ã€triggers ä»¥åŠæ›´å¤šå¤´ç–¼ä¸œè¥¿æ‰“äº¤é“ï¼Œä¸ä»…ä¼šè®©äººéš¾ä»¥æ‹›æž¶ä¹ŸçœŸçš„ä¼šè®©å¼€å‘è¿‡ç¨‹å®Œå…¨å¤±åŽ»ä¹è¶£ã€‚&lt;/p>
&lt;!--
In the following, weâ€™ll first discuss the overall development setup, then review tools of the trade, and last but not least do a hands-on walkthrough of three exemplary tools that allow for iterative, local app development against Kubernetes.
-->
&lt;p>åœ¨ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é¦–å…ˆè®¨è®º Kubernetes æ€»ä½“å¼€å‘çŽ¯å¢ƒï¼Œç„¶åŽå›žé¡¾å¸¸ç”¨å·¥å…·ï¼Œæœ€åŽè¿›è¡Œä¸‰ä¸ªç¤ºä¾‹æ€§å·¥å…·çš„å®žè·µæ¼”ç»ƒã€‚è¿™äº›å·¥å…·å…è®¸é’ˆå¯¹ Kubernetes è¿›è¡Œæœ¬åœ°åº”ç”¨ç¨‹åºçš„å¼€å‘å’Œè¿­ä»£ã€‚&lt;/p>
&lt;!--
## Where to run your cluster?
-->
&lt;h2 id="æ‚¨çš„é›†ç¾¤è¿è¡Œåœ¨å“ªé‡Œ">æ‚¨çš„é›†ç¾¤è¿è¡Œåœ¨å“ªé‡Œï¼Ÿ&lt;/h2>
&lt;!--
As a developer you want to think about where the Kubernetes cluster youâ€™re developing against runs as well as where the development environment sits. Conceptually there are four development modes:
-->
&lt;p>ä½œä¸ºå¼€å‘äººå‘˜ï¼Œæ‚¨æ—¢éœ€è¦è€ƒè™‘æ‰€é’ˆå¯¹å¼€å‘çš„ Kubernetes é›†ç¾¤è¿è¡Œåœ¨å“ªé‡Œï¼Œä¹Ÿéœ€è¦æ€è€ƒå¼€å‘çŽ¯å¢ƒå¦‚ä½•é…ç½®ã€‚æ¦‚å¿µä¸Šï¼Œæœ‰å››ç§å¼€å‘æ¨¡å¼ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-devmodes_preview.png" alt="Dev Modes">&lt;/p>
&lt;!--
A number of tools support pure offline development including Minikube, Docker for Mac/Windows, Minishift, and the ones we discuss in detail below. Sometimes, for example, in a microservices setup where certain microservices already run in the cluster, a proxied setup (forwarding traffic into and from the cluster) is preferable and Telepresence is an example tool in this category. The live mode essentially means youâ€™re building and/or deploying against a remote cluster and, finally, the pure online mode means both your development environment and the cluster are remote, as this is the case with, for example, [Eclipse Che](https://www.eclipse.org/che/docs/kubernetes-single-user.html) or [Cloud 9](https://github.com/errordeveloper/k9c). Letâ€™s now have a closer look at the basics of offline development: running Kubernetes locally.
-->
&lt;p>è®¸å¤šå·¥å…·æ”¯æŒçº¯ offline å¼€å‘ï¼ŒåŒ…æ‹¬ Minikubeã€Dockerï¼ˆMac ç‰ˆ/Windows ç‰ˆï¼‰ã€Minishift ä»¥åŠä¸‹æ–‡ä¸­æˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºçš„å‡ ç§ã€‚æœ‰æ—¶ï¼Œæ¯”å¦‚è¯´åœ¨ä¸€ä¸ªå¾®æœåŠ¡ç³»ç»Ÿä¸­ï¼Œå·²ç»æœ‰è‹¥å¹²å¾®æœåŠ¡åœ¨è¿è¡Œï¼Œproxied æ¨¡å¼ï¼ˆé€šè¿‡è½¬å‘æŠŠæ•°æ®æµä¼ è¿›ä¼ å‡ºé›†ç¾¤ï¼‰å°±éžå¸¸åˆé€‚ï¼ŒTelepresence å°±æ˜¯æ­¤ç±»å·¥å…·çš„ä¸€ä¸ªå®žä¾‹ã€‚live æ¨¡å¼ï¼Œæœ¬è´¨ä¸Šæ˜¯æ‚¨åŸºäºŽä¸€ä¸ªè¿œç¨‹é›†ç¾¤è¿›è¡Œæž„å»ºå’Œéƒ¨ç½²ã€‚æœ€åŽï¼Œçº¯ online æ¨¡å¼æ„å‘³ç€æ‚¨çš„å¼€å‘çŽ¯å¢ƒå’Œè¿è¡Œé›†ç¾¤éƒ½æ˜¯è¿œç¨‹çš„ï¼Œå…¸åž‹çš„ä¾‹å­æ˜¯ &lt;a href="https://www.eclipse.org/che/docs/kubernetes-single-user.html">Eclipse Che&lt;/a> æˆ–è€… &lt;a href="https://github.com/errordeveloper/k9c">Cloud 9&lt;/a>ã€‚çŽ°åœ¨è®©æˆ‘ä»¬ä»”ç»†çœ‹çœ‹ç¦»çº¿å¼€å‘çš„åŸºç¡€ï¼šåœ¨æœ¬åœ°è¿è¡Œ Kubernetesã€‚&lt;/p>
&lt;!--
[Minikube](/docs/getting-started-guides/minikube/) is a popular choice for those who prefer to run Kubernetes in a local VM. More recently Docker for [Mac](https://docs.docker.com/docker-for-mac/kubernetes/) and [Windows](https://docs.docker.com/docker-for-windows/kubernetes/) started shipping Kubernetes as an experimental package (in the â€œedgeâ€ channel). Some reasons why you may want to prefer using Minikube over the Docker desktop option are:
-->
&lt;p>&lt;a href="https://kubernetes.io/docs/getting-started-guides/minikube/">Minikube&lt;/a> åœ¨æ›´åŠ å–œæ¬¢äºŽæœ¬åœ° VM ä¸Šè¿è¡Œ Kubernetes çš„å¼€å‘äººå‘˜ä¸­ï¼Œéžå¸¸å—æ¬¢è¿Žã€‚ä¸ä¹…å‰ï¼ŒDocker çš„ &lt;a href="https://docs.docker.com/docker-for-mac/kubernetes/">Mac&lt;/a> ç‰ˆå’Œ &lt;a href="https://docs.docker.com/docker-for-windows/kubernetes/">Windows&lt;/a> ç‰ˆï¼Œéƒ½è¯•éªŒæ€§åœ°å¼€å§‹è‡ªå¸¦ Kubernetesï¼ˆéœ€è¦ä¸‹è½½ â€œedgeâ€ å®‰è£…åŒ…ï¼‰ã€‚åœ¨ä¸¤è€…ä¹‹é—´ï¼Œä»¥ä¸‹åŽŸå› ä¹Ÿè®¸ä¼šä¿ƒä½¿æ‚¨é€‰æ‹© Minikube è€Œä¸æ˜¯ Docker æ¡Œé¢ç‰ˆï¼š&lt;/p>
&lt;!--
* You already have Minikube installed and running
* You prefer to wait until Docker ships a stable package
* Youâ€™re a Linux desktop user
* You are a Windows user who doesnâ€™t have Windows 10 Pro with Hyper-V
-->
&lt;ul>
&lt;li>æ‚¨å·²ç»å®‰è£…äº† Minikube å¹¶ä¸”å®ƒè¿è¡Œè‰¯å¥½&lt;/li>
&lt;li>æ‚¨æƒ³ç­‰åˆ° Docker å‡ºç¨³å®šç‰ˆæœ¬&lt;/li>
&lt;li>æ‚¨æ˜¯ Linux æ¡Œé¢ç”¨æˆ·&lt;/li>
&lt;li>æ‚¨æ˜¯ Windows ç”¨æˆ·ï¼Œä½†æ˜¯æ²¡æœ‰é…æœ‰ Hyper-V çš„ Windows 10 Pro&lt;/li>
&lt;/ul>
&lt;!--
Running a local cluster allows folks to work offline and that you donâ€™t have to pay for using cloud resources. Cloud provider costs are often rather affordable and free tiers exists, however some folks prefer to avoid having to approve those costs with their manager as well as potentially incur unexpected costs, for example, when leaving cluster running over the weekend.
-->
&lt;p>è¿è¡Œä¸€ä¸ªæœ¬åœ°é›†ç¾¤ï¼Œå¼€å‘äººå‘˜å¯ä»¥ç¦»çº¿å·¥ä½œï¼Œä¸ç”¨æ”¯ä»˜äº‘æœåŠ¡ã€‚äº‘æœåŠ¡æ”¶è´¹ä¸€èˆ¬ä¸ä¼šå¤ªé«˜ï¼Œå¹¶ä¸”å…è´¹çš„ç­‰çº§ä¹Ÿæœ‰ï¼Œä½†æ˜¯ä¸€äº›å¼€å‘äººå‘˜ä¸å–œæ¬¢ä¸ºäº†ä½¿ç”¨äº‘æœåŠ¡è€Œå¿…é¡»å¾—åˆ°ç»ç†çš„æ‰¹å‡†ï¼Œä¹Ÿä¸æ„¿æ„æ”¯ä»˜æ„æƒ³ä¸åˆ°çš„è´¹ç”¨ï¼Œæ¯”å¦‚è¯´å¿˜äº†ä¸‹çº¿è€Œé›†ç¾¤åœ¨å‘¨æœ«ä¹Ÿåœ¨è¿è½¬ã€‚&lt;/p>
&lt;!--
Some developers prefer to use a remote Kubernetes cluster, and this is usually to allow for larger compute and storage capacity and also enable collaborative workflows more easily. This means itâ€™s easier for you to pull in a colleague to help with debugging or share access to an app in the team. Additionally, for some developers it can be critical to mirror production environment as closely as possible, especially when it comes down to external cloud services, say, proprietary databases, object stores, message queues, external load balancer, or mail delivery systems.
-->
&lt;p>æœ‰äº›å¼€å‘äººå‘˜å´æ›´å–œæ¬¢è¿œç¨‹çš„ Kubernetes é›†ç¾¤ï¼Œè¿™æ ·ä»–ä»¬é€šå¸¸å¯ä»¥èŽ·å¾—æ›´å¤§çš„è®¡ç®—èƒ½åŠ›å’Œå­˜å‚¨å®¹é‡ï¼Œä¹Ÿç®€åŒ–äº†ååŒå·¥ä½œæµç¨‹ã€‚æ‚¨å¯ä»¥æ›´å®¹æ˜“çš„æ‹‰ä¸Šä¸€ä¸ªåŒäº‹æ¥å¸®æ‚¨è°ƒè¯•ï¼Œæˆ–è€…åœ¨å›¢é˜Ÿå†…å…±äº«ä¸€ä¸ªåº”ç”¨çš„ä½¿ç”¨ã€‚å†è€…ï¼Œå¯¹æŸäº›å¼€å‘äººå‘˜æ¥è¯´ï¼Œå°½å¯èƒ½çš„è®©å¼€å‘çŽ¯å¢ƒç±»ä¼¼ç”Ÿäº§çŽ¯å¢ƒè‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯æ‚¨ä¾èµ–å¤–éƒ¨åŽ‚å•†çš„äº‘æœåŠ¡æ—¶ï¼Œå¦‚ï¼šä¸“æœ‰æ•°æ®åº“ã€äº‘å¯¹è±¡å­˜å‚¨ã€æ¶ˆæ¯é˜Ÿåˆ—ã€å¤–å•†çš„è´Ÿè½½å‡è¡¡å™¨æˆ–è€…é‚®ä»¶æŠ•é€’ç³»ç»Ÿã€‚&lt;/p>
&lt;!--
In summary, there are good reasons for you to develop against a local cluster as well as a remote one. It very much depends on in which phase you are: from early prototyping and/or developing alone to integrating a set of more stable microservices.
-->
&lt;p>æ€»ä¹‹ï¼Œæ— è®ºæ‚¨é€‰æ‹©æœ¬åœ°æˆ–è€…è¿œç¨‹é›†ç¾¤ï¼Œç†ç”±éƒ½è¶³å¤Ÿå¤šã€‚è¿™å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºŽæ‚¨æ‰€å¤„çš„é˜¶æ®µï¼šä»Žæ—©æœŸçš„åŽŸåž‹è®¾è®¡/å•äººå¼€å‘åˆ°åŽæœŸé¢å¯¹ä¸€æ‰¹ç¨³å®šå¾®æœåŠ¡çš„é›†æˆã€‚&lt;/p>
&lt;!--
Now that you have a basic idea of the options around the runtime environment, letâ€™s move on to how to iteratively develop and deploy your app.
-->
&lt;p>æ—¢ç„¶æ‚¨å·²ç»äº†è§£åˆ°è¿è¡ŒçŽ¯å¢ƒçš„åŸºæœ¬é€‰é¡¹ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±æŽ¥ç€è®¨è®ºå¦‚ä½•è¿­ä»£å¼çš„å¼€å‘å¹¶éƒ¨ç½²æ‚¨çš„åº”ç”¨ã€‚&lt;/p>
&lt;!--
## The tools of the trade
-->
&lt;h2 id="å¸¸ç”¨å·¥å…·">å¸¸ç”¨å·¥å…·&lt;/h2>
&lt;!--
We are now going to review tooling allowing you to develop apps on Kubernetes with the focus on having minimal impact on your existing workflow. We strive to provide an unbiased description including implications of using each of the tools in general terms.
-->
&lt;p>æˆ‘ä»¬çŽ°åœ¨å›žé¡¾æ—¢å¯ä»¥å…è®¸æ‚¨å¯ä»¥åœ¨ Kubernetes ä¸Šå¼€å‘åº”ç”¨ç¨‹åºåˆå°½å¯èƒ½æœ€å°åœ°æ”¹å˜æ‚¨çŽ°æœ‰çš„å·¥ä½œæµç¨‹çš„ä¸€äº›å·¥å…·ã€‚æˆ‘ä»¬è‡´åŠ›äºŽæä¾›ä¸€ä»½ä¸åä¸å€šçš„æè¿°ï¼Œä¹Ÿä¼šæåŠä½¿ç”¨æŸä¸ªå·¥å…·å°†ä¼šæ„å‘³ç€ä»€ä¹ˆã€‚&lt;/p>
&lt;!--
Note that this is a tricky area since even for established technologies such as, for example, JSON vs YAML vs XML or REST vs gRPC vs SOAP a lot depends on your background, your preferences and organizational settings. Itâ€™s even harder to compare tooling in the Kubernetes ecosystem as things evolve very rapidly and new tools are announced almost on a weekly basis; during the preparation of this post alone, for example, [Gitkube](https://gitkube.sh/) and [Watchpod](https://github.com/MinikubeAddon/watchpod) came out. To cover these new tools as well as related, existing tooling such as [Weave Flux](https://github.com/weaveworks/flux) and OpenShiftâ€™s [S2I](https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html) we are planning a follow-up blog post to the one youâ€™re reading.
-->
&lt;p>è¯·æ³¨æ„è¿™å¾ˆæ£˜æ‰‹ï¼Œå› ä¸ºå³ä½¿åœ¨æˆç†Ÿå®šåž‹çš„æŠ€æœ¯ä¸­åšé€‰æ‹©ï¼Œæ¯”å¦‚è¯´åœ¨ JSONã€YAMLã€XMLã€RESTã€gRPC æˆ–è€… SOAP ä¹‹é—´åšé€‰æ‹©ï¼Œå¾ˆå¤§ç¨‹åº¦ä¹Ÿå–å†³äºŽæ‚¨çš„èƒŒæ™¯ã€å–œå¥½ä»¥åŠå…¬å¸çŽ¯å¢ƒã€‚åœ¨ Kubernetes ç”Ÿæ€ç³»ç»Ÿå†…æ¯”è¾ƒå„ç§å·¥å…·å°±æ›´åŠ å›°éš¾ï¼Œå› ä¸ºæŠ€æœ¯å‘å±•å¤ªå¿«ï¼Œå‡ ä¹Žæ¯å‘¨éƒ½æœ‰æ–°å·¥å…·é¢å¸‚ï¼›ä¸¾ä¸ªä¾‹å­ï¼Œä»…åœ¨å‡†å¤‡è¿™ç¯‡åšå®¢çš„æœŸé—´ï¼Œ&lt;a href="https://gitkube.sh/">Gitkube&lt;/a> å’Œ &lt;a href="https://github.com/MinikubeAddon/watchpod">Watchpod&lt;/a> ç›¸ç»§å‡ºå“ã€‚ä¸ºäº†è¿›ä¸€æ­¥è¦†ç›–åˆ°è¿™äº›æ–°çš„ï¼Œä»¥åŠä¸€äº›ç›¸å…³çš„å·²æŽ¨å‡ºçš„å·¥å…·ï¼Œä¾‹å¦‚ &lt;a href="https://github.com/weaveworks/flux">Weave Flux&lt;/a> å’Œ OpenShift çš„ &lt;a href="https://docs.openshift.com/container-platform/3.9/creating_images/s2i.html">S2I&lt;/a>ï¼Œæˆ‘ä»¬è®¡åˆ’å†å†™ä¸€ç¯‡è·Ÿè¿›çš„åšå®¢ã€‚&lt;/p>
&lt;h3 id="draft">Draft&lt;/h3>
&lt;!--
[Draft](https://github.com/Azure/draft) aims to help you get started deploying any app to Kubernetes. It is capable of applying heuristics as to what programming language your app is written in and generates a Dockerfile along with a Helm chart. It then runs the build for you and deploys resulting image to the target cluster via the Helm chart. It also allows user to setup port forwarding to localhost very easily.
-->
&lt;p>&lt;a href="https://github.com/Azure/draft">Draft&lt;/a> æ—¨åœ¨å¸®åŠ©æ‚¨å°†ä»»ä½•åº”ç”¨ç¨‹åºéƒ¨ç½²åˆ° Kubernetesã€‚å®ƒèƒ½å¤Ÿæ£€æµ‹åˆ°æ‚¨çš„åº”ç”¨æ‰€ä½¿ç”¨çš„ç¼–ç¨‹è¯­è¨€ï¼Œå¹¶ä¸”ç”Ÿæˆä¸€ä»½ Dockerfile å’Œ Helm å›¾è¡¨ã€‚ç„¶åŽå®ƒæ›¿æ‚¨å¯åŠ¨æž„å»ºå¹¶ä¸”ä¾ç…§ Helm å›¾è¡¨æŠŠæ‰€ç”Ÿäº§çš„é•œåƒéƒ¨ç½²åˆ°ç›®æ ‡é›†ç¾¤ã€‚å®ƒä¹Ÿå¯ä»¥è®©æ‚¨å¾ˆå®¹æ˜“åœ°è®¾ç½®åˆ° localhost çš„ç«¯å£æ˜ å°„ã€‚&lt;/p>
&lt;!--
Implications:
-->
&lt;p>è¿™æ„å‘³ç€ï¼š&lt;/p>
&lt;!--
* User can customise the chart and Dockerfile templates however they like, or even create a [custom pack](https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md) (with Dockerfile, the chart and more) for future use
-->
&lt;ul>
&lt;li>ç”¨æˆ·å¯ä»¥ä»»æ„åœ°è‡ªå®šä¹‰ Helm å›¾è¡¨å’Œ Dockerfile æ¨¡ç‰ˆï¼Œæˆ–è€…ç”šè‡³åˆ›å»ºä¸€ä¸ª &lt;a href="https://github.com/Azure/draft/blob/master/docs/reference/dep-003.md">custom pack&lt;/a>ï¼ˆä½¿ç”¨ Dockerfileã€Helm å›¾è¡¨ä»¥åŠå…¶ä»–ï¼‰ä»¥å¤‡åŽç”¨&lt;/li>
&lt;/ul>
&lt;!--
* Itâ€™s not very simple to guess how just any app is supposed to be built, in some cases user may need to tweak Dockerfile and the Helm chart that Draft generates
-->
&lt;ul>
&lt;li>è¦æƒ³ç†è§£ä¸€ä¸ªåº”ç”¨åº”è¯¥æ€Žä¹ˆæž„å»ºå¹¶ä¸å®¹æ˜“ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹ï¼Œç”¨æˆ·ä¹Ÿè®¸éœ€è¦ä¿®æ”¹ Draft ç”Ÿæˆçš„ Dockerfile å’Œ Heml å›¾è¡¨&lt;/li>
&lt;/ul>
&lt;!--
* With [Draft version 0.12.0](https://github.com/Azure/draft/releases/tag/v0.12.0) or older, every time user wants to test a change, they need to wait for Draft to copy the code to the cluster, then run the build, push the image and release updated chart; this can timely, but it results in an image being for every single change made by the user (whether it was committed to git or not)
-->
&lt;ul>
&lt;li>å¦‚æžœä½¿ç”¨ &lt;a href="https://github.com/Azure/draft/releases/tag/v0.12.0">Draft version 0.12.0&lt;/a>&lt;sup>1&lt;/sup> æˆ–è€…æ›´è€ç‰ˆæœ¬ï¼Œæ¯ä¸€æ¬¡ç”¨æˆ·æƒ³è¦æµ‹è¯•ä¸€ä¸ªæ”¹åŠ¨ï¼Œä»–ä»¬éœ€è¦ç­‰ Draft æŠŠä»£ç æ‹·è´åˆ°é›†ç¾¤ï¼Œè¿è¡Œæž„å»ºï¼ŒæŽ¨é€é•œåƒå¹¶ä¸”å‘å¸ƒæ›´æ–°åŽçš„å›¾è¡¨ï¼›è¿™äº›æ­¥éª¤å¯èƒ½è¿›è¡Œå¾—å¾ˆå¿«ï¼Œä½†æ˜¯æ¯ä¸€æ¬¡ç”¨æˆ·çš„æ”¹åŠ¨éƒ½ä¼šäº§ç”Ÿä¸€ä¸ªé•œåƒï¼ˆæ— è®ºæ˜¯å¦æäº¤åˆ° git ï¼‰&lt;/li>
&lt;/ul>
&lt;!--
* As of Draft version 0.12.0, builds are executed locally
* User doesnâ€™t have an option to choose something other than Helm for deployment
* It can watch local changes and trigger deployments, but this feature is not enabled by default
-->
&lt;ul>
&lt;li>åœ¨ Draft 0.12.0ç‰ˆæœ¬ï¼Œæž„å»ºæ˜¯æœ¬åœ°è¿›è¡Œçš„&lt;/li>
&lt;li>ç”¨æˆ·ä¸èƒ½é€‰æ‹© Helm ä»¥å¤–çš„å·¥å…·è¿›è¡Œéƒ¨ç½²&lt;/li>
&lt;li>å®ƒå¯ä»¥ç›‘æŽ§æœ¬åœ°çš„æ”¹åŠ¨å¹¶ä¸”è§¦å‘éƒ¨ç½²ï¼Œä½†æ˜¯è¿™ä¸ªåŠŸèƒ½é»˜è®¤æ˜¯å…³é—­çš„&lt;/li>
&lt;/ul>
&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* Deploying to production is up to the user, Draft authors recommend their other project â€“ Brigade
* Can be used instead of Skaffold, and along the side of Squash
-->
&lt;ul>
&lt;li>å®ƒå…è®¸å¼€å‘äººå‘˜ä½¿ç”¨æœ¬åœ°æˆ–è€…è¿œç¨‹çš„ Kubernates é›†ç¾¤&lt;/li>
&lt;li>å¦‚ä½•éƒ¨ç½²åˆ°ç”Ÿäº§çŽ¯å¢ƒå–å†³äºŽç”¨æˆ·ï¼Œ Draft çš„ä½œè€…æŽ¨èäº†ä»–ä»¬çš„å¦ä¸€ä¸ªé¡¹ç›® - Brigade&lt;/li>
&lt;li>å¯ä»¥ä»£æ›¿ Skaffoldï¼Œ å¹¶ä¸”å¯ä»¥å’Œ Squash ä¸€èµ·ä½¿ç”¨&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>æ›´å¤šä¿¡æ¯ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/blog/2017/05/draft-kubernetes-container-development">Draft: Kubernetes container development made easy&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/Azure/draft/blob/master/docs/getting-started.md">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>ã€1ã€‘ï¼šæ­¤å¤„ç–‘ä¸º 0.11.0ï¼Œå› ä¸º 0.12.0 å·²ç»æ”¯æŒæœ¬åœ°æž„å»ºï¼Œè§ä¸‹ä¸€æ¡&lt;/p>
&lt;h3 id="skaffold">Skaffold&lt;/h3>
&lt;!--
[Skaffold](https://github.com/GoogleCloudPlatform/skaffold) is a tool that aims to provide portability for CI integrations with different build system, image registry and deployment tools. It is different from Draft, yet somewhat comparable. It has a basic capability for generating manifests, but itâ€™s not a prominent feature. Skaffold is extendible and lets user pick tools for use in each of the steps in building and deploying their app.
-->
&lt;p>&lt;a href="https://github.com/GoogleCloudPlatform/skaffold">Skaffold&lt;/a> è®© CI é›†æˆå…·æœ‰å¯ç§»æ¤æ€§çš„ï¼Œå®ƒå…è®¸ç”¨æˆ·é‡‡ç”¨ä¸åŒçš„æž„å»ºç³»ç»Ÿï¼Œé•œåƒä»“åº“å’Œéƒ¨ç½²å·¥å…·ã€‚å®ƒä¸åŒäºŽ Draftï¼ŒåŒæ—¶ä¹Ÿå…·æœ‰ä¸€å®šçš„å¯æ¯”æ€§ã€‚å®ƒå…·æœ‰ç”Ÿæˆç³»ç»Ÿæ¸…å•çš„åŸºæœ¬èƒ½åŠ›ï¼Œä½†é‚£ä¸æ˜¯ä¸€ä¸ªé‡è¦åŠŸèƒ½ã€‚Skaffold æ˜“äºŽæ‰©å±•ï¼Œå…è®¸ç”¨æˆ·åœ¨æž„å»ºå’Œéƒ¨ç½²åº”ç”¨çš„æ¯ä¸€æ­¥é€‰å–ç›¸åº”çš„å·¥å…·ã€‚&lt;/p>
&lt;!--
Implications:
-->
&lt;p>è¿™æ„å‘³ç€ï¼š&lt;/p>
&lt;!--
* Modular by design
* Works independently of CI vendor, user doesnâ€™t need Docker or Kubernetes plugin
* Works without CI as such, i.e. from the developerâ€™s laptop
* It can watch local changes and trigger deployments
-->
&lt;ul>
&lt;li>æ¨¡å—åŒ–è®¾è®¡&lt;/li>
&lt;li>ä¸ä¾èµ–äºŽ CIï¼Œç”¨æˆ·ä¸éœ€è¦ Docker æˆ–è€… Kubernetes æ’ä»¶&lt;/li>
&lt;li>æ²¡æœ‰ CI ä¹Ÿå¯ä»¥å·¥ä½œï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¯ä»¥åœ¨å¼€å‘äººå‘˜çš„ç”µè„‘ä¸Šå·¥ä½œ&lt;/li>
&lt;li>å®ƒå¯ä»¥ç›‘æŽ§æœ¬åœ°çš„æ”¹åŠ¨å¹¶ä¸”è§¦å‘éƒ¨ç½²&lt;/li>
&lt;/ul>
&lt;!--
* It allows developer to use either local or remote Kubernetes cluster
* It can be used to deploy to production, user can configure how exactly they prefer to do it and provide different kind of pipeline for each target environment
* Can be used instead of Draft, and along the side with most other tools
-->
&lt;ul>
&lt;li>å®ƒå…è®¸å¼€å‘äººå‘˜ä½¿ç”¨æœ¬åœ°æˆ–è€…è¿œç¨‹çš„ Kubernetes é›†ç¾¤&lt;/li>
&lt;li>å®ƒå¯ä»¥ç”¨äºŽéƒ¨ç½²ç”Ÿäº§çŽ¯å¢ƒï¼Œç”¨æˆ·å¯ä»¥ç²¾ç¡®é…ç½®ï¼Œä¹Ÿå¯ä»¥ä¸ºæ¯ä¸€å¥—ç›®æ ‡çŽ¯å¢ƒæä¾›ä¸åŒçš„ç”Ÿäº§çº¿&lt;/li>
&lt;li>å¯ä»¥ä»£æ›¿ Draftï¼Œå¹¶ä¸”å’Œå…¶ä»–å·¥å…·ä¸€èµ·ä½¿ç”¨&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>æ›´å¤šä¿¡æ¯ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://cloudplatform.googleblog.com/2018/03/introducing-Skaffold-Easy-and-repeatable-Kubernetes-development.html">Introducing Skaffold: Easy and repeatable Kubernetes development&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/GoogleCloudPlatform/skaffold#getting-started-with-local-tooling">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="squash">Squash&lt;/h3>
&lt;!--
[Squash](https://github.com/solo-io/squash) consists of a debug server that is fully integrated with Kubernetes, and a IDE plugin. It allows you to insert breakpoints and do all the fun stuff you are used to doing when debugging an application using an IDE. It bridges IDE debugging experience with your Kubernetes cluster by allowing you to attach the debugger to a pod running in your Kubernetes cluster.
-->
&lt;p>&lt;a href="https://github.com/solo-io/squash">Squash&lt;/a> åŒ…å«ä¸€ä¸ªä¸Ž Kubernetes å…¨é¢é›†æˆçš„è°ƒè¯•æœåŠ¡å™¨ï¼Œä»¥åŠä¸€ä¸ª IDE æ’ä»¶ã€‚å®ƒå…è®¸æ‚¨æ’å…¥æ–­ç‚¹å’Œæ‰€æœ‰çš„è°ƒè¯•æ“ä½œï¼Œå°±åƒæ‚¨æ‰€ä¹ æƒ¯çš„ä½¿ç”¨ IDE è°ƒè¯•ä¸€ä¸ªç¨‹åºä¸€èˆ¬ã€‚å®ƒå…è®¸æ‚¨å°†è°ƒè¯•å™¨åº”ç”¨åˆ° Kubernetes é›†ç¾¤ä¸­è¿è¡Œçš„ pod ä¸Šï¼Œä»Žè€Œè®©æ‚¨å¯ä»¥ä½¿ç”¨ IDE è°ƒè¯• Kubernetes é›†ç¾¤ã€‚&lt;/p>
&lt;!--
Implications:
-->
&lt;p>è¿™æ„å‘³ç€ï¼š&lt;/p>
&lt;!--
* Can be used independently of other tools you chose
* Requires a privileged DaemonSet
* Integrates with popular IDEs
* Supports Go, Python, Node.js, Java and gdb
-->
&lt;ul>
&lt;li>ä¸ä¾èµ–æ‚¨é€‰æ‹©çš„å…¶å®ƒå·¥å…·&lt;/li>
&lt;li>éœ€è¦ä¸€ç»„ç‰¹æƒ DaemonSet&lt;/li>
&lt;li>å¯ä»¥å’Œæµè¡Œ IDE é›†æˆ&lt;/li>
&lt;li>æ”¯æŒ Goã€Pythonã€Node.jsã€Java å’Œ gdb&lt;/li>
&lt;/ul>
&lt;!--
* User must ensure application binaries inside the container image are compiled with debug symbols
* Can be used in combination with any other tools described here
* It can be used with either local or remote Kubernetes cluster
-->
&lt;ul>
&lt;li>ç”¨æˆ·å¿…é¡»ç¡®ä¿å®¹å™¨ä¸­çš„åº”ç”¨ç¨‹åºä½¿ç¼–è¯‘æ—¶ä½¿ç”¨äº†è°ƒè¯•ç¬¦å·&lt;/li>
&lt;li>å¯ä¸Žæ­¤å¤„æè¿°çš„ä»»ä½•å…¶ä»–å·¥å…·ç»“åˆä½¿ç”¨&lt;/li>
&lt;li>å®ƒå¯ä»¥ä¸Žæœ¬åœ°æˆ–è¿œç¨‹ Kubernetes é›†ç¾¤ä¸€èµ·ä½¿ç”¨&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>æ›´å¤šä¿¡æ¯ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=5TrV3qzXlgI">Squash: A Debugger for Kubernetes Apps&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/solo-io/squash/blob/master/docs/getting-started.md">Getting Started Guide&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="telepresence">Telepresence&lt;/h3>
&lt;!--
[Telepresence](https://www.telepresence.io/) connects containers running on developerâ€™s workstation with a remote Kubernetes cluster using a two-way proxy and emulates in-cluster environment as well as provides access to config maps and secrets. It aims to improve iteration time for container app development by eliminating the need for deploying app to the cluster and leverages local container to abstract network and filesystem interface in order to make it appear as if the app was running in the cluster.
-->
&lt;p>&lt;a href="https://www.telepresence.io/">Telepresence&lt;/a> ä½¿ç”¨åŒå‘ä»£ç†å°†å¼€å‘äººå‘˜å·¥ä½œç«™ä¸Šè¿è¡Œçš„å®¹å™¨ä¸Žè¿œç¨‹ Kubernetes é›†ç¾¤è¿žæŽ¥èµ·æ¥ï¼Œå¹¶æ¨¡æ‹Ÿé›†ç¾¤å†…çŽ¯å¢ƒä»¥åŠæä¾›å¯¹é…ç½®æ˜ å°„å’Œæœºå¯†çš„è®¿é—®ã€‚å®ƒæ¶ˆé™¤äº†å°†åº”ç”¨éƒ¨ç½²åˆ°é›†ç¾¤çš„éœ€è¦ï¼Œå¹¶åˆ©ç”¨æœ¬åœ°å®¹å™¨æŠ½è±¡å‡ºç½‘ç»œå’Œæ–‡ä»¶ç³»ç»ŸæŽ¥å£ï¼Œä»¥ä½¿å…¶çœ‹èµ·æ¥åº”ç”¨å¥½åƒå°±åœ¨é›†ç¾¤ä¸­è¿è¡Œï¼Œä»Žè€Œæ”¹è¿›å®¹å™¨åº”ç”¨ç¨‹åºå¼€å‘çš„è¿­ä»£æ—¶é—´ã€‚&lt;/p>
&lt;!--
Implications:
-->
&lt;p>è¿™æ„å‘³ç€ï¼š&lt;/p>
&lt;!--
* It can be used independently of other tools you chose
* Using together with Squash is possible, although Squash would have to be used for pods in the cluster, while conventional/local debugger would need to be used for debugging local container thatâ€™s connected to the cluster via Telepresence
* Telepresence imposes some network latency
-->
&lt;ul>
&lt;li>å®ƒä¸ä¾èµ–äºŽå…¶å®ƒæ‚¨é€‰å–çš„å·¥å…·&lt;/li>
&lt;li>å¯ä»¥åŒ Squash ä¸€èµ·ä½¿ç”¨ï¼Œä½†æ˜¯ Squash å¿…é¡»ç”¨äºŽè°ƒè¯•é›†ç¾¤ä¸­çš„ podsï¼Œè€Œä¼ ç»Ÿ/æœ¬åœ°è°ƒè¯•å™¨éœ€è¦ç”¨äºŽè°ƒè¯•é€šè¿‡ Telepresence è¿žæŽ¥åˆ°é›†ç¾¤çš„æœ¬åœ°å®¹å™¨&lt;/li>
&lt;li>Telepresence ä¼šäº§ç”Ÿä¸€äº›ç½‘ç»œå»¶è¿Ÿ&lt;/li>
&lt;/ul>
&lt;!--
* It provides connectivity via a side-car process - sshuttle, which is based on SSH
* More intrusive dependency injection mode with LD_PRELOAD/DYLD_INSERT_LIBRARIES is also available
* It is most commonly used with a remote Kubernetes cluster, but can be used with a local one also
-->
&lt;ul>
&lt;li>å®ƒé€šè¿‡è¾…åŠ©è¿›ç¨‹æä¾›è¿žæŽ¥ - sshuttleï¼ŒåŸºäºŽSSHçš„ä¸€ä¸ªå·¥å…·&lt;/li>
&lt;li>è¿˜æä¾›äº†ä½¿ç”¨ LD_PRELOAD/DYLD_INSERT_LIBRARIES çš„æ›´å…·ä¾µå…¥æ€§çš„ä¾èµ–æ³¨å…¥æ¨¡å¼&lt;/li>
&lt;li>å®ƒæœ€å¸¸ç”¨äºŽè¿œç¨‹ Kubernetes é›†ç¾¤ï¼Œä½†ä¹Ÿå¯ä»¥ä¸Žæœ¬åœ°é›†ç¾¤ä¸€èµ·ä½¿ç”¨&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>æ›´å¤šä¿¡æ¯ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://www.telepresence.io/">Telepresence: fast, realistic local development for Kubernetes microservices&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.telepresence.io/tutorials/docker">Getting Started Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.telepresence.io/discussion/how-it-works">How It Works&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="ksync">Ksync&lt;/h3>
&lt;!--
[Ksync](https://github.com/vapor-ware/ksync) synchronizes application code (and configuration) between your local machine and the container running in Kubernetes, akin to what [oc rsync](https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html) does in OpenShift. It aims to improve iteration time for app development by eliminating build and deployment steps.
-->
&lt;p>&lt;a href="https://github.com/vapor-ware/ksync">Ksync&lt;/a> åœ¨æœ¬åœ°è®¡ç®—æœºå’Œè¿è¡Œåœ¨ Kubernetes ä¸­çš„å®¹å™¨ä¹‹é—´åŒæ­¥åº”ç”¨ç¨‹åºä»£ç ï¼ˆå’Œé…ç½®ï¼‰ï¼Œç±»ä¼¼äºŽ &lt;a href="https://docs.openshift.com/container-platform/3.9/dev_guide/copy_files_to_container.html">oc rsync&lt;/a> åœ¨ OpenShift ä¸­çš„è§’è‰²ã€‚å®ƒæ—¨åœ¨é€šè¿‡æ¶ˆé™¤æž„å»ºå’Œéƒ¨ç½²æ­¥éª¤æ¥ç¼©çŸ­åº”ç”¨ç¨‹åºå¼€å‘çš„è¿­ä»£æ—¶é—´ã€‚&lt;/p>
&lt;!--
Implications:
-->
&lt;p>è¿™æ„å‘³ç€ï¼š&lt;/p>
&lt;!--
* It bypasses container image build and revision control
* Compiled language users have to run builds inside the pod (TBC)
* Two-way sync â€“ remote files are copied to local directory
* Container is restarted each time remote filesystem is updated
* No security features â€“ development only
-->
&lt;ul>
&lt;li>å®ƒç»•è¿‡å®¹å™¨å›¾åƒæž„å»ºå’Œä¿®è®¢æŽ§åˆ¶&lt;/li>
&lt;li>ä½¿ç”¨ç¼–è¯‘è¯­è¨€çš„ç”¨æˆ·å¿…é¡»åœ¨ podï¼ˆTBCï¼‰å†…è¿è¡Œæž„å»º&lt;/li>
&lt;li>åŒå‘åŒæ­¥ - è¿œç¨‹æ–‡ä»¶ä¼šå¤åˆ¶åˆ°æœ¬åœ°ç›®å½•&lt;/li>
&lt;li>æ¯æ¬¡æ›´æ–°è¿œç¨‹æ–‡ä»¶ç³»ç»Ÿæ—¶éƒ½ä¼šé‡å¯å®¹å™¨&lt;/li>
&lt;li>æ— å®‰å…¨åŠŸèƒ½ - ä»…é™å¼€å‘&lt;/li>
&lt;/ul>
&lt;!--
* Utilizes [Syncthing](https://github.com/syncthing/syncthing), a Go library for peer-to-peer sync
* Requires a privileged DaemonSet running in the cluster
* Node has to use Docker with overlayfs2 â€“ no other CRI implementations are supported at the time of writing
-->
&lt;ul>
&lt;li>ä½¿ç”¨ &lt;a href="https://github.com/syncthing/syncthing">Syncthing&lt;/a>ï¼Œä¸€ä¸ªç”¨äºŽç‚¹å¯¹ç‚¹åŒæ­¥çš„ Go è¯­è¨€åº“&lt;/li>
&lt;li>éœ€è¦ä¸€ä¸ªåœ¨é›†ç¾¤ä¸­è¿è¡Œçš„ç‰¹æƒ DaemonSet&lt;/li>
&lt;li>Node å¿…é¡»ä½¿ç”¨å¸¦æœ‰ overlayfs2 çš„ Docker - åœ¨å†™ä½œæœ¬æ–‡æ—¶ï¼Œå°šä¸æ”¯æŒå…¶ä»– CRI å®žçŽ°&lt;/li>
&lt;/ul>
&lt;!--
More info:
-->
&lt;p>æ›´å¤šä¿¡æ¯ï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/vapor-ware/ksync#getting-started">Getting Started Guide&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/vapor-ware/ksync/blob/master/docs/architecture.md">How It Works&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.katacoda.com/vaporio/scenarios/ksync">Katacoda scenario to try out ksync in your browser&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://docs.syncthing.net/specs/">Syncthing Specification&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
## Hands-on walkthroughs
-->
&lt;h2 id="å®žè·µæ¼”ç»ƒ">å®žè·µæ¼”ç»ƒ&lt;/h2>
&lt;!--
The app we will be using for the hands-on walkthroughs of the tools in the following is a simple [stock market simulator](https://github.com/kubernauts/dok-example-us), consisting of two microservices:
-->
&lt;p>æˆ‘ä»¬æŽ¥ä¸‹æ¥ç”¨äºŽç»ƒä¹ ä½¿ç”¨å·¥å…·çš„åº”ç”¨æ˜¯ä¸€ä¸ªç®€å•çš„&lt;a href="https://github.com/kubernauts/dok-example-us">è‚¡å¸‚æ¨¡æ‹Ÿå™¨&lt;/a>ï¼ŒåŒ…å«ä¸¤ä¸ªå¾®æœåŠ¡ï¼š&lt;/p>
&lt;!--
* The `stock-gen` microservice is written in Go and generates stock data randomly and exposes it via HTTP endpoint `/stockdata`.
â€Ž* A second microservice, `stock-con` is a Node.js app that consumes the stream of stock data from `stock-gen` and provides an aggregation in form of a moving average via the HTTP endpoint `/average/$SYMBOL` as well as a health-check endpoint at `/healthz`.
-->
&lt;ul>
&lt;li>&lt;code>stock-gen&lt;/code>ï¼ˆè‚¡å¸‚æ•°æ®ç”Ÿæˆå™¨ï¼‰å¾®æœåŠ¡æ˜¯ç”¨ Go ç¼–å†™çš„ï¼Œéšæœºç”Ÿæˆè‚¡ç¥¨æ•°æ®å¹¶é€šè¿‡ HTTP ç«¯ç‚¹ &lt;code>/ stockdata&lt;/code> å…¬å¼€&lt;/li>
&lt;li>ç¬¬äºŒä¸ªå¾®æœåŠ¡ï¼Œ&lt;code>stock-con&lt;/code>ï¼ˆè‚¡å¸‚æ•°æ®æ¶ˆè´¹è€…ï¼‰æ˜¯ä¸€ä¸ª Node.js åº”ç”¨ç¨‹åºï¼Œå®ƒä½¿ç”¨æ¥è‡ª &lt;code>stock-gen&lt;/code> çš„è‚¡ç¥¨æ•°æ®æµï¼Œå¹¶é€šè¿‡ HTTP ç«¯ç‚¹ &lt;code>/average/$SYMBOL&lt;/code> æä¾›è‚¡ä»·ç§»åŠ¨å¹³å‡çº¿ï¼Œä¹Ÿæä¾›ä¸€ä¸ªå¥åº·æ£€æŸ¥ç«¯ç‚¹ &lt;code>/healthz&lt;/code>ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Overall, the default setup of the app looks as follows:
-->
&lt;p>æ€»ä½“ä¸Šï¼Œæ­¤åº”ç”¨çš„é»˜è®¤é…ç½®å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-architecture_preview.png" alt="Default Setup">&lt;/p>
&lt;!--
In the following weâ€™ll do a hands-on walkthrough for a representative selection of tools discussed above: ksync, Minikube with local build, as well as Skaffold. For each of the tools we do the following:
-->
&lt;p>åœ¨ä¸‹æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†é€‰å–ä»¥ä¸Šè®¨è®ºçš„ä»£è¡¨æ€§å·¥å…·è¿›è¡Œå®žè·µæ¼”ç»ƒï¼šksyncï¼Œå…·æœ‰æœ¬åœ°æž„å»ºçš„ Minikube ä»¥åŠ Skaffoldã€‚å¯¹äºŽæ¯ä¸ªå·¥å…·ï¼Œæˆ‘ä»¬æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š&lt;/p>
&lt;!--
* Set up the respective tool incl. preparations for the deployment and local consumption of the `stock-con` microservice.
* Perform a code update, that is, change the source code of the `/healthz` endpoint in the `stock-con` microservice and observe the updates.
-->
&lt;ul>
&lt;li>è®¾ç½®ç›¸åº”çš„å·¥å…·ï¼ŒåŒ…æ‹¬éƒ¨ç½²å‡†å¤‡å’Œ &lt;code>stock-con&lt;/code> å¾®æœåŠ¡æ•°æ®çš„æœ¬åœ°è¯»å–&lt;/li>
&lt;li>æ‰§è¡Œä»£ç æ›´æ–°ï¼Œå³æ›´æ”¹ &lt;code>stock-con&lt;/code> å¾®æœåŠ¡çš„ &lt;code>/healthz&lt;/code> ç«¯ç‚¹çš„æºä»£ç å¹¶è§‚å¯Ÿç½‘é¡µåˆ·æ–°&lt;/li>
&lt;/ul>
&lt;!--
Note that for the target Kubernetes cluster weâ€™ve been using Minikube locally, but you can also a remote cluster for ksync and Skaffold if you want to follow along.
-->
&lt;p>è¯·æ³¨æ„ï¼Œæˆ‘ä»¬ä¸€ç›´ä½¿ç”¨ Minikube çš„æœ¬åœ° Kubernetes é›†ç¾¤ï¼Œä½†æ˜¯æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ ksync å’Œ Skaffold çš„è¿œç¨‹é›†ç¾¤è·Ÿéšç»ƒä¹ ã€‚&lt;/p>
&lt;!--
### Walkthrough: ksync
-->
&lt;h3 id="å®žè·µæ¼”ç»ƒ-ksync">å®žè·µæ¼”ç»ƒï¼šksync&lt;/h3>
&lt;!--
As a preparation, install [ksync](https://vapor-ware.github.io/ksync/#installation) and then carry out the following steps to prepare the development setup:
-->
&lt;p>ä½œä¸ºå‡†å¤‡ï¼Œå®‰è£… &lt;a href="https://vapor-ware.github.io/ksync/#installation">ksync&lt;/a>ï¼Œç„¶åŽæ‰§è¡Œä»¥ä¸‹æ­¥éª¤é…ç½®å¼€å‘çŽ¯å¢ƒï¼š&lt;/p>
&lt;pre>&lt;code>$ mkdir -p $(pwd)/ksync
$ kubectl create namespace dok
$ ksync init -n dok
&lt;/code>&lt;/pre>&lt;!--
With the basic setup completed we're ready to tell ksyncâ€™s local client to watch a certain Kubernetes namespace and then we create a spec to define what we want to sync (the directory `$(pwd)/ksync` locally with `/app` in the container). Note that target pod is specified via the selector parameter:
-->
&lt;p>å®ŒæˆåŸºæœ¬è®¾ç½®åŽï¼Œæˆ‘ä»¬å¯ä»¥å‘Šè¯‰ ksync çš„æœ¬åœ°å®¢æˆ·ç«¯ç›‘æŽ§ Kubernetes çš„æŸä¸ªå‘½åç©ºé—´ï¼Œç„¶åŽæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªè§„èŒƒæ¥å®šä¹‰æˆ‘ä»¬æƒ³è¦åŒæ­¥çš„æ–‡ä»¶å¤¹ï¼ˆæœ¬åœ°çš„ &lt;code>$(pwd)/ksync&lt;/code> å’Œå®¹å™¨ä¸­çš„ &lt;code>/ app&lt;/code> ï¼‰ã€‚è¯·æ³¨æ„ï¼Œç›®æ ‡ pod æ˜¯ç”¨ selector å‚æ•°æŒ‡å®šï¼š&lt;/p>
&lt;pre>&lt;code>$ ksync watch -n dok
$ ksync create -n dok --selector=app=stock-con $(pwd)/ksync /app
$ ksync get -n dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
-->
&lt;p>çŽ°åœ¨æˆ‘ä»¬éƒ¨ç½²è‚¡ä»·æ•°æ®ç”Ÿæˆå™¨å’Œè‚¡ä»·æ•°æ®æ¶ˆè´¹è€…å¾®æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply \
-f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-gen/app.yaml
$ kubectl -n=dok apply \
-f https://raw.githubusercontent.com/kubernauts/dok-example-us/master/stock-con/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
-->
&lt;p>ä¸€æ—¦ä¸¤ä¸ªéƒ¨ç½²å»ºå¥½å¹¶ä¸” pod å¼€å§‹è¿è¡Œï¼Œæˆ‘ä»¬è½¬å‘ &lt;code>stock-con&lt;/code> æœåŠ¡ä»¥ä¾›æœ¬åœ°è¯»å–ï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯çª—å£ï¼‰ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl get -n dok po --selector=app=stock-con \
-o=custom-columns=:metadata.name --no-headers | \
xargs -IPOD kubectl -n dok port-forward POD 9898:9898
&lt;/code>&lt;/pre>&lt;!--
With that we should be able to consume the `stock-con` service from our local machine; we do this by regularly checking the response of the `healthz` endpoint like so (in a separate terminal session):
-->
&lt;p>è¿™æ ·ï¼Œé€šè¿‡å®šæœŸæŸ¥è¯¢ &lt;code>healthz&lt;/code> ç«¯ç‚¹ï¼Œæˆ‘ä»¬å°±åº”è¯¥èƒ½å¤Ÿä»Žæœ¬åœ°æœºå™¨ä¸Šè¯»å– &lt;code>stock-con&lt;/code> æœåŠ¡ï¼ŒæŸ¥è¯¢å‘½ä»¤å¦‚ä¸‹ï¼ˆåœ¨ä¸€ä¸ªå•ç‹¬çš„ç»ˆç«¯çª—å£ï¼‰ï¼š&lt;/p>
&lt;pre>&lt;code>$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;!--
Now change the code in the `ksync/stock-con`directory, for example update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response and observe how the pod gets updated and the response of the `curl localhost:9898/healthz` command changes. Overall you should have something like the following in the end:
-->
&lt;p>çŽ°åœ¨ï¼Œæ”¹åŠ¨ &lt;code>ksync/stock-con&lt;/code> ç›®å½•ä¸­çš„ä»£ç ï¼Œä¾‹å¦‚æ”¹åŠ¨ &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> ä¸­å®šä¹‰çš„ &lt;code>/healthz&lt;/code> ç«¯ç‚¹ä»£ç &lt;/a>ï¼Œåœ¨å…¶ JSON å½¢å¼çš„å“åº”ä¸­æ–°æ·»ä¸€ä¸ªå­—æ®µå¹¶è§‚å¯Ÿ pod å¦‚ä½•æ›´æ–°ä»¥åŠ &lt;code>curl localhostï¼š9898/healthz&lt;/code> å‘½ä»¤çš„è¾“å‡ºå‘ç”Ÿä½•ç§å˜åŒ–ã€‚æ€»çš„æ¥è¯´ï¼Œæ‚¨æœ€åŽåº”è¯¥çœ‹åˆ°ç±»ä¼¼çš„å†…å®¹ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-ksync_preview.png" alt="Preview">&lt;/p>
&lt;!--
### Walkthrough: Minikube with local build
-->
&lt;h3 id="å®žè·µæ¼”ç»ƒ-å¸¦æœ¬åœ°æž„å»ºçš„-minikube">å®žè·µæ¼”ç»ƒï¼šå¸¦æœ¬åœ°æž„å»ºçš„ Minikube&lt;/h3>
&lt;!--
For the following you will need to have Minikube up and running and we will leverage the Minikube-internal Docker daemon for building images, locally. As a preparation, do the following
-->
&lt;p>å¯¹äºŽä»¥ä¸‹å†…å®¹ï¼Œæ‚¨éœ€è¦å¯åŠ¨å¹¶è¿è¡Œ Minikubeï¼Œæˆ‘ä»¬å°†åˆ©ç”¨ Minikube è‡ªå¸¦çš„ Docker daemon åœ¨æœ¬åœ°æž„å»ºé•œåƒã€‚ä½œä¸ºå‡†å¤‡ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹æ“ä½œ&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ eval $(minikube docker-env)
$ kubectl create namespace dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator and the stock consumer microservice:
-->
&lt;p>çŽ°åœ¨æˆ‘ä»¬éƒ¨ç½²è‚¡ä»·æ•°æ®ç”Ÿæˆå™¨å’Œè‚¡ä»·æ•°æ®æ¶ˆè´¹è€…å¾®æœåŠ¡ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply -f stock-gen/app.yaml
$ kubectl -n=dok apply -f stock-con/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Once both deployments are created and the pods are running, we forward the `stock-con` service for local consumption (in a separate terminal session):
-->
&lt;p>ä¸€æ—¦ä¸¤ä¸ªéƒ¨ç½²å»ºå¥½å¹¶ä¸” pod å¼€å§‹è¿è¡Œï¼Œæˆ‘ä»¬è½¬å‘ &lt;code>stock-con&lt;/code> æœåŠ¡ä»¥ä¾›æœ¬åœ°è¯»å–ï¼ˆå¦å¼€ä¸€ä¸ªç»ˆç«¯çª—å£ï¼‰ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl get -n dok po --selector=app=stock-con \
-o=custom-columns=:metadata.name --no-headers | \
xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;!--
Now change the code in the `stock-con`directory, for example, update the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response. Once youâ€™re done with your code update, the last step is to build a new container image and kick off a new deployment like shown below:
-->
&lt;p>çŽ°åœ¨ï¼Œæ”¹ä¸€ä¸‹ &lt;code>ksync/stock-con&lt;/code> ç›®å½•ä¸­çš„ä»£ç ï¼Œä¾‹å¦‚ä¿®æ”¹ &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> ä¸­å®šä¹‰çš„ &lt;code>/healthz&lt;/code> ç«¯ç‚¹ä»£ç &lt;/a>ï¼Œåœ¨å…¶ JSON å½¢å¼çš„å“åº”ä¸­æ·»åŠ ä¸€ä¸ªå­—æ®µã€‚åœ¨æ‚¨æ›´æ–°å®Œä»£ç åŽï¼Œæœ€åŽä¸€æ­¥æ˜¯æž„å»ºæ–°çš„å®¹å™¨é•œåƒå¹¶å¯åŠ¨æ–°éƒ¨ç½²ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/p>
&lt;pre>&lt;code>$ docker build -t stock-con:dev -f Dockerfile .
$ kubectl -n dok set image deployment/stock-con *=stock-con:dev
&lt;/code>&lt;/pre>&lt;!--
Overall you should have something like the following in the end:
-->
&lt;p>æ€»çš„æ¥è¯´ï¼Œæ‚¨æœ€åŽåº”è¯¥çœ‹åˆ°ç±»ä¼¼çš„å†…å®¹ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-minikube-localdev_preview.png" alt="Local Preview">&lt;/p>
&lt;!--
### Walkthrough: Skaffold
-->
&lt;h3 id="å®žè·µæ¼”ç»ƒ-skaffold">å®žè·µæ¼”ç»ƒï¼šSkaffold&lt;/h3>
&lt;!--
To perform this walkthrough you first need to install [Skaffold](https://github.com/GoogleContainerTools/skaffold#installation). Once that is done, you can do the following steps to prepare the development setup:
-->
&lt;p>è¦è¿›è¡Œæ­¤æ¼”ç»ƒï¼Œé¦–å…ˆéœ€è¦å®‰è£… &lt;a href="https://github.com/GoogleContainerTools/skaffold#installation">Skaffold&lt;/a>ã€‚å®ŒæˆåŽï¼Œæ‚¨å¯ä»¥æ‰§è¡Œä»¥ä¸‹æ­¥éª¤æ¥é…ç½®å¼€å‘çŽ¯å¢ƒï¼š&lt;/p>
&lt;pre>&lt;code>$ git clone https://github.com/kubernauts/dok-example-us.git &amp;amp;&amp;amp; cd dok-example-us
$ kubectl create namespace dok
&lt;/code>&lt;/pre>&lt;!--
Now we deploy the stock generator (but not the stock consumer microservice, that is done via Skaffold):
-->
&lt;p>çŽ°åœ¨æˆ‘ä»¬éƒ¨ç½²è‚¡ä»·æ•°æ®ç”Ÿæˆå™¨ï¼ˆä½†æ˜¯æš‚ä¸éƒ¨ç½²è‚¡ä»·æ•°æ®æ¶ˆè´¹è€…ï¼Œæ­¤æœåŠ¡å°†ä½¿ç”¨ Skaffold å®Œæˆï¼‰ï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl -n=dok apply -f stock-gen/app.yaml
&lt;/code>&lt;/pre>&lt;!--
Note that initially we experienced an authentication error when doing `skaffold dev` and needed to apply a fix as described in [Issue 322](https://github.com/GoogleContainerTools/skaffold/issues/322). Essentially it means changing the content of `~/.docker/config.json` to:
-->
&lt;p>è¯·æ³¨æ„ï¼Œæœ€åˆæˆ‘ä»¬åœ¨æ‰§è¡Œ &lt;code>skaffold dev&lt;/code> æ—¶å‘ç”Ÿèº«ä»½éªŒè¯é”™è¯¯ï¼Œä¸ºé¿å…æ­¤é”™è¯¯éœ€è¦å®‰è£…&lt;a href="https://github.com/GoogleContainerTools/skaffold/issues/322">é—®é¢˜322&lt;/a> ä¸­æ‰€è¿°çš„ä¿®å¤ã€‚æœ¬è´¨ä¸Šï¼Œéœ€è¦å°† &lt;code>ã€œ/.docker/config.json&lt;/code> çš„å†…å®¹æ”¹ä¸ºï¼š&lt;/p>
&lt;pre>&lt;code>{
&amp;quot;auths&amp;quot;: {}
}
&lt;/code>&lt;/pre>&lt;!--
Next, we had to patch `stock-con/app.yaml` slightly to make it work with Skaffold:
-->
&lt;p>æŽ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦ç•¥å¾®æ”¹åŠ¨ &lt;code>stock-con/app.yaml&lt;/code>ï¼Œè¿™æ · Skaffold æ‰èƒ½æ­£å¸¸ä½¿ç”¨æ­¤æ–‡ä»¶ï¼š&lt;/p>
&lt;!--
Add a `namespace` field to both the `stock-con` deployment and the service with the value of `dok`.
Change the `image` field of the container spec to `quay.io/mhausenblas/stock-con` since Skaffold manages the container image tag on the fly.
-->
&lt;p>åœ¨ &lt;code>stock-con&lt;/code> éƒ¨ç½²å’ŒæœåŠ¡ä¸­æ·»åŠ ä¸€ä¸ª &lt;code>namespace&lt;/code> å­—æ®µï¼Œå…¶å€¼ä¸º &lt;code>dok&lt;/code>&lt;/p>
&lt;p>å°†å®¹å™¨è§„èŒƒçš„ &lt;code>image&lt;/code> å­—æ®µæ›´æ”¹ä¸º &lt;code>quay.io/mhausenblas/stock-con&lt;/code>ï¼Œå› ä¸º Skaffold å¯ä»¥å³æ—¶ç®¡ç†å®¹å™¨é•œåƒæ ‡ç­¾ã€‚&lt;/p>
&lt;!--
The resulting `app.yaml` file stock-con looks as follows:
-->
&lt;p>æœ€ç»ˆçš„ stock-con çš„ &lt;code>app.yaml&lt;/code> æ–‡ä»¶çœ‹èµ·æ¥å¦‚ä¸‹ï¼š&lt;/p>
&lt;pre>&lt;code>apiVersion: apps/v1beta1
kind: Deployment
metadata:
labels:
app: stock-con
name: stock-con
namespace: dok
spec:
replicas: 1
template:
metadata:
labels:
app: stock-con
spec:
containers:
- name: stock-con
image: quay.io/mhausenblas/stock-con
env:
- name: DOK_STOCKGEN_HOSTNAME
value: stock-gen
- name: DOK_STOCKGEN_PORT
value: &amp;quot;9999&amp;quot;
ports:
- containerPort: 9898
protocol: TCP
livenessProbe:
initialDelaySeconds: 2
periodSeconds: 5
httpGet:
path: /healthz
port: 9898
readinessProbe:
initialDelaySeconds: 2
periodSeconds: 5
httpGet:
path: /healthz
port: 9898
---
apiVersion: v1
kind: Service
metadata:
labels:
app: stock-con
name: stock-con
namespace: dok
spec:
type: ClusterIP
ports:
- name: http
port: 80
protocol: TCP
targetPort: 9898
selector:
app: stock-con
&lt;/code>&lt;/pre>&lt;!--
The final step before we can start development is to configure Skaffold. So, create a file `skaffold.yaml` in the `stock-con/` directory with the following content:
-->
&lt;p>æˆ‘ä»¬èƒ½å¤Ÿå¼€å§‹å¼€å‘ä¹‹å‰çš„æœ€åŽä¸€æ­¥æ˜¯é…ç½® Skaffoldã€‚å› æ­¤ï¼Œåœ¨ &lt;code>stock-con/&lt;/code> ç›®å½•ä¸­åˆ›å»ºæ–‡ä»¶ &lt;code>skaffold.yaml&lt;/code>ï¼Œå…¶ä¸­åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š&lt;/p>
&lt;pre>&lt;code>apiVersion: skaffold/v1alpha2
kind: Config
build:
artifacts:
- imageName: quay.io/mhausenblas/stock-con
workspace: .
docker: {}
local: {}
deploy:
kubectl:
manifests:
- app.yaml
&lt;/code>&lt;/pre>&lt;!--
Now weâ€™re ready to kick off the development. For that execute the following in the `stock-con/` directory:
-->
&lt;p>çŽ°åœ¨æˆ‘ä»¬å‡†å¤‡å¥½å¼€å§‹å¼€å‘äº†ã€‚ä¸ºæ­¤ï¼Œåœ¨ &lt;code>stock-con/&lt;/code> ç›®å½•ä¸­æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š&lt;/p>
&lt;pre>&lt;code>$ skaffold dev
&lt;/code>&lt;/pre>&lt;!--
Above command triggers a build of the `stock-con` image and then a deployment. Once the pod of the `stock-con` deployment is running, we again forward the `stock-con` service for local consumption (in a separate terminal session) and check the response of the `healthz` endpoint:
-->
&lt;p>ä¸Šé¢çš„å‘½ä»¤å°†è§¦å‘ &lt;code>stock-con&lt;/code> å›¾åƒçš„æž„å»ºå’Œéƒ¨ç½²ã€‚ä¸€æ—¦ &lt;code>stock-con&lt;/code> éƒ¨ç½²çš„ pod å¼€å§‹è¿è¡Œï¼Œæˆ‘ä»¬å†æ¬¡è½¬å‘ &lt;code>stock-con&lt;/code> æœåŠ¡ä»¥ä¾›æœ¬åœ°è¯»å–ï¼ˆåœ¨å•ç‹¬çš„ç»ˆç«¯çª—å£ä¸­ï¼‰å¹¶æ£€æŸ¥ &lt;code>healthz&lt;/code> ç«¯ç‚¹çš„å“åº”ï¼š&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl get -n dok po --selector&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b8860b">app&lt;/span>&lt;span style="color:#666">=&lt;/span>stock-con &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> -o&lt;span style="color:#666">=&lt;/span>custom-columns&lt;span style="color:#666">=&lt;/span>:metadata.name --no-headers | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> xargs -IPOD kubectl -n dok port-forward POD 9898:9898 &amp;amp;
$ watch curl localhost:9898/healthz
&lt;/code>&lt;/pre>&lt;/div>&lt;!--
If you now change the code in the `stock-con`directory, for example, by updating the [`/healthz` endpoint code in `service.js`](https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52) by adding a field to the JSON response, you should see Skaffold noticing the change and create a new image as well as deploy it. The resulting screen would look something like this:
-->
&lt;p>çŽ°åœ¨ï¼Œå¦‚æžœæ‚¨ä¿®æ”¹ä¸€ä¸‹ &lt;code>stock-con&lt;/code> ç›®å½•ä¸­çš„ä»£ç ï¼Œä¾‹å¦‚ &lt;a href="https://github.com/kubernauts/dok-example-us/blob/2334ee8fb11f8813370122bd46285cf45bdd4c48/stock-con/service.js#L52">&lt;code>service.js&lt;/code> ä¸­å®šä¹‰çš„ &lt;code>/healthz&lt;/code> ç«¯ç‚¹ä»£ç &lt;/a>ï¼Œåœ¨å…¶ JSON å½¢å¼çš„å“åº”ä¸­æ·»åŠ ä¸€ä¸ªå­—æ®µï¼Œæ‚¨åº”è¯¥çœ‹åˆ° Skaffold å¯ä»¥æ£€æµ‹åˆ°ä»£ç æ”¹åŠ¨å¹¶åˆ›å»ºæ–°å›¾åƒä»¥åŠéƒ¨ç½²å®ƒã€‚æ‚¨çš„å±å¹•çœ‹èµ·æ¥åº”è¯¥ç±»ä¼¼è¿™æ ·ï¼š&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2018-05-01-developing-on-kubernetes/dok-skaffold_preview.png" alt="Skaffold Preview">&lt;/p>
&lt;!--
By now you should have a feeling how different tools enable you to develop apps on Kubernetes and if youâ€™re interested to learn more about tools and or methods, check out the following resources:
-->
&lt;p>è‡³æ­¤ï¼Œæ‚¨åº”è¯¥å¯¹ä¸åŒçš„å·¥å…·å¦‚ä½•å¸®æ‚¨åœ¨ Kubernetes ä¸Šå¼€å‘åº”ç”¨ç¨‹åºæœ‰äº†ä¸€å®šçš„æ¦‚å¿µï¼Œå¦‚æžœæ‚¨æœ‰å…´è¶£äº†è§£æœ‰å…³å·¥å…·å’Œ/æˆ–æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ä»¥ä¸‹èµ„æºï¼š&lt;/p>
&lt;ul>
&lt;li>Blog post by Shahidh K Muhammed on &lt;a href="https://blog.hasura.io/draft-vs-gitkube-vs-helm-vs-ksonnet-vs-metaparticle-vs-skaffold-f5aa9561f948">Draft vs Gitkube vs Helm vs Ksonnet vs Metaparticle vs Skaffold&lt;/a> (03/2018)&lt;/li>
&lt;li>Blog post by Gergely Nemeth on &lt;a href="https://nemethgergely.com/using-kubernetes-for-local-development/index.html">Using Kubernetes for Local Development&lt;/a>, with a focus on Skaffold (03/2018)&lt;/li>
&lt;li>Blog post by Richard Li on &lt;a href="https://hackernoon.com/locally-developing-kubernetes-services-without-waiting-for-a-deploy-f63995de7b99">Locally developing Kubernetes services (without waiting for a deploy)&lt;/a>, with a focus on Telepresence&lt;/li>
&lt;li>Blog post by Abhishek Tiwari on &lt;a href="https://abhishek-tiwari.com/local-development-environment-for-kubernetes-using-minikube/">Local Development Environment for Kubernetes using Minikube&lt;/a> (09/2017)&lt;/li>
&lt;li>Blog post by Aymen El Amri on &lt;a href="https://medium.com/devopslinks/using-kubernetes-minikube-for-local-development-c37c6e56e3db">Using Kubernetes for Local Developmentâ€Šâ€”â€ŠMinikube&lt;/a> (08/2017)&lt;/li>
&lt;li>Blog post by Alexis Richardson on &lt;a href="https://www.weave.works/blog/gitops-operations-by-pull-request">â€‹GitOps - Operations by Pull Request&lt;/a> (08/2017)&lt;/li>
&lt;li>Slide deck &lt;a href="https://docs.google.com/presentation/d/1d3PigRVt_m5rO89Ob2XZ16bW8lRSkHHH5k816-oMzZo/">GitOps: Drive operations through git&lt;/a>, with a focus on Gitkube by Tirumarai Selvan (03/2018)&lt;/li>
&lt;li>Slide deck &lt;a href="https://speakerdeck.com/mhausenblas/developing-apps-on-kubernetes">Developing apps on Kubernetes&lt;/a>, a talk Michael Hausenblas gave at a CNCF Paris meetup (04/2018)&lt;/li>
&lt;li>YouTube videos:
&lt;ul>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=QW85Y0Ug3KY">TGI Kubernetes 029: Developing Apps with Ksync&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=McwwWhCXMxc">TGI Kubernetes 030: Exploring Skaffold&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=zezeBAJ_3w8">TGI Kubernetes 031: Connecting with Telepresence&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://www.youtube.com/watch?v=8B1D7cTMPgA">TGI Kubernetes 033: Developing with Draft&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Raw responses to the &lt;a href="https://docs.google.com/spreadsheets/d/12ilRCly2eHKPuicv1P_BD6z__PXAqpiaR-tDYe2eudE/edit">Kubernetes Application Survey&lt;/a> 2018 by SIG Apps&lt;/li>
&lt;/ul>
&lt;!--
With that we wrap up this post on how to go about developing apps on Kubernetes, we hope you learned something and if you have feedback and/or want to point out a tool that you found useful, please let us know via Twitter: [Ilya](https://twitter.com/errordeveloper) and [Michael](https://twitter.com/mhausenblas).
-->
&lt;p>æœ‰äº†è¿™äº›ï¼Œæˆ‘ä»¬è¿™ç¯‡å…³äºŽå¦‚ä½•åœ¨ Kubernetes ä¸Šå¼€å‘åº”ç”¨ç¨‹åºçš„åšå®¢å°±å¯ä»¥æ”¶å°¾äº†ï¼Œå¸Œæœ›æ‚¨æœ‰æ‰€æ”¶èŽ·ï¼Œå¦‚æžœæ‚¨æœ‰åé¦ˆå’Œ/æˆ–æƒ³è¦æŒ‡å‡ºæ‚¨è®¤ä¸ºæœ‰ç”¨çš„å·¥å…·ï¼Œè¯·é€šè¿‡ Twitter å‘Šè¯‰æˆ‘ä»¬ï¼š&lt;a href="https://twitter.com/errordeveloper">Ilya&lt;/a> å’Œ &lt;a href="https://twitter.com/mhausenblas">Michael&lt;/a>&lt;/p></description></item><item><title>Blog: Kubernetes ç¤¾åŒº - 2017 å¹´å¼€æºæŽ’è¡Œæ¦œæ¦œé¦–</title><link>https://kubernetes.io/zh/blog/2018/04/25/open-source-charts-2017/</link><pubDate>Wed, 25 Apr 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2018/04/25/open-source-charts-2017/</guid><description>
&lt;!--
---
title: Kubernetes Community - Top of the Open Source Charts in 2017
date: 2018-04-25
slug: open-source-charts-2017
---
--->
&lt;!--
2017 was a huge year for Kubernetes, and GitHubâ€™s latest [Octoverse report](https://octoverse.github.com) illustrates just how much attention this project has been getting.
Kubernetes, an [open source platform for running application containers](/docs/concepts/overview/what-is-kubernetes/), provides a consistent interface that enables developers and ops teams to automate the deployment, management, and scaling of a wide variety of applications on just about any infrastructure.
--->
&lt;p>å¯¹äºŽ Kubernetes æ¥è¯´ï¼Œ2017 å¹´æ˜¯ä¸°æ”¶çš„ä¸€å¹´ï¼ŒGitHubçš„æœ€æ–° &lt;a href="https://octoverse.github.com">Octoverse æŠ¥å‘Š&lt;/a> è¯´æ˜Žäº†è¯¥é¡¹ç›®èŽ·å¾—äº†å¤šå°‘å…³æ³¨ã€‚&lt;/p>
&lt;p>Kubernetes æ˜¯ &lt;a href="https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/">ç”¨äºŽè¿è¡Œåº”ç”¨ç¨‹åºå®¹å™¨çš„å¼€æºå¹³å°&lt;/a>ï¼Œå®ƒæä¾›äº†ä¸€ä¸ªç»Ÿä¸€çš„ç•Œé¢ï¼Œä½¿å¼€å‘äººå‘˜å’Œæ“ä½œå›¢é˜Ÿèƒ½å¤Ÿè‡ªåŠ¨æ‰§è¡Œéƒ¨ç½²ã€ç®¡ç†å’Œæ‰©å±•å‡ ä¹Žä»»ä½•åŸºç¡€æž¶æž„ä¸Šçš„å„ç§åº”ç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
Solving these shared challenges by leveraging a wide community of expertise and industrial experience, as Kubernetes does, helps engineers focus on building their own products at the top of the stack, rather than needlessly duplicating work that now exists as a standard part of the â€œcloud nativeâ€ toolkit.
However, achieving these gains via ad-hoc collective organizing is its own unique challenge, one which makes it increasingly difficult to support open source, community-driven efforts through periods of rapid growth.
Read on to find out how the Kubernetes Community has addressed these scaling challenges to reach the top of the charts in GitHubâ€™s 2017 Octoverse report.
--->
&lt;p>Kubernetes æ‰€åšçš„ï¼Œæ˜¯é€šè¿‡åˆ©ç”¨å¹¿æ³›çš„ä¸“ä¸šçŸ¥è¯†å’Œè¡Œä¸šç»éªŒæ¥è§£å†³è¿™äº›å…±åŒçš„æŒ‘æˆ˜ï¼Œå¯ä»¥å¸®åŠ©å·¥ç¨‹å¸ˆä¸“æ³¨äºŽåœ¨å †æ ˆçš„é¡¶éƒ¨æž„å»ºè‡ªå·±çš„äº§å“ï¼Œè€Œä¸æ˜¯ä¸å¿…è¦åœ°è¿›è¡Œé‡å¤å·¥ä½œï¼Œæ¯”å¦‚çŽ°åœ¨å·²ç»å­˜åœ¨çš„ â€œäº‘åŽŸç”Ÿâ€ å·¥å…·åŒ…çš„æ ‡å‡†éƒ¨åˆ†ã€‚&lt;/p>
&lt;p>ä½†æ˜¯ï¼Œé€šè¿‡ä¸´æ—¶çš„é›†ä½“ç»„ç»‡æ¥å®žçŽ°è¿™äº›æ”¶ç›Šæ˜¯å®ƒç‹¬æœ‰çš„æŒ‘æˆ˜ï¼Œè¿™ä½¿å¾—æ”¯æŒå¼€æºï¼Œç¤¾åŒºé©±åŠ¨çš„å·¥ä½œå˜å¾—è¶Šæ¥è¶Šå›°éš¾ã€‚&lt;/p>
&lt;p>ç»§ç»­é˜…è¯»ä»¥äº†è§£ Kubernetes ç¤¾åŒºå¦‚ä½•è§£å†³è¿™äº›æŒ‘æˆ˜ï¼Œä»Žè€Œåœ¨ GitHub çš„ 2017 Octoverse æŠ¥å‘Šä¸­ä½å±…æ¦œé¦–ã€‚&lt;/p>
&lt;!--
## Most-Discussed on GitHub
The top two most-discussed repos of 2017 are both based on Kubernetes:
![Most Discussed](/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png)
Of all the open source repositories on GitHub, none received more issue comments than [kubernetes/kubernetes](https://github.com/kubernetes/kubernetes/). [OpenShift](http://openshift.com/), a [CNCF certified distribution of Kubernetes](https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/), took second place.
Open discussion with ample time for community feedback and review helps build shared infrastructure and establish new standards for cloud native computing.
--->
&lt;h2 id="github-ä¸Šè®¨è®ºæœ€å¤šçš„">GitHub ä¸Šè®¨è®ºæœ€å¤šçš„&lt;/h2>
&lt;p>2017 å¹´è®¨è®ºæœ€å¤šçš„ä¸¤ä¸ªä»“åº“éƒ½æ˜¯åŸºäºŽ Kubernetes çš„ï¼š&lt;/p>
&lt;p>ï¼&lt;a href="https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-discussed.png">è®¨è®ºæœ€å¤š&lt;/a>&lt;/p>
&lt;p>åœ¨ GitHub çš„æ‰€æœ‰å¼€æºå­˜å‚¨åº“ä¸­ï¼Œæ²¡æœ‰æ¯” &lt;a href="https://github.com/kubernetes/kubernetes/">kubernetes/kubernetes&lt;/a> æ”¶åˆ°æ›´å¤šçš„è¯„è®ºã€‚ &lt;a href="http://openshift.com/">OpenShift&lt;/a>ï¼Œ &lt;a href="https://www.cncf.io/announcement/2017/11/13/cloud-native-computing-foundation-launches-certified-kubernetes-program-32-conformant-distributions-platforms/">CNCF è®¤è¯çš„ Kubernetes å‘è¡Œç‰ˆ&lt;/a> æŽ’åç¬¬äºŒã€‚&lt;/p>
&lt;p>åˆ©ç”¨å……è¶³æ—¶é—´è¿›è¡Œå…¬å¼€è®¨è®ºæ¥èŽ·å–ç¤¾åŒºåé¦ˆå’Œå®¡æŸ¥ï¼Œæœ‰åŠ©äºŽå»ºç«‹å…±äº«çš„åŸºç¡€æž¶æž„å¹¶ä¸ºäº‘åŽŸç”Ÿè®¡ç®—å»ºç«‹æ–°æ ‡å‡†ã€‚&lt;/p>
&lt;!--
## Most Reviewed on GitHub
Successfully scaling an open source effortâ€™s communications often leads to better coordination and higher-quality feature delivery. The Kubernetes projectâ€™s [Special Interest Group (SIG)](https://github.com/kubernetes/community/blob/master/sig-list.md) structure has helped it become GitHubâ€™s second most reviewed project:
![Most Reviewed](/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png)
Using SIGs to segment and standardize mechanisms for community participation helps channel more frequent reviews from better-qualified community members.
When managed effectively, active community discussions indicate more than just a highly contentious codebase, or a project with an extensive list of unmet needs.
--->
&lt;h2 id="github-ä¸Šå®¡é˜…æœ€å¤šçš„">GitHub ä¸Šå®¡é˜…æœ€å¤šçš„&lt;/h2>
&lt;p>æˆåŠŸæ‰©å±•å¼€æ”¾æºä»£ç å·¥ä½œçš„é€šä¿¡é€šå¸¸ä¼šå¸¦æ¥æ›´å¥½çš„åè°ƒå’Œæ›´é«˜è´¨é‡çš„åŠŸèƒ½äº¤ä»˜ã€‚Kubernetes é¡¹ç›®çš„ &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groupï¼ˆSIGï¼‰&lt;/a> ç»“æž„å·²ä½¿å…¶æˆä¸º GitHub å®¡é˜…ç¬¬äºŒå¤šçš„é¡¹ç›®ï¼š&lt;/p>
&lt;p>ï¼&lt;a href="https://kubernetes.io/images/blog-logging/2018-04-24-open-source-charts-2017/most-reviews.png">å®¡é˜…æœ€å¤š&lt;/a>&lt;/p>
&lt;p>ä½¿ç”¨ SIG å¯¹ç¤¾åŒºå‚ä¸Žæœºåˆ¶è¿›è¡Œç»†åˆ†å’Œæ ‡å‡†åŒ–æœ‰åŠ©äºŽä»Žèµ„æ ¼æ›´é«˜çš„ç¤¾åŒºæˆå‘˜ä¸­èŽ·å¾—æ›´é¢‘ç¹çš„å®¡é˜…ã€‚&lt;/p>
&lt;p>å¦‚æžœå¾—åˆ°æœ‰æ•ˆç®¡ç†ï¼Œæ´»è·ƒçš„ç¤¾åŒºè®¨è®ºä¸ä»…è¡¨æ˜Žä»£ç åº“å­˜åœ¨å¾ˆå¤§çš„äº‰è®®ï¼Œä¹Ÿå¯èƒ½è¡¨æ˜Žé¡¹ç›®åŒ…å«å¤§é‡æœªæ»¡è¶³çš„éœ€æ±‚ã€‚&lt;/p>
&lt;!--
Scaling a projectâ€™s capacity to handle issues and community interactions helps to expand the conversation. Meanwhile, large communities come with more diverse use cases and a larger array of support problems to manage. The Kubernetes [SIG organization structure](https://github.com/kubernetes/community#sigs) helps to address the challenges of complex communication at scale.
SIG meetings provide focused opportunities for users, maintainers, and specialists from various disciplines to collaborate together in support of this community effort. These investments in organizing help create an environment where itâ€™s easier to prioritize architecture discussion and planning over commit velocity; enabling the project to sustain this kind of scale.
--->
&lt;p>æ‰©å±•é¡¹ç›®å¤„ç†é—®é¢˜å’Œç¤¾åŒºäº’åŠ¨çš„èƒ½åŠ›æœ‰åŠ©äºŽæ‰©å¤§äº¤æµã€‚åŒæ—¶ï¼Œå¤§åž‹ç¤¾åŒºå…·æœ‰æ›´å¤šä¸åŒçš„ç”¨ä¾‹å’Œæ›´å¤šçš„æ”¯æŒé—®é¢˜éœ€è¦ç®¡ç†ã€‚Kubernetes &lt;a href="https://github.com/kubernetes/community#sigs">SIG ç»„ç»‡ç»“æž„&lt;/a> å¸®åŠ©åº”å¯¹å¤§è§„æ¨¡å¤æ‚é€šä¿¡çš„æŒ‘æˆ˜ã€‚&lt;/p>
&lt;p>SIG ä¼šè®®ä¸ºä¸åŒå­¦ç§‘çš„ç”¨æˆ·ã€ç»´æŠ¤è€…å’Œä¸“å®¶æä¾›äº†é‡ç‚¹åˆä½œçš„æœºä¼šï¼Œä»¥å…±åŒåä½œæ¥æ”¯æŒç¤¾åŒºçš„å·¥ä½œã€‚è¿™äº›åœ¨ç»„ç»‡ä¸Šçš„æŠ•èµ„æœ‰åŠ©äºŽåˆ›å»ºä¸€ä¸ªçŽ¯å¢ƒï¼Œåœ¨è¿™æ ·çš„çŽ¯å¢ƒä¸­ï¼Œå¯ä»¥æ›´è½»æ¾åœ°å°†æž¶æž„è®¨è®ºå’Œè§„åˆ’çš„ä¼˜å…ˆçº§æŽ’åˆ°æäº¤é€Ÿåº¦å‰é¢ï¼Œå¹¶ä½¿é¡¹ç›®èƒ½å¤Ÿç»´æŒè¿™ç§è§„æ¨¡ã€‚&lt;/p>
&lt;!--
## Join the party!
You may already be using solutions that are successfully managed and scaled on Kubernetes. For example, GitHub.com, which hosts Kubernetesâ€™ upstream source code, [now runs on Kubernetes](https://githubengineering.com/kubernetes-at-github/) as well!
Check out the [Kubernetes Contributorsâ€™ guide](https://github.com/kubernetes/community/blob/master/contributors/guide/README.md) for more information on how to get started as a contributor.
You can also join the [weekly Kubernetes Community meeting](https://github.com/kubernetes/community/tree/master/communication#weekly-meeting) and consider [joining a SIG or two](https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list).
--->
&lt;h2 id="åŠ å…¥æˆ‘ä»¬">åŠ å…¥æˆ‘ä»¬ï¼&lt;/h2>
&lt;p>æ‚¨å¯èƒ½å·²ç»åœ¨ Kubernetes ä¸ŠæˆåŠŸä½¿ç”¨ç®¡ç†å’Œæ‰©å±•çš„è§£å†³æ–¹æ¡ˆã€‚ä¾‹å¦‚ï¼Œæ‰˜ç®¡ Kubernetes ä¸Šæ¸¸æºä»£ç çš„ GitHub.com &lt;a href="https://githubengineering.com/kubernetes-at-github/">çŽ°åœ¨ä¹Ÿå¯ä»¥åœ¨ Kubernetes ä¸Šè¿è¡Œ&lt;/a> ï¼&lt;/p>
&lt;p>è¯·æŸ¥çœ‹ &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/guide/README.md">Kubernetes è´¡çŒ®è€…æŒ‡å—&lt;/a> ï¼Œä»¥èŽ·å–æœ‰å…³å¦‚ä½•å¼€å§‹ä½œä¸ºè´¡çŒ®è€…çš„æ›´å¤šä¿¡æ¯ã€‚&lt;/p>
&lt;p>æ‚¨ä¹Ÿå¯ä»¥å‚åŠ  &lt;a href="https://github.com/kubernetes/community/tree/master/communication#weekly-meeting">æ¯å‘¨ Kubernetes çš„ç¤¾åŒºä¼šè®®&lt;/a> å¹¶è€ƒè™‘ &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md#master-sig-list">åŠ å…¥ä¸€ä¸ªæˆ–ä¸¤ä¸ª SIG&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.9 å¯¹ Windows Server å®¹å™¨æä¾› Beta ç‰ˆæœ¬æ”¯æŒ</title><link>https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/</link><pubDate>Tue, 09 Jan 2018 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2018/01/kubernetes-v19-beta-windows-support/</guid><description>
&lt;!--
---
title: Kubernetes v1.9 releases beta support for Windows Server Containers
date: 2018-01-09
slug: kubernetes-v19-beta-windows-support
url: /blog/2018/01/Kubernetes-V19-Beta-Windows-Support
---
--->
&lt;!--
With the release of Kubernetes v1.9, our mission of ensuring Kubernetes works well everywhere and for everyone takes a great step forward. Weâ€™ve advanced support for Windows Server to beta along with continued feature and functional advancements on both the Kubernetes and Windows platforms. SIG-Windows has been working since March of 2016 to open the door for many Windows-specific applications and workloads to run on Kubernetes, significantly expanding the implementation scenarios and the enterprise reach of Kubernetes.
--->
&lt;p>éšç€ Kubernetes v1.9 çš„å‘å¸ƒï¼Œæˆ‘ä»¬ç¡®ä¿æ‰€æœ‰äººåœ¨ä»»ä½•åœ°æ–¹éƒ½èƒ½æ­£å¸¸è¿è¡Œ Kubernetes çš„ä½¿å‘½å‰è¿›äº†ä¸€å¤§æ­¥ã€‚æˆ‘ä»¬çš„ Beta ç‰ˆæœ¬å¯¹ Windows Server çš„æ”¯æŒè¿›è¡Œäº†å‡çº§ï¼Œå¹¶ä¸”åœ¨ Kubernetes å’Œ Windows å¹³å°ä¸Šéƒ½æä¾›äº†æŒç»­çš„åŠŸèƒ½æ”¹è¿›ã€‚ä¸ºäº†åœ¨ Kubernetes ä¸Šè¿è¡Œè®¸å¤šç‰¹å®šäºŽ Windows çš„åº”ç”¨ç¨‹åºå’Œå·¥ä½œè´Ÿè½½ï¼ŒSIG-Windows è‡ª2016å¹´3æœˆä»¥æ¥ä¸€ç›´åœ¨åŠªåŠ›ï¼Œå¤§å¤§æ‰©å±•äº† Kubernetes çš„å®žçŽ°åœºæ™¯å’Œä¼ä¸šé€‚ç”¨èŒƒå›´ã€‚&lt;/p>
&lt;!--
Enterprises of all sizes have made significant investments in .NET and Windows based applications. Many enterprise portfolios today contain .NET and Windows, with Gartner claiming that [80%](http://www.gartner.com/document/3446217) of enterprise apps run on Windows. According to StackOverflow Insights, 40% of professional developers use the .NET programming languages (including .NET Core).
--->
&lt;p>å„ç§è§„æ¨¡çš„ä¼ä¸šéƒ½åœ¨ .NET å’ŒåŸºäºŽ Windows çš„åº”ç”¨ç¨‹åºä¸Šè¿›è¡Œäº†å¤§é‡æŠ•èµ„ã€‚å¦‚ä»Šè®¸å¤šä¼ä¸šäº§å“ç»„åˆéƒ½åŒ…å« .NET å’Œ Windowsï¼ŒGartner å£°ç§° &lt;a href="http://www.gartner.com/document/3446217">80%&lt;/a> çš„ä¼ä¸šåº”ç”¨éƒ½åœ¨ Windows ä¸Šè¿è¡Œã€‚æ ¹æ® StackOverflow Insightsï¼Œ40% çš„ä¸“ä¸šå¼€å‘äººå‘˜ä½¿ç”¨ .NET ç¼–ç¨‹è¯­è¨€ï¼ˆåŒ…æ‹¬ .NET Coreï¼‰ã€‚&lt;/p>
&lt;!--
But why is all this information important? It means that enterprises have both legacy and new born-in-the-cloud (microservice) applications that utilize a wide array of programming frameworks. There is a big push in the industry to modernize existing/legacy applications to containers, using an approach similar to â€œlift and shiftâ€. Modernizing existing applications into containers also provides added flexibility for new functionality to be introduced in additional Windows or Linux containers. Containers are becoming the de facto standard for packaging, deploying, and managing both existing and microservice applications. IT organizations are looking for an easier and homogenous way to orchestrate and manage containers across their Linux and Windows environments. Kubernetes v1.9 now offers beta support for Windows Server containers, making it the clear choice for orchestrating containers of any kind.
--->
&lt;p>ä½†ä¸ºä»€ä¹ˆè¿™äº›ä¿¡æ¯éƒ½å¾ˆé‡è¦ï¼Ÿè¿™æ„å‘³ç€ä¼ä¸šæ—¢æœ‰ä¼ ç»Ÿçš„ï¼Œä¹Ÿæœ‰æ–°ç”Ÿçš„äº‘ï¼ˆmicroserviceï¼‰åº”ç”¨ç¨‹åºï¼Œåˆ©ç”¨äº†å¤§é‡çš„ç¼–ç¨‹æ¡†æž¶ã€‚ä¸šç•Œæ­£åœ¨å¤§åŠ›æŽ¨åŠ¨å°†çŽ°æœ‰/é—ç•™åº”ç”¨ç¨‹åºçŽ°ä»£åŒ–åˆ°å®¹å™¨ä¸­ï¼Œä½¿ç”¨ç±»ä¼¼äºŽâ€œæå‡å’Œè½¬ç§»â€çš„æ–¹æ³•ã€‚åŒæ—¶ï¼Œä¹Ÿèƒ½çµæ´»åœ°å‘å…¶ä»– Windows æˆ– Linux å®¹å™¨å¼•å…¥æ–°åŠŸèƒ½ã€‚å®¹å™¨æ­£åœ¨æˆä¸ºæ‰“åŒ…ã€éƒ¨ç½²å’Œç®¡ç†çŽ°æœ‰ç¨‹åºå’Œå¾®æœåŠ¡åº”ç”¨ç¨‹åºçš„ä¸šç•Œæ ‡å‡†ã€‚IT ç»„ç»‡æ­£åœ¨å¯»æ‰¾ä¸€ç§æ›´ç®€å•ä¸”ä¸€è‡´çš„æ–¹æ³•æ¥è·¨ Linux å’Œ Windows çŽ¯å¢ƒè¿›è¡Œåè°ƒå’Œç®¡ç†å®¹å™¨ã€‚Kubernetes v1.9 çŽ°åœ¨å¯¹ Windows Server å®¹å™¨æä¾›äº† Beta ç‰ˆæœ¬æ”¯æŒï¼Œä½¿ä¹‹æˆä¸ºç­–åˆ’ä»»ä½•ç±»åž‹å®¹å™¨çš„æ˜Žç¡®é€‰æ‹©ã€‚&lt;/p>
&lt;!--
### Features
Alpha support for Windows Server containers in Kubernetes was great for proof-of-concept projects and visualizing the road map for support of Windows in Kubernetes. The alpha release had significant drawbacks, however, and lacked many features, especially in networking. SIG-Windows, Microsoft, Cloudbase Solutions, Apprenda, and other community members banded together to create a comprehensive beta release, enabling Kubernetes users to start evaluating and using Windows.
--->
&lt;h3 id="ç‰¹ç‚¹">ç‰¹ç‚¹&lt;/h3>
&lt;p>Kubernetes ä¸­å¯¹ Windows Server å®¹å™¨çš„ Alpha æ”¯æŒæ˜¯éžå¸¸æœ‰ç”¨çš„ï¼Œå°¤å…¶æ˜¯å¯¹äºŽæ¦‚å¿µé¡¹ç›®å’Œå¯è§†åŒ– Kubernetes ä¸­ Windows æ”¯æŒçš„è·¯çº¿å›¾ã€‚ç„¶è€Œï¼ŒAlpha ç‰ˆæœ¬æœ‰æ˜Žæ˜¾çš„ç¼ºç‚¹ï¼Œå¹¶ä¸”ç¼ºå°‘è®¸å¤šç‰¹æ€§ï¼Œç‰¹åˆ«æ˜¯åœ¨ç½‘ç»œæ–¹é¢ã€‚SIG Windowsã€Microsoftã€Cloudbase Solutionsã€Apprenda å’Œå…¶ä»–ç¤¾åŒºæˆå‘˜è”åˆåˆ›å»ºäº†ä¸€ä¸ªå…¨é¢çš„ Beta ç‰ˆæœ¬ï¼Œä½¿ Kubernetes ç”¨æˆ·èƒ½å¤Ÿå¼€å§‹è¯„ä¼°å’Œä½¿ç”¨ Windowsã€‚&lt;/p>
&lt;!--
Some key feature improvements for Windows Server containers on Kubernetes include:
- Improved support for pods! Multiple Windows Server containers in a pod can now share the network namespace using network compartments in Windows Server. This feature brings the concept of a pod to parity with Linux-based containers
- Reduced network complexity by using a single network endpoint per pod
- Kernel-Based load-balancing using the Virtual Filtering Platform (VFP) Hyper-v Switch Extension (analogous to Linux iptables)
- Container Runtime Interface (CRI) pod and node level statistics. Windows Server containers can now be profiled for Horizontal Pod Autoscaling using performance metrics gathered from the pod and the node
--->
&lt;p>Kubernetes å¯¹ Windows æœåŠ¡å™¨å®¹å™¨çš„ä¸€äº›å…³é”®åŠŸèƒ½æ”¹è¿›åŒ…æ‹¬ï¼š&lt;/p>
&lt;ul>
&lt;li>æ”¹è¿›äº†å¯¹ Pod çš„æ”¯æŒï¼Pod ä¸­å¤šä¸ª Windows Server å®¹å™¨çŽ°åœ¨å¯ä»¥ä½¿ç”¨ Windows Server ä¸­çš„ç½‘ç»œéš”ç¦»ä¸“åŒºå…±äº«ç½‘ç»œå‘½åç©ºé—´ã€‚æ­¤åŠŸèƒ½ä¸­ Pod çš„æ¦‚å¿µç›¸å½“äºŽåŸºäºŽ Linux çš„å®¹å™¨&lt;/li>
&lt;li>å¯é€šè¿‡æ¯ä¸ª Pod ä½¿ç”¨å•ä¸ªç½‘ç»œç«¯ç‚¹æ¥é™ä½Žç½‘ç»œå¤æ‚æ€§&lt;/li>
&lt;li>å¯ä»¥ä½¿ç”¨ Virtual Filtering Platformï¼ˆVFPï¼‰çš„ Hyper-v Switch Extensionï¼ˆç±»ä¼¼äºŽ Linux iptablesï¼‰è¾¾åˆ°åŸºäºŽå†…æ ¸çš„è´Ÿè½½å¹³è¡¡&lt;/li>
&lt;li>å…·å¤‡ Container Runtime Interfaceï¼ˆCRIï¼‰çš„ Pod å’Œ Node çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯ã€‚å¯ä»¥ä½¿ç”¨ä»Ž Pod å’ŒèŠ‚ç‚¹æ”¶é›†çš„æ€§èƒ½æŒ‡æ ‡é…ç½® Windows Server å®¹å™¨çš„ Horizontal Pod Autoscaling&lt;/li>
&lt;/ul>
&lt;!--
- Support for kubeadm commands to add Windows Server nodes to a Kubernetes environment. Kubeadm simplifies the provisioning of a Kubernetes cluster, and with the support for Windows Server, you can use a single tool to deploy Kubernetes in your infrastructure
- Support for ConfigMaps, Secrets, and Volumes. These are key features that allow you to separate, and in some cases secure, the configuration of the containers from the implementation
The crown jewels of Kubernetes 1.9 Windows support, however, are the networking enhancements. With the release of Windows Server 1709, Microsoft has enabled key networking capabilities in the operating system and the Windows Host Networking Service (HNS) that paved the way to produce a number of CNI plugins that work with Windows Server containers in Kubernetes. The Layer-3 routed and network overlay plugins that are supported with Kubernetes 1.9 are listed below:
--->
&lt;ul>
&lt;li>æ”¯æŒ kubeadm å‘½ä»¤å°† Windows Server çš„ Node æ·»åŠ åˆ° Kubernetes çŽ¯å¢ƒã€‚Kubeadm ç®€åŒ–äº† Kubernetes é›†ç¾¤çš„é…ç½®ï¼Œé€šè¿‡å¯¹ Windows Server çš„æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨æ‚¨çš„åŸºç¡€é…ç½®ä¸­ä½¿ç”¨å•ä¸€çš„å·¥å…·éƒ¨ç½² Kubernetes&lt;/li>
&lt;li>æ”¯æŒ ConfigMaps, Secrets, å’Œ Volumesã€‚è¿™äº›æ˜¯éžå¸¸å…³é”®çš„ç‰¹æ€§ï¼Œæ‚¨å¯ä»¥å°†å®¹å™¨çš„é…ç½®ä»Žå®žæ–½ä½“ç³»ä¸­åˆ†ç¦»å‡ºæ¥ï¼Œå¹¶ä¸”åœ¨å¤§éƒ¨åˆ†æƒ…å†µä¸‹æ˜¯å®‰å…¨çš„ &lt;br>
ç„¶è€Œï¼Œkubernetes 1.9 windows æ”¯æŒçš„æœ€å¤§äº®ç‚¹æ˜¯ç½‘ç»œå¢žå¼ºã€‚éšç€ Windows æœåŠ¡å™¨ 1709 çš„å‘å¸ƒï¼Œå¾®è½¯åœ¨æ“ä½œç³»ç»Ÿå’Œ Windows Host Networking Serviceï¼ˆHNSï¼‰ä¸­å¯ç”¨äº†å…³é”®çš„ç½‘ç»œåŠŸèƒ½ï¼Œè¿™ä¸ºåˆ›é€ å¤§é‡ä¸Ž Kubernetes ä¸­çš„ Windows æœåŠ¡å™¨å®¹å™¨ä¸€èµ·å·¥ä½œçš„ CNI æ’ä»¶é“ºå¹³äº†é“è·¯ã€‚Kubernetes 1.9 æ”¯æŒçš„ç¬¬ä¸‰å±‚è·¯ç”±å’Œç½‘ç»œè¦†ç›–æ’ä»¶å¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/li>
&lt;/ul>
&lt;!--
1. Upstream L3 Routing - IP routes configured in upstream ToR
2. Host-Gateway - IP routes configured on each host
3. Open vSwitch (OVS) &amp; Open Virtual Network (OVN) with Overlay - Supports STT and Geneve tunneling types
You can read more about each of their [configuration, setup, and runtime capabilities](/docs/getting-started-guides/windows/) to make an informed selection for your networking stack in Kubernetes.
--->
&lt;ol>
&lt;li>ä¸Šæ¸¸ L3 è·¯ç”± - ä¸Šæ¸¸ ToR ä¸­é…ç½®çš„ IP è·¯ç”±&lt;/li>
&lt;li>Host-Gateway - åœ¨æ¯ä¸ªä¸»æœºä¸Šé…ç½®çš„ IP è·¯ç”±&lt;/li>
&lt;li>å…·æœ‰ Overlay çš„ Open vSwitchï¼ˆOVSï¼‰å’Œ Open Virtual Networkï¼ˆOVNï¼‰ - æ”¯æŒ STT å’Œ Geneve çš„ tunneling ç±»åž‹
æ‚¨å¯ä»¥é˜…è¯»æ›´å¤šæœ‰å…³ &lt;a href="https://kubernetes.io/docs/getting-started-guides/windows/">é…ç½®ã€è®¾ç½®å’Œè¿è¡Œæ—¶åŠŸèƒ½&lt;/a> çš„ä¿¡æ¯ï¼Œä»¥ä¾¿åœ¨ Kubernetes ä¸­ä¸ºæ‚¨çš„ç½‘ç»œå †æ ˆåšå‡ºæ˜Žæ™ºçš„é€‰æ‹©ã€‚&lt;/li>
&lt;/ol>
&lt;!--
Even though you have to continue running the Kubernetes Control Plane and Master Components in Linux, you are now able to introduce Windows Server as a Node in Kubernetes. As a community, this is a huge milestone and achievement. We will now start seeing .NET, .NET Core, ASP.NET, IIS, Windows Services, Windows executables and many more windows-based applications in Kubernetes.
--->
&lt;p>å¦‚æžœæ‚¨éœ€è¦ç»§ç»­åœ¨ Linux ä¸­è¿è¡Œ Kubernetes Control Plane å’Œ Master Componentsï¼ŒçŽ°åœ¨ä¹Ÿå¯ä»¥å°† Windows Server ä½œä¸º Kubernetes ä¸­çš„ä¸€ä¸ªèŠ‚ç‚¹å¼•å…¥ã€‚å¯¹ä¸€ä¸ªç¤¾åŒºæ¥è¯´ï¼Œè¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„é‡Œç¨‹ç¢‘å’Œæˆå°±ã€‚çŽ°åœ¨ï¼Œæˆ‘ä»¬å°†ä¼šåœ¨ Kubernetes ä¸­çœ‹åˆ° .NETï¼Œ.NET Coreï¼ŒASP.NETï¼ŒIISï¼ŒWindows æœåŠ¡ï¼ŒWindows å¯æ‰§è¡Œæ–‡ä»¶ä»¥åŠæ›´å¤šåŸºäºŽ Windows çš„åº”ç”¨ç¨‹åºã€‚&lt;/p>
&lt;!--
### Whatâ€™s coming next
A lot of work went into this beta release, but the community realizes there are more areas of investment needed before we can release Windows support as GA (General Availability) for production workloads. Some keys areas of focus for the first two quarters of 2018 include:
--->
&lt;h3 id="æŽ¥ä¸‹æ¥è¿˜ä¼šæœ‰ä»€ä¹ˆ">æŽ¥ä¸‹æ¥è¿˜ä¼šæœ‰ä»€ä¹ˆ&lt;/h3>
&lt;p>è¿™ä¸ª Beta ç‰ˆæœ¬è¿›è¡Œäº†å¤§é‡å·¥ä½œï¼Œä½†æ˜¯ç¤¾åŒºæ„è¯†åˆ°åœ¨å°† Windows æ”¯æŒä½œä¸ºç”Ÿäº§å·¥ä½œè´Ÿè½½å‘å¸ƒä¸º GAï¼ˆGeneral Availabilityï¼‰ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦æ›´å¤šé¢†åŸŸçš„æŠ•èµ„ã€‚2018å¹´å‰ä¸¤ä¸ªå­£åº¦çš„é‡ç‚¹å…³æ³¨é¢†åŸŸåŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
1. Continue to make progress in the area of networking. Additional CNI plugins are under development and nearing completion
- Overlay - win-overlay (vxlan or IP-in-IP encapsulation using Flannel)&amp;nbsp;
- Win-l2bridge (host-gateway)&amp;nbsp;
- OVN using cloud networking - without overlays
- Support for Kubernetes network policies in ovn-kubernetes
- Support for Hyper-V Isolation
- Support for StatefulSet functionality for stateful applications
- Produce installation artifacts and documentation that work on any infrastructure and across many public cloud providers like Microsoft Azure, Google Cloud, and Amazon AWS
- Continuous Integration/Continuous Delivery (CI/CD) infrastructure for SIG-Windows
- Scalability and Performance testing
Even though we have not committed to a timeline for GA, SIG-Windows estimates a GA release in the first half of 2018.
--->
&lt;ol>
&lt;li>ç»§ç»­åœ¨ç½‘ç»œé¢†åŸŸå–å¾—æ›´å¤šè¿›å±•ã€‚å…¶ä»– CNI æ’ä»¶æ­£åœ¨å¼€å‘ä¸­ï¼Œå¹¶ä¸”å³å°†å®Œæˆ&lt;/li>
&lt;/ol>
&lt;ul>
&lt;li>Overlay - win-Overlayï¼ˆvxlan æˆ– IP-in-IP ä½¿ç”¨ Flannel å°è£…ï¼‰&lt;/li>
&lt;li>Win-l2bridgeï¼ˆhost-gatewayï¼‰&lt;/li>
&lt;li>ä½¿ç”¨äº‘ç½‘ç»œçš„ OVN - ä¸å†ä¾èµ– Overlay&lt;/li>
&lt;li>åœ¨ ovn-Kubernetes ä¸­æ”¯æŒ Kubernetes ç½‘ç»œç­–ç•¥&lt;/li>
&lt;li>æ”¯æŒ Hyper-V Isolation&lt;/li>
&lt;li>æ”¯æŒæœ‰çŠ¶æ€åº”ç”¨ç¨‹åºçš„ StatefulSet åŠŸèƒ½&lt;/li>
&lt;li>ç”Ÿæˆé€‚ç”¨äºŽä»»ä½•åŸºç¡€æž¶æž„ä»¥åŠè·¨å¤šå…¬å…±äº‘æä¾›å•†ï¼ˆä¾‹å¦‚ Microsoft Azureï¼ŒGoogle Cloud å’Œ Amazon AWSï¼‰çš„å®‰è£…å·¥å…·å’Œæ–‡æ¡£&lt;/li>
&lt;li>SIG-Windows çš„ Continuous Integration/Continuous Deliveryï¼ˆCI/CDï¼‰åŸºç¡€ç»“æž„&lt;/li>
&lt;li>å¯ä¼¸ç¼©æ€§å’Œæ€§èƒ½æµ‹è¯•
å°½ç®¡æˆ‘ä»¬å°šæœªæ‰¿è¯ºæ­£å¼ç‰ˆçš„å…·ä½“æ—¶é—´çº¿ï¼Œä½†ä¼°è®¡ SIG-Windows å°†äºŽ2018å¹´ä¸ŠåŠå¹´æ­£å¼å‘å¸ƒã€‚&lt;/li>
&lt;/ul>
&lt;!--
### Get Involved
As we continue to make progress towards General Availability of this feature in Kubernetes, we welcome you to get involved, contribute code, provide feedback, deploy Windows Server containers to your Kubernetes cluster, or simply join our community.
--->
&lt;h3 id="åŠ å…¥æˆ‘ä»¬">åŠ å…¥æˆ‘ä»¬&lt;/h3>
&lt;p>éšç€æˆ‘ä»¬åœ¨ Kubernetes çš„æ™®éå¯ç”¨æ€§æ–¹å‘ä¸æ–­å–å¾—è¿›å±•ï¼Œæˆ‘ä»¬æ¬¢è¿Žæ‚¨å‚ä¸Žè¿›æ¥ï¼Œè´¡çŒ®ä»£ç ã€æä¾›åé¦ˆï¼Œå°† Windows æœåŠ¡å™¨å®¹å™¨éƒ¨ç½²åˆ° Kubernetes é›†ç¾¤ï¼Œæˆ–è€…å¹²è„†åŠ å…¥æˆ‘ä»¬çš„ç¤¾åŒºã€‚&lt;/p>
&lt;!--
- If you want to get started on deploying Windows Server containers in Kubernetes, read our getting started guide at [/docs/getting-started-guides/windows/](/docs/getting-started-guides/windows/)
- We meet every other Tuesday at 12:30 Eastern Standard Time (EST) at [https://zoom.us/my/sigwindows](https://zoom.us/my/sigwindows). All our meetings are recorded on youtube and referenced at [https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4](https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4)
- Chat with us on Slack at [https://kubernetes.slack.com/messages/sig-windows](https://kubernetes.slack.com/messages/sig-windows)
- Find us on GitHub at [https://github.com/kubernetes/community/tree/master/sig-windows](https://github.com/kubernetes/community/tree/master/sig-windows)
--->
&lt;ul>
&lt;li>å¦‚æžœä½ æƒ³è¦å¼€å§‹åœ¨ Kubernetes ä¸­éƒ¨ç½² Windows Server å®¹å™¨ï¼Œè¯·é˜…è¯»æˆ‘ä»¬çš„å¼€å§‹å¯¼è§ˆ &lt;a href="https://kubernetes.io/docs/getting-started-guides/windows/">/docs/getting-started-guides/windows/&lt;/a>&lt;/li>
&lt;li>æˆ‘ä»¬æ¯éš”ä¸€ä¸ªæ˜ŸæœŸäºŒåœ¨ç¾Žå›½ä¸œéƒ¨æ ‡å‡†æ—¶é—´ï¼ˆESTï¼‰çš„12:30åœ¨ &lt;a href="https://zoom.us/my/sigwindows">https://zoom.us/my/sigwindows&lt;/a> å¼€ä¼šã€‚æ‰€æœ‰ä¼šè®®å†…å®¹éƒ½è®°å½•åœ¨ Youtube å¹¶é™„ä¸Šäº†å‚è€ƒææ–™ &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4">https://www.youtube.com/playlist?list=PL69nYSiGNLP2OH9InCcNkWNu2bl-gmIU4&lt;/a>&lt;/li>
&lt;li>é€šè¿‡ Slack è”ç³»æˆ‘ä»¬ &lt;a href="https://kubernetes.slack.com/messages/sig-windows">https://kubernetes.slack.com/messages/sig-windows&lt;/a>&lt;/li>
&lt;li>åœ¨ Github ä¸Šæ‰¾åˆ°æˆ‘ä»¬ &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">https://github.com/kubernetes/community/tree/master/sig-windows&lt;/a>&lt;/li>
&lt;/ul>
&lt;!--
Thank you,
Michael Michael (@michmike77)
SIG-Windows Lead
Senior Director of Product Management, Apprenda
--->
&lt;p>è°¢è°¢å¤§å®¶ï¼Œ&lt;/p>
&lt;p>Michael Michael (@michmike77)&lt;br>
SIG-Windows é¢†å¯¼äºº&lt;br>
Apprenda äº§å“ç®¡ç†é«˜çº§æ€»ç›‘&lt;/p></description></item><item><title>Blog: Kubernetes ä¸­è‡ªåŠ¨ç¼©æ”¾</title><link>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</link><pubDate>Fri, 17 Nov 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2017/11/17/autoscaling-in-kubernetes/</guid><description>
&lt;!--
---
title: " Autoscaling in Kubernetes "
date: 2017-11-17
slug: autoscaling-in-kubernetes
url: /blog/2017/11/Autoscaling-In-Kubernetes
---
-->
&lt;!--
Kubernetes allows developers to automatically adjust cluster sizes and the number of pod replicas based on current traffic and load. These adjustments reduce the amount of unused nodes, saving money and resources. In this talk, Marcin Wielgus of Google walks you through the current state of pod and node autoscaling in Kubernetes: .how it works, and how to use it, including best practices for deployments in production applications.
-->
&lt;p>Kubernetes å…è®¸å¼€å‘äººå‘˜æ ¹æ®å½“å‰çš„æµé‡å’Œè´Ÿè½½è‡ªåŠ¨è°ƒæ•´é›†ç¾¤å¤§å°å’Œ pod å‰¯æœ¬çš„æ•°é‡ã€‚è¿™äº›è°ƒæ•´å‡å°‘äº†æœªä½¿ç”¨èŠ‚ç‚¹çš„æ•°é‡ï¼ŒèŠ‚çœäº†èµ„é‡‘å’Œèµ„æºã€‚
åœ¨è¿™æ¬¡æ¼”è®²ä¸­ï¼Œè°·æ­Œçš„ Marcin Wielgus å°†å¸¦é¢†æ‚¨äº†è§£ Kubernetes ä¸­ pod å’Œ node è‡ªåŠ¨è°ƒç„¦çš„å½“å‰çŠ¶æ€ï¼šå®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨å®ƒï¼ŒåŒ…æ‹¬åœ¨ç”Ÿäº§åº”ç”¨ç¨‹åºä¸­éƒ¨ç½²çš„æœ€ä½³å®žè·µã€‚&lt;/p>
&lt;!--
Enjoyed this talk? Join us for more exciting sessions on scaling and automating your Kubernetes clusters at KubeCon in Austin on December 6-8. [Register Now](https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006)
-->
&lt;p>å–œæ¬¢è¿™ä¸ªæ¼”è®²å—ï¼Ÿ 12 æœˆ 6 æ—¥è‡³ 8 æ—¥ï¼Œåœ¨ Austin å‚åŠ  KubeCon å…³äºŽæ‰©å±•å’Œè‡ªåŠ¨åŒ–æ‚¨çš„ Kubernetes é›†ç¾¤çš„æ›´ä»¤äººå…´å¥‹çš„ä¼šè®®ã€‚&lt;a href="https://www.eventbrite.com/e/kubecon-cloudnativecon-north-america-registration-37824050754?_ga=2.9666039.317115486.1510003873-1623727562.1496428006">çŽ°åœ¨æ³¨å†Œ&lt;/a>ã€‚&lt;/p>
&lt;!--
Be sure to check out [Automating and Testing Production Ready Kubernetes Clusters in the Public Cloud](http://sched.co/CU64) by Ron Lipke, Senior Developer, Platform as a Service, Gannet/USA Today Network.
-->
&lt;p>ä¸€å®šè¦æŸ¥çœ‹ç”± Ron Lipkeï¼Œ Gannet/USA Today Network, å¹³å°å³æœåŠ¡é«˜çº§å¼€å‘äººå‘˜ï¼Œåœ¨&lt;a href="http://sched.co/CU64">å…¬å…±äº‘ä¸­è‡ªåŠ¨åŒ–å’Œæµ‹è¯•äº§å“å°±ç»ªçš„ Kubernetes é›†ç¾¤&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: Kubernetes 1.8 çš„äº”å¤©</title><link>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</link><pubDate>Tue, 24 Oct 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2017/10/24/five-days-of-kubernetes-18/</guid><description>
&lt;!--
---
title: " Five Days of Kubernetes 1.8 "
date: 2017-10-24
slug: five-days-of-kubernetes-18
url: /blog/2017/10/Five-Days-Of-Kubernetes-18
---
-->
&lt;!--
Kubernetes 1.8 is live, made possible by hundreds of contributors pushing thousands of commits in this latest releases.
-->
&lt;p>Kubernetes 1.8 å·²ç»æŽ¨å‡ºï¼Œæ•°ç™¾åè´¡çŒ®è€…åœ¨è¿™ä¸ªæœ€æ–°ç‰ˆæœ¬ä¸­æŽ¨å‡ºäº†æˆåƒä¸Šä¸‡çš„æäº¤ã€‚&lt;/p>
&lt;!--
The community has tallied more than 66,000 commits in the main repo and continues rapid growth outside of the main repo, which signals growing maturity and stability for the project. The community has logged more than 120,000 commits across all repos and 17,839 commits across all repos for v1.7.0 to v1.8.0 alone.
-->
&lt;p>ç¤¾åŒºå·²ç»æœ‰è¶…è¿‡ 66,000 ä¸ªæäº¤åœ¨ä¸»ä»“åº“ï¼Œå¹¶åœ¨ä¸»ä»“åº“ä¹‹å¤–ç»§ç»­å¿«é€Ÿå¢žé•¿ï¼Œè¿™æ ‡å¿—ç€è¯¥é¡¹ç›®æ—¥ç›Šæˆç†Ÿå’Œç¨³å®šã€‚ä»… v1.7.0 åˆ° v1.8.0ï¼Œç¤¾åŒºå°±è®°å½•äº†æ‰€æœ‰ä»“åº“çš„è¶…è¿‡ 120,000 æ¬¡æäº¤å’Œ 17839 æ¬¡æäº¤ã€‚&lt;/p>
&lt;!--
With the help of our growing community of 1,400 plus contributors, we issued more than 3,000 PRs and pushed more than 5,000 commits to deliver Kubernetes 1.8 with significant security and workload support updates. This all points to increased stability, a result of our project-wide focus on maturing [process](https://github.com/kubernetes/sig-release), formalizing [architecture](https://github.com/kubernetes/community/tree/master/sig-architecture), and strengthening Kubernetesâ€™ [governance model](https://github.com/kubernetes/community/tree/master/community/elections/2017).
-->
&lt;p>åœ¨æ‹¥æœ‰ 1400 å¤šåè´¡çŒ®è€…ï¼Œå¹¶ä¸”ä¸æ–­å‘å±•å£®å¤§çš„ç¤¾åŒºçš„å¸®åŠ©ä¸‹ï¼Œæˆ‘ä»¬åˆå¹¶äº† 3000 å¤šä¸ª PRï¼Œå¹¶å‘å¸ƒäº† 5000 å¤šä¸ªæäº¤ï¼Œæœ€åŽçš„ Kubernetes 1.8 åœ¨å®‰å…¨å’Œå·¥ä½œè´Ÿè½½æ–¹é¢æ·»åŠ äº†å¾ˆå¤šçš„æ›´æ–°ã€‚
è¿™ä¸€åˆ‡éƒ½è¡¨æ˜Žç¨³å®šæ€§çš„æé«˜ï¼Œè¿™æ˜¯æˆ‘ä»¬æ•´ä¸ªé¡¹ç›®å…³æ³¨æˆç†Ÿ&lt;a href="https://github.com/kubernetes/sig-release">æµç¨‹&lt;/a>ã€å½¢å¼åŒ–&lt;a href="https://github.com/kubernetes/community/tree/master/sig-architecture">æž¶æž„&lt;/a>å’ŒåŠ å¼º Kubernetes çš„&lt;a href="https://github.com/kubernetes/community/tree/master/community/elections/2017">æ²»ç†æ¨¡åž‹&lt;/a>çš„ç»“æžœã€‚&lt;/p>
&lt;!--
While many improvements have been contributed, we highlight key features in this series of in-depth&amp;nbsp;posts listed below. [Follow along](https://twitter.com/kubernetesio) and see whatâ€™s new and improved with storage, security and more.
-->
&lt;p>è™½ç„¶æœ‰å¾ˆå¤šæ”¹è¿›ï¼Œä½†æˆ‘ä»¬åœ¨ä¸‹é¢åˆ—å‡ºçš„è¿™ä¸€ç³»åˆ—æ·±åº¦æ–‡ç« ä¸­çªå‡ºäº†ä¸€äº›å…³é”®ç‰¹æ€§ã€‚&lt;a href="https://twitter.com/kubernetesio">è·Ÿéš&lt;/a>å¹¶äº†è§£å­˜å‚¨ï¼Œå®‰å…¨ç­‰æ–¹é¢çš„æ–°åŠŸèƒ½å’Œæ”¹è¿›åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
**Day 1:** [5 Days of Kubernetes 1.8](https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18)
**Day 2:** [kubeadm v1.8 Introduces Easy Upgrades for Kubernetes Clusters](https://kubernetes.io/blog/2017/10/kubeadm-v18-released)
**Day 3:** [Kubernetes v1.8 Retrospective: It Takes a Village to Raise a Kubernetes](https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes)
**Day 4:** [Using RBAC, Generally Available in Kubernetes v1.8](https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18)
**Day 5:** [Enforcing Network Policies in Kubernetes](https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes)
-->
&lt;p>&lt;strong>ç¬¬ä¸€å¤©ï¼š&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/five-days-of-kubernetes-18">Kubernetes 1.8 çš„äº”å¤©&lt;/a>
&lt;strong>ç¬¬äºŒå¤©ï¼š&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/kubeadm-v18-released">kubeadm v1.8 ä¸º Kubernetes é›†ç¾¤å¼•å…¥äº†ç®€å•çš„å‡çº§&lt;/a>
&lt;strong>ç¬¬ä¸‰å¤©ï¼š&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/it-takes-village-to-raise-kubernetes">Kubernetes v1.8 å›žé¡¾ï¼šæå‡ä¸€ä¸ª Kubernetes éœ€è¦ä¸€ä¸ª Village&lt;/a>
&lt;strong>ç¬¬å››å¤©ï¼š&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/using-rbac-generally-available-18">ä½¿ç”¨ RBACï¼Œä¸€èˆ¬åœ¨ Kubernetes v1.8 ä¸­æä¾›&lt;/a>
&lt;strong>ç¬¬äº”å¤©ï¼š&lt;/strong> &lt;a href="https://kubernetes.io/blog/2017/10/enforcing-network-policies-in-kubernetes">åœ¨ Kubernetes æ‰§è¡Œç½‘ç»œç­–ç•¥&lt;/a>&lt;/p>
&lt;!--
**Connect**
-->
&lt;p>&lt;strong>é“¾æŽ¥&lt;/strong>&lt;/p>
&lt;!--
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Join the community portal for advocates on [K8sPort](http://k8sport.org/)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates&amp;nbsp;
- Connect with the community on [Slack](http://slack.k8s.io/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
-->
&lt;ul>
&lt;li>åœ¨ &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a> ä¸Šå‘å¸ƒé—®é¢˜ï¼ˆæˆ–å›žç­”é—®é¢˜ï¼‰&lt;/li>
&lt;li>åŠ å…¥ &lt;a href="http://k8sport.org/">K8sPort&lt;/a> å¸ƒé“å¸ˆçš„ç¤¾åŒºé—¨æˆ·ç½‘ç«™&lt;/li>
&lt;li>åœ¨ Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> å…³æ³¨æˆ‘ä»¬ä»¥èŽ·å–æœ€æ–°æ›´æ–°&lt;/li>
&lt;li>ä¸Ž &lt;a href="http://slack.k8s.io/">Slack&lt;/a> ä¸Šçš„ç¤¾åŒºè”ç³»&lt;/li>
&lt;li>å‚ä¸Ž &lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a> ä¸Šçš„ Kubernetes é¡¹ç›®&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes ç¤¾åŒºæŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾ç»“æžœ</title><link>https://kubernetes.io/blog/2017/10/kubernetes-community-steering-committee-election-results/</link><pubDate>Thu, 05 Oct 2017 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2017/10/kubernetes-community-steering-committee-election-results/</guid><description>
&lt;!--
---
title: " Kubernetes Community Steering Committee Election Results "
date: 2017-10-05
slug: kubernetes-community-steering-committee-election-results
url: /blog/2017/10/Kubernetes-Community-Steering-Committee-Election-Results
---
-->
&lt;!--
Beginning with the announcement of Kubernetes 1.0 at OSCON in 2015, there has been a concerted effort to share the power and burden of leadership across the Kubernetes community.
-->
&lt;p>è‡ª 2015 å¹´ OSCON å‘å¸ƒ Kubernetes 1.0 ä»¥æ¥ï¼Œå¤§å®¶ä¸€ç›´åœ¨å…±åŒåŠªåŠ›ï¼Œåœ¨ Kubernetes ç¤¾åŒºä¸­å…±åŒåˆ†äº«é¢†å¯¼åŠ›å’Œè´£ä»»ã€‚&lt;/p>
&lt;!--
With the work of the Bootstrap Governance Committee, consisting of Brandon Philips, Brendan Burns, Brian Grant, Clayton Coleman, Joe Beda, Sarah Novotny and Tim Hockin - a cross section of long-time leaders representing 5 different companies with major investments of talent and effort in the Kubernetes Ecosystem - we wrote an initial [Steering Committee Charter](https://github.com/kubernetes/steering/blob/master/charter.md) and launched a community wide election to seat a Kubernetes Steering Committee.
-->
&lt;p>åœ¨ Brandon Philipsã€Brendan Burnsã€Brian Grantã€Clayton Colemanã€Joe Bedaã€Sarah Novotny å’Œ Tim Hockin ç»„æˆçš„è‡ªä¸¾æ²»ç†å§”å‘˜ä¼šçš„å·¥ä½œä¸‹ - ä»£è¡¨ 5 å®¶ä¸åŒå…¬å¸çš„é•¿æœŸé¢†å¯¼è€…ï¼Œä»–ä»¬å¯¹ Kubernetes ç”Ÿæ€ç³»ç»Ÿè¿›è¡Œäº†å¤§é‡çš„äººæ‰æŠ•èµ„å’ŒåŠªåŠ› - ç¼–å†™äº†åˆå§‹çš„&lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">æŒ‡å¯¼å§”å‘˜ä¼šç« ç¨‹&lt;/a>ï¼Œå¹¶å‘èµ·äº†ä¸€æ¬¡ç¤¾åŒºé€‰ä¸¾ï¼Œä»¥é€‰ä¸¾ Kubernetes æŒ‡å¯¼å§”å‘˜ä¼šæˆå‘˜ã€‚&lt;/p>
&lt;!--
To quote from the Charter -
-->
&lt;p>å¼•ç”¨ç« ç¨‹ -&lt;/p>
&lt;!--
_The initial role of the steering committee is to **instantiate the formal process for Kubernetes governance**. In addition to defining the initial governance process, the bootstrap committee strongly believes that **it is important to provide a means for iterating** the processes defined by the steering committee. We do not believe that we will get it right the first time, or possibly ever, and wonâ€™t even complete the governance development in a single shot. The role of the steering committee is to be a live, responsive body that can refactor and reform as necessary to adapt to a changing project and community._
-->
&lt;p>_æŒ‡å¯¼å§”å‘˜ä¼šçš„æœ€åˆèŒè´£æ˜¯&lt;strong>å®žä¾‹åŒ– Kubernetes æ²»ç†çš„æ­£å¼è¿‡ç¨‹&lt;/strong>ã€‚é™¤å®šä¹‰åˆå§‹æ²»ç†è¿‡ç¨‹å¤–ï¼ŒæŒ‡å¯¼å§”å‘˜ä¼šè¿˜åšä¿¡&lt;strong>æä¾›ä¸€ç§æ–¹æ³•æ¥è¿­ä»£æŒ‡å¯¼å§”å‘˜ä¼šå®šä¹‰çš„æ–¹æ³•å¾ˆé‡è¦&lt;/strong>ã€‚æˆ‘ä»¬ä¸ç›¸ä¿¡æˆ‘ä»¬ä¼šåœ¨ç¬¬ä¸€æ¬¡æˆ–ä»¥åŽæŠŠè¿™äº›åšå¥½ï¼Œä¹Ÿä¸ä¼šä¸€å£æ°”å®Œæˆæ²»ç†å¼€å‘å·¥ä½œã€‚æŒ‡å¯¼å§”å‘˜ä¼šçš„ä½œç”¨æ˜¯æˆä¸ºä¸€ä¸ªç§¯æžå“åº”çš„æœºæž„ï¼Œå¯ä»¥æ ¹æ®éœ€è¦è¿›è¡Œé‡æž„å’Œæ”¹é€ ï¼Œä»¥é€‚åº”ä¸æ–­å˜åŒ–çš„é¡¹ç›®å’Œç¤¾åŒºã€‚&lt;/p>
&lt;!--
This is our largest step yet toward making an implicit governance structure explicit. Kubernetes vision has been one of an inclusive and broad community seeking to build software which empowers our users with the portability of containers. The Steering Committee will be a strong leadership voice guiding the project toward success.
-->
&lt;p>è¿™æ˜¯å°†æˆ‘ä»¬éšå¼æ²»ç†ç»“æž„æ˜Žç¡®åŒ–çš„æœ€å¤§ä¸€æ­¥ã€‚Kubernetes çš„æ„¿æ™¯ä¸€ç›´æ˜¯æˆä¸ºä¸€ä¸ªåŒ…å®¹è€Œå¹¿æ³›çš„ç¤¾åŒºï¼Œç”¨æˆ‘ä»¬çš„è½¯ä»¶å¸¦ç»™ç”¨æˆ·å®¹å™¨çš„ä¾¿åˆ©æ€§ã€‚æŒ‡å¯¼å§”å‘˜ä¼šå°†æ˜¯ä¸€ä¸ªå¼ºæœ‰åŠ›çš„å¼•é¢†å£°éŸ³ï¼ŒæŒ‡å¯¼è¯¥é¡¹ç›®å–å¾—æˆåŠŸã€‚&lt;/p>
&lt;!--
The Kubernetes Community is pleased to announce the results of the 2017 Steering Committee Elections. **Please congratulate Aaron Crickenberger, Derek Carr, Michelle Noorali, Phillip Wittrock, Quinton Hoole and Timothy St. Clair** , who will be joining the members of the Bootstrap Governance committee on the newly formed Kubernetes Steering Committee. Derek, Michelle, and Phillip will serve for 2 years. Aaron, Quinton, and Timothy will serve for 1 year.
-->
&lt;p>Kubernetes ç¤¾åŒºå¾ˆé«˜å…´åœ°å®£å¸ƒ 2017 å¹´æŒ‡å¯¼å§”å‘˜ä¼šé€‰ä¸¾çš„ç»“æžœã€‚ &lt;strong>è¯·ç¥è´º Aaron Crickenbergerã€Derek Carrã€Michelle Nooraliã€Phillip Wittrockã€Quinton Hoole å’Œ Timothy St. Clair&lt;/strong>ï¼Œä»–ä»¬å°†æˆä¸ºæ–°æˆç«‹çš„ Kubernetes æŒ‡å¯¼å§”å‘˜ä¼šçš„è‡ªä¸¾æ²»ç†å§”å‘˜ä¼šæˆå‘˜ã€‚Derekã€Michelle å’Œ Phillip å°†ä»»èŒ 2 å¹´ã€‚Aaronã€Quintonã€å’Œ Timothy å°†ä»»èŒ 1 å¹´ã€‚&lt;/p>
&lt;!--
This group will meet regularly in order to clarify and streamline the structure and operation of the project. Early work will include electing a representative to the CNCF Governing Board, evolving project processes, refining and documenting the vision and scope of the project, and chartering and delegating to more topical community groups.
-->
&lt;p>è¯¥å°ç»„å°†å®šæœŸå¼€ä¼šï¼Œä»¥é˜æ˜Žå’Œç®€åŒ–é¡¹ç›®çš„ç»“æž„å’Œè¿è¡Œã€‚æ—©æœŸçš„å·¥ä½œå°†åŒ…æ‹¬é€‰ä¸¾ CNCF ç†äº‹ä¼šçš„ä»£è¡¨ï¼Œå‘å±•é¡¹ç›®æµç¨‹ï¼Œå®Œå–„å’Œè®°å½•é¡¹ç›®çš„æ„¿æ™¯å’ŒèŒƒå›´ï¼Œä»¥åŠæŽˆæƒå’Œå§”æ´¾æ›´å¤šä¸»é¢˜ç¤¾åŒºå›¢ä½“ã€‚&lt;/p>
&lt;!--
Please see [the full Steering Committee backlog](https://github.com/kubernetes/steering/blob/master/backlog.md) for more details.
-->
&lt;p>è¯·å‚é˜…&lt;a href="https://github.com/kubernetes/steering/blob/master/backlog.md">å®Œæ•´çš„æŒ‡å¯¼å§”å‘˜ä¼šå¾…åŠžäº‹é¡¹åˆ—è¡¨&lt;/a>ä»¥èŽ·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ã€‚&lt;/p></description></item><item><title>Blog: ä½¿ç”¨ Kubernetes Pet Sets å’Œ Datera Elastic Data Fabric çš„ FlexVolume æ‰©å±•æœ‰çŠ¶æ€çš„åº”ç”¨ç¨‹åº</title><link>https://kubernetes.io/zh/blog/2016/08/stateful-applications-using-kubernetes-datera/</link><pubDate>Mon, 29 Aug 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/08/stateful-applications-using-kubernetes-datera/</guid><description>
&lt;!--
---
title: " Scaling Stateful Applications using Kubernetes Pet Sets and FlexVolumes with Datera Elastic Data Fabric "
date: 2016-08-29
slug: stateful-applications-using-kubernetes-datera
url: /blog/2016/08/Stateful-Applications-Using-Kubernetes-Datera
---
--->
&lt;!--
_Editorâ€™s note: todayâ€™s guest post is by Shailesh Mittal, Software Architect and Ashok Rajagopalan, Sr Director Product at Datera Inc, talking about Stateful Application provisioning with Kubernetes on Datera Elastic Data Fabric._
--->
&lt;p>&lt;em>ç¼–è€…æ³¨ï¼šä»Šå¤©çš„é‚€è¯·å¸–å­æ¥è‡ª Datera å…¬å¸çš„è½¯ä»¶æž¶æž„å¸ˆ Shailesh Mittal å’Œé«˜çº§äº§å“æ€»ç›‘ Ashok Rajagopalanï¼Œä»‹ç»åœ¨ Datera Elastic Data Fabric ä¸Šç”¨ Kubernetes é…ç½®çŠ¶æ€åº”ç”¨ç¨‹åºã€‚&lt;/em>&lt;/p>
&lt;!--
**Introduction**
Persistent volumes in Kubernetes are foundational as customers move beyond stateless workloads to run stateful applications. While Kubernetes has supported stateful applications such as MySQL, Kafka, Cassandra, and Couchbase for a while, the introduction of Pet Sets has significantly improved this support. In particular, the procedure to sequence the provisioning and startup, the ability to scale and associate durably by [Pet Sets](/docs/user-guide/petset/) has provided the ability to automate to scale the â€œPetsâ€ (applications that require consistent handling and durable placement).
--->
&lt;p>&lt;strong>ç®€ä»‹&lt;/strong>&lt;/p>
&lt;p>ç”¨æˆ·ä»Žæ— çŠ¶æ€å·¥ä½œè´Ÿè½½è½¬ç§»åˆ°è¿è¡Œæœ‰çŠ¶æ€åº”ç”¨ç¨‹åºï¼ŒKubernetes ä¸­çš„æŒä¹…å·æ˜¯åŸºç¡€ã€‚è™½ç„¶ Kubernetes æ—©å·²æ”¯æŒæœ‰çŠ¶æ€çš„åº”ç”¨ç¨‹åºï¼Œæ¯”å¦‚ MySQLã€Kafkaã€Cassandra å’Œ Couchbaseï¼Œä½†æ˜¯ Pet Sets çš„å¼•å…¥æ˜Žæ˜¾æ”¹å–„äº†æƒ…å†µã€‚ç‰¹åˆ«æ˜¯ï¼Œ&lt;a href="https://kubernetes.io/docs/user-guide/petset/">Pet Sets&lt;/a> å…·æœ‰æŒç»­æ‰©å±•å’Œå…³è”çš„èƒ½åŠ›ï¼Œåœ¨é…ç½®å’Œå¯åŠ¨çš„é¡ºåºè¿‡ç¨‹ä¸­ï¼Œå¯ä»¥è‡ªåŠ¨ç¼©æ”¾â€œPetsâ€ï¼ˆéœ€è¦è¿žç»­å¤„ç†å’ŒæŒä¹…æ”¾ç½®çš„åº”ç”¨ç¨‹åºï¼‰ã€‚&lt;/p>
&lt;!--
Datera, elastic block storage for cloud deployments, has [seamlessly integrated with Kubernetes](http://datera.io/blog-library/8/19/datera-simplifies-stateful-containers-on-kubernetes-13) through the [FlexVolume](/docs/user-guide/volumes/#flexvolume) framework. Based on the first principles of containers, Datera allows application resource provisioning to be decoupled from the underlying physical infrastructure. This brings clean contracts (aka, no dependency or direct knowledge of the underlying physical infrastructure), declarative formats, and eventually portability to stateful applications.
--->
&lt;p>Datera æ˜¯ç”¨äºŽäº‘éƒ¨ç½²çš„å¼¹æ€§å—å­˜å‚¨ï¼Œå¯ä»¥é€šè¿‡ &lt;a href="https://kubernetes.io/docs/user-guide/volumes/#flexvolume">FlexVolume&lt;/a> æ¡†æž¶ä¸Ž &lt;a href="http://datera.io/blog-library/8/19/datera-simplifies-stateful-containers-on-kubernetes-13">Kubernetes æ— ç¼é›†æˆ&lt;/a>ã€‚åŸºäºŽå®¹å™¨çš„åŸºæœ¬åŽŸåˆ™ï¼ŒDatera å…è®¸åº”ç”¨ç¨‹åºçš„èµ„æºé…ç½®ä¸Žåº•å±‚ç‰©ç†åŸºç¡€æž¶æž„åˆ†ç¦»ï¼Œä¸ºæœ‰çŠ¶æ€çš„åº”ç”¨ç¨‹åºæä¾›ç®€æ´çš„åè®®ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œä¸ä¾èµ–åº•å±‚ç‰©ç†åŸºç¡€ç»“æž„åŠå…¶ç›¸å…³å†…å®¹ï¼‰ã€å£°æ˜Žå¼æ ¼å¼å’Œæœ€åŽç§»æ¤çš„èƒ½åŠ›ã€‚&lt;/p>
&lt;!--
While Kubernetes allows for great flexibility to define the underlying application infrastructure through yaml configurations, Datera allows for that configuration to be passed to the storage infrastructure to provide persistence. Through the notion of Datera AppTemplates, in a Kubernetes environment, stateful applications can be automated to scale.
--->
&lt;p>Kubernetes å¯ä»¥é€šè¿‡ yaml é…ç½®æ¥çµæ´»å®šä¹‰åº•å±‚åº”ç”¨ç¨‹åºåŸºç¡€æž¶æž„ï¼Œè€Œ Datera å¯ä»¥å°†è¯¥é…ç½®ä¼ é€’ç»™å­˜å‚¨åŸºç¡€ç»“æž„ä»¥æä¾›æŒä¹…æ€§ã€‚é€šè¿‡ Datera AppTemplates å£°æ˜Žï¼Œåœ¨ Kubernetes çŽ¯å¢ƒä¸­ï¼Œæœ‰çŠ¶æ€çš„åº”ç”¨ç¨‹åºå¯ä»¥è‡ªåŠ¨æ‰©å±•ã€‚&lt;/p>
&lt;!--
**Deploying Persistent Storage**
Persistent storage is defined using the Kubernetes [PersistentVolume](/docs/user-guide/persistent-volumes/#persistent-volumes) subsystem. PersistentVolumes are volume plugins and define volumes that live independently of the lifecycle of the pod that is using it. They are implemented as NFS, iSCSI, or by cloud provider specific storage system. Datera has developed a volume plugin for PersistentVolumes that can provision iSCSI block storage on the Datera Data Fabric for Kubernetes pods.
--->
&lt;p>&lt;strong>éƒ¨ç½²æ°¸ä¹…æ€§å­˜å‚¨&lt;/strong>&lt;/p>
&lt;p>æ°¸ä¹…æ€§å­˜å‚¨æ˜¯é€šè¿‡ Kubernetes çš„å­ç³»ç»Ÿ &lt;a href="https://kubernetes.io/docs/user-guide/persistent-volumes/#persistent-volumes">PersistentVolume&lt;/a> å®šä¹‰çš„ã€‚PersistentVolumes æ˜¯å·æ’ä»¶ï¼Œå®ƒå®šä¹‰çš„å·çš„ç”Ÿå‘½å‘¨æœŸå’Œä½¿ç”¨å®ƒçš„ Pod ç›¸äº’ç‹¬ç«‹ã€‚PersistentVolumes ç”± NFSã€iSCSI æˆ–äº‘æä¾›å•†çš„ç‰¹å®šå­˜å‚¨ç³»ç»Ÿå®žçŽ°ã€‚Datera å¼€å‘äº†ç”¨äºŽ PersistentVolumes çš„å·æ’ä»¶ï¼Œå¯ä»¥åœ¨ Datera Data Fabric ä¸Šä¸º Kubernetes çš„ Pod é…ç½® iSCSI å—å­˜å‚¨ã€‚&lt;/p>
&lt;!--
The Datera volume plugin gets invoked by kubelets on minion nodes and relays the calls to the Datera Data Fabric over its REST API. Below is a sample deployment of a PersistentVolume with the Datera plugin:
--->
&lt;p>Datera å·æ’ä»¶ä»Ž minion nodes ä¸Šçš„ kubelet è°ƒç”¨ï¼Œå¹¶é€šè¿‡ REST API å›žä¼ åˆ° Datera Data Fabricã€‚ä»¥ä¸‹æ˜¯å¸¦æœ‰ Datera æ’ä»¶çš„ PersistentVolume çš„éƒ¨ç½²ç¤ºä¾‹ï¼š&lt;/p>
&lt;pre>&lt;code> apiVersion: v1
kind: PersistentVolume
metadata:
name: pv-datera-0
spec:
capacity:
storage: 100Gi
accessModes:
- ReadWriteOnce
persistentVolumeReclaimPolicy: Retain
flexVolume:
driver: &amp;quot;datera/iscsi&amp;quot;
fsType: &amp;quot;xfs&amp;quot;
options:
volumeID: &amp;quot;kube-pv-datera-0&amp;quot;
size: â€œ100&amp;quot;
replica: &amp;quot;3&amp;quot;
backstoreServer: &amp;quot;[tlx170.tlx.daterainc.com](http://tlx170.tlx.daterainc.com/):7717â€
&lt;/code>&lt;/pre>&lt;!--
This manifest defines a PersistentVolume of 100 GB to be provisioned in the Datera Data Fabric, should a pod request the persistent storage.
--->
&lt;p>ä¸º Pod ç”³è¯· PersistentVolumeï¼Œè¦æŒ‰ç…§ä»¥ä¸‹æ¸…å•åœ¨ Datera Data Fabric ä¸­é…ç½® 100 GB çš„ PersistentVolumeã€‚&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pv
NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE
pv-datera-0 100Gi RWO Available 8s
pv-datera-1 100Gi RWO Available 2s
pv-datera-2 100Gi RWO Available 7s
pv-datera-3 100Gi RWO Available 4s
&lt;/code>&lt;/pre>&lt;!--
**Configuration**
The Datera PersistenceVolume plugin is installed on all minion nodes. When a pod lands on a minion node with a valid claim bound to the persistent storage provisioned earlier, the Datera plugin forwards the request to create the volume on the Datera Data Fabric. All the options that are specified in the PersistentVolume manifest are sent to the plugin upon the provisioning request.
--->
&lt;p>&lt;strong>é…ç½®&lt;/strong>&lt;/p>
&lt;p>Datera PersistenceVolume æ’ä»¶å®‰è£…åœ¨æ‰€æœ‰ minion node ä¸Šã€‚minion node çš„å£°æ˜Žæ˜¯ç»‘å®šåˆ°ä¹‹å‰è®¾ç½®çš„æ°¸ä¹…æ€§å­˜å‚¨ä¸Šçš„ï¼Œå½“ Pod è¿›å…¥å…·å¤‡æœ‰æ•ˆå£°æ˜Žçš„ minion node ä¸Šæ—¶ï¼ŒDatera æ’ä»¶ä¼šè½¬å‘è¯·æ±‚ï¼Œä»Žè€Œåœ¨ Datera Data Fabric ä¸Šåˆ›å»ºå·ã€‚æ ¹æ®é…ç½®è¯·æ±‚ï¼ŒPersistentVolume æ¸…å•ä¸­æ‰€æœ‰æŒ‡å®šçš„é€‰é¡¹éƒ½å°†å‘é€åˆ°æ’ä»¶ã€‚&lt;/p>
&lt;!--
Once a volume is provisioned in the Datera Data Fabric, volumes are presented as an iSCSI block device to the minion node, and kubelet mounts this device for the containers (in the pod) to access it.
--->
&lt;p>åœ¨ Datera Data Fabric ä¸­é…ç½®çš„å·ä¼šä½œä¸º iSCSI å—è®¾å¤‡å‘ˆçŽ°ç»™ minion nodeï¼Œå¹¶ä¸” kubelet å°†è¯¥è®¾å¤‡å®‰è£…åˆ°å®¹å™¨ï¼ˆåœ¨ Pod ä¸­ï¼‰è¿›è¡Œè®¿é—®ã€‚&lt;/p>
&lt;p>&lt;img src="https://lh4.googleusercontent.com/ILlUm1HrWhGa8uTt97dQ786Gn20FHFZkavfucz05NHv6moZWiGDG7GlELM6o4CSzANWvZckoAVug5o4jMg17a-PbrfD1FRbDPeUCIc8fKVmVBNUsUPshWanXYkBa3gIJy5BnhLmZ" alt="">&lt;/p>
&lt;!--
**Using Persistent Storage**
Kubernetes PersistentVolumes are used along with a pod using PersistentVolume Claims. Once a claim is defined, it is bound to a PersistentVolume matching the claimâ€™s specification. A typical claim for the PersistentVolume defined above would look like below:
--->
&lt;p>&lt;strong>ä½¿ç”¨æ°¸ä¹…æ€§å­˜å‚¨&lt;/strong>&lt;/p>
&lt;p>Kubernetes PersistentVolumes ä¸Žå…·å¤‡ PersistentVolume Claims çš„ Pod ä¸€èµ·ä½¿ç”¨ã€‚å®šä¹‰å£°æ˜ŽåŽï¼Œä¼šè¢«ç»‘å®šåˆ°ä¸Žå£°æ˜Žè§„èŒƒåŒ¹é…çš„ PersistentVolume ä¸Šã€‚ä¸Šé¢æåˆ°çš„å®šä¹‰ PersistentVolume çš„å…¸åž‹å£°æ˜Žå¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/p>
&lt;pre>&lt;code>kind: PersistentVolumeClaim
apiVersion: v1
metadata:
name: pv-claim-test-petset-0
spec:
accessModes:
- ReadWriteOnce
resources:
requests:
storage: 100Gi
&lt;/code>&lt;/pre>&lt;!--
When this claim is defined and it is bound to a PersistentVolume, resources can be used with the pod specification:
--->
&lt;p>å®šä¹‰è¿™ä¸ªå£°æ˜Žå¹¶å°†å…¶ç»‘å®šåˆ° PersistentVolume æ—¶ï¼Œèµ„æºä¸Ž Pod è§„èŒƒå¯ä»¥ä¸€èµ·ä½¿ç”¨ï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pv
NAME CAPACITY ACCESSMODES STATUS CLAIM REASON AGE
pv-datera-0 100Gi RWO Bound default/pv-claim-test-petset-0 6m
pv-datera-1 100Gi RWO Bound default/pv-claim-test-petset-1 6m
pv-datera-2 100Gi RWO Available 7s
pv-datera-3 100Gi RWO Available 4s
[root@tlx241 /]# kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESSMODES AGE
pv-claim-test-petset-0 Bound pv-datera-0 0 3m
pv-claim-test-petset-1 Bound pv-datera-1 0 3m
&lt;/code>&lt;/pre>&lt;!--
A pod can use a PersistentVolume Claim like below:
--->
&lt;p>Pod å¯ä»¥ä½¿ç”¨ PersistentVolume å£°æ˜Žï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/p>
&lt;pre>&lt;code>apiVersion: v1
kind: Pod
metadata:
name: kube-pv-demo
spec:
containers:
- name: data-pv-demo
image: nginx
volumeMounts:
- name: test-kube-pv1
mountPath: /data
ports:
- containerPort: 80
volumes:
- name: test-kube-pv1
persistentVolumeClaim:
claimName: pv-claim-test-petset-0
&lt;/code>&lt;/pre>&lt;!--
The result is a pod using a PersistentVolume Claim as a volume. It in-turn sends the request to the Datera volume plugin to provision storage in the Datera Data Fabric.
--->
&lt;p>ç¨‹åºçš„ç»“æžœæ˜¯ Pod å°† PersistentVolume Claim ä½œä¸ºå·ã€‚ä¾æ¬¡å°†è¯·æ±‚å‘é€åˆ° Datera å·æ’ä»¶ï¼Œç„¶åŽåœ¨ Datera Data Fabric ä¸­é…ç½®å­˜å‚¨ã€‚&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl describe pods kube-pv-demo
Name: kube-pv-demo
Namespace: default
Node: tlx243/172.19.1.243
Start Time: Sun, 14 Aug 2016 19:17:31 -0700
Labels: \&amp;lt;none\&amp;gt;
Status: Running
IP: 10.40.0.3
Controllers: \&amp;lt;none\&amp;gt;
Containers:
data-pv-demo:
Container ID: [docker://ae2a50c25e03143d0dd721cafdcc6543fac85a301531110e938a8e0433f74447](about:blank)
Image: nginx
Image ID: [docker://sha256:0d409d33b27e47423b049f7f863faa08655a8c901749c2b25b93ca67d01a470d](about:blank)
Port: 80/TCP
State: Running
Started: Sun, 14 Aug 2016 19:17:34 -0700
Ready: True
Restart Count: 0
Environment Variables: \&amp;lt;none\&amp;gt;
Conditions:
Type Status
Initialized True
Ready True
PodScheduled True
Volumes:
test-kube-pv1:
Type: PersistentVolumeClaim (a reference to a PersistentVolumeClaim in the same namespace)
ClaimName: pv-claim-test-petset-0
ReadOnly: false
default-token-q3eva:
Type: Secret (a volume populated by a Secret)
SecretName: default-token-q3eva
QoS Tier: BestEffort
Events:
FirstSeen LastSeen Count From SubobjectPath Type Reason Message
--------- -------- ----- ---- ------------- -------- ------ -------
43s 43s 1 {default-scheduler } Normal Scheduled Successfully assigned kube-pv-demo to tlx243
42s 42s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulling pulling image &amp;quot;nginx&amp;quot;
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Pulled Successfully pulled image &amp;quot;nginx&amp;quot;
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Created Created container with docker id ae2a50c25e03
40s 40s 1 {kubelet tlx243} spec.containers{data-pv-demo} Normal Started Started container with docker id ae2a50c25e03
&lt;/code>&lt;/pre>&lt;!--
The persistent volume is presented as iSCSI device at minion node (tlx243 in this case):
--->
&lt;p>æ°¸ä¹…å·åœ¨ minion nodeï¼ˆåœ¨æœ¬ä¾‹ä¸­ä¸º tlx243ï¼‰ä¸­æ˜¾ç¤ºä¸º iSCSI è®¾å¤‡ï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx243 ~]# lsscsi
[0:2:0:0] disk SMC SMC2208 3.24 /dev/sda
[11:0:0:0] disk DATERA IBLOCK 4.0 /dev/sdb
[root@tlx243 datera~iscsi]# mount ``` grep sdb
/dev/sdb on /var/lib/kubelet/pods/6b99bd2a-628e-11e6-8463-0cc47ab41442/volumes/datera~iscsi/pv-datera-0 type xfs (rw,relatime,attr2,inode64,noquota)
&lt;/code>&lt;/pre>&lt;!--
Containers running in the pod see this device mounted at /data as specified in the manifest:
--->
&lt;p>åœ¨ Pod ä¸­è¿è¡Œçš„å®¹å™¨æŒ‰ç…§æ¸…å•ä¸­å°†è®¾å¤‡å®‰è£…åœ¨ /data ä¸Šï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl exec kube-pv-demo -c data-pv-demo -it bash
root@kube-pv-demo:/# mount ``` grep data
/dev/sdb on /data type xfs (rw,relatime,attr2,inode64,noquota)
&lt;/code>&lt;/pre>&lt;!--
**Using Pet Sets**
Typically, pods are treated as stateless units, so if one of them is unhealthy or gets superseded, Kubernetes just disposes it. In contrast, a PetSet is a group of stateful pods that has a stronger notion of identity. The goal of a PetSet is to decouple this dependency by assigning identities to individual instances of an application that are not anchored to the underlying physical infrastructure.
--->
&lt;p>&lt;strong>ä½¿ç”¨ Pet Sets&lt;/strong>&lt;/p>
&lt;p>é€šå¸¸ï¼ŒPod è¢«è§†ä¸ºæ— çŠ¶æ€å•å…ƒï¼Œå› æ­¤ï¼Œå¦‚æžœå…¶ä¸­ä¹‹ä¸€çŠ¶æ€å¼‚å¸¸æˆ–è¢«å–ä»£ï¼ŒKubernetes ä¼šå°†å…¶ä¸¢å¼ƒã€‚ç›¸åï¼ŒPetSet æ˜¯ä¸€ç»„æœ‰çŠ¶æ€çš„ Podï¼Œå…·æœ‰æ›´å¼ºçš„èº«ä»½æ¦‚å¿µã€‚PetSet å¯ä»¥å°†æ ‡è¯†åˆ†é…ç»™åº”ç”¨ç¨‹åºçš„å„ä¸ªå®žä¾‹ï¼Œè¿™äº›åº”ç”¨ç¨‹åºæ²¡æœ‰ä¸Žåº•å±‚ç‰©ç†ç»“æž„è¿žæŽ¥ï¼ŒPetSet å¯ä»¥æ¶ˆé™¤è¿™ç§ä¾èµ–æ€§ã€‚&lt;/p>
&lt;!--
A PetSet requires {0..n-1} Pets. Each Pet has a deterministic name, PetSetName-Ordinal, and a unique identity. Each Pet has at most one pod, and each PetSet has at most one Pet with a given identity. A PetSet ensures that a specified number of â€œpetsâ€ with unique identities are running at any given time. The identity of a Pet is comprised of:
- a stable hostname, available in DNS
- an ordinal index
- stable storage: linked to the ordinal &amp; hostname
A typical PetSet definition using a PersistentVolume Claim looks like below:
--->
&lt;p>æ¯ä¸ª PetSet éœ€è¦{0..n-1}ä¸ª Petã€‚æ¯ä¸ª Pet éƒ½æœ‰ä¸€ä¸ªç¡®å®šçš„åå­—ã€PetSetName-Ordinal å’Œå”¯ä¸€çš„èº«ä»½ã€‚æ¯ä¸ª Pet æœ€å¤šæœ‰ä¸€ä¸ª Podï¼Œæ¯ä¸ª PetSet æœ€å¤šåŒ…å«ä¸€ä¸ªç»™å®šèº«ä»½çš„ Petã€‚è¦ç¡®ä¿æ¯ä¸ª PetSet åœ¨ä»»ä½•ç‰¹å®šæ—¶é—´è¿è¡Œæ—¶ï¼Œå…·æœ‰å”¯ä¸€æ ‡è¯†çš„â€œpetâ€çš„æ•°é‡éƒ½æ˜¯ç¡®å®šçš„ã€‚Pet çš„èº«ä»½æ ‡è¯†åŒ…æ‹¬ä»¥ä¸‹å‡ ç‚¹ï¼š&lt;/p>
&lt;ul>
&lt;li>ä¸€ä¸ªç¨³å®šçš„ä¸»æœºåï¼Œå¯ä»¥åœ¨ DNS ä¸­ä½¿ç”¨&lt;/li>
&lt;li>ä¸€ä¸ªåºå·ç´¢å¼•&lt;/li>
&lt;li>ç¨³å®šçš„å­˜å‚¨ï¼šé“¾æŽ¥åˆ°åºå·å’Œä¸»æœºå&lt;/li>
&lt;/ul>
&lt;p>ä½¿ç”¨ PersistentVolume Claim å®šä¹‰ PetSet çš„å…¸åž‹ä¾‹å­å¦‚ä¸‹æ‰€ç¤ºï¼š&lt;/p>
&lt;pre>&lt;code># A headless service to create DNS records
apiVersion: v1
kind: Service
metadata:
name: test-service
labels:
app: nginx
spec:
ports:
- port: 80
name: web
clusterIP: None
selector:
app: nginx
---
apiVersion: apps/v1alpha1
kind: PetSet
metadata:
name: test-petset
spec:
serviceName: &amp;quot;test-service&amp;quot;
replicas: 2
template:
metadata:
labels:
app: nginx
annotations:
[pod.alpha.kubernetes.io/initialized:](http://pod.alpha.kubernetes.io/initialized:) &amp;quot;true&amp;quot;
spec:
terminationGracePeriodSeconds: 0
containers:
- name: nginx
image: [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
ports:
- containerPort: 80
name: web
volumeMounts:
- name: pv-claim
mountPath: /data
volumeClaimTemplates:
- metadata:
name: pv-claim
annotations:
[volume.alpha.kubernetes.io/storage-class:](http://volume.alpha.kubernetes.io/storage-class:) anything
spec:
accessModes: [&amp;quot;ReadWriteOnce&amp;quot;]
resources:
requests:
storage: 100Gi
&lt;/code>&lt;/pre>&lt;!--
We have the following PersistentVolume Claims available:
--->
&lt;p>æˆ‘ä»¬æä¾›ä»¥ä¸‹ PersistentVolume Claimï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESSMODES AGE
pv-claim-test-petset-0 Bound pv-datera-0 0 41m
pv-claim-test-petset-1 Bound pv-datera-1 0 41m
pv-claim-test-petset-2 Bound pv-datera-2 0 5s
pv-claim-test-petset-3 Bound pv-datera-3 0 2s
&lt;/code>&lt;/pre>&lt;!--
When this PetSet is provisioned, two pods get instantiated:
--->
&lt;p>é…ç½® PetSet æ—¶ï¼Œå°†å®žä¾‹åŒ–ä¸¤ä¸ª Podï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl get pods
NAMESPACE NAME READY STATUS RESTARTS AGE
default test-petset-0 1/1 Running 0 7s
default test-petset-1 1/1 Running 0 3s
&lt;/code>&lt;/pre>&lt;!--
Here is how the PetSet test-petset instantiated earlier looks like:
--->
&lt;p>ä»¥ä¸‹æ˜¯ä¸€ä¸ª PetSetï¼štest-petset å®žä¾‹åŒ–ä¹‹å‰çš„æ ·å­ï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl describe petset test-petset
Name: test-petset
Namespace: default
Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
Selector: app=nginx
Labels: app=nginx
Replicas: 2 current / 2 desired
Annotations: \&amp;lt;none\&amp;gt;
CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700
Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
No events.
&lt;/code>&lt;/pre>&lt;!--
Once a PetSet is instantiated, such as test-petset below, upon increasing the number of replicas (i.e. the number of pods started with that PetSet), more pods get instantiated and more PersistentVolume Claims get bound to new pods:
--->
&lt;p>ä¸€æ—¦å®žä¾‹åŒ– PetSetï¼ˆä¾‹å¦‚ä¸‹é¢çš„ test-petsetï¼‰ï¼Œéšç€å‰¯æœ¬æ•°ï¼ˆä»Ž PetSet çš„åˆå§‹ Pod æ•°é‡ç®—èµ·ï¼‰çš„å¢žåŠ ï¼Œå®žä¾‹åŒ–çš„ Pod å°†å˜å¾—æ›´å¤šï¼Œå¹¶ä¸”æ›´å¤šçš„ PersistentVolume Claim ä¼šç»‘å®šåˆ°æ–°çš„ Pod ä¸Šï¼š&lt;/p>
&lt;pre>&lt;code>[root@tlx241 /]# kubectl patch petset test-petset -p'{&amp;quot;spec&amp;quot;:{&amp;quot;replicas&amp;quot;:&amp;quot;3&amp;quot;}}'
&amp;quot;test-petsetâ€ patched
[root@tlx241 /]# kubectl describe petset test-petset
Name: test-petset
Namespace: default
Image(s): [gcr.io/google\_containers/nginx-slim:0.8](http://gcr.io/google_containers/nginx-slim:0.8)
Selector: app=nginx
Labels: app=nginx
Replicas: 3 current / 3 desired
Annotations: \&amp;lt;none\&amp;gt;
CreationTimestamp: Sun, 14 Aug 2016 19:46:30 -0700
Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed
No volumes.
No events.
[root@tlx241 /]# kubectl get pods
NAME READY STATUS RESTARTS AGE
test-petset-0 1/1 Running 0 29m
test-petset-1 1/1 Running 0 28m
test-petset-2 1/1 Running 0 9s
&lt;/code>&lt;/pre>&lt;!--
Now the PetSet is running 3 pods after patch application.
--->
&lt;p>çŽ°åœ¨ï¼Œåº”ç”¨ä¿®è¡¥ç¨‹åºåŽï¼ŒPetSet æ­£åœ¨è¿è¡Œ3ä¸ª Podã€‚&lt;/p>
&lt;!--
When the above PetSet definition is patched to have one more replica, it introduces one more pod in the system. This in turn results in one more volume getting provisioned on the Datera Data Fabric. So volumes get dynamically provisioned and attached to a pod upon the PetSet scaling up.
--->
&lt;p>å½“ä¸Šè¿° PetSet å®šä¹‰ä¿®è¡¥å®Œæˆï¼Œä¼šäº§ç”Ÿå¦ä¸€ä¸ªå‰¯æœ¬ï¼ŒPetSet å°†åœ¨ç³»ç»Ÿä¸­å¼•å…¥å¦ä¸€ä¸ª podã€‚åä¹‹ï¼Œè¿™ä¼šå¯¼è‡´åœ¨ Datera Data Fabric ä¸Šé…ç½®æ›´å¤šçš„å·ã€‚å› æ­¤ï¼Œåœ¨ PetSet è¿›è¡Œæ‰©å±•æ—¶ï¼Œè¦é…ç½®åŠ¨æ€å·å¹¶å°†å…¶é™„åŠ åˆ° Pod ä¸Šã€‚&lt;/p>
&lt;!--
To support the notion of durability and consistency, if a pod moves from one minion to another, volumes do get attached (mounted) to the new minion node and detached (unmounted) from the old minion to maintain persistent access to the data.
--->
&lt;p>ä¸ºäº†å¹³è¡¡æŒä¹…æ€§å’Œä¸€è‡´æ€§çš„æ¦‚å¿µï¼Œå¦‚æžœ Pod ä»Žä¸€ä¸ª Minion è½¬ç§»åˆ°å¦ä¸€ä¸ªï¼Œå·ç¡®å®žä¼šé™„åŠ ï¼ˆå®‰è£…ï¼‰åˆ°æ–°çš„ minion node ä¸Šï¼Œå¹¶ä¸Žæ—§çš„ Minion åˆ†ç¦»ï¼ˆå¸è½½ï¼‰ï¼Œä»Žè€Œå®žçŽ°å¯¹æ•°æ®çš„æŒä¹…è®¿é—®ã€‚&lt;/p>
&lt;!--
**Conclusion**
This demonstrates Kubernetes with Pet Sets orchestrating stateful and stateless workloads. While the Kubernetes community is working on expanding the FlexVolume frameworkâ€™s capabilities, we are excited that this solution makes it possible for Kubernetes to be run more widely in the datacenters.
--->
&lt;p>&lt;strong>ç»“è®º&lt;/strong>&lt;/p>
&lt;p>æœ¬æ–‡å±•ç¤ºäº†å…·å¤‡ Pet Sets çš„ Kubernetes åè°ƒæœ‰çŠ¶æ€å’Œæ— çŠ¶æ€å·¥ä½œè´Ÿè½½ã€‚å½“ Kubernetes ç¤¾åŒºè‡´åŠ›äºŽæ‰©å±• FlexVolume æ¡†æž¶çš„åŠŸèƒ½æ—¶ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´è¿™ä¸ªè§£å†³æ–¹æ¡ˆä½¿ Kubernetes èƒ½å¤Ÿåœ¨æ•°æ®ä¸­å¿ƒå¹¿æ³›è¿è¡Œã€‚&lt;/p>
&lt;!--
Join and contribute: Kubernetes [Storage SIG](https://groups.google.com/forum/#!forum/kubernetes-sig-storage).
--->
&lt;p>åŠ å…¥æˆ‘ä»¬å¹¶ä½œå‡ºè´¡çŒ®ï¼šKubernetes &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-storage">Storage SIG&lt;/a>.&lt;/p>
&lt;!--
- [Download Kubernetes](http://get.k8s.io/)
- Get involved with the Kubernetes project on [GitHub](https://github.com/kubernetes/kubernetes)
- Post questions (or answer questions) on [Stack Overflow](http://stackoverflow.com/questions/tagged/kubernetes)
- Connect with the community on the [k8s Slack](http://slack.k8s.io/)
- Follow us on Twitter [@Kubernetesio](https://twitter.com/kubernetesio) for latest updates
--->
&lt;ul>
&lt;li>&lt;a href="http://get.k8s.io/">ä¸‹è½½ Kubernetes&lt;/a>&lt;/li>
&lt;li>å‚ä¸Ž Kubernetes é¡¹ç›® &lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a>&lt;/li>
&lt;li>å‘å¸ƒé—®é¢˜ï¼ˆæˆ–è€…å›žç­”é—®é¢˜ï¼‰ &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>è”ç³»ç¤¾åŒº &lt;a href="http://slack.k8s.io/">k8s Slack&lt;/a>&lt;/li>
&lt;li>åœ¨ Twitter ä¸Šå…³æ³¨æˆ‘ä»¬ &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes ç”Ÿæ—¥å¿«ä¹ã€‚å“¦ï¼Œè¿™æ˜¯ä½ è¦åŽ»çš„åœ°æ–¹ï¼</title><link>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</link><pubDate>Thu, 21 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/21/oh-the-places-you-will-go/</guid><description>
&lt;!--
---
title: " Happy Birthday Kubernetes. Oh, the places youâ€™ll go! "
date: 2016-07-21
slug: oh-the-places-you-will-go
url: /blog/2016/07/Oh-The-Places-You-Will-Go
---
-->
&lt;!--
_Editorâ€™s note, Todayâ€™s guest post is from an independent Kubernetes contributor, Justin Santa Barbara, sharing his reflection on growth of the project from inception to its future._
**Dear K8s,**
_Itâ€™s hard to believe youâ€™re only one - youâ€™ve grown up so fast. On the occasion of your first birthday, I thought I would write a little note about why I was so excited when you were born, why I feel fortunate to be part of the group that is raising you, and why Iâ€™m eager to watch you continue to grow up!_
-->
&lt;p>&lt;em>ç¼–è€…æŒ‰ï¼Œä»Šå¤©çš„å˜‰å®¾å¸–å­æ¥è‡ªä¸€ä½ç‹¬ç«‹çš„ kubernetes æ’°ç¨¿äºº Justin Santa Barbaraï¼Œåˆ†äº«äº†ä»–å¯¹é¡¹ç›®ä»Žä¸€å¼€å§‹åˆ°æœªæ¥å‘å±•çš„æ€è€ƒã€‚&lt;/em>&lt;/p>
&lt;p>&lt;strong>äº²çˆ±çš„ K8s,&lt;/strong>&lt;/p>
&lt;p>&lt;em>å¾ˆéš¾ç›¸ä¿¡ä½ æ˜¯å”¯ä¸€çš„ä¸€ä¸ª - æˆé•¿è¿™ä¹ˆå¿«çš„ã€‚åœ¨ä½ ä¸€å²ç”Ÿæ—¥çš„æ—¶å€™ï¼Œæˆ‘æƒ³æˆ‘å¯ä»¥å†™ä¸€ä¸ªå°çº¸æ¡ï¼Œå‘Šè¯‰ä½ ä¸ºä»€ä¹ˆæˆ‘åœ¨ä½ å‡ºç”Ÿçš„æ—¶å€™é‚£ä¹ˆå…´å¥‹ï¼Œä¸ºä»€ä¹ˆæˆ‘è§‰å¾—å¾ˆå¹¸è¿èƒ½æˆä¸ºæŠšå…»ä½ é•¿å¤§çš„ä¸€å‘˜ï¼Œä¸ºä»€ä¹ˆæˆ‘æ¸´æœ›çœ‹åˆ°ä½ ç»§ç»­æˆé•¿ï¼&lt;/em>&lt;/p>
&lt;!--
_--Justin_
You started with an excellent foundation - good declarative functionality, built around a solid API with a well defined schema and the machinery so that we could evolve going forwards. And sure enough, over your first year you grew so fast: autoscaling, HTTP load-balancing support (Ingress), support for persistent workloads including clustered databases (PetSets). Youâ€™ve made friends with more clouds (welcome Azure &amp; OpenStack to the family), and even started to span zones and clusters (Federation). And these are just some of the most visible changes - thereâ€™s so much happening inside that brain of yours!
I think itâ€™s wonderful youâ€™ve remained so open in all that you do - you seem to write down everything on GitHub - for better or worse. I think weâ€™ve all learned a lot about that on the way, like the perils of having engineers make scaling statements that are then weighed against claims made without quite the same framework of precision and rigor. But Iâ€™m proud that you chose not to lower your standards, but rose to the challenge and just ran faster instead - it might not be the most realistic approach, but it is the only way to move mountains!
-->
&lt;p>&lt;em>--Justin&lt;/em>&lt;/p>
&lt;p>ä½ ä»Žä¸€ä¸ªä¼˜ç§€çš„åŸºç¡€ - è‰¯å¥½çš„å£°æ˜Žæ€§åŠŸèƒ½å¼€å§‹ï¼Œå®ƒæ˜¯å›´ç»•ä¸€ä¸ªå…·æœ‰è‰¯å¥½å®šä¹‰çš„æ¨¡å¼å’Œæœºåˆ¶çš„åšå®žçš„ API æž„å»ºçš„ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å‘å‰å‘å±•äº†ã€‚æžœç„¶ï¼Œåœ¨ä½ çš„ç¬¬ä¸€å¹´é‡Œï¼Œä½ å¢žé•¿å¾—å¦‚æ­¤ä¹‹å¿«ï¼šautoscalingã€HTTP load-balancing support (Ingress)ã€support for persistent workloads including clustered databases (PetSets)ã€‚ä½ å·²ç»å’Œæ›´å¤šçš„äº‘äº¤äº†æœ‹å‹(æ¬¢è¿Ž azure å’Œ openstack åŠ å…¥å®¶åº­)ï¼Œç”šè‡³å¼€å§‹è·¨è¶ŠåŒºåŸŸå’Œé›†ç¾¤(Federation)ã€‚è¿™äº›åªæ˜¯ä¸€äº›æœ€æ˜Žæ˜¾çš„å˜åŒ– - åœ¨ä½ çš„å¤§è„‘é‡Œå‘ç”Ÿäº†å¤ªå¤šçš„å˜åŒ–ï¼&lt;/p>
&lt;p>æˆ‘è§‰å¾—ä½ ä¸€ç›´ä¿æŒå¼€æ”¾çš„æ€åº¦çœŸæ˜¯å¤ªå¥½äº† - ä½ å¥½åƒæŠŠæ‰€æœ‰çš„ä¸œè¥¿éƒ½å†™åœ¨ github ä¸Š - ä¸ç®¡æ˜¯å¥½æ˜¯åã€‚æˆ‘æƒ³æˆ‘ä»¬åœ¨è¿™æ–¹é¢éƒ½å­¦åˆ°äº†å¾ˆå¤šï¼Œæ¯”å¦‚è®©å·¥ç¨‹å¸ˆåšç¼©æ”¾å£°æ˜Žçš„é£Žé™©ï¼Œç„¶åŽåœ¨æ²¡æœ‰å®Œå…¨ç›¸åŒçš„ç²¾ç¡®æ€§å’Œä¸¥è°¨æ€§æ¡†æž¶çš„æƒ…å†µä¸‹ï¼Œå°†è¿™äº›å£°æ˜Žä¸Žç´¢èµ”è¿›è¡Œæƒè¡¡ã€‚ä½†æˆ‘å¾ˆè‡ªè±ªä½ é€‰æ‹©äº†ä¸é™ä½Žä½ çš„æ ‡å‡†ï¼Œè€Œæ˜¯ä¸Šå‡åˆ°æŒ‘æˆ˜ï¼Œåªæ˜¯è·‘å¾—æ›´å¿« - è¿™å¯èƒ½ä¸æ˜¯æœ€çŽ°å®žçš„åŠžæ³•ï¼Œä½†è¿™æ˜¯å”¯ä¸€çš„æ–¹å¼èƒ½ç§»åŠ¨å±±ï¼&lt;/p>
&lt;!--
And yet, somehow, youâ€™ve managed to avoid a lot of the common dead-ends that other open source software has fallen into, particularly as those projects got bigger and the developers end up working on it more than they use it directly. How did you do that? Thereâ€™s a probably-apocryphal story of an employee at IBM that makes a huge mistake, and is summoned to meet with the big boss, expecting to be fired, only to be told â€œWe just spent several million dollars training you. Why would we want to fire you?â€. Despite all the investment google is pouring into you (along with Redhat and others), I sometimes wonder if the mistakes we are avoiding could be worth even more. There is a very open development process, yet thereâ€™s also an â€œoracleâ€ that will sometimes course-correct by telling us what happens two years down the road if we make a particular design decision. This is a parent you should probably listen to!
-->
&lt;p>ç„¶è€Œï¼Œä¸çŸ¥ä½•æ•…ï¼Œä½ å·²ç»è®¾æ³•é¿å…äº†è®¸å¤šå…¶ä»–å¼€æºè½¯ä»¶é™·å…¥çš„å…±åŒæ­»èƒ¡åŒï¼Œç‰¹åˆ«æ˜¯å½“é‚£äº›é¡¹ç›®è¶Šæ¥è¶Šå¤§ï¼Œå¼€å‘äººå‘˜æœ€ç»ˆè¦åšçš„æ¯”ç›´æŽ¥ä½¿ç”¨å®ƒæ›´å¤šçš„æ—¶å€™ã€‚ä½ æ˜¯æ€Žä¹ˆåšåˆ°çš„ï¼Ÿæœ‰ä¸€ä¸ªå¾ˆå¯èƒ½æ˜¯è™šæž„çš„æ•…äº‹ï¼Œè®²çš„æ˜¯ IBM çš„ä¸€åå‘˜å·¥çŠ¯äº†ä¸€ä¸ªå·¨å¤§çš„é”™è¯¯ï¼Œè¢«ä¼ å”¤åŽ»è§å¤§è€æ¿ï¼Œå¸Œæœ›è¢«è§£é›‡ï¼Œå´è¢«å‘ŠçŸ¥â€œæˆ‘ä»¬åˆšåˆšèŠ±äº†å‡ ç™¾ä¸‡ç¾Žå…ƒåŸ¹è®­ä½ ã€‚æˆ‘ä»¬ä¸ºä»€ä¹ˆè¦è§£é›‡ä½ ï¼Ÿâ€œã€‚å°½ç®¡è°·æ­Œå¯¹ä½ è¿›è¡Œäº†å¤§é‡çš„æŠ•èµ„(åŒ…æ‹¬ redhat å’Œå…¶ä»–å…¬å¸)ï¼Œä½†æˆ‘æœ‰æ—¶æƒ³çŸ¥é“ï¼Œæˆ‘ä»¬æ­£åœ¨é¿å…çš„é”™è¯¯æ˜¯å¦æ›´æœ‰ä»·å€¼ã€‚æœ‰ä¸€ä¸ªéžå¸¸å¼€æ”¾çš„å¼€å‘è¿‡ç¨‹ï¼Œä½†ä¹Ÿæœ‰ä¸€ä¸ªâ€œoracleâ€ï¼Œå®ƒæœ‰æ—¶ä¼šé€šè¿‡å‘Šè¯‰æˆ‘ä»¬ä¸¤å¹´åŽå¦‚æžœæˆ‘ä»¬åšä¸€ä¸ªç‰¹å®šçš„è®¾è®¡å†³ç­–ä¼šå‘ç”Ÿä»€ä¹ˆæ¥çº æ­£é”™è¯¯ã€‚è¿™æ˜¯ä½ åº”è¯¥å¬çš„çˆ¶æ¯ï¼&lt;/p>
&lt;!--
And so although youâ€™re only a year old, you really have an [old soul](http://queue.acm.org/detail.cfm?id=2898444). Iâ€™m just one of the [many people raising you](https://kubernetes.io/blog/2016/07/happy-k8sbday-1), but itâ€™s a wonderful learning experience for me to be able to work with the people that have built these incredible systems and have all this domain knowledge. Yet because we started from scratch (rather than taking the existing Borg code) weâ€™re at the same level and can still have genuine discussions about how to raise you. Well, at least as close to the same level as we could ever be, but itâ€™s to their credit that they are all far too nice ever to mention it!
If I would pick just two of the wise decisions those brilliant people made:
-->
&lt;p>æ‰€ä»¥ï¼Œå°½ç®¡ä½ åªæœ‰ä¸€å²ï¼Œä½ çœŸçš„æœ‰ä¸€ä¸ª&lt;a href="http://queue.acm.org/detail.cfm?ID=2898444">æ—§çµé­‚&lt;/a>ã€‚æˆ‘åªæ˜¯&lt;a href="https://kubernetes.io/blog/2016/07/happy-k8sbday-1">å¾ˆå¤šäººæŠšå…»ä½ &lt;/a>ä¸­çš„ä¸€å‘˜ï¼Œä½†å¯¹æˆ‘æ¥è¯´ï¼Œèƒ½å¤Ÿä¸Žé‚£äº›å»ºç«‹äº†è¿™äº›ä»¤äººéš¾ä»¥ç½®ä¿¡çš„ç³»ç»Ÿå¹¶æ‹¥æœ‰æ‰€æœ‰è¿™äº›é¢†åŸŸçŸ¥è¯†çš„äººä¸€èµ·å·¥ä½œæ˜¯ä¸€æ¬¡æžå¥½çš„å­¦ä¹ ç»åŽ†ã€‚ç„¶è€Œï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ç™½æ‰‹èµ·å®¶(è€Œä¸æ˜¯é‡‡ç”¨çŽ°æœ‰çš„ Borg ä»£ç )ï¼Œæˆ‘ä»¬å¤„äºŽåŒä¸€æ°´å¹³ï¼Œä»ç„¶å¯ä»¥å°±å¦‚ä½•åŸ¹å…»ä½ è¿›è¡ŒçœŸæ­£çš„è®¨è®ºã€‚å¥½å§ï¼Œè‡³å°‘å’Œæˆ‘ä»¬çš„æ°´å¹³ä¸€æ ·æŽ¥è¿‘ï¼Œä½†å€¼å¾—ç§°èµžçš„æ˜¯ï¼Œä»–ä»¬éƒ½å¤ªå¥½äº†ï¼Œä»Žæ¥æ²¡æè¿‡ï¼&lt;/p>
&lt;p>å¦‚æžœæˆ‘é€‰æ‹©ä¸¤ä¸ªèªæ˜Žäººåšå‡ºçš„æ˜Žæ™ºå†³å®šï¼š&lt;/p>
&lt;!--
- Labels &amp; selectors give us declarative â€œpointersâ€, so we can say â€œwhyâ€ we want things, rather than listing the things directly. Itâ€™s the secret to how you can scale to [great heights](https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set); not by naming each step, but saying â€œa thousand more steps just like that first oneâ€.
- Controllers are state-synchronizers: we specify the goals, and your controllers will indefatigably work to bring the system to that state. They work through that strongly-typed API foundation, and are used throughout the code, so Kubernetes is more of a set of a hundred small programs than one big one. Itâ€™s not enough to scale to thousands of nodes technically; the project also has to scale to thousands of developers and features; and controllers help us get there.
-->
&lt;ul>
&lt;li>æ ‡ç­¾å’Œé€‰æ‹©å™¨ç»™æˆ‘ä»¬å£°æ˜Žæ€§çš„â€œpointersâ€ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥è¯´â€œä¸ºä»€ä¹ˆâ€æˆ‘ä»¬æƒ³è¦ä¸œè¥¿ï¼Œè€Œä¸æ˜¯ç›´æŽ¥åˆ—å‡ºä¸œè¥¿ã€‚è¿™æ˜¯å¦‚ä½•æ‰©å±•åˆ°[ä¼Ÿå¤§é«˜åº¦]çš„ç§˜å¯†(&lt;a href="https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set">https://kubernetes.io/blog/2016/07/thousand-instances-of-cassandra-using-kubernetes-pet-set&lt;/a>)ï¼›ä¸æ˜¯å‘½åæ¯ä¸€æ­¥ï¼Œè€Œæ˜¯è¯´â€œåƒç¬¬ä¸€æ­¥ä¸€æ ·å¤šèµ°ä¸€åƒæ­¥â€ã€‚&lt;/li>
&lt;li>æŽ§åˆ¶å™¨æ˜¯çŠ¶æ€åŒæ­¥å™¨ï¼šæˆ‘ä»¬æŒ‡å®šç›®æ ‡ï¼Œæ‚¨çš„æŽ§åˆ¶å™¨å°†ä¸é—ä½™åŠ›åœ°å·¥ä½œï¼Œä½¿ç³»ç»Ÿè¾¾åˆ°è¯¥çŠ¶æ€ã€‚å®ƒä»¬å·¥ä½œåœ¨å¼ºç±»åž‹ API åŸºç¡€ä¸Šï¼Œå¹¶ä¸”è´¯ç©¿æ•´ä¸ªä»£ç ï¼Œå› æ­¤ Kubernetes æ¯”ä¸€ä¸ªå¤§çš„ç¨‹åºå¤šä¸€ç™¾ä¸ªå°ç¨‹åºã€‚ä»…ä»…ä»ŽæŠ€æœ¯ä¸Šæ‰©å±•åˆ°æ•°åƒä¸ªèŠ‚ç‚¹æ˜¯ä¸å¤Ÿçš„ï¼›è¿™ä¸ªé¡¹ç›®è¿˜å¿…é¡»æ‰©å±•åˆ°æ•°åƒä¸ªå¼€å‘äººå‘˜å’Œç‰¹æ€§ï¼›æŽ§åˆ¶å™¨å¸®åŠ©æˆ‘ä»¬è¾¾åˆ°ç›®çš„ã€‚&lt;/li>
&lt;/ul>
&lt;!--
And so on we will go! Weâ€™ll be replacing those controllers and building on more, and the API-foundation lets us build anything we can express in that way - with most things just a label or annotation away! But your thoughts will not be defined by language: with third party resources you can express anything you choose. Now we can build Kubernetes without building in Kubernetes, creating things that feel as much a part of Kubernetes as anything else. Many of the recent additions, like ingress, DNS integration, autoscaling and network policies were done or could be done in this way. Eventually it will be hard to imagine you before these things, but tomorrowâ€™s standard functionality can start today, with no obstacles or gatekeeper, maybe even for an audience of one.
So Iâ€™m looking forward to seeing more and more growth happen further and further from the core of Kubernetes. We had to work our way through those phases; starting with things that needed to happen in the kernel of Kubernetes - like replacing replication controllers with deployments. Now weâ€™re starting to build things that donâ€™t require core changes. But weâ€™re still still talking about infrastructure separately from applications. Itâ€™s what comes next that gets really interesting: when we start building applications that rely on the Kubernetes APIs. Weâ€™ve always had the Cassandra example that uses the Kubernetes API to self-assemble, but we havenâ€™t really even started to explore this more widely yet. In the same way that the S3 APIs changed how we build things that remember, I think the k8s APIs are going to change how we build things that think.
-->
&lt;p>ç­‰ç­‰æˆ‘ä»¬å°±èµ°ï¼æˆ‘ä»¬å°†å–ä»£é‚£äº›æŽ§åˆ¶å™¨ï¼Œå»ºç«‹æ›´å¤šï¼ŒAPI åŸºé‡‘ä¼šè®©æˆ‘ä»¬æž„å»ºä»»ä½•æˆ‘ä»¬å¯ä»¥ç”¨è¿™ç§æ–¹å¼è¡¨è¾¾çš„ä¸œè¥¿ - å¤§å¤šæ•°ä¸œè¥¿åªæ˜¯æ ‡ç­¾æˆ–æ³¨é‡Šè¿œç¦»ï¼ä½†ä½ çš„æ€æƒ³ä¸ä¼šç”±è¯­è¨€æ¥å®šä¹‰ï¼šæœ‰äº†ç¬¬ä¸‰æ–¹èµ„æºï¼Œä½ å¯ä»¥è¡¨è¾¾ä»»ä½•ä½ é€‰æ‹©çš„ä¸œè¥¿ã€‚çŽ°åœ¨æˆ‘ä»¬å¯ä»¥ä¸ç”¨åœ¨ Kubernetes å»ºé€ Kubernetes äº†ï¼Œåˆ›é€ å‡ºä¸Žå…¶ä»–ä»»ä½•ä¸œè¥¿ä¸€æ ·æ„Ÿè§‰æ˜¯ Kubernetes çš„ä¸€éƒ¨åˆ†çš„ä¸œè¥¿ã€‚æœ€è¿‘æ·»åŠ çš„è®¸å¤šåŠŸèƒ½ï¼Œå¦‚ingressã€DNS integrationã€autoscaling and network policies ï¼Œéƒ½å·²ç»å®Œæˆæˆ–å¯ä»¥é€šè¿‡è¿™ç§æ–¹å¼å®Œæˆã€‚æœ€ç»ˆï¼Œåœ¨è¿™äº›äº‹æƒ…å‘ç”Ÿä¹‹å‰å¾ˆéš¾æƒ³è±¡ä½ ä¼šæ˜¯æ€Žæ ·çš„ä¸€ä¸ªäººï¼Œä½†æ˜¯æ˜Žå¤©çš„æ ‡å‡†åŠŸèƒ½å¯ä»¥ä»Žä»Šå¤©å¼€å§‹ï¼Œæ²¡æœ‰ä»»ä½•éšœç¢æˆ–çœ‹é—¨äººï¼Œç”šè‡³å¯¹ä¸€ä¸ªå¬ä¼—æ¥è¯´ä¹Ÿæ˜¯è¿™æ ·ã€‚&lt;/p>
&lt;p>æ‰€ä»¥æˆ‘æœŸå¾…ç€çœ‹åˆ°è¶Šæ¥è¶Šå¤šçš„å¢žé•¿å‘ç”Ÿåœ¨ç¦» Kubernetes æ ¸å¿ƒè¶Šæ¥è¶Šè¿œçš„åœ°æ–¹ã€‚æˆ‘ä»¬å¿…é¡»é€šè¿‡è¿™äº›é˜¶æ®µæ¥å·¥ä½œï¼›ä»Žéœ€è¦åœ¨ kubernetes å†…æ ¸ä¸­å‘ç”Ÿçš„äº‹æƒ…å¼€å§‹â€”â€”æ¯”å¦‚ç”¨éƒ¨ç½²æ›¿æ¢å¤åˆ¶æŽ§åˆ¶å™¨ã€‚çŽ°åœ¨æˆ‘ä»¬å¼€å§‹æž„å»ºä¸éœ€è¦æ ¸å¿ƒæ›´æ”¹çš„ä¸œè¥¿ã€‚ä½†æˆ‘ä»¬ä»ç„¶åœ¨è®¨è®ºåŸºç¡€è®¾æ–½å’Œåº”ç”¨ç¨‹åºã€‚æŽ¥ä¸‹æ¥çœŸæ­£æœ‰è¶£çš„æ˜¯ï¼šå½“æˆ‘ä»¬å¼€å§‹æž„å»ºä¾èµ–äºŽ kubernetes api çš„åº”ç”¨ç¨‹åºæ—¶ã€‚æˆ‘ä»¬ä¸€ç›´æœ‰ä½¿ç”¨ kubernetes api è¿›è¡Œè‡ªç»„è£…çš„ cassandra ç¤ºä¾‹ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰çœŸæ­£å¼€å§‹æ›´å¹¿æ³›åœ°æŽ¢è®¨è¿™ä¸ªé—®é¢˜ã€‚æ­£å¦‚ S3 APIs æ”¹å˜äº†æˆ‘ä»¬æž„å»ºè®°å¿†äº‹ç‰©çš„æ–¹å¼ä¸€æ ·ï¼Œæˆ‘è®¤ä¸º k8s APIs ä¹Ÿå°†æ”¹å˜æˆ‘ä»¬æž„å»ºæ€è€ƒäº‹ç‰©çš„æ–¹å¼ã€‚&lt;/p>
&lt;!--
So Iâ€™m looking forward to your second birthday: I can try to predict what youâ€™ll look like then, but I know youâ€™ll surpass even the most audacious things I can imagine. Oh, the places youâ€™ll go!
_-- Justin Santa Barbara, Independent Kubernetes Contributor_
-->
&lt;p>æ‰€ä»¥æˆ‘å¾ˆæœŸå¾…ä½ çš„äºŒå²ç”Ÿæ—¥ï¼šæˆ‘å¯ä»¥è¯•ç€é¢„æµ‹ä½ é‚£æ—¶çš„æ ·å­ï¼Œä½†æˆ‘çŸ¥é“ä½ ä¼šè¶…è¶Šæˆ‘æ‰€èƒ½æƒ³è±¡çš„æœ€å¤§èƒ†çš„ä¸œè¥¿ã€‚å“¦ï¼Œè¿™æ˜¯ä½ è¦åŽ»çš„åœ°æ–¹ï¼&lt;/p>
&lt;p>&lt;em>-- Justin Santa Barbara, ç‹¬ç«‹çš„ Kubernetes è´¡çŒ®è€…&lt;/em>&lt;/p></description></item><item><title>Blog: Dashboard - Kubernetes çš„å…¨åŠŸèƒ½ Web ç•Œé¢</title><link>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</link><pubDate>Fri, 15 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/15/dashboard-web-interface-for-kubernetes/</guid><description>
&lt;!--
---
title: " Dashboard - Full Featured Web Interface for Kubernetes "
date: 2016-07-15
slug: dashboard-web-interface-for-kubernetes
url: /blog/2016/07/Dashboard-Web-Interface-For-Kubernetes
---
-->
&lt;!--
_Editorâ€™s note: this post is part of a [series of in-depth articles](https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3) on what's new in Kubernetes 1.3_
[Kubernetes Dashboard](http://github.com/kubernetes/dashboard) is a project that aims to bring a general purpose monitoring and operational web interface to the Kubernetes world.&amp;nbsp;Three months ago we [released](https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes) the first production ready version, and since then the dashboard has made massive improvements. In a single UI, youâ€™re able to perform majority of possible interactions with your Kubernetes clusters without ever leaving your browser. This blog post breaks down new features introduced in the latest release and outlines the roadmap for the future.&amp;nbsp;
-->
&lt;p>&lt;em>ç¼–è€…æŒ‰ï¼šè¿™ç¯‡æ–‡ç« æ˜¯&lt;a href="https://kubernetes.io/blog/2016/07/five-days-of-kubernetes-1-3">ä¸€ç³»åˆ—æ·±å…¥çš„æ–‡ç« &lt;/a> ä¸­å…³äºŽKubernetes 1.3çš„æ–°å†…å®¹çš„ä¸€éƒ¨åˆ†&lt;/em>
&lt;a href="http://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a>æ˜¯ä¸€ä¸ªæ—¨åœ¨ä¸º Kubernetes ä¸–ç•Œå¸¦æ¥é€šç”¨ç›‘æŽ§å’Œæ“ä½œ Web ç•Œé¢çš„é¡¹ç›®ã€‚ä¸‰ä¸ªæœˆå‰ï¼Œæˆ‘ä»¬&lt;a href="https://kubernetes.io/blog/2016/04/building-awesome-user-interfaces-for-kubernetes">å‘å¸ƒ&lt;/a>ç¬¬ä¸€ä¸ªé¢å‘ç”Ÿäº§çš„ç‰ˆæœ¬ï¼Œä»Žé‚£æ—¶èµ· dashboard å·²ç»åšäº†å¤§é‡çš„æ”¹è¿›ã€‚åœ¨ä¸€ä¸ª UI ä¸­ï¼Œæ‚¨å¯ä»¥åœ¨ä¸ç¦»å¼€æµè§ˆå™¨çš„æƒ…å†µä¸‹ï¼Œä¸Ž Kubernetes é›†ç¾¤æ‰§è¡Œå¤§å¤šæ•°å¯èƒ½çš„äº¤äº’ã€‚è¿™ç¯‡åšå®¢æ–‡ç« åˆ†è§£äº†æœ€æ–°ç‰ˆæœ¬ä¸­å¼•å…¥çš„æ–°åŠŸèƒ½ï¼Œå¹¶æ¦‚è¿°äº†æœªæ¥çš„è·¯çº¿å›¾ã€‚&lt;/p>
&lt;!--
**Full-Featured Dashboard**
Thanks to a large number of contributions from the community and project members, we were able to deliver many new features for [Kubernetes 1.3 release](https://kubernetes.io/blog/2016/07/kubernetes-1.3-bridging-cloud-native-and-enterprise-workloads). We have been carefully listening to all the great feedback we have received from our users (see the [summary infographics](http://static.lwy.io/img/kubernetes_dashboard_infographic.png)) and addressed the highest priority requests and pain points.
-->
&lt;p>&lt;strong>å…¨åŠŸèƒ½çš„ Dashboard&lt;/strong>&lt;/p>
&lt;p>ç”±äºŽç¤¾åŒºå’Œé¡¹ç›®æˆå‘˜çš„å¤§é‡è´¡çŒ®ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¸º&lt;a href="https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/">Kubernetes 1.3å‘è¡Œç‰ˆ&lt;/a>æä¾›è®¸å¤šæ–°åŠŸèƒ½ã€‚æˆ‘ä»¬ä¸€ç›´åœ¨è®¤çœŸå¬å–ç”¨æˆ·çš„åé¦ˆ(å‚è§&lt;a href="http://static.lwy.io/img/kubernetes_dashboard_infographic.png">æ‘˜è¦ä¿¡æ¯å›¾è¡¨&lt;/a>)ï¼Œå¹¶è§£å†³äº†æœ€é«˜ä¼˜å…ˆçº§çš„è¯·æ±‚å’Œéš¾ç‚¹ã€‚
--&amp;gt;&lt;/p>
&lt;!--
The Dashboard UI now handles all workload resources. This means that no matter what workload type you run, it is visible in the web interface and you can do operational changes on it. For example, you can modify your stateful MySQL installation with [Pet Sets](/docs/user-guide/petset/), do a rolling update of your web server with Deployments or install cluster monitoring with DaemonSets.&amp;nbsp;
[![](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz) ](https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz)
-->
&lt;p>Dashboard UI çŽ°åœ¨å¤„ç†æ‰€æœ‰å·¥ä½œè´Ÿè½½èµ„æºã€‚è¿™æ„å‘³ç€æ— è®ºæ‚¨è¿è¡Œä»€ä¹ˆå·¥ä½œè´Ÿè½½ç±»åž‹ï¼Œå®ƒéƒ½åœ¨ web ç•Œé¢ä¸­å¯è§ï¼Œå¹¶ä¸”æ‚¨å¯ä»¥å¯¹å…¶è¿›è¡Œæ“ä½œæ›´æ”¹ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥ä½¿ç”¨&lt;a href="https://kubernetes.io/docs/user-guide/petset/">Pet Sets&lt;/a>ä¿®æ”¹æœ‰çŠ¶æ€çš„ mysql å®‰è£…ï¼Œä½¿ç”¨éƒ¨ç½²å¯¹ web æœåŠ¡å™¨è¿›è¡Œæ»šåŠ¨æ›´æ–°ï¼Œæˆ–ä½¿ç”¨å®ˆæŠ¤ç¨‹åºå®‰è£…ç¾¤é›†ç›‘è§†ã€‚&lt;/p>
&lt;p>&lt;a href="https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz">&lt;img src="https://lh3.googleusercontent.com/p9bMGxPx4jE6_Z2KB-MktmyuAxyFst-bEk29M_Bn0Bj5ul7uzinH6u5WjHsMmqhGvBwlABZt06dwQ5qkBZiLq_EM1oddCmpwChvXDNXZypaS5l8uzkKuZj3PBUmzTQT4dgDxSXgz" alt=""> &lt;/a>&lt;/p>
&lt;!--
In addition to viewing resources, you can create, edit, update, and delete them. This feature enables many use cases. For example, you can kill a failed Pod, do a rolling update on a Deployment, or just organize your resources. You can also export and import YAML configuration files of your cloud apps and store them in a version control system.
![](https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O)
-->
&lt;p>é™¤äº†æŸ¥çœ‹èµ„æºå¤–ï¼Œè¿˜å¯ä»¥åˆ›å»ºã€ç¼–è¾‘ã€æ›´æ–°å’Œåˆ é™¤èµ„æºã€‚è¿™ä¸ªç‰¹æ€§æ”¯æŒè®¸å¤šç”¨ä¾‹ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥æ€æ­»ä¸€ä¸ªå¤±è´¥çš„ podï¼Œå¯¹éƒ¨ç½²è¿›è¡Œæ»šåŠ¨æ›´æ–°ï¼Œæˆ–è€…åªç»„ç»‡èµ„æºã€‚æ‚¨è¿˜å¯ä»¥å¯¼å‡ºå’Œå¯¼å…¥äº‘åº”ç”¨ç¨‹åºçš„ yaml é…ç½®æ–‡ä»¶ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨ç‰ˆæœ¬æŽ§åˆ¶ç³»ç»Ÿä¸­ã€‚&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/zz-qjNcGgvWXrK1LIipUdIdPyeWJ1EyPVJxRnSvI6pMcLBkxDxpQt-ObsIiZsS_X0RjVBWtXYO5TCvhsymb__CGXFzKuPUnUrB4HKnAMsxtYdWLwMmHEb8c9P9Chzlo5ePHRKf5O" alt="">&lt;/p>
&lt;!--
The release includes a beta view of cluster nodes for administration and operational use cases. The UI lists all nodes in the cluster to allow for overview analysis and quick screening for problematic nodes. The details view shows all information about the node and links to pods running on it.
![](https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_)
-->
&lt;p>è¿™ä¸ªç‰ˆæœ¬åŒ…æ‹¬ä¸€ä¸ªç”¨äºŽç®¡ç†å’Œæ“ä½œç”¨ä¾‹çš„é›†ç¾¤èŠ‚ç‚¹çš„ beta è§†å›¾ã€‚UI åˆ—å‡ºé›†ç¾¤ä¸­çš„æ‰€æœ‰èŠ‚ç‚¹ï¼Œä»¥ä¾¿è¿›è¡Œæ€»ä½“åˆ†æžå’Œå¿«é€Ÿç­›é€‰æœ‰é—®é¢˜çš„èŠ‚ç‚¹ã€‚details è§†å›¾æ˜¾ç¤ºæœ‰å…³è¯¥èŠ‚ç‚¹çš„æ‰€æœ‰ä¿¡æ¯ä»¥åŠæŒ‡å‘åœ¨å…¶ä¸Šè¿è¡Œçš„ pod çš„é“¾æŽ¥ã€‚&lt;/p>
&lt;p>&lt;img src="https://lh6.googleusercontent.com/3CSTUy-8Tz-yAL9tCqxNUqMcWJYKK0dwk7kidE9zy-L-sXFiD4A4Y2LKEqbJKgI6Fl6xbzYxsziI8dULVXPJbu6eU0ci7hNtqi3tTuhdbVD6CG3EXw151fvt2MQuqumHRbab6g-_" alt="">&lt;/p>
&lt;!--
There are also many smaller scope new features that the we shipped with the release, namely: support for namespaced resources, internationalization, performance improvements, and many bug fixes (find out more in the [release notes](https://github.com/kubernetes/dashboard/releases/tag/v1.1.0)). All these improvements result in a better and simpler user experience of the product.
-->
&lt;p>æˆ‘ä»¬éšå‘è¡Œç‰ˆæä¾›çš„è¿˜æœ‰è®¸å¤šå°èŒƒå›´çš„æ–°åŠŸèƒ½ï¼Œå³ï¼šæ”¯æŒå‘½åç©ºé—´èµ„æºã€å›½é™…åŒ–ã€æ€§èƒ½æ”¹è¿›å’Œè®¸å¤šé”™è¯¯ä¿®å¤(è¯·å‚é˜…&lt;a href="https://github.com/kubernetes/dashboard/releases/tag/v1.1.0">å‘è¡Œè¯´æ˜Ž&lt;/a>ä¸­çš„æ›´å¤šå†…å®¹)ã€‚æ‰€æœ‰è¿™äº›æ”¹è¿›éƒ½ä¼šå¸¦æ¥æ›´å¥½ã€æ›´ç®€å•çš„äº§å“ç”¨æˆ·ä½“éªŒã€‚&lt;/p>
&lt;!--
**Future Work**
The team has ambitious plans for the future spanning across multiple use cases. We are also open to all feature requests, which you can post on our [issue tracker](https://github.com/kubernetes/dashboard/issues).
-->
&lt;p>&lt;strong>Future Work&lt;/strong>&lt;/p>
&lt;p>è¯¥å›¢é˜Ÿå¯¹è·¨è¶Šå¤šä¸ªç”¨ä¾‹çš„æœªæ¥æœ‰ç€é›„å¿ƒå‹ƒå‹ƒçš„è®¡åˆ’ã€‚æˆ‘ä»¬è¿˜å¯¹æ‰€æœ‰åŠŸèƒ½è¯·æ±‚å¼€æ”¾ï¼Œæ‚¨å¯ä»¥åœ¨æˆ‘ä»¬çš„&lt;a href="https://github.com/kubernetes/dashboard/issues">é—®é¢˜è·Ÿè¸ªç¨‹åº&lt;/a>ä¸Šå‘å¸ƒè¿™äº›è¯·æ±‚ã€‚&lt;/p>
&lt;!--
Here is a list of our focus areas for the following months:
- [Handle more Kubernetes resources](https://github.com/kubernetes/dashboard/issues/961) - To show all resources that a cluster user may potentially interact with. Once done, Dashboard can act as a complete replacement for CLI.&amp;nbsp;
- [Monitoring and troubleshooting](https://github.com/kubernetes/dashboard/issues/962) - To add resource usage statistics/graphs to the objects shown in Dashboard. This focus area will allow for actionable debugging and troubleshooting of cloud applications.
- [Security, auth and logging in](https://github.com/kubernetes/dashboard/issues/964) - Make Dashboard accessible from networks external to a Cluster and work with custom authentication systems.
-->
&lt;p>ä»¥ä¸‹æ˜¯æˆ‘ä»¬æŽ¥ä¸‹æ¥å‡ ä¸ªæœˆçš„é‡ç‚¹é¢†åŸŸï¼š&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/961">Handle more Kubernetes resources&lt;/a> - æ˜¾ç¤ºç¾¤é›†ç”¨æˆ·å¯èƒ½ä¸Žä¹‹äº¤äº’çš„æ‰€æœ‰èµ„æºã€‚ä¸€æ—¦å®Œæˆï¼Œdashboard å°±å¯ä»¥å®Œå…¨æ›¿ä»£cliã€‚&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/962">Monitoring and troubleshooting&lt;/a> - å°†èµ„æºä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯/å›¾è¡¨æ·»åŠ åˆ° Dashboard ä¸­æ˜¾ç¤ºçš„å¯¹è±¡ã€‚è¿™ä¸ªé‡ç‚¹é¢†åŸŸå°†å…è®¸å¯¹äº‘åº”ç”¨ç¨‹åºè¿›è¡Œå¯æ“ä½œçš„è°ƒè¯•å’Œæ•…éšœæŽ’é™¤ã€‚&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/dashboard/issues/964">Security, auth and logging in&lt;/a> - ä½¿ä»ªè¡¨æ¿å¯ä»Žç¾¤é›†å¤–éƒ¨çš„ç½‘ç»œè®¿é—®ï¼Œå¹¶ä½¿ç”¨è‡ªå®šä¹‰èº«ä»½éªŒè¯ç³»ç»Ÿã€‚&lt;/li>
&lt;/ul>
&lt;!--
**Connect With Us**
We would love to talk with you and hear your feedback!
- Email us at the [SIG-UI mailing list](https://groups.google.com/forum/#!forum/kubernetes-sig-ui)
- Chat with us on the Kubernetes Slack&amp;nbsp;[#SIG-UI channel](https://kubernetes.slack.com/messages/sig-ui/)
- Join our meetings: 4PM CEST. See the [SIG-UI calendar](https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;ctz=Europe/Warsaw) for details.
_-- Piotr Bryk, Software Engineer, Google_
-->
&lt;p>&lt;strong>è”ç³»æˆ‘ä»¬&lt;/strong>&lt;/p>
&lt;p>æˆ‘ä»¬å¾ˆä¹æ„ä¸Žæ‚¨äº¤è°ˆå¹¶å¬å–æ‚¨çš„åé¦ˆï¼&lt;/p>
&lt;ul>
&lt;li>è¯·åœ¨[SIG-UIé‚®ä»¶åˆ—è¡¨](&lt;a href="https://groups.google.com/forum/">https://groups.google.com/forum/&lt;/a>å‘æˆ‘ä»¬å‘é€ç”µå­é‚®ä»¶ï¼è®ºå›/kubernetes sig ui)&lt;/li>
&lt;li>åœ¨ kubernetes slack ä¸Šä¸Žæˆ‘ä»¬èŠå¤©ã€‚&lt;a href="https://kubernetes.slack.com/messages/sig-ui/">#SIG-UI channel&lt;/a>&lt;/li>
&lt;li>å‚åŠ æˆ‘ä»¬çš„ä¼šè®®ï¼šä¸œéƒ¨æ—¶é—´ä¸‹åˆ4ç‚¹ã€‚è¯·å‚é˜…&lt;a href="https://calendar.google.com/calendar/embed?src=google.com_52lm43hc2kur57dgkibltqc6kc%40group.calendar.google.com&amp;amp;ctz=Europe/Warsaw">SIG-UIæ—¥åŽ†&lt;/a>äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚&lt;/li>
&lt;/ul></description></item><item><title>Blog: Citrix + Kubernetes = å…¨åž’æ‰“</title><link>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</link><pubDate>Thu, 14 Jul 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/07/14/citrix-netscaler-and-kubernetes/</guid><description>
&lt;!--
---
title: " Citrix + Kubernetes = A Home Run "
date: 2016-07-14
slug: citrix-netscaler-and-kubernetes
url: /blog/2016/07/Citrix-Netscaler-And-Kubernetes
---
-->
&lt;!--
_Editorâ€™s note: todayâ€™s guest post is by Mikko Disini, a Director of Product Management at Citrix Systems, sharing their collaboration experience on a Kubernetes integration.&amp;nbsp;_
-->
&lt;p>ç¼–è€…æŒ‰ï¼šä»Šå¤©çš„å®¢åº§æ–‡ç« æ¥è‡ª Citrix Systems çš„äº§å“ç®¡ç†æ€»ç›‘ Mikko Disiniï¼Œä»–åˆ†äº«äº†ä»–ä»¬åœ¨ Kubernetes é›†æˆä¸Šçš„åˆä½œç»éªŒã€‚Â _&lt;/p>
&lt;!--
Technical collaboration is like sports. If you work together as a team, you can go down the homestretch and pull through for a win. Thatâ€™s our experience with the Google Cloud Platform team.
-->
&lt;p>æŠ€æœ¯åˆä½œå°±åƒä½“è‚²è¿åŠ¨ã€‚å¦‚æžœä½ èƒ½åƒä¸€ä¸ªå›¢é˜Ÿä¸€æ ·åˆä½œï¼Œä½ å°±èƒ½åœ¨æœ€åŽå…³å¤´å–å¾—èƒœåˆ©ã€‚è¿™å°±æ˜¯æˆ‘ä»¬å¯¹è°·æ­Œäº‘å¹³å°å›¢é˜Ÿçš„ç»éªŒã€‚&lt;/p>
&lt;!--
Recently, we approached Google Cloud Platform (GCP) to collaborate on behalf of Citrix customers and the broader enterprise market looking to migrate workloads.&amp;nbsp;This migration required including the [NetScaler Docker load balancer](https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/), CPX, into Kubernetes nodes and resolving any issues with getting traffic into the CPX proxies. &amp;nbsp;
-->
&lt;p>æœ€è¿‘ï¼Œæˆ‘ä»¬ä¸Ž Google äº‘å¹³å°ï¼ˆGCPï¼‰è”ç³»ï¼Œä»£è¡¨ Citrix å®¢æˆ·ä»¥åŠæ›´å¹¿æ³›çš„ä¼ä¸šå¸‚åœºï¼Œå¸Œæœ›å°±å·¥ä½œè´Ÿè½½çš„è¿ç§»è¿›è¡Œåä½œã€‚æ­¤è¿ç§»éœ€è¦å°† [NetScaler Docker è´Ÿè½½å‡è¡¡å™¨]https://www.citrix.com/blogs/2016/06/20/the-best-docker-load-balancer-at-dockercon-in-seattle-this-week/) CPX åŒ…å«åˆ° Kubernetes èŠ‚ç‚¹ä¸­ï¼Œå¹¶è§£å†³å°†æµé‡å¼•å…¥ CPX ä»£ç†çš„ä»»ä½•é—®é¢˜ã€‚&lt;/p>
&lt;!--
**Why NetScaler and Kubernetes?**
-->
&lt;p>&lt;strong>ä¸ºä»€ä¹ˆæ˜¯ NetScaler å’Œ Kubernetes&lt;/strong>&lt;/p>
&lt;!--
1. Citrix customers want the same Layer 4 to Layer 7 capabilities from NetScaler that they have on-prem as they move to the cloud as they begin deploying their container and microservices architecture with Kubernetes&amp;nbsp;
2. Kubernetes provides a proven infrastructure for running containers and VMs with automated workload delivery
3. NetScaler CPX provides Layer 4 to Layer 7 services and highly efficient telemetry data to a logging and analytics platform, [NetScaler Management and Analytics System](https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/)
-->
&lt;ol>
&lt;li>Citrix çš„å®¢æˆ·å¸Œæœ›ä»–ä»¬å¼€å§‹ä½¿ç”¨ Kubernetes éƒ¨ç½²ä»–ä»¬çš„å®¹å™¨å’Œå¾®æœåŠ¡ä½“ç³»ç»“æž„æ—¶ï¼Œèƒ½å¤Ÿåƒå½“åˆè¿ç§»åˆ°äº‘è®¡ç®—æ—¶ä¸€æ ·ï¼Œäº«æœ‰ NetScaler æ‰€æä¾›çš„ç¬¬ 4 å±‚åˆ°ç¬¬ 7 å±‚èƒ½åŠ›Â &lt;/li>
&lt;li>Kubernetes æä¾›äº†ä¸€å¥—ç»è¿‡éªŒè¯çš„åŸºç¡€è®¾æ–½ï¼Œå¯ç”¨æ¥è¿è¡Œå®¹å™¨å’Œè™šæ‹Ÿæœºï¼Œå¹¶è‡ªåŠ¨äº¤ä»˜å·¥ä½œè´Ÿè½½ï¼›&lt;/li>
&lt;li>NetScaler CPX æä¾›ç¬¬ 4 å±‚åˆ°ç¬¬ 7 å±‚çš„æœåŠ¡ï¼Œå¹¶ä¸ºæ—¥å¿—å’Œåˆ†æžå¹³å° &lt;a href="https://www.citrix.com/blogs/2016/05/24/introducing-the-next-generation-netscaler-management-and-analytics-system/">NetScaler ç®¡ç†å’Œåˆ†æžç³»ç»Ÿ&lt;/a> æä¾›é«˜æ•ˆçš„åº¦é‡æ•°æ®ã€‚&lt;/li>
&lt;/ol>
&lt;!--
I wish all our experiences working together with a technical partner were as good as working with GCP. We had a list of issues to enable our use cases and were able to collaborate swiftly on a solution. To resolve these, GCP team offered in depth technical assistance, working with Citrix such that NetScaler CPX can spin up and take over as a client-side proxy running on each host.&amp;nbsp;
-->
&lt;p>æˆ‘å¸Œæœ›æˆ‘ä»¬æ‰€æœ‰ä¸ŽæŠ€æœ¯åˆä½œä¼™ä¼´ä¸€èµ·å·¥ä½œçš„ç»éªŒéƒ½èƒ½åƒä¸Ž GCP ä¸€èµ·å·¥ä½œä¸€æ ·å¥½ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆ—è¡¨ï¼ŒåŒ…å«æ”¯æŒæˆ‘ä»¬çš„ç”¨ä¾‹æ‰€éœ€è¦è§£å†³çš„é—®é¢˜ã€‚æˆ‘ä»¬èƒ½å¤Ÿå¿«é€Ÿåä½œå½¢æˆè§£å†³æ–¹æ¡ˆã€‚ä¸ºäº†è§£å†³è¿™äº›é—®é¢˜ï¼ŒGCP å›¢é˜Ÿæä¾›äº†æ·±å…¥çš„æŠ€æœ¯æ”¯æŒï¼Œä¸Ž Citrix åˆä½œï¼Œä»Žè€Œä½¿å¾— NetScaler CPX èƒ½å¤Ÿåœ¨æ¯å°ä¸»æœºä¸Šä½œä¸ºå®¢æˆ·ç«¯ä»£ç†å¯åŠ¨è¿è¡Œã€‚&lt;/p>
&lt;!--
Next, NetScaler CPX needed to be inserted in the data path of GCP ingress load balancer so that NetScaler CPX can spread traffic to front end web servers. The NetScaler team made modifications so that NetScaler CPX listens to API server events and configures itself to create a VIP, IP table rules and server rules to take ingress traffic and load balance across front end applications. Google Cloud Platform team provided feedback and assistance to verify modifications made to overcome the technical hurdles. Done!
-->
&lt;p>æŽ¥ä¸‹æ¥ï¼Œéœ€è¦åœ¨ GCP å…¥å£è´Ÿè½½å‡è¡¡å™¨çš„æ•°æ®è·¯å¾„ä¸­æ’å…¥ NetScaler CPXï¼Œä½¿ NetScaler CPX èƒ½å¤Ÿå°†æµé‡åˆ†æ•£åˆ°å‰ç«¯ web æœåŠ¡å™¨ã€‚NetScaler å›¢é˜Ÿè¿›è¡Œäº†ä¿®æ”¹ï¼Œä»¥ä¾¿ NetScaler CPX ç›‘å¬ API æœåŠ¡å™¨äº‹ä»¶ï¼Œå¹¶é…ç½®è‡ªå·±æ¥åˆ›å»º VIPã€IP è¡¨è§„åˆ™å’ŒæœåŠ¡å™¨è§„åˆ™ï¼Œä»¥ä¾¿è·¨å‰ç«¯åº”ç”¨ç¨‹åºæŽ¥æ”¶æµé‡å’Œè´Ÿè½½å‡è¡¡ã€‚è°·æ­Œäº‘å¹³å°å›¢é˜Ÿæä¾›åé¦ˆå’Œå¸®åŠ©ï¼ŒéªŒè¯ä¸ºå…‹æœæŠ€æœ¯éšœç¢æ‰€åšçš„ä¿®æ”¹ã€‚å®Œæˆäº†!&lt;/p>
&lt;!--
NetScaler CPX use case is supported in [Kubernetes 1.3](https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/). Citrix customers and the broader enterprise market will have the opportunity to leverage NetScaler with Kubernetes, thereby lowering the friction to move workloads to the cloud.&amp;nbsp;
-->
&lt;p>NetScaler CPX ç”¨ä¾‹åœ¨ &lt;a href="https://kubernetes.io/blog/2016/07/kubernetes-1-3-bridging-cloud-native-and-enterprise-workloads/">Kubernetes 1.3&lt;/a> ä¸­æä¾›æ”¯æŒã€‚Citrix çš„å®¢æˆ·å’Œæ›´å¹¿æ³›çš„ä¼ä¸šå¸‚åœºå°†æœ‰æœºä¼šåŸºäºŽ Kubernetes äº«ç”¨ NetScaler æœåŠ¡ï¼Œä»Žè€Œé™ä½Žå°†å·¥ä½œè´Ÿè½½è½¬ç§»åˆ°äº‘å¹³å°çš„é˜»åŠ›ã€‚Â &lt;/p>
&lt;!--
You can learn more about&amp;nbsp;NetScaler CPX [here](https://www.citrix.com/networking/microservices.html).
-->
&lt;p>æ‚¨å¯ä»¥åœ¨&lt;a href="https://www.citrix.com/networking/microservices.html">æ­¤å¤„&lt;/a>äº†è§£æœ‰å…³ NetScaler CPX çš„æ›´å¤šä¿¡æ¯ã€‚&lt;/p>
&lt;!--
_&amp;nbsp;-- Mikko Disini, Director of Product Management - NetScaler, Citrix Systems_
-->
&lt;p>_Â -- Mikko Disiniï¼ŒCitrix Systems NetScaler äº§å“ç®¡ç†æ€»ç›‘&lt;/p></description></item><item><title>Blog: KubeCon EU 2016ï¼šä¼¦æ•¦ Kubernetes ç¤¾åŒº</title><link>https://kubernetes.io/zh/blog/2016/02/24/kubecon-eu-2016-kubernetes-community-in/</link><pubDate>Wed, 24 Feb 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/02/24/kubecon-eu-2016-kubernetes-community-in/</guid><description>
&lt;!--
---
title: " KubeCon EU 2016: Kubernetes Community in London "
date: 2016-02-24
slug: kubecon-eu-2016-kubernetes-community-in
url: /blog/2016/02/Kubecon-Eu-2016-Kubernetes-Community-In
---
-->
&lt;!--
KubeCon EU 2016 is the inaugural [European Kubernetes](http://kubernetes.io/) community conference that follows on the American launch in November 2015. KubeCon is fully dedicated to education and community engagement for[Kubernetes](http://kubernetes.io/) enthusiasts, production users and the surrounding ecosystem.
-->
&lt;p>KubeCon EU 2016 æ˜¯é¦–å±Š&lt;a href="http://kubernetes.io/">æ¬§æ´² Kubernetes&lt;/a> ç¤¾åŒºä¼šè®®ï¼Œç´§éš 2015 å¹´ 11 æœˆå¬å¼€çš„åŒ—ç¾Žä¼šè®®ã€‚KubeCon è‡´åŠ›äºŽä¸º &lt;a href="http://kubernetes.io/">Kubernetes&lt;/a> çˆ±å¥½è€…ã€äº§å“ç”¨æˆ·å’Œå‘¨å›´çš„ç”Ÿæ€ç³»ç»Ÿæä¾›æ•™è‚²å’Œç¤¾åŒºå‚ä¸Žã€‚&lt;/p>
&lt;!--
Come join us in London and hang out with hundreds from the Kubernetes community and experience a wide variety of deep technical expert talks and use cases.
-->
&lt;p>å¿«æ¥åŠ å…¥æˆ‘ä»¬åœ¨ä¼¦æ•¦ï¼Œä¸Ž Kubernetes ç¤¾åŒºçš„æ•°ç™¾äººä¸€èµ·å‡ºåŽ»ï¼Œä½“éªŒå„ç§æ·±å…¥çš„æŠ€æœ¯ä¸“å®¶è®²åº§å’Œç”¨ä¾‹ã€‚&lt;/p>
&lt;!--
Donâ€™t miss these great speaker sessions at the conference:
-->
&lt;p>ä¸è¦é”™è¿‡è¿™äº›ä¼˜è´¨çš„æ¼”è®²ï¼š&lt;/p>
&lt;!--
* â€œKubernetes Hardware Hacks: Exploring the Kubernetes API Through Knobs, Faders, and Slidersâ€ by Ian Lewis and Brian Dorsey, Developer Advocate, Google -* [http://sched.co/6Bl3](http://sched.co/6Bl3)
* â€œrktnetes: what's new with container runtimes and Kubernetesâ€ by Jonathan Boulle, Developer and Team Lead at CoreOS -* [http://sched.co/6BY7](http://sched.co/6BY7)
* â€œKubernetes Documentation: Contributing, fixing issues, collecting bountiesâ€ by John Mulhausen, Lead Technical Writer, Google -* [http://sched.co/6BUP](http://sched.co/6BUP)&amp;nbsp;
* â€œ[What is OpenStack's role in a Kubernetes world?](https://kubeconeurope2016.sched.org/event/6BYC/what-is-openstacks-role-in-a-kubernetes-world?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)â€ By Thierry Carrez, Director of Engineering, OpenStack Foundation -* http://sched.co/6BYC
* â€œA Practical Guide to Container Schedulingâ€ by Mandy Waite, Developer Advocate, Google -* [http://sched.co/6BZa](http://sched.co/6BZa)
* â€œ[Kubernetes in Production in The New York Times newsroom](https://kubeconeurope2016.sched.org/event/67f2/kubernetes-in-production-in-the-new-york-times-newsroom?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)â€ Eric Lewis, Web Developer, New York Times -* [http://sched.co/67f2](http://sched.co/67f2)
* â€œ[Creating an Advanced Load Balancing Solution for Kubernetes with NGINX](https://kubeconeurope2016.sched.org/event/6Bc9/creating-an-advanced-load-balancing-solution-for-kubernetes-with-nginx?iframe=yes&amp;w=i:0;&amp;sidebar=yes&amp;bg=no#?iframe=yes&amp;w=i:100;&amp;sidebar=yes&amp;bg=no)â€ by Andrew Hutchings, Technical Product Manager, NGINX -* http://sched.co/6Bc9
* And many more http://kubeconeurope2016.sched.org/
-->
&lt;ul>
&lt;li>
&lt;p>â€œKubernetes ç¡¬ä»¶é»‘å®¢ï¼šé€šè¿‡æ—‹é’®ã€æŽ¨æ†å’Œæ»‘å—æŽ¢ç´¢ Kubernetes APIâ€ æ¼”è®²è€… Ian Lewis å’Œ Brian Dorseyï¼Œè°·æ­Œå¼€å‘å¸ƒé“å¸ˆ* &lt;a href="http://sched.co/6Bl3">http://sched.co/6Bl3&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œrktnetes: å®¹å™¨è¿è¡Œæ—¶å’Œ Kubernetes çš„æ–°åŠŸèƒ½â€ æ¼”è®²è€… Jonathan Boulle, CoreOS çš„ä¸»ç¨‹ -* &lt;a href="http://sched.co/6BY7">http://sched.co/6BY7&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œKubernetes æ–‡æ¡£ï¼šè´¡çŒ®ã€ä¿®å¤é—®é¢˜ã€æ”¶é›†å¥–é‡‘â€ ä½œè€…ï¼šJohn Mulhausenï¼Œé¦–å¸­æŠ€æœ¯ä½œå®¶ï¼Œè°·æ­Œ -* &lt;a href="http://sched.co/6BUP">http://sched.co/6BUP&lt;/a>Â &lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œ&lt;a href="https://kubeconeurope2016.sched.org/event/6BYC/what-is-openstacks-role-in-a-kubernetes-world?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no">OpenStack åœ¨ Kubernetes çš„ä¸–ç•Œä¸­æ‰®æ¼”ä»€ä¹ˆè§’è‰²ï¼Ÿ&lt;/a>â€ ä½œè€…ï¼šThierry carez, OpenStack åŸºé‡‘ä¼šå·¥ç¨‹æ€»ç›‘ -* &lt;a href="http://sched.co/6BYC">http://sched.co/6BYC&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œå®¹å™¨è°ƒåº¦çš„å®žç”¨æŒ‡å—â€ ä½œè€…ï¼šMandy Waiteï¼Œå¼€å‘è€…å€¡å¯¼è€…ï¼Œè°·æ­Œ -* &lt;a href="http://sched.co/6BZa">http://sched.co/6BZa&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œ&lt;a href="https://kubeconeurope2016.sched.org/event/67f2/kubernetes-in-production-in-the-new-york-times-newsroom?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no">ã€Šçº½çº¦æ—¶æŠ¥ã€‹ç¼–è¾‘éƒ¨æ­£åœ¨åˆ¶ä½œ Kubernetes&lt;/a>â€ Eric Lewisï¼Œã€Šçº½çº¦æ—¶æŠ¥ã€‹ç½‘ç«™å¼€å‘äººå‘˜ -* &lt;a href="http://sched.co/67f2">http://sched.co/67f2&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>â€œ&lt;a href="https://kubeconeurope2016.sched.org/event/6Bc9/creating-an-advanced-load-balancing-solution-for-kubernetes-with-nginx?iframe=yes&amp;amp;w=i:0;&amp;amp;sidebar=yes&amp;amp;bg=no#?iframe=yes&amp;amp;w=i:100;&amp;amp;sidebar=yes&amp;amp;bg=no">ä½¿ç”¨ NGINX ä¸º Kubernetes åˆ›å»ºä¸€ä¸ªé«˜çº§è´Ÿè½½å‡è¡¡è§£å†³æ–¹æ¡ˆ&lt;/a>â€ ä½œè€…ï¼šAndrew Hutchings, NGINX æŠ€æœ¯äº§å“ç»ç† -* &lt;a href="http://sched.co/6Bc9">http://sched.co/6Bc9&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è¿˜æœ‰æ›´å¤š &lt;a href="http://kubeconeurope2016.sched.org/">http://kubeconeurope2016.sched.org/&lt;/a>&lt;/p>
&lt;/li>
&lt;/ul>
&lt;!--
Get your KubeCon EU [tickets here](https://ti.to/kubecon/kubecon-eu-2016).
-->
&lt;p>&lt;a href="https://ti.to/kubecon/kubecon-eu-2016">åœ¨è¿™é‡Œ&lt;/a>èŽ·å–æ‚¨çš„ KubeCon EU é—¨ç¥¨ã€‚&lt;/p>
&lt;!--
Venue Location: CodeNode * 10 South Pl, London, United Kingdom
Accommodations: [hotels](https://skillsmatter.com/contact-us#hotels)
Website: [kubecon.io](https://www.kubecon.io/)
Twitter: [@KubeConio](https://twitter.com/kubeconio) #KubeCon
Google is a proud Diamond sponsor of KubeCon EU 2016. Come to London next month, March 10th &amp; 11th, and visit booth #13 to learn all about Kubernetes, Google Container Engine (GKE) and Google Cloud Platform!
-->
&lt;p>ä¼šåœºåœ°å€ï¼šCodeNode * è‹±å›½ä¼¦æ•¦å—å¹¿åœº 10 å·
é…’åº—ä½å®¿ï¼š&lt;a href="https://skillsmatter.com/contact-us">é…’åº—&lt;/a>
ç½‘ç«™ï¼š[kubecon.io] (&lt;a href="https://www.kubecon.io/">https://www.kubecon.io/&lt;/a>)
æŽ¨ç‰¹ï¼š[@KubeConio] (&lt;a href="https://twitter.com/kubeconio">https://twitter.com/kubeconio&lt;/a>)
è°·æ­Œæ˜¯ KubeCon EU 2016 çš„é’»çŸ³èµžåŠ©å•†ã€‚ä¸‹ä¸ªæœˆ 3 æœˆ 10 - 11 å·æ¥ä¼¦æ•¦ï¼Œå‚è§‚ 13 å·å±•ä½ï¼Œäº†è§£ Kubernetesï¼ŒGoogle Container Engineï¼ˆGKEï¼‰ï¼ŒGoogle Cloud Platform çš„æ‰€æœ‰ä¿¡æ¯!&lt;/p>
&lt;!--
_KubeCon is organized by KubeAcademy, LLC, a community-driven group of developers focused on the education of developers and the promotion of Kubernetes._
-* Sarah Novotny, Kubernetes Community Manager, Google
-->
&lt;p>_KubeCon æ˜¯ç”± KubeAcademyã€LLC ç»„ç»‡çš„ï¼Œè¿™æ˜¯ä¸€ä¸ªç”±ç¤¾åŒºé©±åŠ¨çš„å¼€å‘è€…å›¢ä½“ï¼Œä¸“æ³¨äºŽå¼€å‘äººå‘˜çš„æ•™è‚²å’Œ kubernet.com çš„æŽ¨å¹¿
-* Sarah Novotny, è°·æ­Œçš„ Kubernetes ç¤¾åŒºç»ç†&lt;/p></description></item><item><title>Blog: Kubernetes ç¤¾åŒºä¼šè®®è®°å½• - 20160204</title><link>https://kubernetes.io/zh/blog/2016/02/09/kubernetes-community-meeting-notes/</link><pubDate>Tue, 09 Feb 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/02/09/kubernetes-community-meeting-notes/</guid><description>
&lt;!--
---
title: " Kubernetes community meeting notes - 20160204 "
date: 2016-02-09
slug: kubernetes-community-meeting-notes
url: /blog/2016/02/Kubernetes-Community-Meeting-Notes
---
-->
&lt;!--
#### February 4th - rkt demo (congratulations on the 1.0, CoreOS!), eBay puts k8s on Openstack and considers Openstack on k8s, SIGs, and flaky test surge makes progress.
-->
&lt;h4 id="2æœˆ4æ—¥-rktæ¼”ç¤º-ç¥è´º-1-0-ç‰ˆæœ¬-coreos-ebay-å°†-k8s-æ”¾åœ¨-openstack-ä¸Šå¹¶è®¤ä¸º-openstack-åœ¨-k8s-sig-å’Œç‰‡çŠ¶æµ‹è¯•æ¿€å¢žæ–¹é¢å–å¾—äº†è¿›å±•">2æœˆ4æ—¥ - rktæ¼”ç¤ºï¼ˆç¥è´º 1.0 ç‰ˆæœ¬ï¼Œ CoreOSï¼ï¼‰ï¼Œ eBay å°† k8s æ”¾åœ¨ Openstack ä¸Šå¹¶è®¤ä¸º Openstack åœ¨ k8sï¼Œ SIG å’Œç‰‡çŠ¶æµ‹è¯•æ¿€å¢žæ–¹é¢å–å¾—äº†è¿›å±•ã€‚&lt;/h4>
&lt;!--
The Kubernetes contributing community meets most Thursdays at 10:00PT to discuss the project's status via a videoconference. Here are the notes from the latest meeting.
-->
&lt;p>Kubernetes è´¡çŒ®ç¤¾åŒºåœ¨æ¯å‘¨å›› 10:00 PT å¼€ä¼š,é€šè¿‡è§†é¢‘ä¼šè®®è®¨è®ºé¡¹ç›®çŠ¶æ€ã€‚ä»¥ä¸‹æ˜¯æœ€è¿‘ä¸€æ¬¡ä¼šè®®çš„ç¬”è®°ã€‚&lt;/p>
&lt;!--
* Note taker: Rob Hirschfeld
* Demo (20 min): CoreOS rkt + Kubernetes [Shaya Potter]
* expect to see integrations w/ rkt &amp; k8s in the coming months ("rkt-netes"). not integrated into the v1.2 release.
* Shaya gave a demo (8 minutes into meeting for video reference)
* CLI of rkt shown spinning up containers
* [note: audio is garbled at points]
* Discussion about integration w/ k8s &amp; rkt
* rkt community sync next week: https://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY
* Dawn Chen:
* The remaining issues of integrating rkt with kubernetes: 1) cadivsor 2) DNS 3) bugs related to logging
* But need more work on e2e test suites
-->
&lt;ul>
&lt;li>ä¹¦è®°å‘˜ï¼šRob Hirschfeld&lt;/li>
&lt;li>æ¼”ç¤ºè§†é¢‘ï¼ˆ20åˆ†é’Ÿï¼‰ï¼šCoreOS rkt + Kubernetes[Shaya Potter]
&lt;ul>
&lt;li>æœŸå¾…åœ¨æœªæ¥å‡ ä¸ªæœˆå†…çœ‹åˆ°ä¸Žrktå’Œk8sçš„æ•´åˆï¼ˆâ€œrkt-netesâ€ï¼‰ã€‚ è¿˜æ²¡æœ‰é›†æˆåˆ° v1.2ç‰ˆæœ¬ä¸­ã€‚&lt;/li>
&lt;li>Shaya åšäº†ä¸€ä¸ªæ¼”ç¤ºï¼ˆ8åˆ†é’Ÿçš„ä¼šè®®è§†é¢‘å‚è€ƒï¼‰
&lt;ul>
&lt;li>rktçš„CLIæ˜¾ç¤ºäº†æ—‹è½¬å®¹å™¨&lt;/li>
&lt;li>[æ³¨æ„ï¼šéŸ³é¢‘åœ¨ç‚¹æ•°ä¸Šæ˜¯ä¹±ç ]&lt;/li>
&lt;li>å…³äºŽ k8s&amp;amp;rkt æ•´åˆçš„è®¨è®º&lt;/li>
&lt;li>ä¸‹å‘¨ rkt ç¤¾åŒºåŒæ­¥ï¼šhttps://groups.google.com/forum/#!topic/rkt-dev/FlwZVIEJGbY&lt;/li>
&lt;li>Dawn Chen:
&lt;ul>
&lt;li>å°† rkt ä¸Ž kubernetes é›†æˆçš„å…¶ä½™é—®é¢˜ï¼š1ï¼‰cadivsor 2ï¼‰ DNS 3ï¼‰ä¸Žæ—¥å¿—è®°å½•ç›¸å…³çš„é”™è¯¯&lt;/li>
&lt;li>ä½†æ˜¯éœ€è¦åœ¨ e2e æµ‹è¯•å¥—ä»¶ä¸Šåšæ›´å¤šçš„å·¥ä½œ&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Use Case (10 min): eBay k8s on OpenStack and OpenStack on k8s [Ashwin Raveendran]
* eBay is currently running Kubernetes on OpenStack
* Goal for eBay is to manage the OpenStack control plane w/ k8s. Goal would be to achieve upgrades
* OpenStack Kolla creates containers for the control plane. Uses Ansible+Docker for management of the containers.
* Working on k8s control plan management - Saltstack is proving to be a management challenge at the scale they want to operate. Looking for automated management of the k8s control plane.
-->
&lt;ul>
&lt;li>ç”¨ä¾‹ï¼ˆ10åˆ†é’Ÿï¼‰ï¼šåœ¨ OpenStack ä¸Šçš„ eBay k8s å’Œ k8s ä¸Šçš„ OpenStack [Ashwin Raveendran]
&lt;ul>
&lt;li>eBay ç›®å‰æ­£åœ¨ OpenStack ä¸Šè¿è¡Œ Kubernetes&lt;/li>
&lt;li>eBay çš„ç›®æ ‡æ˜¯ç®¡ç†å¸¦æœ‰ k8s çš„ OpenStack æŽ§åˆ¶å¹³é¢ã€‚ç›®æ ‡æ˜¯å®žçŽ°å‡çº§ã€‚&lt;/li>
&lt;li>OpenStack Kolla ä¸ºæŽ§åˆ¶å¹³é¢åˆ›å»ºå®¹å™¨ã€‚ä½¿ç”¨ Ansible+Docker æ¥ç®¡ç†å®¹å™¨ã€‚&lt;/li>
&lt;li>è‡´åŠ›äºŽ k8s æŽ§åˆ¶è®¡åˆ’ç®¡ç† - Saltstack è¢«è¯æ˜Žæ˜¯ä»–ä»¬æƒ³è¿è¥çš„è§„æ¨¡çš„ç®¡ç†æŒ‘æˆ˜ã€‚å¯»æ‰¾ k8s æŽ§åˆ¶å¹³é¢çš„è‡ªåŠ¨åŒ–ç®¡ç†ã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* SIG Report
* Testing update [Jeff, Joe, and Erick]
* Working to make the workflow about contributing to K8s easier to understanding
* [pull/19714][1] has flow chart of the bot flow to help users understand
* Need a consistent way to run tests w/ hacking config scripts (you have to fake a Jenkins process right now)
* Want to create necessary infrastructure to make test setup less flaky
* want to decouple test start (single or full) from Jenkins
* goal is to get to point where you have 1 script to run that can be pointed to any cluster
* demo included Google internal views - working to try get that external.
* want to be able to collect test run results
* Bob Wise calls for testing infrastructure to be a blocker on v1.3
* Long discussion about testing practicesâ€¦
* consensus that we want to have tests work over multiple platforms.
* would be helpful to have a comprehensive state dump for test reports
* "phone-home" to collect stack traces - should be available
-->
&lt;ul>
&lt;li>SIG æŠ¥å‘Š&lt;/li>
&lt;li>æµ‹è¯•æ›´æ–° [Jeff, Joe, å’Œ Erick]
&lt;ul>
&lt;li>åŠªåŠ›ä½¿æœ‰åŠ©äºŽ K8s çš„å·¥ä½œæµç¨‹æ›´å®¹æ˜“ç†è§£
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/pull/19714">pull/19714&lt;/a>æœ‰ bot æµç¨‹å›¾æ¥å¸®åŠ©ç”¨æˆ·ç†è§£&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>éœ€è¦ä¸€ç§ä¸€è‡´çš„æ–¹æ³•æ¥è¿è¡Œæµ‹è¯• w/hacking é…ç½®è„šæœ¬ï¼ˆä½ çŽ°åœ¨å¿…é¡»ä¼ªé€ ä¸€ä¸ª Jenkins è¿›ç¨‹ï¼‰&lt;/li>
&lt;li>æƒ³è¦åˆ›å»ºå¿…è¦çš„åŸºç¡€è®¾æ–½ï¼Œä½¿æµ‹è¯•è®¾ç½®ä¸é‚£ä¹ˆè–„å¼±&lt;/li>
&lt;li>æƒ³è¦å°†æµ‹è¯•å¼€å§‹ï¼ˆå•æ¬¡æˆ–å®Œæ•´ï¼‰ä¸Ž Jenkinsåˆ†ç¦»&lt;/li>
&lt;li>ç›®æ ‡æ˜¯æŒ‡å‡ºä½ æœ‰ä¸€ä¸ªå¯ä»¥æŒ‡å‘ä»»ä½•é›†ç¾¤çš„è„šæœ¬&lt;/li>
&lt;li>æ¼”ç¤ºåŒ…æ‹¬ Google å†…éƒ¨è§†å›¾ - åŠªåŠ›å°è¯•èŽ·å–å¤–éƒ¨è§†å›¾ã€‚&lt;/li>
&lt;li>å¸Œæœ›èƒ½å¤Ÿæ”¶é›†æµ‹è¯•è¿è¡Œç»“æžœ&lt;/li>
&lt;li>Bob Wise ä¸èµžåŒåœ¨ v1.3 ç‰ˆæœ¬è¿›è¡Œæµ‹è¯•æ–¹é¢çš„åŸºç¡€è®¾æ–½å»ºè®¾ã€‚&lt;/li>
&lt;li>å…³äºŽæµ‹è¯•å®žè·µçš„é•¿æœŸè®¨è®ºâ€¦
&lt;ul>
&lt;li>æˆ‘ä»¬å¸Œæœ›åœ¨å¤šä¸ªå¹³å°ä¸Šè¿›è¡Œæµ‹è¯•çš„å…±è¯†ã€‚&lt;/li>
&lt;li>ä¸ºæµ‹è¯•æŠ¥å‘Šæä¾›ä¸€ä¸ªå…¨é¢è½¬å‚¨ä¼šå¾ˆæœ‰å¸®åŠ©&lt;/li>
&lt;li>å¯ä»¥ä½¿ç”¨&amp;quot;phone-home&amp;quot;æ”¶é›†å¼‚å¸¸&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* 1.2 Release Watch
* CoC [Sarah]
* GSoC [Sarah]
-->
&lt;ul>
&lt;li>1.2å‘å¸ƒè§‚å¯Ÿ&lt;/li>
&lt;li>CoC [Sarah]&lt;/li>
&lt;li>GSoC [Sarah]&lt;/li>
&lt;/ul>
&lt;!--
To get involved in the Kubernetes community consider joining our [Slack channel][2], taking a look at the [Kubernetes project][3] on GitHub, or join the [Kubernetes-dev Google group][4]. If you're really excited, you can do all of the above and join us for the next community conversation -- February 11th, 2016. Please add yourself or a topic you want to know about to the [agenda][5] and get a calendar invitation by joining [this group][6].
-->
&lt;p>è¦å‚ä¸Ž Kubernetes ç¤¾åŒºï¼Œè¯·è€ƒè™‘åŠ å…¥æˆ‘ä»¬çš„&lt;a href="http://slack.k8s.io/">Slack é¢‘é“&lt;/a>ï¼ŒæŸ¥çœ‹ GitHubä¸Šçš„ &lt;a href="https://github.com/kubernetes/">Kubernetes é¡¹ç›®&lt;/a>ï¼Œæˆ–åŠ å…¥&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-dev">Kubernetes-dev Google å°ç»„&lt;/a>ã€‚å¦‚æžœä½ çœŸçš„å¾ˆå…´å¥‹ï¼Œä½ å¯ä»¥å®Œæˆä¸Šè¿°æ‰€æœ‰å·¥ä½œå¹¶åŠ å…¥æˆ‘ä»¬çš„ä¸‹ä¸€æ¬¡ç¤¾åŒºå¯¹è¯-2016å¹´2æœˆ11æ—¥ã€‚è¯·å°†æ‚¨è‡ªå·±æˆ–æ‚¨æƒ³è¦äº†è§£çš„ä¸»é¢˜æ·»åŠ åˆ°&lt;a href="https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#">è®®ç¨‹&lt;/a>å¹¶é€šè¿‡åŠ å…¥&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-community-video-chat">æ­¤ç»„&lt;/a>æ¥èŽ·å–æ—¥åŽ†é‚€è¯·ã€‚&lt;/p>
&lt;p>&amp;quot;https://youtu.be/IScpP8Cj0hw?list=PL69nYSiGNLP1pkHsbPjzAewvMgGUpkCnJ&amp;quot;&lt;/p></description></item><item><title>Blog: Kubernetes ç¤¾åŒºä¼šè®®è®°å½• - 20160114</title><link>https://kubernetes.io/blog/2016/01/kubernetes-community-meeting-notes/</link><pubDate>Thu, 28 Jan 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2016/01/kubernetes-community-meeting-notes/</guid><description>
&lt;!--
---
title: " Kubernetes Community Meeting Notes - 20160114 "
date: 2016-01-28
slug: kubernetes-community-meeting-notes
url: /blog/2016/01/Kubernetes-Community-Meeting-Notes
---
-->
&lt;!--
##### January 14 - RackN demo, testing woes, and KubeCon EU CFP.
---
Note taker: Joe Beda
---
-->
&lt;h5 id="1-æœˆ-14-æ—¥-rackn-æ¼”ç¤º-æµ‹è¯•é—®é¢˜å’Œ-kubecon-eu-cfp">1 æœˆ 14 æ—¥ - RackN æ¼”ç¤ºã€æµ‹è¯•é—®é¢˜å’Œ KubeCon EU CFPã€‚&lt;/h5>
&lt;hr>
&lt;h2 id="è®°å½•è€…-joe-beda">è®°å½•è€…ï¼šJoe Beda&lt;/h2>
&lt;!--
* Demonstration: Automated Deploy on Metal, AWS and others w/ Digital Rebar, Rob Hirschfeld and Greg Althaus from RackN
* Greg Althaus. CTO. Digital Rebar is the product. Bare metal provisioning tool.
* Detect hardware, bring it up, configure raid, OS and get workload deployed.
* Been working on Kubernetes workload.
* Seeing trend to start in cloud and then move back to bare metal.
* New provider model to use provisioning system on both cloud and bare metal.
* UI, REST API, CLI
* Demo: Packet -- bare metal as a service
* 4 nodes running grouped into a "deployment"
* Functional roles/operations selected per node.
* Decomposed the kubernetes bring up into units that can be ordered and synchronized. Dependency tree -- things like wait for etcd to be up before starting k8s master.
* Using the Ansible playbook under the covers.
* Demo brings up 5 more nodes -- packet will build those nodes
* Pulled out basic parameters from the ansible playbook. Things like the network config, dns set up, etc.
* Hierarchy of roles pulls in other components -- making a node a master brings in a bunch of other roles that are necessary for that.
* Has all of this combined into a command line tool with a simple config file.
* Forward: extending across multiple clouds for test deployments. Also looking to create split/replicated across bare metal and cloud.
* Q: secrets?
A: using ansible playbooks. Builds own certs and then distributes them. Wants to abstract them out and push that stuff upstream.
* Q: Do you support bringing up from real bare metal with PXE boot?
A: yes -- will discover bare metal systems and install OS, install ssh keys, build networking, etc.
-->
&lt;ul>
&lt;li>
&lt;p>æ¼”ç¤ºï¼šåœ¨ Metalï¼ŒAWS å’Œå…¶ä»–å¹³å°ä¸Šä½¿ç”¨ Digital Rebar è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼Œæ¥è‡ª RackN çš„ Rob Hirschfeld å’Œ Greg Althausã€‚&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Greg Althausã€‚é¦–å¸­æŠ€æœ¯å®˜ã€‚Digital Rebar æ˜¯äº§å“ã€‚è£¸æœºç½®å¤‡å·¥å…·ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ£€æµ‹ç¡¬ä»¶ï¼Œå¯åŠ¨ç¡¬ä»¶ï¼Œé…ç½® RAIDã€æ“ä½œç³»ç»Ÿå¹¶éƒ¨ç½²å·¥ä½œè´Ÿè½½ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å¤„ç† Kubernetes çš„å·¥ä½œè´Ÿè½½ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>çœ‹åˆ°å§‹äºŽäº‘ï¼Œç„¶åŽåˆå›žåˆ°è£¸æœºçš„è¶‹åŠ¿ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ–°çš„æä¾›å•†æ¨¡åž‹å¯ä»¥åœ¨äº‘å’Œè£¸æœºä¸Šä½¿ç”¨é¢„é…ç½®ç³»ç»Ÿã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>UI, REST API, CLI&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ¼”ç¤ºï¼šæ•°æ®åŒ…--è£¸æœºå³æœåŠ¡&lt;/p>
&lt;ul>
&lt;li>
&lt;p>4ä¸ªæ­£åœ¨è¿è¡Œçš„èŠ‚ç‚¹å½’ä¸ºä¸€ä¸ªâ€œéƒ¨ç½²â€&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ¯ä¸ªèŠ‚ç‚¹é€‰æ‹©çš„åŠŸèƒ½æ€§è§’è‰²/æ“ä½œã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åˆ†è§£çš„ kubernetes å¸¦å…¥å¯ä»¥è®¢è´­å’ŒåŒæ­¥çš„å•å…ƒã€‚ä¾èµ–å…³ç³»æ ‘--è¯¸å¦‚åœ¨å¯åŠ¨ k8s master ä¹‹å‰ç­‰å¾… etcd å¯åŠ¨çš„äº‹æƒ…ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åœ¨å°é¢ä¸‹ä½¿ç”¨ Ansibleã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ¼”ç¤ºå¸¦æ¥äº†å¦å¤–5ä¸ªèŠ‚ç‚¹--æ•°æ®åŒ…å°†æž„å»ºè¿™äº›èŠ‚ç‚¹&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä»Ž ansible ä¸­æå–åŸºæœ¬å‚æ•°ã€‚è¯¸å¦‚ç½‘ç»œé…ç½®ï¼ŒDNS è®¾ç½®ç­‰å†…å®¹ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è§’è‰²å±‚æ¬¡ç»“æž„å¼•å…¥äº†å…¶ä»–ç»„ä»¶--ä½¿èŠ‚ç‚¹æˆä¸ºä¸»èŠ‚ç‚¹ä¼šå¸¦æ¥ä¸€ç³»åˆ—å…¶ä»–å¿…è¦çš„è§’è‰²ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é€šè¿‡ç®€å•çš„é…ç½®æ–‡ä»¶å°†æ‰€æœ‰è¿™äº›ç»„åˆåˆ°å‘½ä»¤è¡Œå·¥å…·ä¸­ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>è½¬å‘ï¼šæ‰©å±•åˆ°å¤šä¸ªäº‘ä»¥è¿›è¡Œæµ‹è¯•éƒ¨ç½²ã€‚è¿˜å¸Œæœ›åœ¨è£¸æœºå’Œäº‘ä¹‹é—´åˆ›å»ºæ‹†åˆ†/å¤åˆ¶ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é—®ï¼šç§˜å¯†ï¼Ÿ&lt;br>
ç­”ï¼šä½¿ç”¨ Ansibleã€‚æž„å»ºè‡ªå·±çš„è¯ä¹¦ï¼Œç„¶åŽåˆ†å‘å®ƒä»¬ã€‚æƒ³è¦å°†å®ƒä»¬æŠ½è±¡å‡ºæ¥å¹¶å°†å…¶æŽ¨å…¥ä¸Šæ¸¸ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é—®ï¼šæ‚¨æ˜¯å¦æ”¯æŒä½¿ç”¨ PXE å¼•å¯¼ä»ŽçœŸæ­£çš„è£¸æœºå¯åŠ¨ï¼Ÿ&lt;br>
ç­”ï¼šæ˜¯çš„--å°†å‘çŽ°è£¸æœºç³»ç»Ÿå¹¶å®‰è£…æ“ä½œç³»ç»Ÿï¼Œå®‰è£… ssh å¯†é’¥ï¼Œå»ºç«‹ç½‘ç»œç­‰ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* [from SIG-scalability] Q: What is the status of moving to golang 1.5?
A: At HEAD we are 1.5 but will support 1.4 also. Some issues with flakiness but looks like things are stable now.
* Also looking to use the 1.5 vendor experiment. Move away from godep. But can't do that until 1.5 is the baseline.
* Sarah: one of the things we are working on is rewards for doing stuff like this. Cloud credits, tshirts, poker chips, ponies.
* [from SIG-scalability] Q: What is the status of cleaning up the jenkins based submit queue? What can the community do to help out?
A: It has been rocky the last few days. There should be issues associated with each of these. There is a [flake label][1] on those issues.
* Still working on test federation. More test resources now. Happening slowly but hopefully faster as new people come up to speed. Will be great to having lots of folks doing e2e tests on their environments.
* Erick Fjeta is the new test lead
* Brendan is happy to help share details on Jenkins set up but that shouldn't be necessary.
* Federation may use Jenkins API but doesn't require Jenkins itself.
* Joe bitches about the fact that running the e2e tests in the way Jenkins is tricky. Brendan says it should be runnable easily. Joe will take another look.
* Conformance tests? etune did this but he isn't here. - revisit 20150121
-->
&lt;ul>
&lt;li>
&lt;p>[æ¥è‡ª SIG-scalability]é—®ï¼šè½¬åˆ° golang 1.5 çš„çŠ¶æ€å¦‚ä½•ï¼Ÿ&lt;br>
ç­”ï¼šåœ¨ HEADï¼Œæˆ‘ä»¬æ˜¯1.5ï¼Œä½†ä¹Ÿä¼šæ”¯æŒ1.4ã€‚æœ‰ä¸€äº›è·Ÿç¨³å®šæ€§ç›¸å…³çš„é—®é¢˜ï¼Œä½†çœ‹èµ·æ¥çŽ°åœ¨æƒ…å†µç¨³å®šäº†ã€‚&lt;/p>
&lt;ul>
&lt;li>
&lt;p>è¿˜å¸Œæœ›ä½¿ç”¨1.5ä¾›åº”å•†å®žéªŒã€‚ä¸å†ä½¿ç”¨ godepã€‚ä½†åªæœ‰åœ¨åŸºçº¿ä¸º1.5ä¹‹å‰ï¼Œæ‰èƒ½è¿™æ ·åšã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Sarahï¼šçŽ°åœ¨æˆ‘ä»¬æ­£åœ¨å·¥ä½œçš„äº‹æƒ…ä¹‹ä¸€å°±æ˜¯æä¾›åšè¿™äº›äº‹çš„å°å¥–å“ã€‚äº‘ç§¯åˆ†ï¼ŒTæ¤ï¼Œæ‰‘å…‹ç­¹ç ï¼Œå°é©¬ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>[æ¥è‡ªå¯ä¼¸ç¼©æ€§å…´è¶£å°ç»„]é—®ï¼šæ¸…ç†åŸºäºŽ jenkins çš„æäº¤é˜Ÿåˆ—çš„çŠ¶æ€å¦‚ä½•ï¼Ÿç¤¾åŒºå¯ä»¥åšäº›ä»€ä¹ˆæ¥å¸®åŠ©æ‚¨ï¼Ÿ&lt;br>
ç­”ï¼šæœ€è¿‘å‡ å¤©ä¸€ç›´å¾ˆè‰°éš¾ã€‚æ¯ä¸ªæ–¹é¢éƒ½åº”è¯¥æœ‰ç›¸å…³çš„é—®é¢˜ã€‚åœ¨è¿™äº›é—®é¢˜ä¸Šæœ‰ä¸€ä¸ª&lt;a href="https://github.com/kubernetes/kubernetes/labels/kind%2Fflake">ç‰‡çŠ¶æ ‡ç­¾&lt;/a>ã€‚&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ä»åœ¨è¿›è¡Œè”ç›Ÿæµ‹è¯•ã€‚çŽ°åœ¨æœ‰æ›´å¤šæµ‹è¯•èµ„æºã€‚éšç€æ–°äººä»¬çš„è¿›æ­¥ï¼Œäº‹æƒ…æœ‰å¸Œæœ›è¿›å±•çš„æ›´å¿«ã€‚è¿™å¯¹è®©å¾ˆå¤šäººåœ¨ä»–ä»¬çš„çŽ¯å¢ƒä¸­è¿›è¡Œç«¯åˆ°ç«¯æµ‹è¯•éžå¸¸æœ‰ç”¨ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Erick Fjeta æ˜¯æ–°çš„æµ‹è¯•è´Ÿè´£äºº&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Brendanå¾ˆä¹æ„å¸®åŠ©åˆ†äº«æœ‰å…³ Jenkins è®¾ç½®çš„è¯¦ç»†ä¿¡æ¯ï¼Œä½†è¿™ä¸æ˜¯å¿…é¡»çš„ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è”ç›Ÿå¯ä»¥ä½¿ç”¨ Jenkins APIï¼Œä½†ä¸éœ€è¦ Jenkins æœ¬èº«ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Joe å˜²ç¬‘ä»¥ Jenkins çš„æ–¹å¼è¿è¡Œ e2e æµ‹è¯•æ˜¯ä¸€ä»¶æ£˜æ‰‹çš„äº‹å®žã€‚å¸ƒä¼¦ä¸¹è¯´ï¼Œå®ƒåº”è¯¥æ˜“äºŽè¿è¡Œã€‚ä¹”å†çœ‹ä¸€çœ¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ç¬¦åˆæ€§æµ‹è¯•ï¼Ÿ etune åšåˆ°äº†ï¼Œä½†ä»–ä¸åœ¨ã€‚ -é‡æ–°è®¿é—®20150121&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* * March 10-11 in London. Venue to be announced this week.
* Please send talks! CFP deadline looks to be Feb 5.
* Lots of excitement. Looks to be 700-800 people. Bigger than SF version (560 ppl).
* Buy tickets early -- early bird prices will end soon and price will go up 100 GBP.
* Accommodations provided for speakers?
* Q from Bob @ Samsung: Can we get more warning/planning for stuff like this:
* A: Sarah -- I don't hear about this stuff much in advance but will try to pull together a list. Working to make the events page on kubernetes.io easier to use.
* A: JJ -- we'll make sure we give more info earlier for the next US conf.
-->
&lt;ul>
&lt;li>
&lt;pre>&lt;code>* 3æœˆ10æ—¥è‡³11æ—¥åœ¨ä¼¦æ•¦ä¸¾è¡Œã€‚åœ°ç‚¹å°†äºŽæœ¬å‘¨å®£å¸ƒã€‚
&lt;/code>&lt;/pre>
&lt;ul>
&lt;li>
&lt;p>è¯·å‘é€è®²è¯ï¼CFP æˆªæ­¢æ—¥æœŸä¸º2æœˆ5æ—¥ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä»¤äººå…´å¥‹ã€‚çœ‹èµ·æ¥æ˜¯700-800äººã€‚æ¯”SFç‰ˆæœ¬å¤§ï¼ˆ560 äººï¼‰ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æå‰è´­ä¹°é—¨ç¥¨--æ—©èµ·çš„é¸Ÿå„¿ä»·æ ¼å°†å¾ˆå¿«ç»“æŸï¼Œä»·æ ¼å°†ä¸Šæ¶¨åˆ°100è‹±é•‘ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä¸ºæ¼”è®²è€…æä¾›ä½å®¿å—ï¼Ÿ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä¸‰æ˜Ÿ Bob çš„æé—®ï¼šæˆ‘ä»¬å¯ä»¥é’ˆå¯¹ä»¥ä¸‹é—®é¢˜èŽ·å¾—æ›´å¤šè­¦å‘Š/è®¡åˆ’å—ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ç­”ï¼šSarah -- æˆ‘ä¸å¤ªæ—©å°±å¬è¯´è¿‡è¿™äº›ä¸œè¥¿ï¼Œä½†ä¼šå°è¯•æ•´ç†ä¸€ä¸‹æ¸…å•ã€‚åŠªåŠ›ä½¿ kubernetes.io ä¸Šçš„äº‹ä»¶é¡µé¢æ›´æ˜“äºŽä½¿ç”¨ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ç­”ï¼šJJ -- æˆ‘ä»¬å°†ç¡®ä¿æˆ‘ä»¬æ—©æ—¥ä¸ºä¸‹å±Šç¾Žå›½ä¼šè®®æä¾›æ›´å¤šä¿¡æ¯ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Scale tests [Rob Hirschfeld from RackN] -- if you want to help coordinate on scale tests we'd love to help.
* Bob invited Rob to join the SIG-scale group.
* There is also a big bare metal cluster through the CNCF (from Intel) that will be useful too. No hard dates yet on that.
* Notes/video going to be posted on k8s blog. (Video for 20150114 wasn't recorded. Fail.)
To get involved in the Kubernetes community consider joining our [Slack channel][2], taking a look at the [Kubernetes project][3] on GitHub, or join the [Kubernetes-dev Google group][4]. If you're really excited, you can do all of the above and join us for the next community conversation - January 27th, 2016. Please add yourself or a topic you want to know about to the [agenda][5] and get a calendar invitation by joining [this group][6].
-->
&lt;ul>
&lt;li>
&lt;p>è§„æ¨¡æµ‹è¯•[RackN çš„ Rob Hirschfeld] -- å¦‚æžœæ‚¨æƒ³å¸®åŠ©è¿›è¡Œè§„æ¨¡æµ‹è¯•ï¼Œæˆ‘ä»¬å°†éžå¸¸ä¹æ„ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Bob é‚€è¯· Rob åŠ å…¥è§„æ¨¡ç‰¹åˆ«å…´è¶£å°ç»„ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è¿˜æœ‰ä¸€ä¸ªå¤§åž‹çš„è£¸æœºé›†ç¾¤ï¼Œé€šè¿‡ CNCFï¼ˆæ¥è‡ª Intelï¼‰ä¹Ÿå¾ˆæœ‰ç”¨ã€‚å°šæ— ç¡®åˆ‡æ—¥æœŸã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>ç¬”è®°/è§†é¢‘å°†å‘å¸ƒåœ¨ k8s åšå®¢ä¸Šã€‚ï¼ˆæœªå½•åˆ¶20150114çš„è§†é¢‘ã€‚å¤±è´¥ã€‚ï¼‰&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>è¦åŠ å…¥ Kubernetes ç¤¾åŒºï¼Œè¯·è€ƒè™‘åŠ å…¥æˆ‘ä»¬çš„&lt;a href="http://slack.k8s.io/">Slack é¢‘é“&lt;/a>ï¼Œçœ‹çœ‹GitHubä¸Šçš„&lt;a href="https://github.com/kubernetes/">Kubernetes é¡¹ç›®&lt;/a>ï¼Œæˆ–åŠ å…¥&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-dev">Kubernetes-dev Google è®ºå›&lt;/a>ã€‚å¦‚æžœæ‚¨çœŸçš„å¯¹æ­¤æ„Ÿåˆ°å…´å¥‹ï¼Œåˆ™å¯ä»¥å®Œæˆä¸Šè¿°æ‰€æœ‰æ“ä½œï¼Œå¹¶åŠ å…¥æˆ‘ä»¬çš„ä¸‹ä¸€æ¬¡ç¤¾åŒºå¯¹è¯-2016å¹´1æœˆ27æ—¥ã€‚è¯·å°†æ‚¨è‡ªå·±æˆ–æ‚¨æƒ³äº†è§£çš„è¯é¢˜æ·»åŠ åˆ°&lt;a href="https://docs.google.com/document/d/1VQDIAB0OqiSjIHI8AWMvSdceWhnz56jNpZrLs6o7NJY/edit#">è®®ç¨‹&lt;/a>ä¸­ï¼Œå¹¶èŽ·å¾—ä¸€ä¸ªåŠ å…¥&lt;a href="https://groups.google.com/forum/#!forum/kubernetes-community-video-chat">æ­¤ç¾¤ç»„&lt;/a>è¿›è¡Œæ—¥åŽ†é‚€è¯·ã€‚&lt;/p></description></item><item><title>Blog: ä¸ºä»€ä¹ˆ Kubernetes ä¸ç”¨ libnetwork</title><link>https://kubernetes.io/zh/blog/2016/01/14/why-kubernetes-doesnt-use-libnetwork/</link><pubDate>Thu, 14 Jan 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/01/14/why-kubernetes-doesnt-use-libnetwork/</guid><description>
&lt;!-- ---
title: " Why Kubernetes doesnâ€™t use libnetwork "
date: 2016-01-14
slug: why-kubernetes-doesnt-use-libnetwork
url: /blog/2016/01/Why-Kubernetes-Doesnt-Use-Libnetwork
--- -->
&lt;!-- Kubernetes has had a very basic form of network plugins since before version 1.0 was released â€” around the same time as Docker's [libnetwork](https://github.com/docker/libnetwork) and Container Network Model ([CNM](https://github.com/docker/libnetwork/blob/master/docs/design.md)) was introduced. Unlike libnetwork, the Kubernetes plugin system still retains its "alpha" designation. Now that Docker's network plugin support is released and supported, an obvious question we get is why Kubernetes has not adopted it yet. After all, vendors will almost certainly be writing plugins for Docker â€” we would all be better off using the same drivers, right? -->
&lt;p>åœ¨ 1.0 ç‰ˆæœ¬å‘å¸ƒä¹‹å‰ï¼ŒKubernetes å·²ç»æœ‰äº†ä¸€ä¸ªéžå¸¸åŸºç¡€çš„ç½‘ç»œæ’ä»¶å½¢å¼-å¤§çº¦åœ¨å¼•å…¥ Dockerâ€™s &lt;a href="https://github.com/docker/libnetwork">libnetwork&lt;/a> å’Œ Container Network Model (&lt;a href="https://github.com/docker/libnetwork/blob/master/docs/design.md">CNM&lt;/a>) çš„æ—¶å€™ã€‚ä¸Ž libnetwork ä¸åŒï¼ŒKubernetes æ’ä»¶ç³»ç»Ÿä»ç„¶ä¿ç•™å®ƒçš„ 'alpha' åç§°ã€‚çŽ°åœ¨ Docker çš„ç½‘ç»œæ’ä»¶æ”¯æŒå·²ç»å‘å¸ƒå¹¶å¾—åˆ°æ”¯æŒï¼Œæˆ‘ä»¬å‘çŽ°ä¸€ä¸ªæ˜Žæ˜¾çš„é—®é¢˜æ˜¯ Kubernetes å°šæœªé‡‡ç”¨å®ƒã€‚æ¯•ç«Ÿï¼Œä¾›åº”å•†å‡ ä¹Žè‚¯å®šä¼šä¸º Docker ç¼–å†™æ’ä»¶-æˆ‘ä»¬æœ€å¥½è¿˜æ˜¯ç”¨ç›¸åŒçš„é©±åŠ¨ç¨‹åºï¼Œå¯¹å§ï¼Ÿ&lt;/p>
&lt;!-- Before going further, it's important to remember that Kubernetes is a system that supports multiple container runtimes, of which Docker is just one. Configuring networking is a facet of each runtime, so when people ask "will Kubernetes support CNM?" what they really mean is "will kubernetes support CNM drivers with the Docker runtime?" It would be great if we could achieve common network support across runtimes, but thatâ€™s not an explicit goal. -->
&lt;p>åœ¨è¿›ä¸€æ­¥è¯´æ˜Žä¹‹å‰ï¼Œé‡è¦çš„æ˜¯è®°ä½ Kubernetes æ˜¯ä¸€ä¸ªæ”¯æŒå¤šç§å®¹å™¨è¿è¡Œæ—¶çš„ç³»ç»Ÿï¼Œ Docker åªæ˜¯å…¶ä¸­ä¹‹ä¸€ã€‚é…ç½®ç½‘ç»œåªæ˜¯æ¯ä¸€ä¸ªè¿è¡Œæ—¶çš„ä¸€ä¸ªæ–¹é¢ï¼Œæ‰€ä»¥å½“äººä»¬é—®èµ·â€œ Kubernetes ä¼šæ”¯æŒCNMå—ï¼Ÿâ€ï¼Œä»–ä»¬çœŸæ­£çš„æ„æ€æ˜¯â€œ Kubernetes ä¼šæ”¯æŒ Docker è¿è¡Œæ—¶çš„ CNM é©±åŠ¨å—ï¼Ÿâ€å¦‚æžœæˆ‘ä»¬èƒ½å¤Ÿè·¨è¿è¡Œæ—¶å®žçŽ°é€šç”¨çš„ç½‘ç»œæ”¯æŒä¼šå¾ˆæ£’ï¼Œä½†è¿™ä¸æ˜¯ä¸€ä¸ªæ˜Žç¡®çš„ç›®æ ‡ã€‚&lt;/p>
&lt;!-- Indeed, Kubernetes has not adopted CNM/libnetwork for the Docker runtime. In fact, weâ€™ve been investigating the alternative Container Network Interface ([CNI](https://github.com/appc/cni/blob/master/SPEC.md)) model put forth by CoreOS and part of the App Container ([appc](https://github.com/appc)) specification. Why? There are a number of reasons, both technical and non-technical. -->
&lt;p>å®žé™…ä¸Šï¼Œ Kubernetes è¿˜æ²¡æœ‰ä¸º Docker è¿è¡Œæ—¶é‡‡ç”¨ CNM/libnetwork ã€‚äº‹å®žä¸Šï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ç ”ç©¶ CoreOS æå‡ºçš„æ›¿ä»£ Container Network Interface (&lt;a href="https://github.com/appc/cni/blob/master/SPEC.md">CNI&lt;/a>) æ¨¡åž‹ä»¥åŠ App Container (&lt;a href="https://github.com/appc">appc&lt;/a>) è§„èŒƒçš„ä¸€éƒ¨åˆ†ã€‚ä¸ºä»€ä¹ˆæˆ‘ä»¬è¦è¿™ä¹ˆåšï¼Ÿæœ‰å¾ˆå¤šæŠ€æœ¯å’ŒéžæŠ€æœ¯çš„åŽŸå› ã€‚&lt;/p>
&lt;!-- First and foremost, there are some fundamental assumptions in the design of Docker's network drivers that cause problems for us. -->
&lt;p>é¦–å…ˆï¼ŒDocker çš„ç½‘ç»œé©±åŠ¨ç¨‹åºè®¾è®¡ä¸­å­˜åœ¨ä¸€äº›åŸºæœ¬å‡è®¾ï¼Œè¿™äº›å‡è®¾ä¼šç»™æˆ‘ä»¬å¸¦æ¥é—®é¢˜ã€‚&lt;/p>
&lt;!-- Docker has a concept of "local" and "global" drivers. Local drivers (such as "bridge") are machine-centric and donâ€™t do any cross-node coordination. Global drivers (such as "overlay") rely on [libkv](https://github.com/docker/libkv) (a key-value store abstraction) to coordinate across machines. This key-value store is a another plugin interface, and is very low-level (keys and values, no semantic meaning). To run something like Docker's overlay driver in a Kubernetes cluster, we would either need cluster admins to run a whole different instance of [consul](https://github.com/hashicorp/consul), [etcd](https://github.com/coreos/etcd) or [zookeeper](https://zookeeper.apache.org/) (see [multi-host networking](https://docs.docker.com/engine/userguide/networking/get-started-overlay/)), or else we would have to provide our own libkv implementation that was backed by Kubernetes. -->
&lt;p>Docker æœ‰ä¸€ä¸ªâ€œæœ¬åœ°â€å’Œâ€œå…¨å±€â€é©±åŠ¨ç¨‹åºçš„æ¦‚å¿µã€‚æœ¬åœ°é©±åŠ¨ç¨‹åºï¼ˆä¾‹å¦‚ &amp;quot;bridge&amp;quot; ï¼‰ä»¥æœºå™¨ä¸ºä¸­å¿ƒï¼Œä¸è¿›è¡Œä»»ä½•è·¨èŠ‚ç‚¹åè°ƒã€‚å…¨å±€é©±åŠ¨ç¨‹åºï¼ˆä¾‹å¦‚ &amp;quot;overlay&amp;quot; ï¼‰ä¾èµ–äºŽ &lt;a href="https://github.com/docker/libkv">libkv&lt;/a> ï¼ˆä¸€ä¸ªé”®å€¼å­˜å‚¨æŠ½è±¡åº“ï¼‰æ¥åè°ƒè·¨æœºå™¨ã€‚è¿™ä¸ªé”®å€¼å­˜å‚¨æ˜¯å¦ä¸€ä¸ªæ’ä»¶æŽ¥å£ï¼Œå¹¶ä¸”æ˜¯éžå¸¸ä½Žçº§çš„ï¼ˆé”®å’Œå€¼ï¼Œæ²¡æœ‰å…¶ä»–å«ä¹‰ï¼‰ã€‚ è¦åœ¨ Kubernetes é›†ç¾¤ä¸­è¿è¡Œç±»ä¼¼ Docker's overlay é©±åŠ¨ç¨‹åºï¼Œæˆ‘ä»¬è¦ä¹ˆéœ€è¦é›†ç¾¤ç®¡ç†å‘˜æ¥è¿è¡Œ &lt;a href="https://github.com/hashicorp/consul">consul&lt;/a>, &lt;a href="https://github.com/coreos/etcd">etcd&lt;/a> æˆ– &lt;a href="https://zookeeper.apache.org/">zookeeper&lt;/a> çš„æ•´ä¸ªä¸åŒå®žä¾‹ (see &lt;a href="https://docs.docker.com/engine/userguide/networking/get-started-overlay/">multi-host networking&lt;/a>) å¦åˆ™æˆ‘ä»¬å¿…é¡»æä¾›æˆ‘ä»¬è‡ªå·±çš„ libkv å®žçŽ°ï¼Œé‚£è¢« Kubernetes æ”¯æŒã€‚&lt;/p>
&lt;!-- The latter sounds attractive, and we tried to implement it, but the libkv interface is very low-level, and the schema is defined internally to Docker. We would have to either directly expose our underlying key-value store or else offer key-value semantics (on top of our structured API which is itself implemented on a key-value system). Neither of those are very attractive for performance, scalability and security reasons. The net result is that the whole system would significantly be more complicated, when the goal of using Docker networking is to simplify things. -->
&lt;p>åŽè€…å¬èµ·æ¥å¾ˆæœ‰å¸å¼•åŠ›ï¼Œå¹¶ä¸”æˆ‘ä»¬å°è¯•å®žçŽ°å®ƒï¼Œä½† libkv æŽ¥å£æ˜¯éžå¸¸ä½Žçº§çš„ï¼Œå¹¶ä¸”æž¶æž„åœ¨å†…éƒ¨å®šä¹‰ä¸º Docker ã€‚æˆ‘ä»¬å¿…é¡»ç›´æŽ¥æš´éœ²æˆ‘ä»¬çš„åº•å±‚é”®å€¼å­˜å‚¨ï¼Œæˆ–è€…æä¾›é”®å€¼è¯­ä¹‰ï¼ˆåœ¨æˆ‘ä»¬çš„ç»“æž„åŒ–APIä¹‹ä¸Šï¼Œå®ƒæœ¬èº«æ˜¯åœ¨é”®å€¼ç³»ç»Ÿä¸Šå®žçŽ°çš„ï¼‰ã€‚å¯¹äºŽæ€§èƒ½ï¼Œå¯ä¼¸ç¼©æ€§å’Œå®‰å…¨æ€§åŽŸå› ï¼Œè¿™äº›éƒ½ä¸æ˜¯å¾ˆæœ‰å¸å¼•åŠ›ã€‚æœ€ç»ˆç»“æžœæ˜¯ï¼Œå½“ä½¿ç”¨ Docker ç½‘ç»œçš„ç›®æ ‡æ˜¯ç®€åŒ–äº‹æƒ…æ—¶ï¼Œæ•´ä¸ªç³»ç»Ÿå°†æ˜¾å¾—æ›´åŠ å¤æ‚ã€‚&lt;/p>
&lt;!-- For users that are willing and able to run the requisite infrastructure to satisfy Docker global drivers and to configure Docker themselves, Docker networking should "just work." Kubernetes will not get in the way of such a setup, and no matter what direction the project goes, that option should be available. For default installations, though, the practical conclusion is that this is an undue burden on users and we therefore cannot use Docker's global drivers (including "overlay"), which eliminates a lot of the value of using Docker's plugins at all. -->
&lt;p>å¯¹äºŽæ„¿æ„å¹¶ä¸”èƒ½å¤Ÿè¿è¡Œå¿…éœ€çš„åŸºç¡€æž¶æž„ä»¥æ»¡è¶³ Docker å…¨å±€é©±åŠ¨ç¨‹åºå¹¶è‡ªå·±é…ç½® Docker çš„ç”¨æˆ·ï¼Œ Docker ç½‘ç»œåº”è¯¥â€œæ­£å¸¸å·¥ä½œã€‚â€ Kubernetes ä¸ä¼šå¦¨ç¢è¿™æ ·çš„è®¾ç½®ï¼Œæ— è®ºé¡¹ç›®çš„æ–¹å‘å¦‚ä½•ï¼Œè¯¥é€‰é¡¹éƒ½åº”è¯¥å¯ç”¨ã€‚ä½†æ˜¯å¯¹äºŽé»˜è®¤å®‰è£…ï¼Œå®žé™…çš„ç»“è®ºæ˜¯è¿™å¯¹ç”¨æˆ·æ¥è¯´æ˜¯ä¸€ä¸ªä¸åº”æœ‰çš„è´Ÿæ‹…ï¼Œå› æ­¤æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨ Docker çš„å…¨å±€é©±åŠ¨ç¨‹åºï¼ˆåŒ…æ‹¬ &amp;quot;overlay&amp;quot; ï¼‰ï¼Œè¿™æ¶ˆé™¤äº†ä½¿ç”¨ Docker æ’ä»¶çš„å¾ˆå¤šä»·å€¼ã€‚&lt;/p>
&lt;!-- Docker's networking model makes a lot of assumptions that arenâ€™t valid for Kubernetes. In docker versions 1.8 and 1.9, it includes a fundamentally flawed implementation of "discovery" that results in corrupted `/etc/hosts` files in containers ([docker #17190](https://github.com/docker/docker/issues/17190)) â€” and this cannot be easily turned off. In version 1.10 Docker is planning to [bundle a new DNS server](https://github.com/docker/docker/issues/17195), and itâ€™s unclear whether this will be able to be turned off. Container-level naming is not the right abstraction for Kubernetes â€” we already have our own concepts of service naming, discovery, and binding, and we already have our own DNS schema and server (based on the well-established [SkyDNS](https://github.com/skynetservices/skydns)). The bundled solutions are not sufficient for our needs but are not disableable. -->
&lt;p>Docker çš„ç½‘ç»œæ¨¡åž‹åšå‡ºäº†è®¸å¤šå¯¹ Kubernetes æ— æ•ˆçš„å‡è®¾ã€‚åœ¨ docker 1.8 å’Œ 1.9 ç‰ˆæœ¬ä¸­ï¼Œå®ƒåŒ…å«ä¸€ä¸ªä»Žæ ¹æœ¬ä¸Šæœ‰ç¼ºé™·çš„â€œå‘çŽ°â€å®žçŽ°ï¼Œå¯¼è‡´å®¹å™¨ä¸­çš„ &lt;code>/etc/hosts&lt;/code> æ–‡ä»¶æŸå (&lt;a href="https://github.com/docker/docker/issues/17190">docker #17190&lt;/a>) - å¹¶ä¸”è¿™ä¸å®¹æ˜“è¢«å…³é—­ã€‚åœ¨ 1.10 ç‰ˆæœ¬ä¸­ï¼ŒDocker è®¡åˆ’ &lt;a href="https://github.com/docker/docker/issues/17195">æ†ç»‘ä¸€ä¸ªæ–°çš„DNSæœåŠ¡å™¨&lt;/a>ï¼Œç›®å‰è¿˜ä¸æ¸…æ¥šæ˜¯å¦å¯ä»¥å…³é—­å®ƒã€‚å®¹å™¨çº§å‘½åä¸æ˜¯ Kubernetes çš„æ­£ç¡®æŠ½è±¡ - æˆ‘ä»¬å·²ç»æœ‰äº†è‡ªå·±çš„æœåŠ¡å‘½åï¼Œå‘çŽ°å’Œç»‘å®šæ¦‚å¿µï¼Œå¹¶ä¸”æˆ‘ä»¬å·²ç»æœ‰äº†è‡ªå·±çš„ DNS æ¨¡å¼å’ŒæœåŠ¡å™¨ï¼ˆåŸºäºŽå®Œå–„çš„ &lt;a href="https://github.com/skynetservices/skydns">SkyDNS&lt;/a> ï¼‰ã€‚æ†ç»‘çš„è§£å†³æ–¹æ¡ˆä¸è¶³ä»¥æ»¡è¶³æˆ‘ä»¬çš„éœ€æ±‚ï¼Œä½†ä¸èƒ½ç¦ç”¨ã€‚&lt;/p>
&lt;!-- Orthogonal to the local/global split, Docker has both in-process and out-of-process ("remote") plugins. We investigated whether we could bypass libnetwork (and thereby skip the issues above) and drive Docker remote plugins directly. Unfortunately, this would mean that we could not use any of the Docker in-process plugins, "bridge" and "overlay" in particular, which again eliminates much of the utility of libnetwork. -->
&lt;p>ä¸Žæœ¬åœ°/å…¨å±€æ‹†åˆ†æ­£äº¤ï¼Œ Docker å…·æœ‰è¿›ç¨‹å†…å’Œè¿›ç¨‹å¤–ï¼ˆ &amp;quot;remote&amp;quot; ï¼‰æ’ä»¶ã€‚æˆ‘ä»¬è°ƒæŸ¥äº†æ˜¯å¦å¯ä»¥ç»•è¿‡ libnetwork ï¼ˆä»Žè€Œè·³è¿‡ä¸Šé¢çš„é—®é¢˜ï¼‰å¹¶ç›´æŽ¥é©±åŠ¨ Docker remote æ’ä»¶ã€‚ä¸å¹¸çš„æ˜¯ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬æ— æ³•ä½¿ç”¨ä»»ä½• Docker è¿›ç¨‹ä¸­çš„æ’ä»¶ï¼Œç‰¹åˆ«æ˜¯ &amp;quot;bridge&amp;quot; å’Œ &amp;quot;overlay&amp;quot;ï¼Œè¿™å†æ¬¡æ¶ˆé™¤äº† libnetwork çš„å¤§éƒ¨åˆ†åŠŸèƒ½ã€‚&lt;/p>
&lt;!-- On the other hand, CNI is more philosophically aligned with Kubernetes. It's far simpler than CNM, doesn't require daemons, and is at least plausibly cross-platform (CoreOSâ€™s [rkt](https://coreos.com/rkt/docs/) container runtime supports it). Being cross-platform means that there is a chance to enable network configurations which will work the same across runtimes (e.g. Docker, Rocket, Hyper). It follows the UNIX philosophy of doing one thing well. -->
&lt;p>å¦ä¸€æ–¹é¢ï¼Œ CNI åœ¨å“²å­¦ä¸Šä¸Ž Kubernetes æ›´åŠ ä¸€è‡´ã€‚å®ƒæ¯” CNM ç®€å•å¾—å¤šï¼Œä¸éœ€è¦å®ˆæŠ¤è¿›ç¨‹ï¼Œå¹¶ä¸”è‡³å°‘æ˜¯åˆç†çš„è·¨å¹³å°ï¼ˆ CoreOS çš„ &lt;a href="https://coreos.com/rkt/docs/">rkt&lt;/a> å®¹å™¨è¿è¡Œæ—¶æ”¯æŒå®ƒï¼‰ã€‚è·¨å¹³å°æ„å‘³ç€æœ‰æœºä¼šå¯ç”¨è·¨è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚ Docker ï¼Œ Rocket ï¼Œ Hyper ï¼‰è¿è¡Œç›¸åŒçš„ç½‘ç»œé…ç½®ã€‚ å®ƒéµå¾ª UNIX çš„ç†å¿µï¼Œå³åšå¥½ä¸€ä»¶äº‹ã€‚&lt;/p>
&lt;!-- Additionally, it's trivial to wrap a CNI plugin and produce a more customized CNI plugin â€” it can be done with a simple shell script. CNM is much more complex in this regard. This makes CNI an attractive option for rapid development and iteration. Early prototypes have proven that it's possible to eject almost 100% of the currently hard-coded network logic in kubelet into a plugin. -->
&lt;p>æ­¤å¤–ï¼ŒåŒ…è£… CNI æ’ä»¶å¹¶ç”Ÿæˆæ›´åŠ ä¸ªæ€§åŒ–çš„ CNI æ’ä»¶æ˜¯å¾®ä¸è¶³é“çš„ - å®ƒå¯ä»¥é€šè¿‡ç®€å•çš„ shell è„šæœ¬å®Œæˆã€‚ CNM åœ¨è¿™æ–¹é¢è¦å¤æ‚å¾—å¤šã€‚è¿™ä½¿å¾— CNI å¯¹äºŽå¿«é€Ÿå¼€å‘å’Œè¿­ä»£æ˜¯æœ‰å¸å¼•åŠ›çš„é€‰æ‹©ã€‚æ—©æœŸçš„åŽŸåž‹å·²ç»è¯æ˜Žï¼Œå¯ä»¥å°† kubelet ä¸­å‡ ä¹Ž 100ï¼… çš„å½“å‰ç¡¬ç¼–ç ç½‘ç»œé€»è¾‘å¼¹å‡ºåˆ°æ’ä»¶ä¸­ã€‚&lt;/p>
&lt;!-- We investigated [writing a "bridge" CNM driver](https://groups.google.com/forum/#!topic/kubernetes-sig-network/5MWRPxsURUw) for Docker that ran CNI drivers. This turned out to be very complicated. First, the CNM and CNI models are very different, so none of the "methods" lined up. We still have the global vs. local and key-value issues discussed above. Assuming this driver would declare itself local, we have to get info about logical networks from Kubernetes. -->
&lt;p>æˆ‘ä»¬è°ƒæŸ¥äº†ä¸º Docker &lt;a href="https://groups.google.com/forum/#!topic/kubernetes-sig-network/5MWRPxsURUw">ç¼–å†™ &amp;quot;bridge&amp;quot; CNMé©±åŠ¨ç¨‹åº&lt;/a> å¹¶è¿è¡Œ CNI é©±åŠ¨ç¨‹åºã€‚äº‹å®žè¯æ˜Žè¿™éžå¸¸å¤æ‚ã€‚é¦–å…ˆï¼Œ CNM å’Œ CNI æ¨¡åž‹éžå¸¸ä¸åŒï¼Œå› æ­¤æ²¡æœ‰ä¸€ç§â€œæ–¹æ³•â€åè°ƒä¸€è‡´ã€‚ æˆ‘ä»¬ä»ç„¶æœ‰ä¸Šé¢è®¨è®ºçš„å…¨çƒä¸Žæœ¬åœ°å’Œé”®å€¼é—®é¢˜ã€‚å‡è®¾è¿™ä¸ªé©±åŠ¨ç¨‹åºä¼šå£°æ˜Žè‡ªå·±æ˜¯æœ¬åœ°çš„ï¼Œæˆ‘ä»¬å¿…é¡»ä»Ž Kubernetes èŽ·å–æœ‰å…³é€»è¾‘ç½‘ç»œçš„ä¿¡æ¯ã€‚&lt;/p>
&lt;!-- Unfortunately, Docker drivers are hard to map to other control planes like Kubernetes. Specifically, drivers are not told the name of the network to which a container is being attached â€” just an ID that Docker allocates internally. This makes it hard for a driver to map back to any concept of network that exists in another system. -->
&lt;p>ä¸å¹¸çš„æ˜¯ï¼Œ Docker é©±åŠ¨ç¨‹åºå¾ˆéš¾æ˜ å°„åˆ°åƒ Kubernetes è¿™æ ·çš„å…¶ä»–æŽ§åˆ¶å¹³é¢ã€‚å…·ä½“æ¥è¯´ï¼Œé©±åŠ¨ç¨‹åºä¸ä¼šè¢«å‘ŠçŸ¥è¿žæŽ¥å®¹å™¨çš„ç½‘ç»œåç§° - åªæ˜¯ Docker å†…éƒ¨åˆ†é…çš„ ID ã€‚è¿™ä½¿å¾—é©±åŠ¨ç¨‹åºå¾ˆéš¾æ˜ å°„å›žå¦ä¸€ä¸ªç³»ç»Ÿä¸­å­˜åœ¨çš„ä»»ä½•ç½‘ç»œæ¦‚å¿µã€‚&lt;/p>
&lt;!-- This and other issues have been brought up to Docker developers by network vendors, and are usually closed as "working as intended" ([libnetwork #139](https://github.com/docker/libnetwork/issues/139), [libnetwork #486](https://github.com/docker/libnetwork/issues/486), [libnetwork #514](https://github.com/docker/libnetwork/pull/514), [libnetwork #865](https://github.com/docker/libnetwork/issues/865), [docker #18864](https://github.com/docker/docker/issues/18864)), even though they make non-Docker third-party systems more difficult to integrate with. Throughout this investigation Docker has made it clear that theyâ€™re not very open to ideas that deviate from their current course or that delegate control. This is very worrisome to us, since Kubernetes complements Docker and adds so much functionality, but exists outside of Docker itself. -->
&lt;p>è¿™ä¸ªé—®é¢˜å’Œå…¶ä»–é—®é¢˜å·²ç”±ç½‘ç»œä¾›åº”å•†æå‡ºç»™ Docker å¼€å‘äººå‘˜ï¼Œå¹¶ä¸”é€šå¸¸å…³é—­ä¸ºâ€œæŒ‰é¢„æœŸå·¥ä½œâ€ï¼Œ(&lt;a href="https://github.com/docker/libnetwork/issues/139">libnetwork #139&lt;/a>, &lt;a href="https://github.com/docker/libnetwork/issues/486">libnetwork #486&lt;/a>, &lt;a href="https://github.com/docker/libnetwork/pull/514">libnetwork #514&lt;/a>, &lt;a href="https://github.com/docker/libnetwork/issues/865">libnetwork #865&lt;/a>, &lt;a href="https://github.com/docker/docker/issues/18864">docker #18864&lt;/a>)ï¼Œå³ä½¿å®ƒä»¬ä½¿éž Docker ç¬¬ä¸‰æ–¹ç³»ç»Ÿæ›´éš¾ä»¥ä¸Žä¹‹é›†æˆã€‚åœ¨æ•´ä¸ªè°ƒæŸ¥è¿‡ç¨‹ä¸­ï¼Œ Docker æ˜Žç¡®è¡¨ç¤ºä»–ä»¬å¯¹åç¦»å½“å‰è·¯çº¿æˆ–å§”æ‰˜æŽ§åˆ¶çš„æƒ³æ³•ä¸å¤ªæ¬¢è¿Žã€‚è¿™å¯¹æˆ‘ä»¬æ¥è¯´éžå¸¸ä»¤äººæ‹…å¿§ï¼Œå› ä¸º Kubernetes è¡¥å……äº† Docker å¹¶å¢žåŠ äº†å¾ˆå¤šåŠŸèƒ½ï¼Œä½†å®ƒå­˜åœ¨äºŽ Docker ä¹‹å¤–ã€‚&lt;/p>
&lt;!-- For all of these reasons we have chosen to invest in CNI as the Kubernetes plugin model. There will be some unfortunate side-effects of this. Most of them are relatively minor (for example, `docker inspect` will not show an IP address), but some are significant. In particular, containers started by `docker run` might not be able to communicate with containers started by Kubernetes, and network integrators will have to provide CNI drivers if they want to fully integrate with Kubernetes. On the other hand, Kubernetes will get simpler and more flexible, and a lot of the ugliness of early bootstrapping (such as configuring Docker to use our bridge) will go away. -->
&lt;p>å‡ºäºŽæ‰€æœ‰è¿™äº›åŽŸå› ï¼Œæˆ‘ä»¬é€‰æ‹©æŠ•èµ„ CNI ä½œä¸º Kubernetes æ’ä»¶æ¨¡åž‹ã€‚è¿™ä¼šæœ‰ä¸€äº›ä¸å¹¸çš„å‰¯ä½œç”¨ã€‚å®ƒä»¬ä¸­çš„å¤§å¤šæ•°éƒ½ç›¸å¯¹è¾ƒå°ï¼ˆä¾‹å¦‚ï¼Œ &lt;code>docker inspect&lt;/code> ä¸ä¼šæ˜¾ç¤º IP åœ°å€ï¼‰ï¼Œä½†æœ‰äº›æ˜¯é‡è¦çš„ã€‚ç‰¹åˆ«æ˜¯ï¼Œç”± &lt;code>docker run&lt;/code> å¯åŠ¨çš„å®¹å™¨å¯èƒ½æ— æ³•ä¸Ž Kubernetes å¯åŠ¨çš„å®¹å™¨é€šä¿¡ï¼Œå¦‚æžœç½‘ç»œé›†æˆå•†æƒ³è¦ä¸Ž Kubernetes å®Œå…¨é›†æˆï¼Œåˆ™å¿…é¡»æä¾› CNI é©±åŠ¨ç¨‹åºã€‚å¦ä¸€æ–¹é¢ï¼Œ Kubernetes å°†å˜å¾—æ›´ç®€å•ï¼Œæ›´çµæ´»ï¼Œæ—©æœŸå¼•å…¥çš„è®¸å¤šä¸‘é™‹çš„ï¼ˆä¾‹å¦‚é…ç½® Docker ä½¿ç”¨æˆ‘ä»¬çš„ç½‘æ¡¥ï¼‰å°†ä¼šæ¶ˆå¤±ã€‚&lt;/p>
&lt;!-- As we proceed down this path, weâ€™ll certainly keep our eyes and ears open for better ways to integrate and simplify. If you have thoughts on how we can do that, we really would like to hear them â€” find us on [slack](http://slack.k8s.io/) or on our [network SIG mailing-list](https://groups.google.com/forum/#!forum/kubernetes-sig-network). -->
&lt;p>å½“æˆ‘ä»¬æ²¿ç€è¿™æ¡é“è·¯å‰è¿›æ—¶ï¼Œæˆ‘ä»¬è‚¯å®šä¼šä¿æŒçœ¼ç›å’Œè€³æœµçš„å¼€æ”¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°æ•´åˆå’Œç®€åŒ–ã€‚å¦‚æžœæ‚¨å¯¹æˆ‘ä»¬å¦‚ä½•åšåˆ°è¿™ä¸€ç‚¹æœ‰æ‰€æƒ³æ³•ï¼Œæˆ‘ä»¬çœŸçš„å¸Œæœ›å¬åˆ°å®ƒä»¬ - åœ¨ &lt;a href="http://slack.k8s.io/">slack&lt;/a> æˆ–è€… &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-network">network SIG mailing-list&lt;/a> æ‰¾åˆ°æˆ‘ä»¬ã€‚&lt;/p>
&lt;p>Tim Hockin, Software Engineer, Google&lt;/p></description></item><item><title>Blog: Simple leader election with Kubernetes and Docker</title><link>https://kubernetes.io/zh/blog/2016/01/11/simple-leader-election-with-kubernetes/</link><pubDate>Mon, 11 Jan 2016 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2016/01/11/simple-leader-election-with-kubernetes/</guid><description>
&lt;p>title: &amp;quot;Kubernetes å’Œ Docker ç®€å•çš„ leader election&amp;quot;
date: 2016-01-11
slug: simple-leader-election-with-kubernetes
url: /blog/2016/01/Simple-Leader-Election-With-Kubernetes&lt;/p>
&lt;!--
Kubernetes simplifies the deployment and operational management of services running on clusters. However, it also simplifies the development of these services. In this post we'll see how you can use Kubernetes to easily perform leader election in your distributed application. Distributed applications usually replicate the tasks of a service for reliability and scalability, but often it is necessary to designate one of the replicas as the leader who is responsible for coordination among all of the replicas.
-->
&lt;p>Kubernetes ç®€åŒ–äº†é›†ç¾¤ä¸Šè¿è¡Œçš„æœåŠ¡çš„éƒ¨ç½²å’Œæ“ä½œç®¡ç†ã€‚ä½†æ˜¯ï¼Œå®ƒä¹Ÿç®€åŒ–äº†è¿™äº›æœåŠ¡çš„å‘å±•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å°†çœ‹åˆ°å¦‚ä½•ä½¿ç”¨ Kubernetes åœ¨åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºä¸­è½»æ¾åœ°æ‰§è¡Œ leader electionã€‚åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºé€šå¸¸ä¸ºäº†å¯é æ€§å’Œå¯ä¼¸ç¼©æ€§è€Œå¤åˆ¶æœåŠ¡çš„ä»»åŠ¡ï¼Œä½†é€šå¸¸éœ€è¦æŒ‡å®šå…¶ä¸­ä¸€ä¸ªå‰¯æœ¬ä½œä¸ºè´Ÿè´£æ‰€æœ‰å‰¯æœ¬ä¹‹é—´åè°ƒçš„è´Ÿè´£äººã€‚&lt;/p>
&lt;!--
Typically in leader election, a set of candidates for becoming leader is identified. These candidates all race to declare themselves the leader. One of the candidates wins and becomes the leader. Once the election is won, the leader continually "heartbeats" to renew their position as the leader, and the other candidates periodically make new attempts to become the leader. This ensures that a new leader is identified quickly, if the current leader fails for some reason.
-->
&lt;p>é€šå¸¸åœ¨ leader election ä¸­ï¼Œä¼šç¡®å®šä¸€ç»„æˆä¸ºé¢†å¯¼è€…çš„å€™é€‰äººã€‚è¿™äº›å€™é€‰äººéƒ½ç«žç›¸å®£å¸ƒè‡ªå·±ä¸ºé¢†è¢–ã€‚å…¶ä¸­ä¸€ä½å€™é€‰äººèŽ·èƒœå¹¶æˆä¸ºé¢†è¢–ã€‚ä¸€æ—¦é€‰ä¸¾èŽ·èƒœï¼Œé¢†å¯¼è€…å°±ä¼šä¸æ–­åœ°â€œä¿¡å·â€ä»¥è¡¨ç¤ºä»–ä»¬ä½œä¸ºé¢†å¯¼è€…çš„åœ°ä½ï¼Œå…¶ä»–å€™é€‰äººä¹Ÿä¼šå®šæœŸåœ°åšå‡ºæ–°çš„å°è¯•æ¥æˆä¸ºé¢†å¯¼è€…ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨å½“å‰é¢†å¯¼å› æŸç§åŽŸå› å¤±è´¥æ—¶ï¼Œå¿«é€Ÿç¡®å®šæ–°é¢†å¯¼ã€‚&lt;/p>
&lt;!--
Implementing leader election usually requires either deploying software such as ZooKeeper, etcd or Consul and using it for consensus, or alternately, implementing a consensus algorithm on your own. We will see below that Kubernetes makes the process of using leader election in your application significantly easier.
#### Implementing leader election in Kubernetes
-->
&lt;p>å®žçŽ° leader election é€šå¸¸éœ€è¦éƒ¨ç½² ZooKeeperã€etcd æˆ– Consul ç­‰è½¯ä»¶å¹¶å°†å…¶ç”¨äºŽåå•†ä¸€è‡´ï¼Œæˆ–è€…ä¹Ÿå¯ä»¥è‡ªå·±å®žçŽ°åå•†ä¸€è‡´ç®—æ³•ã€‚æˆ‘ä»¬å°†åœ¨ä¸‹é¢çœ‹åˆ°ï¼ŒKubernetes ä½¿åœ¨åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ leader election çš„è¿‡ç¨‹å¤§å¤§ç®€åŒ–ã€‚&lt;/p>
&lt;p>####åœ¨ Kubernetes å®žæ–½é¢†å¯¼äººé€‰ä¸¾&lt;/p>
&lt;!--
The first requirement in leader election is the specification of the set of candidates for becoming the leader. Kubernetes already uses _Endpoints_ to represent a replicated set of pods that comprise a service, so we will re-use this same object. (aside: You might have thought that we would use _ReplicationControllers_, but they are tied to a specific binary, and generally you want to have a single leader even if you are in the process of performing a rolling update)
To perform leader election, we use two properties of all Kubernetes API objects:
-->
&lt;p>Leader election çš„é¦–è¦æ¡ä»¶æ˜¯ç¡®å®šå€™é€‰äººçš„äººé€‰ã€‚Kubernetes å·²ç»ä½¿ç”¨ &lt;em>Endpoints&lt;/em> æ¥è¡¨ç¤ºç»„æˆæœåŠ¡çš„ä¸€ç»„å¤åˆ¶ podï¼Œå› æ­¤æˆ‘ä»¬å°†é‡ç”¨è¿™ä¸ªç›¸åŒçš„å¯¹è±¡ã€‚ï¼ˆæ—ç™½ï¼šæ‚¨å¯èƒ½è®¤ä¸ºæˆ‘ä»¬ä¼šä½¿ç”¨ &lt;em>ReplicationControllers&lt;/em>ï¼Œä½†å®ƒä»¬æ˜¯ç»‘å®šåˆ°ç‰¹å®šçš„äºŒè¿›åˆ¶æ–‡ä»¶ï¼Œè€Œä¸”é€šå¸¸æ‚¨å¸Œæœ›åªæœ‰ä¸€ä¸ªé¢†å¯¼è€…ï¼Œå³ä½¿æ‚¨æ­£åœ¨æ‰§è¡Œæ»šåŠ¨æ›´æ–°ï¼‰&lt;/p>
&lt;p>è¦æ‰§è¡Œ leader electionï¼Œæˆ‘ä»¬ä½¿ç”¨æ‰€æœ‰ Kubernetes api å¯¹è±¡çš„ä¸¤ä¸ªå±žæ€§ï¼š&lt;/p>
&lt;!--
* ResourceVersions - Every API object has a unique ResourceVersion, and you can use these versions to perform compare-and-swap on Kubernetes objects
* Annotations - Every API object can be annotated with arbitrary key/value pairs to be used by clients.
Given these primitives, the code to use master election is relatively straightforward, and you can find it [here][1]. Let's run it ourselves.
-->
&lt;ul>
&lt;li>ResourceVersions - æ¯ä¸ª API å¯¹è±¡éƒ½æœ‰ä¸€ä¸ªæƒŸä¸€çš„ ResourceVersionï¼Œæ‚¨å¯ä»¥ä½¿ç”¨è¿™äº›ç‰ˆæœ¬å¯¹ Kubernetes å¯¹è±¡æ‰§è¡Œæ¯”è¾ƒå’Œäº¤æ¢&lt;/li>
&lt;li>Annotations - æ¯ä¸ª API å¯¹è±¡éƒ½å¯ä»¥ç”¨å®¢æˆ·ç«¯ä½¿ç”¨çš„ä»»æ„é”®/å€¼å¯¹è¿›è¡Œæ³¨é‡Šã€‚&lt;/li>
&lt;/ul>
&lt;p>ç»™å®šè¿™äº›åŽŸè¯­ï¼Œä½¿ç”¨ master election çš„ä»£ç ç›¸å¯¹ç®€å•ï¼Œæ‚¨å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°&lt;a href="https://github.com/kubernetes/contrib/pull/353">here&lt;/a>ã€‚æˆ‘ä»¬è‡ªå·±æ¥åšå§ã€‚&lt;/p>
&lt;!--
```
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example
```
This creates a leader election set with 3 replicas:
```
$ kubectl get pods
NAME READY STATUS RESTARTS AGE
leader-elector-inmr1 1/1 Running 0 13s
leader-elector-qkq00 1/1 Running 0 13s
leader-elector-sgwcq 1/1 Running 0 13s
```
-->
&lt;pre>&lt;code>$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example
&lt;/code>&lt;/pre>&lt;p>è¿™å°†åˆ›å»ºä¸€ä¸ªåŒ…å«3ä¸ªå‰¯æœ¬çš„ leader election é›†åˆï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods
NAME READY STATUS RESTARTS AGE
leader-elector-inmr1 1/1 Running 0 13s
leader-elector-qkq00 1/1 Running 0 13s
leader-elector-sgwcq 1/1 Running 0 13s
&lt;/code>&lt;/pre>&lt;!--
To see which pod was chosen as the leader, you can access the logs of one of the pods, substituting one of your own pod's names in place of
```
${pod_name}, (e.g. leader-elector-inmr1 from the above)
$ kubectl logs -f ${name}
leader is (leader-pod-name)
```
â€¦ Alternately, you can inspect the endpoints object directly:
-->
&lt;p>è¦æŸ¥çœ‹å“ªä¸ªpodè¢«é€‰ä¸ºé¢†å¯¼ï¼Œæ‚¨å¯ä»¥è®¿é—®å…¶ä¸­ä¸€ä¸ª pod çš„æ—¥å¿—ï¼Œç”¨æ‚¨è‡ªå·±çš„ä¸€ä¸ª pod çš„åç§°æ›¿æ¢&lt;/p>
&lt;pre>&lt;code>${pod_name}, (e.g. leader-elector-inmr1 from the above)
$ kubectl logs -f ${name}
leader is (leader-pod-name)
&lt;/code>&lt;/pre>&lt;p>â€¦æˆ–è€…ï¼Œå¯ä»¥ç›´æŽ¥æ£€æŸ¥ endpoints å¯¹è±¡ï¼š&lt;/p>
&lt;!--
_'example' is the name of the candidate set from the above kubectl run â€¦ command_
```
$ kubectl get endpoints example -o yaml
```
Now to validate that leader election actually works, in a different terminal, run:
```
$ kubectl delete pods (leader-pod-name)
```
-->
&lt;p>_'example' æ˜¯ä¸Šé¢ kubectl run â€¦ å‘½ä»¤_ä¸­å€™é€‰é›†çš„åç§°&lt;/p>
&lt;pre>&lt;code>$ kubectl get endpoints example -o yaml
&lt;/code>&lt;/pre>&lt;p>çŽ°åœ¨ï¼Œè¦éªŒè¯ leader election æ˜¯å¦å®žé™…æœ‰æ•ˆï¼Œè¯·åœ¨å¦ä¸€ä¸ªç»ˆç«¯è¿è¡Œï¼š&lt;/p>
&lt;pre>&lt;code>$ kubectl delete pods (leader-pod-name)
&lt;/code>&lt;/pre>&lt;!--
This will delete the existing leader. Because the set of pods is being managed by a replication controller, a new pod replaces the one that was deleted, ensuring that the size of the replicated set is still three. Via leader election one of these three pods is selected as the new leader, and you should see the leader failover to a different pod. Because pods in Kubernetes have a _grace period_ before termination, this may take 30-40 seconds.
The leader-election container provides a simple webserver that can serve on any address (e.g. http://localhost:4040). You can test this out by deleting the existing leader election group and creating a new one where you additionally pass in a --http=(host):(port) specification to the leader-elector image. This causes each member of the set to serve information about the leader via a webhook.
-->
&lt;p>è¿™å°†åˆ é™¤çŽ°æœ‰é¢†å¯¼ã€‚ç”±äºŽ pod é›†ç”± replication controller ç®¡ç†ï¼Œå› æ­¤æ–°çš„ pod å°†æ›¿æ¢å·²åˆ é™¤çš„podï¼Œç¡®ä¿å¤åˆ¶é›†çš„å¤§å°ä»ä¸º3ã€‚é€šè¿‡ leader electionï¼Œè¿™ä¸‰ä¸ªpodä¸­çš„ä¸€ä¸ªè¢«é€‰ä¸ºæ–°çš„é¢†å¯¼è€…ï¼Œæ‚¨åº”è¯¥ä¼šçœ‹åˆ°é¢†å¯¼è€…æ•…éšœè½¬ç§»åˆ°å¦ä¸€ä¸ªpodã€‚å› ä¸º Kubernetes çš„åŠèˆ±åœ¨ç»ˆæ­¢å‰æœ‰ä¸€ä¸ª &lt;em>grace period&lt;/em>ï¼Œè¿™å¯èƒ½éœ€è¦30-40ç§’ã€‚&lt;/p>
&lt;p>Leader-election container æä¾›äº†ä¸€ä¸ªç®€å•çš„ web æœåŠ¡å™¨ï¼Œå¯ä»¥æœåŠ¡äºŽä»»ä½•åœ°å€(e.g. http://localhost:4040)ã€‚æ‚¨å¯ä»¥é€šè¿‡åˆ é™¤çŽ°æœ‰çš„ leader election ç»„å¹¶åˆ›å»ºä¸€ä¸ªæ–°çš„ leader elector ç»„æ¥æµ‹è¯•è¿™ä¸€ç‚¹ï¼Œåœ¨è¯¥ç»„ä¸­ï¼Œæ‚¨è¿˜å¯ä»¥å‘ leader elector æ˜ åƒä¼ é€’--http=(host):(port) è§„èŒƒã€‚è¿™å°†å¯¼è‡´é›†åˆä¸­çš„æ¯ä¸ªæˆå‘˜é€šè¿‡ webhook æä¾›æœ‰å…³é¢†å¯¼è€…çš„ä¿¡æ¯ã€‚&lt;/p>
&lt;!--
```
# delete the old leader elector group
$ kubectl delete rc leader-elector
# create the new group, note the --http=localhost:4040 flag
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example --http=0.0.0.0:4040
# create a proxy to your Kubernetes api server
$ kubectl proxy
```
-->
&lt;pre>&lt;code># delete the old leader elector group
$ kubectl delete rc leader-elector
# create the new group, note the --http=localhost:4040 flag
$ kubectl run leader-elector --image=gcr.io/google_containers/leader-elector:0.4 --replicas=3 -- --election=example --http=0.0.0.0:4040
# create a proxy to your Kubernetes api server
$ kubectl proxy
&lt;/code>&lt;/pre>&lt;!--
You can then access:
http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/
And you will see:
```
{"name":"(name-of-leader-here)"}
```
#### Leader election with sidecars
-->
&lt;p>ç„¶åŽæ‚¨å¯ä»¥è®¿é—®ï¼š&lt;/p>
&lt;p>http://localhost:8001/api/v1/proxy/namespaces/default/pods/(leader-pod-name):4040/&lt;/p>
&lt;p>ä½ ä¼šçœ‹åˆ°ï¼š&lt;/p>
&lt;pre>&lt;code>{&amp;quot;name&amp;quot;:&amp;quot;(name-of-leader-here)&amp;quot;}
&lt;/code>&lt;/pre>&lt;h4 id="æœ‰å‰¯æ‰‹çš„-leader-election">æœ‰å‰¯æ‰‹çš„ leader election&lt;/h4>
&lt;!--
Ok, that's great, you can do leader election and find out the leader over HTTP, but how can you use it from your own application? This is where the notion of sidecars come in. In Kubernetes, Pods are made up of one or more containers. Often times, this means that you add sidecar containers to your main application to make up a Pod. (for a much more detailed treatment of this subject see my earlier blog post).
The leader-election container can serve as a sidecar that you can use from your own application. Any container in the Pod that's interested in who the current master is can simply access http://localhost:4040 and they'll get back a simple JSON object that contains the name of the current master. Since all containers in a Pod share the same network namespace, there's no service discovery required!
-->
&lt;p>å¥½å§ï¼Œé‚£å¤ªå¥½äº†ï¼Œä½ å¯ä»¥é€šè¿‡ HTTP è¿›è¡Œleader election å¹¶æ‰¾åˆ° leaderï¼Œä½†æ˜¯ä½ å¦‚ä½•ä»Žè‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨å®ƒå‘¢ï¼Ÿè¿™å°±æ˜¯ sidecar çš„ç”±æ¥ã€‚Kubernetes ä¸­ï¼ŒPods ç”±ä¸€ä¸ªæˆ–å¤šä¸ªå®¹å™¨ç»„æˆã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œè¿™æ„å‘³ç€æ‚¨å°† sidecar containers æ·»åŠ åˆ°ä¸»åº”ç”¨ç¨‹åºä¸­ä»¥ç»„æˆ podã€‚ï¼ˆå…³äºŽè¿™ä¸ªä¸»é¢˜çš„æ›´è¯¦ç»†çš„å¤„ç†ï¼Œè¯·å‚é˜…æˆ‘ä¹‹å‰çš„åšå®¢æ–‡ç« ï¼‰ã€‚
Leader-election container å¯ä»¥ä½œä¸ºä¸€ä¸ª sidecarï¼Œæ‚¨å¯ä»¥ä»Žè‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ã€‚Pod ä¸­ä»»ä½•å¯¹å½“å‰ master æ„Ÿå…´è¶£çš„å®¹å™¨éƒ½å¯ä»¥ç®€å•åœ°è®¿é—®http://localhost:4040ï¼Œå®ƒä»¬å°†è¿”å›žä¸€ä¸ªåŒ…å«å½“å‰ master åç§°çš„ç®€å• json å¯¹è±¡ã€‚ç”±äºŽ podä¸­ çš„æ‰€æœ‰å®¹å™¨å…±äº«ç›¸åŒçš„ç½‘ç»œå‘½åç©ºé—´ï¼Œå› æ­¤ä¸éœ€è¦æœåŠ¡å‘çŽ°ï¼&lt;/p>
&lt;!--
For example, here is a simple Node.js application that connects to the leader election sidecar and prints out whether or not it is currently the master. The leader election sidecar sets its identifier to `hostname` by default.
```
var http = require('http');
// This will hold info about the current master
var master = {};
// The web handler for our nodejs application
var handleRequest = function(request, response) {
response.writeHead(200);
response.end("Master is " + master.name);
};
// A callback that is used for our outgoing client requests to the sidecar
var cb = function(response) {
var data = '';
response.on('data', function(piece) { data = data + piece; });
response.on('end', function() { master = JSON.parse(data); });
};
// Make an async request to the sidecar at http://localhost:4040
var updateMaster = function() {
var req = http.get({host: 'localhost', path: '/', port: 4040}, cb);
req.on('error', function(e) { console.log('problem with request: ' + e.message); });
req.end();
};
/ / Set up regular updates
updateMaster();
setInterval(updateMaster, 5000);
// set up the web server
var www = http.createServer(handleRequest);
www.listen(8080);
```
Of course, you can use this sidecar from any language that you choose that supports HTTP and JSON.
-->
&lt;p>ä¾‹å¦‚ï¼Œè¿™é‡Œæœ‰ä¸€ä¸ªç®€å•çš„ Node.js åº”ç”¨ç¨‹åºï¼Œå®ƒè¿žæŽ¥åˆ° leader election sidecar å¹¶æ‰“å°å‡ºå®ƒå½“å‰æ˜¯å¦æ˜¯ masterã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œleader election sidecar å°†å…¶æ ‡è¯†ç¬¦è®¾ç½®ä¸º &lt;code>hostname&lt;/code>ã€‚&lt;/p>
&lt;pre>&lt;code>var http = require('http');
// This will hold info about the current master
var master = {};
// The web handler for our nodejs application
var handleRequest = function(request, response) {
response.writeHead(200);
response.end(&amp;quot;Master is &amp;quot; + master.name);
};
// A callback that is used for our outgoing client requests to the sidecar
var cb = function(response) {
var data = '';
response.on('data', function(piece) { data = data + piece; });
response.on('end', function() { master = JSON.parse(data); });
};
// Make an async request to the sidecar at http://localhost:4040
var updateMaster = function() {
var req = http.get({host: 'localhost', path: '/', port: 4040}, cb);
req.on('error', function(e) { console.log('problem with request: ' + e.message); });
req.end();
};
/ / Set up regular updates
updateMaster();
setInterval(updateMaster, 5000);
// set up the web server
var www = http.createServer(handleRequest);
www.listen(8080);
&lt;/code>&lt;/pre>&lt;p>å½“ç„¶ï¼Œæ‚¨å¯ä»¥ä»Žä»»ä½•æ”¯æŒ HTTP å’Œ JSON çš„è¯­è¨€ä¸­ä½¿ç”¨è¿™ä¸ª sidecarã€‚&lt;/p>
&lt;!--
#### Conclusion
Hopefully I've shown you how easy it is to build leader election for your distributed application using Kubernetes. In future installments we'll show you how Kubernetes is making building distributed systems even easier. In the meantime, head over to [Google Container Engine][2] or [kubernetes.io][3] to get started with Kubernetes.
[1]: https://github.com/kubernetes/contrib/pull/353
[2]: https://cloud.google.com/container-engine/
[3]: http://kubernetes.io/
-->
&lt;h4 id="ç»“è®º">ç»“è®º&lt;/h4>
&lt;p>å¸Œæœ›æˆ‘å·²ç»å‘æ‚¨å±•ç¤ºäº†ä½¿ç”¨ Kubernetes ä¸ºæ‚¨çš„åˆ†å¸ƒå¼åº”ç”¨ç¨‹åºæž„å»º leader election æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚åœ¨ä»¥åŽçš„éƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬å°†å‘æ‚¨å±•ç¤º Kubernetes å¦‚ä½•ä½¿æž„å»ºåˆ†å¸ƒå¼ç³»ç»Ÿå˜å¾—æ›´åŠ å®¹æ˜“ã€‚åŒæ—¶ï¼Œè½¬åˆ°&lt;a href="https://cloud.google.com/container-engine/">Google Container Engine&lt;/a>æˆ–&lt;a href="http://kubernetes.io/">kubernetes.io&lt;/a>å¼€å§‹ä½¿ç”¨Kubernetesã€‚&lt;/p></description></item><item><title>Blog: ä½¿ç”¨ Puppet ç®¡ç† Kubernetes Podsï¼ŒServices å’Œ Replication Controllers</title><link>https://kubernetes.io/zh/blog/2015/12/17/managing-kubernetes-pods-services-and-replication-controllers-with-puppet/</link><pubDate>Thu, 17 Dec 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2015/12/17/managing-kubernetes-pods-services-and-replication-controllers-with-puppet/</guid><description>
&lt;!--
---
title: " Managing Kubernetes Pods, Services and Replication Controllers with Puppet "
date: 2015-12-17
slug: managing-kubernetes-pods-services-and-replication-controllers-with-puppet
url: /blog/2015/12/Managing-Kubernetes-Pods-Services-And-Replication-Controllers-With-Puppet
---
-->
&lt;!--
_Todayâ€™s guest post is written by Gareth Rushgrove, Senior Software Engineer at Puppet Labs, a leader in IT automation. Gareth tells us about a new Puppet module that helps manage resources in Kubernetes.&amp;nbsp;_
People familiar with [Puppet](https://github.com/puppetlabs/puppet)&amp;nbsp;might have used it for managing files, packages and users on host computers. But Puppet is first and foremost a configuration management tool, and config management is a much broader discipline than just managing host-level resources. A good definition of configuration management is that it aims to solve four related problems: identification, control, status accounting and verification and audit. These problems exist in the operation of any complex system, and with the new [Puppet Kubernetes module](https://forge.puppetlabs.com/garethr/kubernetes)&amp;nbsp;weâ€™re starting to look at how we can solve those problems for Kubernetes.
-->
&lt;p>&lt;em>ä»Šå¤©çš„å˜‰å®¾å¸–å­æ˜¯ç”± IT è‡ªåŠ¨åŒ–é¢†åŸŸçš„é¢†å¯¼è€… Puppet Labs çš„é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆ Gareth Rushgrove æ’°å†™çš„ã€‚Garethå‘Šè¯‰æˆ‘ä»¬ä¸€ä¸ªæ–°çš„ Puppet æ¨¡å—ï¼Œå®ƒå¸®åŠ©ç®¡ç† Kubernetes ä¸­çš„èµ„æºã€‚&lt;/em>&lt;/p>
&lt;p>ç†Ÿæ‚‰[Puppet]çš„äºº(&lt;a href="https://github.com/puppetlabs/puppet">https://github.com/puppetlabs/puppet&lt;/a>)å¯èƒ½ä½¿ç”¨å®ƒæ¥ç®¡ç†ä¸»æœºä¸Šçš„æ–‡ä»¶ã€åŒ…å’Œç”¨æˆ·ã€‚ä½†æ˜¯Puppeté¦–å…ˆæ˜¯ä¸€ä¸ªé…ç½®ç®¡ç†å·¥å…·ï¼Œé…ç½®ç®¡ç†æ˜¯ä¸€ä¸ªæ¯”ç®¡ç†ä¸»æœºçº§èµ„æºæ›´å¹¿æ³›çš„è§„ç¨‹ã€‚é…ç½®ç®¡ç†çš„ä¸€ä¸ªå¾ˆå¥½çš„å®šä¹‰æ˜¯å®ƒæ—¨åœ¨è§£å†³å››ä¸ªç›¸å…³çš„é—®é¢˜ï¼šæ ‡è¯†ã€æŽ§åˆ¶ã€çŠ¶æ€æ ¸ç®—å’ŒéªŒè¯å®¡è®¡ã€‚è¿™äº›é—®é¢˜å­˜åœ¨äºŽä»»ä½•å¤æ‚ç³»ç»Ÿçš„æ“ä½œä¸­ï¼Œå¹¶ä¸”æœ‰äº†æ–°çš„&lt;a href="https://forge.puppetlabs.com/garethr/kubernetes">Puppet Kubernetes module&lt;/a>ï¼Œæˆ‘ä»¬å¼€å§‹ç ”ç©¶å¦‚ä½•ä¸º Kubernetes è§£å†³è¿™äº›é—®é¢˜ã€‚&lt;/p>
&lt;!--
### The Puppet Kubernetes Module
The Puppet Kubernetes module currently assumes you already have a Kubernetes cluster [up and running](http://kubernetes.io/gettingstarted/).&amp;nbsp;Its focus is on managing the resources in Kubernetes, like Pods, Replication Controllers and Services, not (yet) on managing the underlying kubelet or etcd services. Hereâ€™s a quick snippet of code describing a Pod in Puppetâ€™s DSL.
-->
&lt;h3 id="puppet-kubernetes-æ¨¡å—">Puppet Kubernetes æ¨¡å—&lt;/h3>
&lt;p>Puppet kubernetes æ¨¡å—ç›®å‰å‡è®¾æ‚¨å·²ç»æœ‰ä¸€ä¸ª kubernetes é›†ç¾¤ [å¯åŠ¨å¹¶è¿è¡Œ]](&lt;a href="http://kubernetes.io/gettingstarted/">http://kubernetes.io/gettingstarted/&lt;/a>)ã€‚å®ƒçš„é‡ç‚¹æ˜¯ç®¡ç† Kubernetesä¸­çš„èµ„æºï¼Œå¦‚ Podsã€Replication Controllers å’Œ Servicesï¼Œè€Œä¸æ˜¯ï¼ˆçŽ°åœ¨ï¼‰ç®¡ç†åº•å±‚çš„ kubelet æˆ– etcd servicesã€‚ä¸‹é¢æ˜¯æè¿° Puppetâ€™s DSL ä¸­ä¸€ä¸ª Pod çš„ç®€çŸ­ä»£ç ç‰‡æ®µã€‚&lt;/p>
&lt;!--
```
kubernetes_pod { 'sample-pod':
ensure => present,
metadata => {
namespace => 'default',
},
spec => {
containers => [{
name => 'container-name',
image => 'nginx',
}]
},
```
}
-->
&lt;pre>&lt;code>kubernetes_pod { 'sample-pod':
ensure =&amp;gt; present,
metadata =&amp;gt; {
namespace =&amp;gt; 'default',
},
spec =&amp;gt; {
containers =&amp;gt; [{
name =&amp;gt; 'container-name',
image =&amp;gt; 'nginx',
}]
},
}
&lt;/code>&lt;/pre>&lt;!--
If youâ€™re familiar with the YAML file format, youâ€™ll probably recognise the structure immediately. The interface is intentionally identical to aid conversion between different formats â€” in fact, the code powering this is autogenerated from the Kubernetes API Swagger definitions. Running the above code, assuming we save it as pod.pp, is as simple as:
```
puppet apply pod.pp
```
-->
&lt;p>å¦‚æžœæ‚¨ç†Ÿæ‚‰ YAML æ–‡ä»¶æ ¼å¼ï¼Œæ‚¨å¯èƒ½ä¼šç«‹å³è¯†åˆ«è¯¥ç»“æž„ã€‚ è¯¥æŽ¥å£æ•…æ„é‡‡å–ç›¸åŒçš„æ ¼å¼ä»¥å¸®åŠ©åœ¨ä¸åŒæ ¼å¼ä¹‹é—´è¿›è¡Œè½¬æ¢ â€” äº‹å®žä¸Šï¼Œä¸ºæ­¤æä¾›æ”¯æŒçš„ä»£ç æ˜¯ä»ŽKubernetes API Swaggerè‡ªåŠ¨ç”Ÿæˆçš„ã€‚ è¿è¡Œä¸Šé¢çš„ä»£ç ï¼Œå‡è®¾æˆ‘ä»¬å°†å…¶ä¿å­˜ä¸º pod.ppï¼Œå°±åƒä¸‹é¢è¿™æ ·ç®€å•ï¼š&lt;/p>
&lt;pre>&lt;code>puppet apply pod.pp
&lt;/code>&lt;/pre>&lt;!--
Authentication uses the standard kubectl configuration file. You can find complete [installation instructions in the module's README](https://github.com/garethr/garethr-kubernetes/blob/master/README.md).
Kubernetes has several resources, from Pods and Services to Replication Controllers and Service Accounts. You can see an example of the module managing these resources in the [Kubernetes guestbook sample in Puppet](https://puppetlabs.com/blog/kubernetes-guestbook-example-puppet)&amp;nbsp;post. This demonstrates converting the canonical hello-world example to use Puppet code. -->
&lt;p>èº«ä»½éªŒè¯ä½¿ç”¨æ ‡å‡†çš„ kubectl é…ç½®æ–‡ä»¶ã€‚æ‚¨å¯ä»¥åœ¨æ¨¡å—çš„è‡ªè¿°æ–‡ä»¶ä¸­æ‰¾åˆ°å®Œæ•´çš„&lt;a href="https://github.com/garethr/garethr-kubernetes/blob/master/README.md">README&lt;/a>ã€‚&lt;/p>
&lt;p>Kubernetes æœ‰å¾ˆå¤šèµ„æºï¼Œæ¥è‡ª Podsã€ Servicesã€ Replication Controllers å’Œ Service Accountsã€‚æ‚¨å¯ä»¥åœ¨&lt;a href="https://puppetlabs.com/blog/kubernetes-guestbook-example-puppet">Puppet ä¸­çš„ kubernetes ç•™è¨€ç°¿ç¤ºä¾‹&lt;/a>æ–‡ç« ä¸­çœ‹åˆ°ç®¡ç†è¿™äº›èµ„æºçš„æ¨¡å—ç¤ºä¾‹ã€‚è¿™æ¼”ç¤ºäº†å¦‚ä½•å°†è§„èŒƒçš„ hello-world ç¤ºä¾‹è½¬æ¢ä¸ºä½¿ç”¨ Puppetä»£ç ã€‚&lt;/p>
&lt;!--
One of the main advantages of using Puppet for this, however, is that you can create your own higher-level and more business-specific interfaces to Kubernetes-managed applications. For instance, for the guestbook, you could create something like the following:
```
guestbook { 'myguestbook':
redis_slave_replicas => 2,
frontend_replicas => 3,
redis_master_image => 'redis',
redis_slave_image => 'gcr.io/google_samples/gb-redisslave:v1',
frontend_image => 'gcr.io/google_samples/gb-frontend:v3',
}
```
-->
&lt;p>ç„¶è€Œï¼Œä½¿ç”¨ Puppet çš„ä¸€ä¸ªä¸»è¦ä¼˜ç‚¹æ˜¯ï¼Œæ‚¨å¯ä»¥åˆ›å»ºè‡ªå·±çš„æ›´é«˜çº§åˆ«å’Œæ›´ç‰¹å®šäºŽä¸šåŠ¡çš„æŽ¥å£ï¼Œä»¥è¿žæŽ¥ kubernetes ç®¡ç†çš„åº”ç”¨ç¨‹åºã€‚ä¾‹å¦‚ï¼Œå¯¹äºŽç•™è¨€ç°¿ï¼Œå¯ä»¥åˆ›å»ºå¦‚ä¸‹å†…å®¹ï¼š&lt;/p>
&lt;pre>&lt;code>guestbook { 'myguestbook':
redis_slave_replicas =&amp;gt; 2,
frontend_replicas =&amp;gt; 3,
redis_master_image =&amp;gt; 'redis',
redis_slave_image =&amp;gt; 'gcr.io/google_samples/gb-redisslave:v1',
frontend_image =&amp;gt; 'gcr.io/google_samples/gb-frontend:v3',
}
&lt;/code>&lt;/pre>&lt;!--
You can read more about using Puppetâ€™s defined types, and see lots more code examples, in the Puppet blog post, [Building Your Own Abstractions for Kubernetes in Puppet](https://puppetlabs.com/blog/building-your-own-abstractions-kubernetes-puppet).
### Conclusions
The advantages of using Puppet rather than just the standard YAML files and kubectl are:
-->
&lt;p>æ‚¨å¯ä»¥åœ¨Puppetåšå®¢æ–‡ç« &lt;a href="https://puppetlabs.com/blog/building-your-own-abstractions-kubernetes-puppet">åœ¨ Puppet ä¸­ä¸º Kubernetes æž„å»ºè‡ªå·±çš„æŠ½è±¡&lt;/a>ä¸­é˜…è¯»æ›´å¤šå…³äºŽä½¿ç”¨ Puppet å®šä¹‰çš„ç±»åž‹çš„ä¿¡æ¯ï¼Œå¹¶çœ‹åˆ°æ›´å¤šçš„ä»£ç ç¤ºä¾‹ã€‚&lt;/p>
&lt;h3 id="ç»“è®º">ç»“è®º&lt;/h3>
&lt;p>ä½¿ç”¨ Puppet è€Œä¸ä»…ä»…æ˜¯ä½¿ç”¨æ ‡å‡†çš„ YAML æ–‡ä»¶å’Œ kubectl çš„ä¼˜ç‚¹æ˜¯ï¼š&lt;/p>
&lt;!--
- The ability to create your own abstractions to cut down on repetition and craft higher-level user interfaces, like the guestbook example above.&amp;nbsp;
- Use of Puppetâ€™s development tools for validating code and for writing unit tests.&amp;nbsp;
- Integration with other tools such as Puppet Server, for ensuring that your model in code matches the state of your cluster, and with PuppetDB for storing reports and tracking changes.
- The ability to run the same code repeatedly against the Kubernetes API, to detect any changes or remediate configuration drift.&amp;nbsp;
-->
&lt;ul>
&lt;li>èƒ½å¤Ÿåˆ›å»ºè‡ªå·±çš„æŠ½è±¡ï¼Œä»¥å‡å°‘é‡å¤å’Œè®¾è®¡æ›´é«˜çº§åˆ«çš„ç”¨æˆ·ç•Œé¢ï¼Œå¦‚ä¸Šé¢çš„ç•™è¨€ç°¿ç¤ºä¾‹ã€‚&lt;/li>
&lt;li>ä½¿ç”¨ Puppet çš„å¼€å‘å·¥å…·éªŒè¯ä»£ç å’Œç¼–å†™å•å…ƒæµ‹è¯•ã€‚&lt;/li>
&lt;li>ä¸Ž Puppet Server ç­‰å…¶ä»–å·¥å…·é…åˆï¼Œä»¥ç¡®ä¿ä»£ç ä¸­çš„æ¨¡åž‹ä¸Žé›†ç¾¤çš„çŠ¶æ€åŒ¹é…ï¼Œå¹¶ä¸Ž PuppetDB é…åˆå·¥ä½œï¼Œä»¥å­˜å‚¨æŠ¥å‘Šå’Œè·Ÿè¸ªæ›´æ”¹ã€‚&lt;/li>
&lt;li>èƒ½å¤Ÿé’ˆå¯¹ Kubernetes API é‡å¤è¿è¡Œç›¸åŒçš„ä»£ç ï¼Œä»¥æ£€æµ‹ä»»ä½•æ›´æ”¹æˆ–ä¿®æ­£é…ç½®ã€‚&lt;/li>
&lt;/ul>
&lt;!--
Itâ€™s also worth noting that most large organisations will have very heterogenous environments, running a wide range of software and operating systems. Having a single toolchain that unifies those discrete systems can make adopting new technology like Kubernetes much easier.
-->
&lt;p>å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå¤§å¤šæ•°å¤§åž‹ç»„ç»‡éƒ½å°†æ‹¥æœ‰éžå¸¸å¼‚æž„çš„çŽ¯å¢ƒï¼Œè¿è¡Œå„ç§å„æ ·çš„è½¯ä»¶å’Œæ“ä½œç³»ç»Ÿã€‚æ‹¥æœ‰ç»Ÿä¸€è¿™äº›ç¦»æ•£ç³»ç»Ÿçš„å•ä¸€å·¥å…·é“¾å¯ä»¥ä½¿é‡‡ç”¨ Kubernetes ç­‰æ–°æŠ€æœ¯å˜å¾—æ›´åŠ å®¹æ˜“ã€‚&lt;/p>
&lt;!--
Itâ€™s safe to say that Kubernetes provides an excellent set of primitives on which to build cloud-native systems. And with Puppet, you can address some of the operational and configuration management issues that come with running any complex system in production. [Let us know](mailto:gareth@puppetlabs.com)&amp;nbsp;what you think if you try the module out, and what else youâ€™d like to see supported in the future.
&amp;nbsp;-&amp;nbsp;Gareth Rushgrove, Senior Software Engineer, Puppet Labs
-->
&lt;p>å¯ä»¥è‚¯å®šåœ°è¯´ï¼ŒKubernetesæä¾›äº†ä¸€ç»„ä¼˜ç§€çš„ç»„ä»¶æ¥æž„å»ºäº‘åŽŸç”Ÿç³»ç»Ÿã€‚ä½¿ç”¨ Puppetï¼Œæ‚¨å¯ä»¥è§£å†³åœ¨ç”Ÿäº§ä¸­è¿è¡Œä»»ä½•å¤æ‚ç³»ç»Ÿæ‰€å¸¦æ¥çš„ä¸€äº›æ“ä½œå’Œé…ç½®ç®¡ç†é—®é¢˜ã€‚&lt;a href="mailto:gareth@puppetlabs.com">å‘Šè¯‰æˆ‘ä»¬&lt;/a>å¦‚æžœæ‚¨è¯•ç”¨äº†è¯¥æ¨¡å—ï¼Œæ‚¨ä¼šæœ‰ä»€ä¹ˆæƒ³æ³•ï¼Œä»¥åŠæ‚¨å¸Œæœ›åœ¨å°†æ¥çœ‹åˆ°å“ªäº›æ”¯æŒã€‚&lt;/p>
&lt;p>Gareth Rushgroveï¼ŒPuppet Labs é«˜çº§è½¯ä»¶å·¥ç¨‹å¸ˆ&lt;/p></description></item><item><title>Blog: Weekly Kubernetes Community Hangout Notes - July 31 2015</title><link>https://kubernetes.io/blog/2015/08/weekly-kubernetes-community-hangout/</link><pubDate>Tue, 04 Aug 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2015/08/weekly-kubernetes-community-hangout/</guid><description>
&lt;hr>
&lt;h2 id="url-blog-2015-08-weekly-kubernetes-community-hangout">title: &amp;quot; Kubernetesç¤¾åŒºæ¯å‘¨çŽ¯èŠç¬”è®°-2015å¹´7æœˆ31æ—¥ &amp;quot;
date: 2015-08-04
slug: weekly-kubernetes-community-hangout
url: /blog/2015/08/Weekly-Kubernetes-Community-Hangout&lt;/h2>
&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.
Here are the notes from today's meeting:
-->
&lt;p>æ¯å‘¨ï¼ŒKubernetes è´¡çŒ®ç¤¾åŒºéƒ½ä¼šé€šè¿‡Google çŽ¯èŠè™šæ‹Ÿå¼€ä¼šã€‚æˆ‘ä»¬å¸Œæœ›ä»»ä½•æœ‰å…´è¶£çš„äººéƒ½çŸ¥é“æœ¬è®ºå›è®¨è®ºçš„å†…å®¹ã€‚&lt;/p>
&lt;p>è¿™æ˜¯ä»Šå¤©ä¼šè®®çš„ç¬”è®°ï¼š&lt;/p>
&lt;!--
* Private Registry Demo - Muhammed
* Run docker-registry as an RC/Pod/Service
* Run a proxy on every node
* Access as localhost:5000
* Discussion:
* Should we back it by GCS or S3 when possible?
* Run real registry backed by $object_store on each node
* DNS instead of localhost?
* disassemble image strings?
* more like DNS policy?
-->
&lt;ul>
&lt;li>
&lt;p>ç§æœ‰é•œåƒä»“åº“æ¼”ç¤º - Muhammed&lt;/p>
&lt;ul>
&lt;li>
&lt;p>å°† docker-registry ä½œä¸º RC/Pod/Service è¿è¡Œ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œä»£ç†&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä»¥ localhost:5000 è®¿é—®&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è®¨è®ºï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æˆ‘ä»¬åº”è¯¥åœ¨å¯èƒ½çš„æƒ…å†µä¸‹é€šè¿‡ GCS æˆ– S3 æ”¯æŒå®ƒå—ï¼Ÿ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>åœ¨æ¯ä¸ªèŠ‚ç‚¹ä¸Šè¿è¡Œç”± $object_store æ”¯æŒçš„çœŸå®žé•œåƒä»“åº“&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DNS ä»£æ›¿ localhostï¼Ÿ&lt;/p>
&lt;ul>
&lt;li>
&lt;p>åˆ†è§£ docker é•œåƒå­—ç¬¦ä¸²ï¼Ÿ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ›´åƒ DNS ç­–ç•¥å—ï¼Ÿ&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Running Large Clusters - Joe
* Samsung keen to see large scale O(1000)
* Starting on AWS
* RH also interested - test plan needed
* Plan for next week: discuss working-groups
* If you are interested in joining conversation on cluster scalability send mail to [joe@0xBEDA.com][4]
-->
&lt;ul>
&lt;li>
&lt;p>è¿è¡Œå¤§åž‹é›†ç¾¤ - Joe&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ä¸‰æ˜Ÿæ¸´æœ›çœ‹åˆ°å¤§è§„æ¨¡ O(1000)&lt;/p>
&lt;ul>
&lt;li>ä»Ž AWS å¼€å§‹&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>RH ä¹Ÿæœ‰å…´è¶£ - éœ€è¦æµ‹è¯•è®¡åˆ’&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è®¡åˆ’ä¸‹å‘¨ï¼šè®¨è®ºå·¥ä½œç»„&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å¦‚æžœæ‚¨æœ‰å…´è¶£åŠ å…¥æœ‰å…³é›†ç¾¤å¯æ‰©å±•æ€§çš„å¯¹è¯ï¼Œè¯·å‘é€é‚®ä»¶è‡³[joe@0xBEDA.com][4]&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Resource API Proposal - Clayton
* New stuff wants more info on resources
* Proposal for resources API - ask apiserver for info on pods
* Send feedback to: #11951
* Discussion on snapshot vs time-series vs aggregates
-->
&lt;ul>
&lt;li>
&lt;p>èµ„æº API ææ¡ˆ - Clayton&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æ–°ä¸œè¥¿éœ€è¦æ›´å¤šèµ„æºä¿¡æ¯&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å…³äºŽèµ„æº API çš„ææ¡ˆ - å‘ apiserver è¯¢é—®æœ‰å…³podçš„ä¿¡æ¯&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å‘é€åé¦ˆè‡³ï¼š#11951&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å…³äºŽå¿«ç…§ï¼Œæ—¶é—´åºåˆ—å’Œèšåˆçš„è®¨è®º&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Containerized kubelet - Clayton
* Open pull
* Docker mount propagation - RH carries patches
* Big issues around whole bootstrap of the system
* dual: boot-docker/system-docker
* Kube-in-docker is really nice, but maybe not critical
* Do the small stuff to make progress
* Keep pressure on docker
-->
&lt;ul>
&lt;li>
&lt;p>å®¹å™¨åŒ– kubelet - Clayton&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æ‰“å¼€ pull&lt;/p>
&lt;/li>
&lt;li>
&lt;p>DockeræŒ‚è½½ä¼ æ’­ - RH å¸¦æœ‰è¡¥ä¸&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æœ‰å…³æ•´ä¸ªç³»ç»Ÿå¼•å¯¼ç¨‹åºçš„å¤§é—®é¢˜&lt;/p>
&lt;ul>
&lt;li>åŒï¼šå¼•å¯¼docker /ç³»ç»Ÿdocker&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>Kube-in-dockeréžå¸¸å¥½ï¼Œä½†å¯èƒ½å¹¶ä¸å…³é”®&lt;/p>
&lt;ul>
&lt;li>
&lt;p>åšäº›å°äº‹ä»¥å–å¾—è¿›æ­¥&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å¯¹ docker æ–½åŠ åŽ‹åŠ›&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Web UI (preilly)
* Where does web UI stand?
* OK to split it back out
* Use it as a container image
* Build image as part of kube release process
* Vendor it back in? Maybe, maybe not.
* Will DNS be split out?
* Probably more tightly integrated, instead
* Other potential spin-outs:
* apiserver
* clients
-->
&lt;ul>
&lt;li>
&lt;p>Web UIï¼ˆpreillyï¼‰&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Web UI æ”¾åœ¨å“ªé‡Œï¼Ÿ&lt;/p>
&lt;ul>
&lt;li>
&lt;p>ç¡®å®šå°†å…¶æ‹†åˆ†å‡ºåŽ»&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å°†å…¶ç”¨ä½œå®¹å™¨é•œåƒ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ä½œä¸º kube å‘å¸ƒè¿‡ç¨‹çš„ä¸€éƒ¨åˆ†æž„å»ºæ˜ åƒ&lt;/p>
&lt;/li>
&lt;li>
&lt;p>vendorå›žæ¥äº†å—ï¼Ÿä¹Ÿè®¸å§ï¼Œä¹Ÿè®¸ä¸æ˜¯ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>DNSå°†è¢«æ‹†åˆ†å—ï¼Ÿ&lt;/p>
&lt;ul>
&lt;li>å¯èƒ½æ›´ç´§å¯†åœ°é›†æˆåœ¨ä¸€èµ·ï¼Œè€Œä¸æ˜¯&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>å…¶ä»–æ½œåœ¨çš„è¡ç”Ÿäº§å“ï¼š&lt;/p>
&lt;ul>
&lt;li>
&lt;p>apiserver&lt;/p>
&lt;/li>
&lt;li>
&lt;p>clients&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: å®£å¸ƒé¦–ä¸ªKubernetesä¼ä¸šåŸ¹è®­è¯¾ç¨‹</title><link>https://kubernetes.io/zh/blog/2015/07/08/announcing-first-kubernetes-enterprise/</link><pubDate>Wed, 08 Jul 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2015/07/08/announcing-first-kubernetes-enterprise/</guid><description>
&lt;!-- ---
title: " Announcing the First Kubernetes Enterprise Training Course "
date: 2015-07-08
slug: announcing-first-kubernetes-enterprise
url: /blog/2015/07/Announcing-First-Kubernetes-Enterprise
--- -->
&lt;!-- At Google we rely on Linux application containers to run our core infrastructure. Everything from Search to Gmail runs in containers. &amp;nbsp;In fact, we like containers so much that even our Google Compute Engine VMs run in containers! &amp;nbsp;Because containers are critical to our business, we have been working with the community on many of the basic container technologies (from cgroups to Dockerâ€™s LibContainer) and even decided to build the next generation of Googleâ€™s container scheduling technology, Kubernetes, in the open. -->
&lt;p>åœ¨è°·æ­Œï¼Œæˆ‘ä»¬ä¾èµ– Linux å®¹å™¨åº”ç”¨ç¨‹åºåŽ»è¿è¡Œæˆ‘ä»¬çš„æ ¸å¿ƒåŸºç¡€æž¶æž„ã€‚æ‰€æœ‰æœåŠ¡ï¼Œä»Žæœç´¢å¼•æ“Žåˆ°GmailæœåŠ¡ï¼Œéƒ½è¿è¡Œåœ¨å®¹å™¨ä¸­ã€‚äº‹å®žä¸Šï¼Œæˆ‘ä»¬éžå¸¸å–œæ¬¢å®¹å™¨ï¼Œç”šè‡³æˆ‘ä»¬çš„è°·æ­Œäº‘è®¡ç®—å¼•æ“Žè™šæ‹Ÿæœºä¹Ÿè¿è¡Œåœ¨å®¹å™¨ä¸Šï¼ç”±äºŽå®¹å™¨å¯¹äºŽæˆ‘ä»¬çš„ä¸šåŠ¡è‡³å…³é‡è¦ï¼Œæˆ‘ä»¬å·²ç»ä¸Žç¤¾åŒºåˆä½œå¼€å‘è®¸å¤šåŸºæœ¬çš„å®¹å™¨æŠ€æœ¯ï¼ˆä»Ž cgroups åˆ° Docker çš„ LibContainerï¼‰,ç”šè‡³å†³å®šåŽ»æž„å»ºè°·æ­Œçš„ä¸‹ä¸€ä»£å¼€æºå®¹å™¨è°ƒåº¦æŠ€æœ¯ï¼ŒKubernetesã€‚&lt;/p>
&lt;!-- One year into the Kubernetes project, and on the eve of our planned V1 release at OSCON, we are pleased to announce the first-ever formal Kubernetes enterprise-focused training session organized by a key Kubernetes contributor, Mesosphere. The inaugural session will be taught by Zed Shaw and Michael Hausenblas from Mesosphere, and will take place on July 20 at OSCON in Portland. [Pre-registration](https://mesosphere.com/training/kubernetes/) is free for early registrants, but space is limited so act soon! -->
&lt;p>åœ¨ Kubernetes é¡¹ç›®è¿›è¡Œä¸€å¹´åŽï¼Œåœ¨ OSCON ä¸Šå‘å¸ƒ V1 ç‰ˆæœ¬çš„å‰å¤•ï¼Œæˆ‘ä»¬å¾ˆé«˜å…´çš„å®£å¸ƒKubernetes çš„ä¸»è¦è´¡çŒ®è€… Mesosphere ç»„ç»‡äº†æœ‰å²ä»¥æ¥ç¬¬ä¸€æ¬¡æ­£è§„çš„ä»¥ä¼ä¸šä¸ºä¸­å¿ƒçš„ Kubernetes åŸ¹è®­ä¼šè®®ã€‚é¦–å±Šä¼šè®®å°†äºŽ 6 æœˆ 20 æ—¥åœ¨æ³¢ç‰¹å…°çš„ OSCON ä¸¾åŠžï¼Œç”±æ¥è‡ª Mesosphere çš„ Zed Shaw å’Œ Michael Hausenblas æ¼”è®²ã€‚&lt;a href="https://mesosphere.com/training/kubernetes/">Pre-registration&lt;/a> å¯¹äºŽä¼˜å…ˆæ³¨å†Œè€…æ˜¯å…è´¹çš„ï¼Œä½†åé¢æœ‰é™ï¼Œç«‹åˆ»è¡ŒåŠ¨å§ï¼&lt;/p>
&lt;!-- This one-day course will cover the basics of building and deploying containerized applications using Kubernetes. It will walk attendees through the end-to-end process of creating a Kubernetes application architecture, building and configuring Docker images, and deploying them on a Kubernetes cluster. Users will also learn the fundamentals of deploying Kubernetes applications and services on our Google Container Engine and Mesosphereâ€™s Datacenter Operating System. -->
&lt;p>è¿™ä¸ªä¸ºæœŸä¸€å¤©çš„è¯¾ç¨‹å°†åŒ…æ¶µä½¿ç”¨ Kubernetes æž„å»ºå’Œéƒ¨ç½²å®¹å™¨åŒ–åº”ç”¨ç¨‹åºçš„åŸºç¡€çŸ¥è¯†ã€‚å®ƒå°†é€šè¿‡å®Œæ•´çš„æµç¨‹å¼•å¯¼ä¸Žå‚ä¼šè€…åˆ›å»ºä¸€ä¸ª Kubernetes çš„åº”ç”¨ç¨‹åºä½“ç³»ç»“æž„ï¼Œåˆ›å»ºå’Œé…ç½® Docker é•œåƒï¼Œå¹¶æŠŠå®ƒä»¬éƒ¨ç½²åˆ° Kubernetes é›†ç¾¤ä¸Šã€‚ç”¨æˆ·è¿˜å°†äº†è§£åœ¨æˆ‘ä»¬çš„è°·æ­Œå®¹å™¨å¼•æ“Žå’Œ Mesosphere çš„æ•°æ®ä¸­å¿ƒæ“ä½œç³»ç»Ÿä¸Šéƒ¨ç½² Kubernetes åº”ç”¨ç¨‹åºå’ŒæœåŠ¡çš„åŸºç¡€çŸ¥è¯†ã€‚&lt;/p>
&lt;!-- The upcoming Kubernetes bootcamp will be a great way to learn how to apply Kubernetes to solve long-standing deployment and application management problems. &amp;nbsp;This is just the first of what we hope are many, and from a broad set of contributors. -->
&lt;p>å³å°†æŽ¨å‡ºçš„ Kubernetes bootcamp å°†æ˜¯å­¦ä¹ å¦‚ä½•åº”ç”¨ Kubernetes è§£å†³é•¿æœŸéƒ¨ç½²å’Œåº”ç”¨ç¨‹åºç®¡ç†é—®é¢˜çš„ä¸€ä¸ªå¥½é€”å¾„ã€‚ç›¸å¯¹äºŽæˆ‘ä»¬æ‰€é¢„æœŸçš„ï¼Œæ¥è‡ªäºŽå¹¿æ³›ç¤¾åŒºçš„ä¼—å¤šåŸ¹è®­é¡¹ç›®è€Œè¨€ï¼Œè¿™åªæ˜¯å…¶ä¸­ä¸€ä¸ªã€‚&lt;/p></description></item><item><title>Blog: å¹»ç¯ç‰‡ï¼šKubernetes é›†ç¾¤ç®¡ç†ï¼Œçˆ±ä¸å ¡å¤§å­¦æ¼”è®²</title><link>https://kubernetes.io/zh/blog/2015/06/26/slides-cluster-management-with/</link><pubDate>Fri, 26 Jun 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2015/06/26/slides-cluster-management-with/</guid><description>
&lt;!--
---
title: " Slides: Cluster Management with Kubernetes, talk given at the University of Edinburgh "
date: 2015-06-26
slug: slides-cluster-management-with
url: /blog/2015/06/Slides-Cluster-Management-With
---
-->
&lt;!--
On Friday 5 June 2015 I gave a talk called [Cluster Management with Kubernetes](https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;loop=false&amp;delayms=3000) to a general audience at the University of Edinburgh. The talk includes an example of a music store system with a Kibana front end UI and an Elasticsearch based back end which helps to make concrete concepts like pods, replication controllers and services.
[Cluster Management with Kubernetes](https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;loop=false&amp;delayms=3000).
-->
&lt;p>2015å¹´6æœˆ5æ—¥æ˜ŸæœŸäº”ï¼Œæˆ‘åœ¨çˆ±ä¸å ¡å¤§å­¦ç»™æ™®é€šå¬ä¼—åšäº†ä¸€ä¸ªæ¼”è®²ï¼Œé¢˜ç›®æ˜¯&lt;a href="https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000">ä½¿ç”¨ Kubernetes è¿›è¡Œé›†ç¾¤ç®¡ç†&lt;/a>ã€‚è¿™æ¬¡æ¼”è®²åŒ…æ‹¬ä¸€ä¸ªå¸¦æœ‰ Kibana å‰ç«¯ UI çš„éŸ³ä¹å­˜å‚¨ç³»ç»Ÿçš„ä¾‹å­ï¼Œä»¥åŠä¸€ä¸ªåŸºäºŽ Elasticsearch çš„åŽç«¯ï¼Œè¯¥åŽç«¯æœ‰åŠ©äºŽç”Ÿæˆå…·ä½“çš„æ¦‚å¿µï¼Œå¦‚ podsã€å¤åˆ¶æŽ§åˆ¶å™¨å’ŒæœåŠ¡ã€‚&lt;/p>
&lt;p>&lt;a href="https://docs.google.com/presentation/d/1H4ywDb4vAJeg8KEjpYfhNqFSig0Q8e_X5I36kM9S6q0/pub?start=false&amp;amp;loop=false&amp;amp;delayms=3000">Kubernetes é›†ç¾¤ç®¡ç†&lt;/a>ã€‚&lt;/p></description></item><item><title>Blog: OpenStack ä¸Šçš„ Kubernetes</title><link>https://kubernetes.io/zh/blog/2015/05/19/kubernetes-on-openstack/</link><pubDate>Tue, 19 May 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2015/05/19/kubernetes-on-openstack/</guid><description>
&lt;!--
---
title: " Kubernetes on OpenStack "
date: 2015-05-19
slug: kubernetes-on-openstack
url: /blog/2015/05/Kubernetes-On-Openstack
---
-->
&lt;p>&lt;a href="https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s1600/Untitled%2Bdrawing.jpg">&lt;img src="https://3.bp.blogspot.com/-EOrCHChZJZE/VVZzq43g6CI/AAAAAAAAF-E/JUilRHk369E/s400/Untitled%2Bdrawing.jpg" alt="">&lt;/a>&lt;/p>
&lt;!--
Today, the [OpenStack foundation](https://www.openstack.org/foundation/) made it even easier for you deploy and manage clusters of Docker containers on OpenStack clouds by including Kubernetes in its [Community App Catalog](http://apps.openstack.org/). &amp;nbsp;At a keynote today at the OpenStack Summit in Vancouver, Mark Collier, COO of the OpenStack Foundation, and Craig Peters, &amp;nbsp;[Mirantis](https://www.mirantis.com/) product line manager, demonstrated the Community App Catalog workflow by launching a Kubernetes cluster in a matter of seconds by leveraging the compute, storage, networking and identity systems already present in an OpenStack cloud.
-->
&lt;p>ä»Šå¤©ï¼Œ&lt;a href="https://www.openstack.org/foundation/">OpenStack åŸºé‡‘ä¼š&lt;/a>é€šè¿‡åœ¨å…¶&lt;a href="http://apps.openstack.org/">ç¤¾åŒºåº”ç”¨ç¨‹åºç›®å½•&lt;/a>ä¸­åŒ…å« Kubernetesï¼Œä½¿æ‚¨æ›´å®¹æ˜“åœ¨ OpenStack äº‘ä¸Šéƒ¨ç½²å’Œç®¡ç† Docker å®¹å™¨é›†ç¾¤ã€‚
ä»Šå¤©åœ¨æ¸©å“¥åŽ OpenStack å³°ä¼šä¸Šçš„ä¸»é¢˜æ¼”è®²ä¸­ï¼ŒOpenStack åŸºé‡‘ä¼šçš„é¦–å¸­è¿è¥å®˜ï¼šMark Collier å’Œ &lt;a href="https://www.mirantis.com/">Mirantis&lt;/a> äº§å“çº¿ç»ç† Craig Peters é€šè¿‡åˆ©ç”¨ OpenStack äº‘ä¸­å·²ç»å­˜åœ¨çš„è®¡ç®—ã€å­˜å‚¨ã€ç½‘ç»œå’Œæ ‡è¯†ç³»ç»Ÿï¼Œåœ¨å‡ ç§’é’Ÿå†…å¯åŠ¨äº† Kubernetes é›†ç¾¤ï¼Œå±•ç¤ºäº†ç¤¾åŒºåº”ç”¨ç¨‹åºç›®å½•çš„å·¥ä½œæµã€‚&lt;/p>
&lt;!--
The entries in the catalog include not just the ability to [start a Kubernetes cluster](http://apps.openstack.org/#tab=murano-apps&amp;asset=Kubernetes%20Cluster), but also a range of applications deployed in Docker containers managed by Kubernetes. These applications include:
-->
&lt;p>ç›®å½•ä¸­çš„æ¡ç›®ä¸ä»…åŒ…æ‹¬&lt;a href="http://apps.openstack.org/#tab=murano-apps&amp;amp;asset=Kubernetes%20Cluster">å¯åŠ¨ Kubernetes é›†ç¾¤&lt;/a>çš„åŠŸèƒ½ï¼Œè¿˜åŒ…æ‹¬éƒ¨ç½²åœ¨ Kubernetes ç®¡ç†çš„ Docker å®¹å™¨ä¸­çš„ä¸€ç³»åˆ—åº”ç”¨ç¨‹åºã€‚è¿™äº›åº”ç”¨åŒ…æ‹¬ï¼š&lt;/p>
&lt;!--
-
Apache web server
-
Nginx web server
-
Crate - The Distributed Database for Docker
-
GlassFish - Java EE 7 Application Server
-
Tomcat - An open-source web server and servlet container
-
InfluxDB - An open-source, distributed, time series database
-
Grafana - Metrics dashboard for InfluxDB
-
Jenkins - An extensible open source continuous integration server
-
MariaDB database
-
MySql database
-
Redis - Key-value cache and store
-
PostgreSQL database
-
MongoDB NoSQL database
-
Zend Server - The Complete PHP Application Platform
-->
&lt;ul>
&lt;li>&lt;/li>
&lt;/ul>
&lt;h2 id="apache-web-æœåŠ¡å™¨">Apache web æœåŠ¡å™¨&lt;/h2>
&lt;h2 id="nginx-web-æœåŠ¡å™¨">Nginx web æœåŠ¡å™¨&lt;/h2>
&lt;h2 id="crate-dockerçš„åˆ†å¸ƒå¼æ•°æ®åº“">Crate - Dockerçš„åˆ†å¸ƒå¼æ•°æ®åº“&lt;/h2>
&lt;h2 id="glassfish-java-ee-7-åº”ç”¨æœåŠ¡å™¨">GlassFish - Java EE 7 åº”ç”¨æœåŠ¡å™¨&lt;/h2>
&lt;h2 id="tomcat-ä¸€ä¸ªå¼€æºçš„-web-æœåŠ¡å™¨å’Œ-servlet-å®¹å™¨">Tomcat - ä¸€ä¸ªå¼€æºçš„ web æœåŠ¡å™¨å’Œ servlet å®¹å™¨&lt;/h2>
&lt;h2 id="influxdb-ä¸€ä¸ªå¼€æºçš„-åˆ†å¸ƒå¼çš„-æ—¶é—´åºåˆ—æ•°æ®åº“">InfluxDB - ä¸€ä¸ªå¼€æºçš„ã€åˆ†å¸ƒå¼çš„ã€æ—¶é—´åºåˆ—æ•°æ®åº“&lt;/h2>
&lt;h2 id="grafana-influxdb-çš„åº¦é‡ä»ªè¡¨æ¿">Grafana - InfluxDB çš„åº¦é‡ä»ªè¡¨æ¿&lt;/h2>
&lt;h2 id="jenkins-ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æ”¾æºç æŒç»­é›†æˆæœåŠ¡å™¨">Jenkins - ä¸€ä¸ªå¯æ‰©å±•çš„å¼€æ”¾æºç æŒç»­é›†æˆæœåŠ¡å™¨&lt;/h2>
&lt;h2 id="mariadb-æ•°æ®åº“">MariaDB æ•°æ®åº“&lt;/h2>
&lt;h2 id="mysql-æ•°æ®åº“">MySql æ•°æ®åº“&lt;/h2>
&lt;h2 id="redis-é”®-å€¼ç¼“å­˜å’Œå­˜å‚¨">Redis - é”®-å€¼ç¼“å­˜å’Œå­˜å‚¨&lt;/h2>
&lt;h2 id="postgresql-æ•°æ®åº“">PostgreSQL æ•°æ®åº“&lt;/h2>
&lt;h2 id="mongodb-nosql-æ•°æ®åº“">MongoDB NoSQL æ•°æ®åº“&lt;/h2>
&lt;p>Zend æœåŠ¡å™¨ - å®Œæ•´çš„ PHP åº”ç”¨ç¨‹åºå¹³å°&lt;/p>
&lt;!--
This list will grow, and is curated [here](https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes). You can examine (and contribute to) the YAML file that tells Murano how to install and start the Kubernetes cluster [here](https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml).
-->
&lt;p>æ­¤åˆ—è¡¨å°†ä¼šå¢žé•¿ï¼Œå¹¶åœ¨&lt;a href="https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes">æ­¤å¤„&lt;/a>è¿›è¡Œç­–åˆ’ã€‚æ‚¨å¯ä»¥æ£€æŸ¥ï¼ˆå¹¶å‚ä¸Žï¼‰YAML æ–‡ä»¶ï¼Œè¯¥æ–‡ä»¶å‘Šè¯‰ Murano å¦‚ä½•æ ¹æ®&lt;a href="https://opendev.org/x/k8s-docker-suite-app-murano/src/branch/master/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml">æ­¤å¤„&lt;/a>å®šä¹‰æ¥å®‰è£…å’Œå¯åŠ¨ ...apps/blob/master/Docker/Kubernetes/KubernetesCluster/package/Classes/KubernetesCluster.yaml)å®‰è£…å’Œå¯åŠ¨ Kubernetes é›†ç¾¤ã€‚&lt;/p>
&lt;!--
[The Kubernetes open source project](https://github.com/GoogleCloudPlatform/kubernetes) has continued to see fantastic community adoption and increasing momentum, with over 11,000 commits and 7,648 stars on GitHub. With supporters ranging from Red Hat and Intel to CoreOS and Box.net, it has come to represent a range of customer interests ranging from enterprise IT to cutting edge startups. We encourage you to give it a try, give us your feedback, and get involved in our growing community.
-->
&lt;p>&lt;a href="https://github.com/GoogleCloudPlatform/kubernetes">Kubernetes å¼€æºé¡¹ç›®&lt;/a>ç»§ç»­å—åˆ°ç¤¾åŒºçš„æ¬¢è¿Žï¼Œå¹¶ä¸”åŠ¿å¤´è¶Šæ¥è¶Šå¥½ï¼ŒGitHub ä¸Šæœ‰è¶…è¿‡ 11000 ä¸ªæäº¤å’Œ 7648 é¢—æ˜Ÿã€‚ä»Ž Red Hat å’Œ Intel åˆ° CoreOS å’Œ Box.netï¼Œå®ƒå·²ç»ä»£è¡¨äº†ä»Žä¼ä¸š IT åˆ°å‰æ²¿åˆ›ä¸šä¼ä¸šçš„ä¸€ç³»åˆ—å®¢æˆ·ã€‚æˆ‘ä»¬é¼“åŠ±æ‚¨å°è¯•ä¸€ä¸‹ï¼Œç»™æˆ‘ä»¬æ‚¨çš„åé¦ˆï¼Œå¹¶å‚ä¸Žåˆ°æˆ‘ä»¬ä¸æ–­å¢žé•¿çš„ç¤¾åŒºä¸­æ¥ã€‚&lt;/p>
&lt;!--
- Martin Buhr, Product Manager, Kubernetes Open Source Project
-->
&lt;ul>
&lt;li>Martin Buhr, Kubernetes å¼€æºé¡¹ç›®äº§å“ç»ç†&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes ç¤¾åŒºæ¯å‘¨èšä¼šç¬”è®°- 2015å¹´5æœˆ1æ—¥</title><link>https://kubernetes.io/zh/blog/2015/05/11/weekly-kubernetes-community-hangout/</link><pubDate>Mon, 11 May 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/zh/blog/2015/05/11/weekly-kubernetes-community-hangout/</guid><description>
&lt;!--
---
title: " Weekly Kubernetes Community Hangout Notes - May 1 2015 "
date: 2015-05-11
slug: weekly-kubernetes-community-hangout
url: /blog/2015/05/Weekly-Kubernetes-Community-Hangout
---
-->
&lt;!--
Every week the Kubernetes contributing community meet virtually over Google Hangouts. We want anyone who's interested to know what's discussed in this forum.
-->
&lt;p>æ¯ä¸ªæ˜ŸæœŸï¼ŒKubernetes è´¡çŒ®è€…ç¤¾åŒºå‡ ä¹Žéƒ½ä¼šåœ¨è°·æ­Œ Hangouts ä¸Šèšä¼šã€‚æˆ‘ä»¬å¸Œæœ›ä»»ä½•å¯¹æ­¤æ„Ÿå…´è¶£çš„äººéƒ½èƒ½äº†è§£è¿™ä¸ªè®ºå›çš„è®¨è®ºå†…å®¹ã€‚&lt;/p>
&lt;!--
* Simple rolling update - Brendan
* Rolling update = nice example of why RCs and Pods are good.
* ...pauseâ€¦ (Brendan needs demo recovery tips from Kelsey)
* Rolling update has recovery: Cancel update and restart, update continues from where it stopped.
* New controller gets name of old controller, so appearance is pure update.
* Can also name versions in update (won't do rename at the end).
-->
&lt;ul>
&lt;li>
&lt;p>ç®€å•çš„æ»šåŠ¨æ›´æ–° - Brendan&lt;/p>
&lt;ul>
&lt;li>
&lt;p>æ»šåŠ¨æ›´æ–° = RCså’ŒPodså¾ˆå¥½çš„ä¾‹å­ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>...pauseâ€¦ (Brendan éœ€è¦ Kelsey çš„æ¼”ç¤ºæ¢å¤æŠ€å·§)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ»šåŠ¨æ›´æ–°å…·æœ‰æ¢å¤åŠŸèƒ½:å–æ¶ˆæ›´æ–°å¹¶é‡æ–°å¯åŠ¨ï¼Œæ›´æ–°ä»Žåœæ­¢çš„åœ°æ–¹ç»§ç»­ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ–°æŽ§åˆ¶å™¨èŽ·å–æ—§æŽ§åˆ¶å™¨çš„åç§°ï¼Œå› æ­¤å¤–è§‚æ˜¯çº¯ç²¹çš„æ›´æ–°ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>è¿˜å¯ä»¥åœ¨ update ä¸­å‘½åç‰ˆæœ¬(æœ€åŽä¸ä¼šé‡å‘½å)ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Rocket demo - CoreOS folks
* 2 major differences between rocket &amp; docker: Rocket is daemonless &amp; pod-centric.
* Rocket has AppContainer format as native, but also supports docker image format.
* Can run AppContainer and docker containers in same pod.
* Changes are close to merged.
-->
&lt;ul>
&lt;li>
&lt;p>Rocket æ¼”ç¤º - CoreOS çš„ä¼™è®¡ä»¬&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Rocket å’Œ docker ä¹‹é—´çš„ä¸»è¦åŒºåˆ«: Rocket æ˜¯æ— å®ˆæŠ¤è¿›ç¨‹å’Œä»¥ pod ä¸ºä¸­å¿ƒã€‚ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Rocket å…·æœ‰åŽŸç”Ÿçš„ AppContainer æ ¼å¼ï¼Œä½†ä¹Ÿæ”¯æŒ docker é•œåƒæ ¼å¼ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å¯ä»¥åœ¨åŒä¸€ä¸ª pod ä¸­è¿è¡Œ AppContainer å’Œ docker å®¹å™¨ã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>å˜æ›´æŽ¥è¿‘äºŽåˆå¹¶ã€‚&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* demo service accounts and secrets being added to pods - Jordan
* Problem: It's hard to get a token to talk to the API.
* New API object: "ServiceAccount"
* ServiceAccount is namespaced, controller makes sure that at least 1 default service account exists in a namespace.
* Typed secret "ServiceAccountToken", controller makes sure there is at least 1 default token.
* DEMO
* * Can create new service account with ServiceAccountToken. Controller will create token for it.
* Can create a pod with service account, pods will have service account secret mounted at /var/run/secrets/kubernetes.io/â€¦
-->
&lt;ul>
&lt;li>
&lt;p>æ¼”ç¤º service accounts å’Œ secrets è¢«æ·»åŠ åˆ° pod - Jordan&lt;/p>
&lt;ul>
&lt;li>
&lt;p>é—®é¢˜ï¼šå¾ˆéš¾èŽ·å¾—ä¸ŽAPIé€šä¿¡çš„ä»¤ç‰Œã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ–°çš„APIå¯¹è±¡ï¼š&amp;quot;ServiceAccount&amp;quot;&lt;/p>
&lt;/li>
&lt;li>
&lt;p>ServiceAccount æ˜¯å‘½åç©ºé—´ï¼ŒæŽ§åˆ¶å™¨ç¡®ä¿å‘½åç©ºé—´ä¸­è‡³å°‘å­˜åœ¨ä¸€ä¸ªä¸ªé»˜è®¤ service accountã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>é”®å…¥ &amp;quot;ServiceAccountToken&amp;quot;ï¼ŒæŽ§åˆ¶å™¨ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªé»˜è®¤ä»¤ç‰Œã€‚&lt;/p>
&lt;/li>
&lt;li>
&lt;p>æ¼”ç¤º&lt;/p>
&lt;/li>
&lt;li>
&lt;pre>&lt;code>* å¯ä»¥ä½¿ç”¨ ServiceAccountToken åˆ›å»ºæ–°çš„ service accountã€‚æŽ§åˆ¶å™¨å°†ä¸ºå®ƒåˆ›å»ºä»¤ç‰Œã€‚
&lt;/code>&lt;/pre>
&lt;/li>
&lt;li>
&lt;p>å¯ä»¥åˆ›å»ºä¸€ä¸ªå¸¦æœ‰ service account çš„ pod, pod å°†åœ¨ /var/run/secrets/kubernets.io/â€¦&lt;/p>
&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;!--
* Kubelet running in a container - Paul
* Kubelet successfully ran pod w/ mounted secret.
-->
&lt;ul>
&lt;li>
&lt;p>Kubelet åœ¨å®¹å™¨ä¸­è¿è¡Œ - Paul&lt;/p>
&lt;ul>
&lt;li>Kubelet æˆåŠŸåœ°è¿è¡Œäº†å¸¦æœ‰ secret çš„ podã€‚&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul></description></item><item><title>Blog: AppC Support for Kubernetes through RKT</title><link>https://kubernetes.io/blog/2015/05/appc-support-for-kubernetes-through-rkt/</link><pubDate>Mon, 04 May 2015 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2015/05/appc-support-for-kubernetes-through-rkt/</guid><description>
&lt;hr>
&lt;h2 id="url-blog-2015-05-appc-support-for-kubernetes-through-rkt">title: &amp;quot; é€šè¿‡ RKT å¯¹ Kubernetes çš„ AppC æ”¯æŒ &amp;quot;
date: 2015-05-04
slug: appc-support-for-kubernetes-through-rkt
url: /blog/2015/05/Appc-Support-For-Kubernetes-Through-Rkt&lt;/h2>
&lt;!--
We very recently accepted a pull request to the Kubernetes project to add appc support for the Kubernetes community. &amp;nbsp;Appc is a new open container specification that was initiated by CoreOS, and is supported through CoreOS rkt container runtime.
-->
&lt;p>æˆ‘ä»¬æœ€è¿‘æŽ¥å—äº†å¯¹ Kubernetes é¡¹ç›®çš„æ‹‰å–è¯·æ±‚ï¼Œä»¥å¢žåŠ å¯¹ Kubernetes ç¤¾åŒºçš„åº”ç”¨ç¨‹åºæ”¯æŒã€‚ Â AppC æ˜¯ç”± CoreOS å‘èµ·çš„æ–°çš„å¼€æ”¾å®¹å™¨è§„èŒƒï¼Œå¹¶é€šè¿‡ CoreOS rkt å®¹å™¨è¿è¡Œæ—¶å—åˆ°æ”¯æŒã€‚&lt;/p>
&lt;!--
This is an important step forward for the Kubernetes project and for the broader containers community. &amp;nbsp;It adds flexibility and choice to the container-verse and brings the promise of &amp;nbsp;compelling new security and performance capabilities to the Kubernetes developer.
-->
&lt;p>å¯¹äºŽKubernetesé¡¹ç›®å’Œæ›´å¹¿æ³›çš„å®¹å™¨ç¤¾åŒºè€Œè¨€ï¼Œè¿™æ˜¯é‡è¦çš„ä¸€æ­¥ã€‚ Â å®ƒä¸ºå®¹å™¨è¯­è¨€å¢žåŠ äº†çµæ´»æ€§å’Œé€‰æ‹©ä½™åœ°ï¼Œå¹¶ä¸ºKuberneteså¼€å‘äººå‘˜å¸¦æ¥äº†ä»¤äººä¿¡æœçš„æ–°å®‰å…¨æ€§å’Œæ€§èƒ½åŠŸèƒ½ã€‚&lt;/p>
&lt;!--
Container based runtimes (like Docker or rkt) when paired with smart orchestration technologies (like Kubernetes and/or Apache Mesos) are a legitimate disruption to the way that developers build and run their applications. &amp;nbsp;While the supporting technologies are relatively nascent, they do offer the promise of some very powerful new ways to assemble, deploy, update, debug and extend solutions. &amp;nbsp;I believe that the world has not yet felt the full potential of containers and the next few years are going to be particularly exciting! &amp;nbsp;With that in mind it makes sense for several projects to emerge with different properties and different purposes. It also makes sense to be able to plug together different pieces (whether it be the container runtime or the orchestrator) based on the specific needs of a given application.
-->
&lt;p>ä¸Žæ™ºèƒ½ç¼–æŽ’æŠ€æœ¯ï¼ˆä¾‹å¦‚ Kubernetes å’Œ/æˆ– Apache Mesosï¼‰é…åˆä½¿ç”¨æ—¶ï¼ŒåŸºäºŽå®¹å™¨çš„è¿è¡Œæ—¶ï¼ˆä¾‹å¦‚ Docker æˆ– rktï¼‰å¯¹å¼€å‘äººå‘˜æž„å»ºå’Œè¿è¡Œå…¶åº”ç”¨ç¨‹åºçš„æ–¹å¼æ˜¯ä¸€ç§åˆæ³•å¹²æ‰°ã€‚ Â å°½ç®¡æ”¯æŒæŠ€æœ¯è¿˜å¤„äºŽæ–°ç”Ÿé˜¶æ®µï¼Œä½†å®ƒä»¬ç¡®å®žä¸ºç»„è£…ï¼Œéƒ¨ç½²ï¼Œæ›´æ–°ï¼Œè°ƒè¯•å’Œæ‰©å±•è§£å†³æ–¹æ¡ˆæä¾›äº†ä¸€äº›éžå¸¸å¼ºå¤§çš„æ–°æ–¹æ³•ã€‚ Â æˆ‘ç›¸ä¿¡ï¼Œä¸–ç•Œè¿˜æ²¡æœ‰æ„è¯†åˆ°å®¹å™¨çš„å…¨éƒ¨æ½œåŠ›ï¼Œæœªæ¥å‡ å¹´å°†ç‰¹åˆ«ä»¤äººå…´å¥‹ï¼ Â è€ƒè™‘åˆ°è¿™ä¸€ç‚¹ï¼Œæœ‰å‡ ä¸ªå…·æœ‰ä¸åŒå±žæ€§å’Œä¸åŒç›®çš„çš„é¡¹ç›®æ‰æœ‰æ„ä¹‰ã€‚èƒ½å¤Ÿæ ¹æ®ç»™å®šåº”ç”¨ç¨‹åºçš„ç‰¹å®šéœ€æ±‚å°†ä¸åŒçš„éƒ¨åˆ†ï¼ˆæ— è®ºæ˜¯å®¹å™¨è¿è¡Œæ—¶è¿˜æ˜¯ç¼–æŽ’å·¥å…·ï¼‰æ’å…¥åœ¨ä¸€èµ·ä¹Ÿæ˜¯æœ‰æ„ä¹‰çš„ã€‚&lt;/p>
&lt;!--
Docker has done an amazing job of democratizing container technologies and making them accessible to the outside world, and we expect Kubernetes to support Docker indefinitely. CoreOS has also started to do interesting work with rkt to create an elegant, clean, simple and open platform that offers some really interesting properties. &amp;nbsp;It looks poised deliver a secure and performant operating environment for containers. &amp;nbsp;The Kubernetes team has been working with the appc team at CoreOS for a while and in many ways they built rkt with Kubernetes in mind as a simple pluggable runtime component. &amp;nbsp;
-->
&lt;p>Docker åœ¨ä½¿å®¹å™¨æŠ€æœ¯æ°‘ä¸»åŒ–å¹¶ä½¿å¤–ç•Œå¯ä»¥è®¿é—®å®ƒä»¬æ–¹é¢åšå¾—éžå¸¸å‡ºè‰²ï¼Œæˆ‘ä»¬å¸Œæœ› Kubernetes èƒ½å¤Ÿæ— é™æœŸåœ°æ”¯æŒ Dockerã€‚CoreOS è¿˜å¼€å§‹ä¸Ž rkt è¿›è¡Œæœ‰è¶£çš„å·¥ä½œï¼Œä»¥åˆ›å»ºä¸€ä¸ªä¼˜é›…ï¼Œå¹²å‡€ï¼Œç®€å•å’Œå¼€æ”¾çš„å¹³å°ï¼Œè¯¥å¹³å°æä¾›äº†ä¸€äº›éžå¸¸æœ‰è¶£çš„å±žæ€§ã€‚ Â è¿™çœ‹èµ·æ¥è“„åŠ¿å¾…å‘ï¼Œå¯ä»¥ä¸ºå®¹å™¨æä¾›å®‰å…¨ï¼Œé«˜æ€§èƒ½çš„æ“ä½œçŽ¯å¢ƒã€‚ Â Kubernetes å›¢é˜Ÿå·²ç»ä¸Ž CoreOS çš„ appc å›¢é˜Ÿåˆä½œäº†ä¸€æ®µæ—¶é—´ï¼Œåœ¨è®¸å¤šæ–¹é¢ï¼Œä»–ä»¬éƒ½å°† Kubernetes ä½œä¸ºç®€å•çš„å¯æ’å…¥è¿è¡Œæ—¶ç»„ä»¶æ¥æž„å»º rktã€‚ Â &lt;/p>
&lt;!--
The really nice thing is that with Kubernetes you can now pick the container runtime that works best for you based on your workloadsâ€™ needs, change runtimes without having the replace your cluster environment, or even mix together applications where different parts are running in different container runtimes in the same cluster. &amp;nbsp;Additional choices canâ€™t help but ultimately benefit the end developer.
-->
&lt;p>çœŸæ­£çš„å¥½å¤„æ˜¯ï¼Œå€ŸåŠ© Kubernetesï¼Œæ‚¨çŽ°åœ¨å¯ä»¥æ ¹æ®å·¥ä½œè´Ÿè½½çš„éœ€æ±‚é€‰æ‹©æœ€é€‚åˆæ‚¨çš„å®¹å™¨è¿è¡Œæ—¶ï¼Œæ— éœ€æ›¿æ¢é›†ç¾¤çŽ¯å¢ƒå³å¯æ›´æ”¹è¿è¡Œæ—¶ï¼Œç”šè‡³å¯ä»¥å°†åœ¨åŒä¸€é›†ç¾¤ä¸­åœ¨ä¸åŒå®¹å™¨ä¸­è¿è¡Œçš„åº”ç”¨ç¨‹åºçš„ä¸åŒéƒ¨åˆ†æ··åˆåœ¨ä¸€èµ·ã€‚ Â å…¶ä»–é€‰æ‹©æ— æµŽäºŽäº‹ï¼Œä½†æœ€ç»ˆä½¿æœ€ç»ˆå¼€å‘äººå‘˜å—ç›Šã€‚&lt;/p>
&lt;!--
-- Craig McLuckie
Google Product Manager and Kubernetes co-founder
-->
&lt;p>-- Craig McLuckie
Google äº§å“ç»ç†å’Œ Kubernetes è”åˆåˆ›å§‹äºº&lt;/p></description></item></channel></rss>