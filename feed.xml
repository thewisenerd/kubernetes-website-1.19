<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Kubernetes – Production-Grade Container Orchestration</title><link>https://kubernetes.io/</link><description>The Kubernetes project blog</description><generator>Hugo -- gohugo.io</generator><image><url>https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.png</url><title>Kubernetes.io</title><link>https://kubernetes.io/</link></image><atom:link href="https://kubernetes.io/feed.xml" rel="self" type="application/rss+xml"/><item><title>Blog: GSoD 2020: Improving the API Reference Experience</title><link>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</link><pubDate>Fri, 04 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/04/gsod-2020-improving-api-reference-experience/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://github.com/feloy">Philippe Martin&lt;/a>&lt;/p>
&lt;p>&lt;em>Editor's note: Better API references have been my goal since I joined Kubernetes docs three and a half years ago. Philippe has succeeded fantastically. More than a better API reference, though, Philippe embodied the best of the Kubernetes community in this project: excellence through collaboration, and a process that made the community itself better. Thanks, Google Season of Docs, for making Philippe's work possible. —Zach Corleissen&lt;/em>&lt;/p>
&lt;h2 id="introduction">Introduction&lt;/h2>
&lt;p>The &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> project brings open source organizations and technical writers together to work closely on a specific documentation project.&lt;/p>
&lt;p>I was selected by the CNCF to work on Kubernetes documentation, specifically to make the API Reference documentation more accessible.&lt;/p>
&lt;p>I'm a software developer with a great interest in documentation systems. In the late 90's I started translating Linux-HOWTO documents into French. From one thing to another, I learned about documentation systems. Eventually, I wrote a Linux-HOWTO to help documentarians learn the language used at that time for writing documents, LinuxDoc/SGML.&lt;/p>
&lt;p>Shortly afterward, Linux documentation adopted the DocBook language. I helped some writers rewrite their documents in this format; for example, the Advanced Bash-Scripting Guide. I also worked on the GNU &lt;code>makeinfo&lt;/code> program to add DocBook output, making it possible to transform &lt;em>GNU Info&lt;/em> documentation into Docbook format.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>The &lt;a href="https://kubernetes.io/docs/home/">Kubernetes website&lt;/a> is built with Hugo from documentation written in Markdown format in the &lt;a href="https://github.com/kubernetes/website">website repository&lt;/a>, using the &lt;a href="https://www.docsy.dev/about/">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The existing API reference documentation is a large HTML file generated from the Kubernetes OpenAPI specification.&lt;/p>
&lt;p>On my side, I wanted for some time to make the API Reference more accessible, by:&lt;/p>
&lt;ul>
&lt;li>building individual and autonomous pages for each Kubernetes resource&lt;/li>
&lt;li>adapting the format to mobile reading&lt;/li>
&lt;li>reusing the website's assets and theme to build, integrate, and display the reference pages&lt;/li>
&lt;li>allowing the search engines to reference the content of the pages&lt;/li>
&lt;/ul>
&lt;p>Around one year ago, I started to work on the generator building the current unique HTML page, to add a DocBook output, so the API Reference could be generated first in DocBook format, and after that in PDF or other formats supported by DocBook processors. The first result has been some &lt;a href="https://github.com/feloy/kubernetes-resources-reference/releases">Ebook files for the API Reference&lt;/a> and an auto-edited paper book.&lt;/p>
&lt;p>I decided later to add another output to this generator, to generate Markdown files and create &lt;a href="https://web.archive.org/web/20201022201911/https://www.k8sref.io/docs/workloads/">a website with the API Reference&lt;/a>.&lt;/p>
&lt;p>When the CNCF proposed a project for the Google Season of Docs to work on the API Reference, I applied, and the match occurred.&lt;/p>
&lt;h2 id="the-project">The Project&lt;/h2>
&lt;h3 id="swagger-ui">swagger-ui&lt;/h3>
&lt;p>The first idea of the CNCF members that proposed this project was to test the &lt;a href="https://swagger.io/tools/swagger-ui/">&lt;code>swagger-ui&lt;/code> tool&lt;/a>, to try and document the Kubernetes API Reference with this standard tool.&lt;/p>
&lt;p>Because the Kubernetes API is much larger than many other APIs, it has been necessary to write a tool to split the complete API Reference by API Groups, and insert in the Documentation website several &lt;code>swagger-ui&lt;/code> components, one for each API Group.&lt;/p>
&lt;p>Generally, APIs are used by developers by calling endpoints with a specific HTTP verb, with specific parameters and waiting for a response. The &lt;code>swagger-ui&lt;/code> interface is built for this usage: the interface displays a list of endpoints and their associated verbs, and for each the parameters and responses formats.&lt;/p>
&lt;p>The Kubernetes API is most of the time used differently: users create manifest files containing resources definitions in YAML format, and use the &lt;code>kubectl&lt;/code> CLI to &lt;em>apply&lt;/em> these manifests to the cluster. In this case, the most important information is the description of the structures used as parameters and responses (the Kubernetes Resources).&lt;/p>
&lt;p>Because of this specificity, we realized that it would be difficult to adapt the &lt;code>swagger-ui&lt;/code> interface to satisfy the users of the Kubernetes API and this direction has been abandoned.&lt;/p>
&lt;h3 id="markdown-pages">Markdown pages&lt;/h3>
&lt;p>The second stage of the project has been to adapt the work I had done to create the k8sref.io website, to include it in the official documentation website.&lt;/p>
&lt;p>The main changes have been to:&lt;/p>
&lt;ul>
&lt;li>use go-templates to represent the output pages, so non-developers can adapt the generated pages without having to edit the generator code&lt;/li>
&lt;li>create a new custom &lt;a href="https://gohugo.io/content-management/shortcodes/">shortcode&lt;/a>, to easily create links from inside the website to specific pages of the API reference&lt;/li>
&lt;li>improve the navigation between the sections of the API reference&lt;/li>
&lt;li>add the code of the generator to the Kubernetes GitHub repository containing the different reference generators&lt;/li>
&lt;/ul>
&lt;p>All the discussions and work done can be found in website &lt;a href="https://github.com/kubernetes/website/pull/23294">pull request #23294&lt;/a>.&lt;/p>
&lt;p>Adding the generator code to the Kubernetes project happened in &lt;a href="https://github.com/kubernetes-sigs/reference-docs/pull/179">kubernetes-sigs/reference-docs#179&lt;/a>.&lt;/p>
&lt;p>Here are the features of the new API Reference to be included in the official documentation website:&lt;/p>
&lt;ul>
&lt;li>the resources are categorized, in the categories Workloads, Services, Config &amp;amp; Storage, Authentication, Authorization, Policies, Extend, Cluster. This structure is configurable with a simple &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/toc.yaml">&lt;code>toc.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>each page displays associated resources at the first level ; for example: Pod, PodSpec, PodStatus, PodList&lt;/li>
&lt;li>most resource pages inline relevant definitions ; the exceptions are when those definitions are common to several resources, or are too complex to be displayed inline. With the old approach, you had to follow a hyperlink to read each extra detail.&lt;/li>
&lt;li>some widely used definitions, such as &lt;code>ObjectMeta&lt;/code>, are documented in a specific page&lt;/li>
&lt;li>required fields are indicated, and placed first&lt;/li>
&lt;li>fields of a resource can be categorized and ordered, with the help of a &lt;a href="https://github.com/kubernetes-sigs/reference-docs/blob/master/gen-resourcesdocs/config/v1.20/fields.yaml">&lt;code>fields.yaml&lt;/code> file&lt;/a>&lt;/li>
&lt;li>&lt;code>map&lt;/code> fields are indicated. For example the &lt;code>.spec.nodeSelector&lt;/code> for a &lt;code>Pod&lt;/code> is &lt;code>map[string]string&lt;/code>, instead of &lt;code>object&lt;/code>, using the value of &lt;code>x-kubernetes-list-type&lt;/code>&lt;/li>
&lt;li>patch strategies are indicated&lt;/li>
&lt;li>&lt;code>apiVersion&lt;/code> and &lt;code>kind&lt;/code> display the value, not the &lt;code>string&lt;/code> type&lt;/li>
&lt;li>At the top of a reference page, the page displays the Go import necessary to use these resources from a Go program.&lt;/li>
&lt;/ul>
&lt;p>The work is currently on hold pending the 1.20 release. When the release finishes and the work is integrated, the API reference will be available at &lt;a href="https://kubernetes.io/docs/reference/">https://kubernetes.io/docs/reference/&lt;/a>.&lt;/p>
&lt;h3 id="future-work">Future Work&lt;/h3>
&lt;p>There are points to improve, particularly:&lt;/p>
&lt;ul>
&lt;li>Some Kubernetes resources are deeply nested. Inlining the definition of these resources makes them difficult to understand.&lt;/li>
&lt;li>The created &lt;code>shortcode&lt;/code> uses the URL of the page to reference a Resource page. It would be easier for documentarians if they could reference a Resource by its group and name.&lt;/li>
&lt;/ul>
&lt;h2 id="appreciation">Appreciation&lt;/h2>
&lt;p>I would like to thank my mentor &lt;a href="https://github.com/zacharysarah">Zach Corleissen&lt;/a> and the lead writers &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a>, &lt;a href="https://github.com/celestehorgan">Celeste Horgan&lt;/a>, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/tengqm">Qiming Teng&lt;/a> who supervised me during all the season. They all have been very encouraging and gave me tons of great advice.&lt;/p></description></item><item><title>Blog: Dockershim Deprecation FAQ</title><link>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dockershim-faq/</guid><description>
&lt;p>This document goes over some frequently asked questions regarding the Dockershim
deprecation announced as a part of the Kubernetes v1.20 release. For more detail
on the deprecation of Docker as a container runtime for Kubernetes kubelets, and
what that means, check out the blog post
&lt;a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/">Don't Panic: Kubernetes and Docker&lt;/a>.&lt;/p>
&lt;h3 id="why-is-dockershim-being-deprecated">Why is dockershim being deprecated?&lt;/h3>
&lt;p>Maintaining dockershim has become a heavy burden on the Kubernetes maintainers.
The CRI standard was created to reduce this burden and allow smooth interoperability
of different container runtimes. Docker itself doesn't currently implement CRI,
thus the problem.&lt;/p>
&lt;p>Dockershim was always intended to be a temporary solution (hence the name: shim).
You can read more about the community discussion and planning in the
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/1985-remove-dockershim">Dockershim Removal Kubernetes Enhancement Proposal&lt;/a>.&lt;/p>
&lt;p>Additionally, features that were largely incompatible with the dockershim, such
as cgroups v2 and user namespaces are being implemented in these newer CRI
runtimes. Removing support for the dockershim will allow further development in
those areas.&lt;/p>
&lt;h3 id="can-i-still-use-docker-in-kubernetes-1-20">Can I still use Docker in Kubernetes 1.20?&lt;/h3>
&lt;p>Yes, the only thing changing in 1.20 is a single warning log printed at &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet&lt;/a>
startup if using Docker as the runtime.&lt;/p>
&lt;h3 id="when-will-dockershim-be-removed">When will dockershim be removed?&lt;/h3>
&lt;p>Given the impact of this change, we are using an extended deprecation timeline.
It will not be removed before Kubernetes 1.22, meaning the earliest release without
dockershim would be 1.23 in late 2021. We will be working closely with vendors
and other ecosystem groups to ensure a smooth transition and will evaluate things
as the situation evolves.&lt;/p>
&lt;h3 id="will-my-existing-docker-images-still-work">Will my existing Docker images still work?&lt;/h3>
&lt;p>Yes, the images produced from &lt;code>docker build&lt;/code> will work with all CRI implementations.
All your existing images will still work exactly the same.&lt;/p>
&lt;h3 id="what-about-private-images">What about private images?&lt;/h3>
&lt;p>Also yes. All CRI runtimes support the same pull secrets configuration used in
Kubernetes, either via the PodSpec or ServiceAccount.&lt;/p>
&lt;h3 id="are-docker-and-containers-the-same-thing">Are Docker and containers the same thing?&lt;/h3>
&lt;p>Docker popularized the Linux containers pattern and has been instrumental in
developing the underlying technology, however containers in Linux have existed
for a long time. The container ecosystem has grown to be much broader than just
Docker. Standards like OCI and CRI have helped many tools grow and thrive in our
ecosystem, some replacing aspects of Docker while others enhance existing
functionality.&lt;/p>
&lt;h3 id="are-there-examples-of-folks-using-other-runtimes-in-production-today">Are there examples of folks using other runtimes in production today?&lt;/h3>
&lt;p>All Kubernetes project produced artifacts (Kubernetes binaries) are validated
with each release.&lt;/p>
&lt;p>Additionally, the &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a> project has been using containerd for some time and has
seen an improvement in stability for its use case. Kind and containerd are leveraged
multiple times every day to validate any changes to the Kubernetes codebase. Other
related projects follow a similar pattern as well, demonstrating the stability and
usability of other container runtimes. As an example, OpenShift 4.x has been
using the &lt;a href="https://cri-o.io/">CRI-O&lt;/a> runtime in production since June 2019.&lt;/p>
&lt;p>For other examples and references you can look at the adopters of containerd and
cri-o, two container runtimes under the Cloud Native Computing Foundation (&lt;a href="https://cncf.io">CNCF&lt;/a>).&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/containerd/containerd/blob/master/ADOPTERS.md">containerd&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cri-o/cri-o/blob/master/ADOPTERS.md">CRI-O&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="people-keep-referencing-oci-what-is-that">People keep referencing OCI, what is that?&lt;/h3>
&lt;p>OCI stands for the &lt;a href="https://opencontainers.org/about/overview/">Open Container Initiative&lt;/a>, which standardized many of the
interfaces between container tools and technologies. They maintain a standard
specification for packaging container images (OCI image-spec) and running containers
(OCI runtime-spec). They also maintain an actual implementation of the runtime-spec
in the form of &lt;a href="https://github.com/opencontainers/runc">runc&lt;/a>, which is the underlying default runtime for both
&lt;a href="https://containerd.io/">containerd&lt;/a> and &lt;a href="https://cri-o.io/">CRI-O&lt;/a>. The CRI builds on these low-level specifications to
provide an end-to-end standard for managing containers.&lt;/p>
&lt;h3 id="which-cri-implementation-should-i-use">Which CRI implementation should I use?&lt;/h3>
&lt;p>That’s a complex question and it depends on a lot of factors. If Docker is
working for you, moving to containerd should be a relatively easy swap and
has have strictly better performance and less overhead. However we encourage you
to explore all the options from the &lt;a href="https://landscape.cncf.io/category=container-runtime&amp;amp;format=card-mode&amp;amp;grouping=category">CNCF landscape&lt;/a> in case another would be an
even better fit for your environment.&lt;/p>
&lt;h3 id="what-should-i-look-out-for-when-changing-cri-implementations">What should I look out for when changing CRI implementations?&lt;/h3>
&lt;p>While the underlying containerization code is the same between Docker and most
CRIs (including containerd), there are a few differences around the edges. Some
common things to consider when migrating are:&lt;/p>
&lt;ul>
&lt;li>Logging configuration&lt;/li>
&lt;li>Runtime resource limitations&lt;/li>
&lt;li>Node provisioning scripts that call docker or use docker via it's control socket&lt;/li>
&lt;li>Kubectl plugins that require docker CLI or the control socket&lt;/li>
&lt;li>Kubernetes tools that require direct access to Docker (e.g. kube-imagepuller)&lt;/li>
&lt;li>Configuration of functionality like &lt;code>registry-mirrors&lt;/code> and insecure registries&lt;/li>
&lt;li>Other support scripts or daemons that expect docker to be available and are run
outside of Kubernetes (e.g. monitoring or security agents)&lt;/li>
&lt;li>GPUs or special hardware and how they integrate with your runtime and Kubernetes&lt;/li>
&lt;/ul>
&lt;p>If you use Kubernetes resource requests/limits or file-based log collection
DaemonSets then they will continue to work the same, but if you’ve customized
your dockerd configuration, you’ll need to adapt that for your new container
runtime where possible.&lt;/p>
&lt;p>Another thing to look out for is anything expecting to run for system maintenance
or nested inside a container when building images will no longer work. For the
former, you can use the &lt;a href="https://github.com/kubernetes-sigs/cri-tools">&lt;code>crictl&lt;/code>&lt;/a> tool as a drop-in replacement and for the
latter you can use newer container build options like &lt;a href="https://github.com/genuinetools/img">img&lt;/a>, &lt;a href="https://github.com/containers/buildah">buildah&lt;/a>, or
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a> that don’t require Docker.&lt;/p>
&lt;p>For containerd, you can start with their &lt;a href="https://github.com/containerd/cri/blob/master/docs/registry.md">documentation&lt;/a> to see what configuration
options are available as you migrate things over.&lt;/p>
&lt;p>For instructions on how to use containerd and CRI-O with Kubernetes, see the
Kubernetes documentation on &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes">Container Runtimes&lt;/a>&lt;/p>
&lt;h3 id="what-if-i-have-more-questions">What if I have more questions?&lt;/h3>
&lt;p>If you use a vendor-supported Kubernetes distribution, you can ask them about
upgrade plans for their products. For end-user questions, please post them
to our end user community forum: &lt;a href="https://discuss.kubernetes.io/">https://discuss.kubernetes.io/&lt;/a>.&lt;/p>
&lt;p>You can also check out the excellent blog post
&lt;a href="https://dev.to/inductor/wait-docker-is-deprecated-in-kubernetes-now-what-do-i-do-e4m">Wait, Docker is deprecated in Kubernetes now?&lt;/a> a more in-depth technical
discussion of the changes.&lt;/p>
&lt;h3 id="can-i-have-a-hug">Can I have a hug?&lt;/h3>
&lt;p>Always and whenever you want! 🤗🤗&lt;/p></description></item><item><title>Blog: Don't Panic: Kubernetes and Docker</title><link>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</link><pubDate>Wed, 02 Dec 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jorge Castro, Duffie Cooley, Kat Cosgrove, Justin Garrison, Noah Kantrowitz, Bob Killen, Rey Lejano, Dan “POP” Papandrea, Jeffrey Sica, Davanum “Dims” Srinivas&lt;/p>
&lt;p>Kubernetes is &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.20.md#deprecation">deprecating
Docker&lt;/a>
as a container runtime after v1.20.&lt;/p>
&lt;p>&lt;strong>You do not need to panic. It’s not as dramatic as it sounds.&lt;/strong>&lt;/p>
&lt;p>tl;dr Docker as an underlying runtime is being deprecated in favor of runtimes
that use the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface(CRI)&lt;/a>
created for Kubernetes. Docker-produced images will continue to work in your
cluster with all runtimes, as they always have.&lt;/p>
&lt;p>If you’re an end-user of Kubernetes, not a whole lot will be changing for you.
This doesn’t mean the death of Docker, and it doesn’t mean you can’t, or
shouldn’t, use Docker as a development tool anymore. Docker is still a useful
tool for building containers, and the images that result from running &lt;code>docker build&lt;/code> can still run in your Kubernetes cluster.&lt;/p>
&lt;p>If you’re using a managed Kubernetes service like GKE, EKS, or AKS (which &lt;a href="https://github.com/Azure/AKS/releases/tag/2020-11-16">defaults to containerd&lt;/a>) you will need to
make sure your worker nodes are using a supported container runtime before
Docker support is removed in a future version of Kubernetes. If you have node
customizations you may need to update them based on your environment and runtime
requirements. Please work with your service provider to ensure proper upgrade
testing and planning.&lt;/p>
&lt;p>If you’re rolling your own clusters, you will also need to make changes to avoid
your clusters breaking. At v1.20, you will get a deprecation warning for Docker.
When Docker runtime support is removed in a future release (currently planned
for the 1.22 release in late 2021) of Kubernetes it will no longer be supported
and you will need to switch to one of the other compliant container runtimes,
like containerd or CRI-O. Just make sure that the runtime you choose supports
the docker daemon configurations you currently use (e.g. logging).&lt;/p>
&lt;h2 id="so-why-the-confusion-and-what-is-everyone-freaking-out-about">So why the confusion and what is everyone freaking out about?&lt;/h2>
&lt;p>We’re talking about two different environments here, and that’s creating
confusion. Inside of your Kubernetes cluster, there’s a thing called a container
runtime that’s responsible for pulling and running your container images. Docker
is a popular choice for that runtime (other common options include containerd
and CRI-O), but Docker was not designed to be embedded inside Kubernetes, and
that causes a problem.&lt;/p>
&lt;p>You see, the thing we call “Docker” isn’t actually one thing -- it’s an entire
tech stack, and one part of it is a thing called “containerd,” which is a
high-level container runtime by itself. Docker is cool and useful because it has
a lot of UX enhancements that make it really easy for humans to interact with
while we’re doing development work, but those UX enhancements aren’t necessary
for Kubernetes, because it isn’t a human.&lt;/p>
&lt;p>As a result of this human-friendly abstraction layer, your Kubernetes cluster
has to use another tool called Dockershim to get at what it really needs, which
is containerd. That’s not great, because it gives us another thing that has to
be maintained and can possibly break. What’s actually happening here is that
Dockershim is being removed from Kubelet as early as v1.23 release, which
removes support for Docker as a container runtime as a result. You might be
thinking to yourself, but if containerd is included in the Docker stack, why
does Kubernetes need the Dockershim?&lt;/p>
&lt;p>Docker isn’t compliant with CRI, the &lt;a href="https://kubernetes.io/blog/2016/12/container-runtime-interface-cri-in-kubernetes/">Container Runtime Interface&lt;/a>.
If it were, we wouldn’t need the shim, and this wouldn’t be a thing. But it’s
not the end of the world, and you don’t need to panic -- you just need to change
your container runtime from Docker to another supported container runtime.&lt;/p>
&lt;p>One thing to note: If you are relying on the underlying docker socket
(/var/run/docker.sock) as part of a workflow within your cluster today, moving
to a different runtime will break your ability to use it. This pattern is often
called Docker in Docker. There are lots of options out there for this specific
use case including things like
&lt;a href="https://github.com/GoogleContainerTools/kaniko">kaniko&lt;/a>,
&lt;a href="https://github.com/genuinetools/img">img&lt;/a>, and
&lt;a href="https://github.com/containers/buildah">buildah&lt;/a>.&lt;/p>
&lt;h2 id="what-does-this-change-mean-for-developers-though-do-we-still-write-dockerfiles-do-we-still-build-things-with-docker">What does this change mean for developers, though? Do we still write Dockerfiles? Do we still build things with Docker?&lt;/h2>
&lt;p>This change addresses a different environment than most folks use to interact
with Docker. The Docker installation you’re using in development is unrelated to
the Docker runtime inside your Kubernetes cluster. It’s confusing, I know. As a
developer, Docker is still useful to you in all the ways it was before this
change was announced. The image that Docker produces isn’t really a
Docker-specific image -- it’s an OCI (&lt;a href="https://opencontainers.org/">Open Container Initiative&lt;/a>) image.
Any OCI-compliant image, regardless of the tool you use to build it, will look
the same to Kubernetes. Both &lt;a href="https://containerd.io/">containerd&lt;/a> and
&lt;a href="https://cri-o.io/">CRI-O&lt;/a> know how to pull those images and run them. This is
why we have a standard for what containers should look like.&lt;/p>
&lt;p>So, this change is coming. It’s going to cause issues for some, but it isn’t
catastrophic, and generally it’s a good thing. Depending on how you interact
with Kubernetes, this could mean nothing to you, or it could mean a bit of work.
In the long run, it’s going to make things easier. If this is still confusing
for you, that’s okay -- there’s a lot going on here, Kubernetes has a lot of
moving parts, and nobody is an expert in 100% of it. We encourage any and all
questions regardless of experience level or complexity! Our goal is to make sure
everyone is educated as much as possible on the upcoming changes. &lt;code>&amp;lt;3&lt;/code> We hope
this has answered most of your questions and soothed some anxieties!&lt;/p>
&lt;p>Looking for more answers? Check out our accompanying &lt;a href="https://kubernetes.io/blog/2020/12/02/dockershim-faq/">Dockershim Deprecation FAQ&lt;/a>.&lt;/p></description></item><item><title>Blog: Cloud native security for your clusters</title><link>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</link><pubDate>Wed, 18 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/18/cloud-native-security-for-your-clusters/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/pudijoglekar">Pushkar Joglekar&lt;/a>&lt;/p>
&lt;p>Over the last few years a small, security focused community has been working diligently to deepen our understanding of security, given the evolving cloud native infrastructure and corresponding iterative deployment practices. To enable sharing of this knowledge with the rest of the community, members of &lt;a href="https://github.com/cncf/sig-security">CNCF SIG Security&lt;/a> (a group which reports into &lt;a href="https://github.com/cncf/toc#sigs">CNCF TOC&lt;/a> and who are friends with &lt;a href="https://github.com/kubernetes/community/tree/master/sig-security">Kubernetes SIG Security&lt;/a>) led by Emily Fox, collaborated on a whitepaper outlining holistic cloud native security concerns and best practices. After over 1200 comments, changes, and discussions from 35 members across the world, we are proud to share &lt;a href="https://www.cncf.io/blog/2020/11/18/announcing-the-cloud-native-security-white-paper">cloud native security whitepaper v1.0&lt;/a> that serves as essential reading for security leadership in enterprises, financial and healthcare industries, academia, government, and non-profit organizations.&lt;/p>
&lt;p>The paper attempts to &lt;em>not&lt;/em> focus on any specific &lt;a href="https://www.cncf.io/projects/">cloud native project&lt;/a>. Instead, the intent is to model and inject security into four logical phases of cloud native application lifecycle: &lt;em>Develop, Distribute, Deploy, and Runtime&lt;/em>.&lt;/p>
&lt;p>&lt;img alt="Cloud native application lifecycle phases"
src="cloud-native-app-lifecycle-phases.svg"
style="width:60em;max-width:100%;">&lt;/p>
&lt;h2 id="kubernetes-native-security-controls">Kubernetes native security controls&lt;/h2>
&lt;p>When using Kubernetes as a workload orchestrator, some of the security controls this version of the whitepaper recommends are:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies&lt;/a>: Implement a single source of truth for “least privilege” workloads across the entire cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Resource requests and limits&lt;/a>: Apply requests (soft constraint) and limits (hard constraint) for shared resources such as memory and CPU&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Audit log analysis&lt;/a>: Enable Kubernetes API auditing and filtering for security relevant events&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/architecture/control-plane-node-communication/">Control plane authentication and certificate root of trust&lt;/a>: Enable mutual TLS authentication with a trusted CA for communication within the cluster&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets management&lt;/a>: Integrate with a built-in or external secrets store&lt;/li>
&lt;/ul>
&lt;h2 id="cloud-native-complementary-security-controls">Cloud native complementary security controls&lt;/h2>
&lt;p>Kubernetes has direct involvement in the &lt;em>deploy&lt;/em> phase and to a lesser extent in the &lt;em>runtime&lt;/em> phase. Ensuring the artifacts are securely &lt;em>developed&lt;/em> and &lt;em>distributed&lt;/em> is necessary for, enabling workloads in Kubernetes to run “secure by default”. Throughout all phases of the Cloud native application life cycle, several complementary security controls exist for Kubernetes orchestrated workloads, which includes but are not limited to:&lt;/p>
&lt;ul>
&lt;li>Develop:
&lt;ul>
&lt;li>Image signing and verification&lt;/li>
&lt;li>Image vulnerability scanners&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Distribute:
&lt;ul>
&lt;li>Pre-deployment checks for detecting excessive privileges&lt;/li>
&lt;li>Enabling observability and logging&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Deploy:
&lt;ul>
&lt;li>Using a service mesh for workload authentication and authorization&lt;/li>
&lt;li>Enforcing “default deny” network policies for inter-workload communication via &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">network plugins&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Runtime:
&lt;ul>
&lt;li>Deploying security monitoring agents for workloads&lt;/li>
&lt;li>Isolating applications that run on the same node using SELinux, AppArmor, etc.&lt;/li>
&lt;li>Scanning configuration against recognized secure baselines for node, workload and orchestrator&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="understand-first-secure-next">Understand first, secure next&lt;/h2>
&lt;p>The cloud native way, including containers, provides great security benefits for its users: immutability, modularity, faster upgrades and consistent state across the environment. Realizing this fundamental change in “the way things are done”, motivates us to look at security with a cloud native lens. One of the things that was evident for all the authors of the paper was the fact that it’s tough to make smarter decisions on how and what to secure in a cloud native ecosystem if you do not understand the tools, patterns, and frameworks at hand (in addition to knowing your own critical assets). Hence, for all the security practitioners out there who want to be partners rather than a gatekeeper for your friends in Operations, Product Development, and Compliance, let’s make an attempt to &lt;em>learn more so we can secure better&lt;/em>.&lt;/p>
&lt;p>We recommend following this &lt;strong>7 step R.U.N.T.I.M.E. path&lt;/strong> to get started on cloud native security:&lt;/p>
&lt;ol>
&lt;li>&lt;b>R&lt;/b>ead the paper and any linked material in it&lt;/li>
&lt;li>&lt;b>U&lt;/b>nderstand challenges and constraints for your environment&lt;/li>
&lt;li>&lt;b>N&lt;/b>ote the content and controls that apply to your environment&lt;/li>
&lt;li>&lt;b>T&lt;/b>alk about your observations with your peers&lt;/li>
&lt;li>&lt;b>I&lt;/b>nvolve your leadership and ask for help&lt;/li>
&lt;li>&lt;b>M&lt;/b>ake a risk profile based on existing and missing security controls&lt;/li>
&lt;li>&lt;b>E&lt;/b>xpend time, money, and resources that improve security posture and reduce risk where appropriate.&lt;/li>
&lt;/ol>
&lt;h2 id="acknowledgements">Acknowledgements&lt;/h2>
&lt;p>Huge shout out to &lt;em>Emily Fox, Tim Bannister (The Scale Factory), Chase Pettet (Mirantis), and Wayne Haber (GitLab)&lt;/em> for contributing with their wonderful suggestions for this blog post.&lt;/p></description></item><item><title>Blog: Remembering Dan Kohn</title><link>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</link><pubDate>Mon, 02 Nov 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/11/02/remembering-dan-kohn/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: The Kubernetes Steering Committee&lt;/p>
&lt;p>Dan Kohn was instrumental in getting Kubernetes and CNCF community to where it is today. He shared our values, motivations, enthusiasm, community spirit, and helped the Kubernetes community to become the best that it could be. Dan loved getting people together to solve problems big and small. He enabled people to grow their individual scope in the community which often helped launch their career in open source software.&lt;/p>
&lt;p>Dan built a coalition around the nascent Kubernetes project and turned that into a cornerstone to build the larger cloud native space. He loved challenges, especially ones where the payoff was great like building worldwide communities, spreading the love of open source, and helping diverse, underprivileged communities and students to get a head start in technology.&lt;/p>
&lt;p>Our heart goes out to his family. Thank you, Dan, for bringing your boys to events in India and elsewhere as we got to know how great you were as a father. Dan, your thoughts and ideas will help us make progress in our journey as a community. Thank you for your life's work!&lt;/p>
&lt;p>If Dan has made an impact on you in some way, please consider adding a memory of him in his &lt;a href="https://github.com/cncf/memorials/blob/master/dan-kohn.md">CNCF memorial&lt;/a>.&lt;/p></description></item><item><title>Blog: Announcing the 2020 Steering Committee Election Results</title><link>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</link><pubDate>Mon, 12 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/12/steering-committee-results-2020/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Kaslin Fields&lt;/p>
&lt;p>The &lt;a href="https://github.com/kubernetes/community/tree/master/events/elections/2020">2020 Steering Committee Election&lt;/a> is now complete. In 2019, the committee arrived at its final allocation of 7 seats, 3 of which were up for election in 2020. Incoming committee members serve a term of 2 years, and all members are elected by the Kubernetes Community.&lt;/p>
&lt;p>This community body is significant since it oversees the governance of the entire Kubernetes project. With that great power comes great responsibility. You can learn more about the steering committee’s role in their &lt;a href="https://github.com/kubernetes/steering/blob/master/charter.md">charter&lt;/a>.&lt;/p>
&lt;h2 id="results">Results&lt;/h2>
&lt;p>Congratulations to the elected committee members whose two year terms begin immediately (listed in alphabetical order by GitHub handle):&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Davanum Srinivas (&lt;a href="https://github.com/dims">@dims&lt;/a>), VMware&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Jordan Liggitt (&lt;a href="https://github.com/liggitt">@liggitt&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;li>&lt;strong>Bob Killen (&lt;a href="https://github.com/mrbobbytables">@mrbobbytables&lt;/a>), Google&lt;/strong>&lt;/li>
&lt;/ul>
&lt;p>They join continuing members Christoph Blecker (&lt;a href="https://github.com/cblecker">@cblecker&lt;/a>), Red Hat; Derek Carr (&lt;a href="https://github.com/derekwaynecarr">@derekwaynecarr&lt;/a>), Red Hat; Nikhita Raghunath (&lt;a href="https://github.com/nikhita">@nikhita&lt;/a>), VMware; and Paris Pittman (&lt;a href="https://github.com/parispittman">@parispittman&lt;/a>), Apple. Davanum Srinivas is returning for his second term on the committee.&lt;/p>
&lt;h2 id="big-thanks">Big Thanks!&lt;/h2>
&lt;ul>
&lt;li>Thank you and congratulations on a successful election to this round’s election officers:
&lt;ul>
&lt;li>Jaice Singer DuMars (&lt;a href="https://github.com/jdumars">@jdumars&lt;/a>), Apple&lt;/li>
&lt;li>Ihor Dvoretskyi (&lt;a href="https://github.com/idvoretskyi">@idvoretskyi&lt;/a>), CNCF&lt;/li>
&lt;li>Josh Berkus (&lt;a href="https://github.com/jberkus">@jberkus&lt;/a>), Red Hat&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Thanks to the Emeritus Steering Committee Members. Your prior service is appreciated by the community:
&lt;ul>
&lt;li>Aaron Crickenberger (&lt;a href="https://github.com/spiffxp">@spiffxp&lt;/a>), Google&lt;/li>
&lt;li>and Lachlan Evenson(&lt;a href="https://github.com/lachie8e">@lachie8e)&lt;/a>), Microsoft&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>And thank you to all the candidates who came forward to run for election. As &lt;a href="https://twitter.com/castrojo/status/1315718627639820288?s=20">Jorge Castro put it&lt;/a>: we are spoiled with capable, kind, and selfless volunteers who put the needs of the project first.&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-with-the-steering-committee">Get Involved with the Steering Committee&lt;/h2>
&lt;p>This governing body, like all of Kubernetes, is open to all. You can follow along with Steering Committee &lt;a href="https://github.com/kubernetes/steering/projects/1">backlog items&lt;/a> and weigh in by filing an issue or creating a PR against their &lt;a href="https://github.com/kubernetes/steering">repo&lt;/a>. They have an open meeting on &lt;a href="https://github.com/kubernetes/steering">the first Monday of the month at 6pm UTC&lt;/a> and regularly attend Meet Our Contributors. They can also be contacted at their public mailing list &lt;a href="mailto:steering@kubernetes.io">steering@kubernetes.io&lt;/a>.&lt;/p>
&lt;p>You can see what the Steering Committee meetings are all about by watching past meetings on the &lt;a href="https://www.youtube.com/playlist?list=PL69nYSiGNLP1yP1B_nd9-drjoxp0Q14qM">YouTube Playlist&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>This post was written by the &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contributor-marketing">Upstream Marketing Working Group&lt;/a>. If you want to write stories about the Kubernetes community, learn more about us.&lt;/em>&lt;/p></description></item><item><title>Blog: Contributing to the Development Guide</title><link>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</link><pubDate>Thu, 01 Oct 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/</guid><description>
&lt;p>When most people think of contributing to an open source project, I suspect they probably think of
contributing code changes, new features, and bug fixes. As a software engineer and a long-time open
source user and contributor, that's certainly what I thought. Although I have written a good quantity
of documentation in different workflows, the massive size of the Kubernetes community was a new kind
of &amp;quot;client.&amp;quot; I just didn't know what to expect when Google asked my compatriots and me at
&lt;a href="https://lionswaycontent.com/">Lion's Way&lt;/a> to make much-needed updates to the Kubernetes Development Guide.&lt;/p>
&lt;p>&lt;em>This article originally appeared on the &lt;a href="https://www.kubernetes.dev/blog/2020/09/28/contributing-to-the-development-guide/">Kubernetes Contributor Community blog&lt;/a>.&lt;/em>&lt;/p>
&lt;h2 id="the-delights-of-working-with-a-community">The Delights of Working With a Community&lt;/h2>
&lt;p>As professional writers, we are used to being hired to write very specific pieces. We specialize in
marketing, training, and documentation for technical services and products, which can range anywhere from relatively fluffy marketing emails to deeply technical white papers targeted at IT and developers. With
this kind of professional service, every deliverable tends to have a measurable return on investment.
I knew this metric wouldn't be present when working on open source documentation, but I couldn't
predict how it would change my relationship with the project.&lt;/p>
&lt;p>One of the primary traits of the relationship between our writing and our traditional clients is that we
always have one or two primary points of contact inside a company. These contacts are responsible
for reviewing our writing and making sure it matches the voice of the company and targets the
audience they're looking for. It can be stressful -- which is why I'm so glad that my writing
partner, eagle-eyed reviewer, and bloodthirsty editor &lt;a href="https://twitter.com/JoelByronBarker">Joel&lt;/a>
handles most of the client contact.&lt;/p>
&lt;p>I was surprised and delighted that all of the stress of client contact went out the window when
working with the Kubernetes community.&lt;/p>
&lt;p>&amp;quot;How delicate do I have to be? What if I screw up? What if I make a developer angry? What if I make
enemies?&amp;quot; These were all questions that raced through my mind and made me feel like I was
approaching a field of eggshells when I first joined the &lt;code>#sig-contribex&lt;/code> channel on the Kubernetes
Slack and announced that I would be working on the
&lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/development.md">Development Guide&lt;/a>.&lt;/p>
&lt;div class="card rounded p-2 td-post-card mb-4 mt-4" style="max-width: 810px">
&lt;img class="card-img-top" src="https://kubernetes.io/blog/2020/10/01/contributing-to-the-development-guide/jorge-castro-code-of-conduct_hu5bc3c30874931ced96ecf71d135c93d2_143155_800x450_fit_q75_catmullrom.jpg" width="800" height="450">
&lt;div class="card-body px-0 pt-2 pb-0">
&lt;p class="card-text">
"The Kubernetes Code of Conduct is in effect, so please be excellent to each other." &amp;mdash; Jorge
Castro, SIG ContribEx co-chair
&lt;/p>
&lt;/div>
&lt;/div>
&lt;p>My fears were unfounded. Immediately, I felt welcome. I like to think this isn't just because I was
working on a much needed task, but rather because the Kubernetes community is filled
with friendly, welcoming people. During the weekly SIG ContribEx meetings, our reports on progress
with the Development Guide were included immediately. In addition, the leader of the meeting would
always stress that the &lt;a href="https://www.kubernetes.dev/resources/code-of-conduct/">Kubernetes Code of Conduct&lt;/a> was in
effect, and that we should, like Bill and Ted, be excellent to each other.&lt;/p>
&lt;h2 id="this-doesn-t-mean-it-s-all-easy">This Doesn't Mean It's All Easy&lt;/h2>
&lt;p>The Development Guide needed a pretty serious overhaul. When we got our hands on it, it was already
packed with information and lots of steps for new developers to go through, but it was getting dusty
with age and neglect. Documentation can really require a global look, not just point fixes.
As a result, I ended up submitting a gargantuan pull request to the
&lt;a href="https://github.com/kubernetes/community">Community repo&lt;/a>: 267 additions and 88 deletions.&lt;/p>
&lt;p>The life cycle of a pull request requires a certain number of Kubernetes organization members to review and approve changes
before they can be merged. This is a great practice, as it keeps both documentation and code in
pretty good shape, but it can be tough to cajole the right people into taking the time for such a hefty
review. As a result, that massive PR took 26 days from my first submission to final merge. But in
the end, &lt;a href="https://github.com/kubernetes/community/pull/5003">it was successful&lt;/a>.&lt;/p>
&lt;p>Since Kubernetes is a pretty fast-moving project, and since developers typically aren't really
excited about writing documentation, I also ran into the problem that sometimes, the secret jewels
that describe the workings of a Kubernetes subsystem are buried deep within the &lt;a href="https://github.com/amwat">labyrinthine mind of
a brilliant engineer&lt;/a>, and not in plain English in a Markdown file. I ran headlong into this issue
when it came time to update the getting started documentation for end-to-end (e2e) testing.&lt;/p>
&lt;p>This portion of my journey took me out of documentation-writing territory and into the role of a
brand new user of some unfinished software. I ended up working with one of the developers of the new
&lt;a href="https://github.com/kubernetes-sigs/kubetest2">&lt;code>kubetest2&lt;/code> framework&lt;/a> to document the latest process of
getting up-and-running for e2e testing, but it required a lot of head scratching on my part. You can
judge the results for yourself by checking out my
&lt;a href="https://github.com/kubernetes/community/pull/5045">completed pull request&lt;/a>.&lt;/p>
&lt;h2 id="nobody-is-the-boss-and-everybody-gives-feedback">Nobody Is the Boss, and Everybody Gives Feedback&lt;/h2>
&lt;p>But while I secretly expected chaos, the process of contributing to the Kubernetes Development Guide
and interacting with the amazing Kubernetes community went incredibly smoothly. There was no
contention. I made no enemies. Everybody was incredibly friendly and welcoming. It was &lt;em>enjoyable&lt;/em>.&lt;/p>
&lt;p>With an open source project, there is no one boss. The Kubernetes project, which approaches being
gargantuan, is split into many different special interest groups (SIGs), working groups, and
communities. Each has its own regularly scheduled meetings, assigned duties, and elected
chairpersons. My work intersected with the efforts of both SIG ContribEx (who watch over and seek to
improve the contributor experience) and SIG Testing (who are in charge of testing). Both of these
SIGs proved easy to work with, eager for contributions, and populated with incredibly friendly and
welcoming people.&lt;/p>
&lt;p>In an active, living project like Kubernetes, documentation continues to need maintenance, revision,
and testing alongside the code base. The Development Guide will continue to be crucial to onboarding
new contributors to the Kubernetes code base, and as our efforts have shown, it is important that
this guide keeps pace with the evolution of the Kubernetes project.&lt;/p>
&lt;p>Joel and I really enjoy interacting with the Kubernetes community and contributing to
the Development Guide. I really look forward to continuing to not only contributing more, but to
continuing to build the new friendships I've made in this vast open source community over the past
few months.&lt;/p></description></item><item><title>Blog: GSoC 2020 - Building operators for cluster addons</title><link>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</link><pubDate>Wed, 16 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/16/gsoc20-building-operators-for-cluster-addons/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Somtochi Onyekwere&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>&lt;a href="https://summerofcode.withgoogle.com/">Google Summer of Code&lt;/a> is a global program that is geared towards introducing students to open source. Students are matched with open-source organizations to work with them for three months during the summer.&lt;/p>
&lt;p>My name is Somtochi Onyekwere from the Federal University of Technology, Owerri (Nigeria) and this year, I was given the opportunity to work with Kubernetes (under the CNCF organization) and this led to an amazing summer spent learning, contributing and interacting with the community.&lt;/p>
&lt;p>Specifically, I worked on the &lt;em>Cluster Addons: Package all the things!&lt;/em> project. The project focused on building operators for better management of various cluster addons, extending the tooling for building these operators and making the creation of these operators a smooth process.&lt;/p>
&lt;h1 id="background">Background&lt;/h1>
&lt;p>Kubernetes has progressed greatly in the past few years with a flourishing community and a large number of contributors. The codebase is gradually moving away from the monolith structure where all the code resides in the &lt;a href="https://github.com/kubernetes/kubernetes">kubernetes/kubernetes&lt;/a> repository to being split into multiple sub-projects. Part of the focus of cluster-addons is to make some of these sub-projects work together in an easy to assemble, self-monitoring, self-healing and Kubernetes-native way. It enables them to work seamlessly without human intervention.&lt;/p>
&lt;p>The community is exploring the use of operators as a mechanism to monitor various resources in the cluster and properly manage these resources. In addition to this, it provides self-healing and it is a kubernetes-native pattern that can encode how best these addons work and manage them properly.&lt;/p>
&lt;p>What are cluster addons? Cluster addons are a collection of resources (like Services and deployment) that are used to give a Kubernetes cluster additional functionalities. They range from things as simple as the Kubernetes dashboards (for visualization) to more complex ones like Calico (for networking). These addons are essential to different applications running in the cluster and the cluster itself. The addon operator provides a nicer way of managing these addons and understanding the health and status of the various resources that comprise the addon. You can get a deeper overview in this &lt;a href="https://kubernetes.io/docs/concepts/overview/components/#addons">article&lt;/a>.&lt;/p>
&lt;p>Operators are custom controllers with custom resource definitions that encode application-specific knowledge and are used for managing complex stateful applications. It is a widely accepted pattern. Managing addons via operators, with these operators encoding knowledge of how best the addons work, introduces a lot of advantages while setting standards that will be easy to follow and scale. This &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator">article&lt;/a> does a good job of explaining operators.&lt;/p>
&lt;p>The addon operators can solve a lot of problems, but they have their challenges. Those under the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons project&lt;/a> had missing pieces and were still a proof of concept. Generating the RBAC configuration for the operators was a pain and sometimes the operators were given too much privilege. The operators weren’t very extensible as it only pulled manifests from local filesystems or HTTP(s) servers and a lot of simple addons were generating the same code.
I spent the summer working on these issues, looking at them with fresh eyes and coming up with solutions for both the known and unknown issues.&lt;/p>
&lt;h1 id="various-additions-to-kubebuilder-declarative-pattern">Various additions to kubebuilder-declarative-pattern&lt;/h1>
&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern">kubebuilder-declarative-pattern&lt;/a> (from here on referred to as KDP) repo is an extra layer of addon specific tooling on top of the &lt;a href="https://github.com/kubernetes-sigs/kubebuilder">kubebuilder&lt;/a> SDK that is enabled by passing the experimental &lt;code>--pattern=addon&lt;/code> flag to &lt;code>kubebuilder create&lt;/code> command. Together, they create the base code for the addon operator. During the internship, I worked on a couple of features in KDP and cluster-addons.&lt;/p>
&lt;h2 id="operator-version-checking">Operator version checking&lt;/h2>
&lt;p>Enabling version checks for operators helped in making upgrades/downgrades safer to different versions of the addon, even though the operator had complex logic. It is a way of matching the version of an addon to the version of the operator that knows how to manage it well. Most addons have different versions and these versions might need to be managed differently. This feature checks the custom resource for the &lt;code>addons.k8s.io/min-operator-version&lt;/code> annotation which states the minimum operator version that is needed to manage the version against the version of the operator. If the operator version is below the minimum version required, the operator pauses with an error telling the user that the version of the operator is too low. This helps to ensure that the correct operator is being used for the addon.&lt;/p>
&lt;h2 id="git-repository-for-storing-the-manifests">Git repository for storing the manifests&lt;/h2>
&lt;p>Previously, there was support for only local file directories and HTTPS repositories for storing manifests. Giving creators of addon operators the ability to store manifest in GitHub repository enables faster development and version control. When starting the controller, you can pass a flag to specify the location of your channels directory. The channels directory contains the manifests for different versions, the controller pulls the manifest from this directory and applies it to the cluster. During the internship period, I extended it to include Git repositories.&lt;/p>
&lt;h2 id="annotations-to-temporarily-disable-reconciliation">Annotations to temporarily disable reconciliation&lt;/h2>
&lt;p>The reconciliation loop that ensures that the desired state matches the actual state prevents modification of objects in the cluster. This makes it hard to experiment or investigate what might be wrong in the cluster as any changes made are promptly reverted. I resolved this by allowing users to place an &lt;code>addons.k8s.io/ignore&lt;/code> annotation on the resource that they don’t want the controller to reconcile. The controller checks for this annotation and doesn’t reconcile that object. To resume reconciliation, the annotation can be removed from the resource.&lt;/p>
&lt;h2 id="unstructured-support-in-kubebuilder-declarative-pattern">Unstructured support in kubebuilder-declarative-pattern&lt;/h2>
&lt;p>One of the operators that I worked on is a generic controller that could manage more than one cluster addon that did not require extra configuration. To do this, the operator couldn’t use a particular type and needed the kubebuilder-declarative-repo to support using the &lt;a href="https://godoc.org/k8s.io/apimachinery/pkg/apis/meta/v1/unstructured#Unstructured">unstructured.Unstructured&lt;/a> type. There were various functions in the kubebuilder-declarative-pattern that couldn’t handle this type and returned an error if the object passed in was not of type &lt;code>addonsv1alpha1.CommonObject&lt;/code>. The functions were modified to handle both &lt;code>unstructured.Unstructured&lt;/code> and &lt;code>addonsv1alpha.CommonObject&lt;/code>.&lt;/p>
&lt;h1 id="tools-and-cli-programs">Tools and CLI programs&lt;/h1>
&lt;p>There were also some command-line programs I wrote that could be used to make working with addon operators easier. Most of them have uses outside the addon operators as they try to solve a specific problem that could surface anywhere while working with Kubernetes. I encourage you to &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools">check them out&lt;/a> when you have the chance!&lt;/p>
&lt;h2 id="rbac-generator">RBAC Generator&lt;/h2>
&lt;p>One of the biggest concerns with the operator was RBAC. You had to manually look through the manifest and add the RBAC rule for each resource as it needs to have RBAC permissions to create, get, update and delete the resources in the manifest when running in-cluster. Building the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">RBAC generator&lt;/a> automated the process of writing the RBAC roles and role bindings. The function of the RBAC generator is simple. It accepts the file name of the manifest as a flag. Then, it parses the manifest and gets the API group and resource name of the resources and adds it to a role. It outputs the role and role binding to stdout or a file if the &lt;code>--out&lt;/code> flag is parsed.&lt;/p>
&lt;p>Additionally, the tool enables you to split the RBAC by separating the cluster roles in the manifest. This lessened the security concern of an operator being over-privileged as it needed to have all the permissions that the clusterrole has. If you want to apply the clusterrole yourself and not give the operator these permissions, you can pass in a &lt;code>--supervisory&lt;/code> boolean flag so that the generator does not add these permissions to the role. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/rbac-gen">here&lt;/a>.&lt;/p>
&lt;h2 id="kubectl-ownerref">Kubectl Ownerref&lt;/h2>
&lt;p>It is hard to find out at a glance which objects were created by an addon custom resource. This kubectl plugin alleviates that pain by displaying all the objects in the cluster that a resource has ownerrefs on. You simply pass the kind and the name of the resource as arguments to the program and it checks the cluster for the objects and gives the kind, name, the namespace of such an object. It could be useful to get a general overview of all the objects that the controller is reconciling by passing in the name and kind of custom resource. The CLI program resides &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/tree/master/tools/kubectl-ownerref">here&lt;/a>.&lt;/p>
&lt;h1 id="addon-operators">Addon Operators&lt;/h1>
&lt;p>To fully understand addons operators and make changes to how they are being created, you have to try creating and using them. Part of the summer was spent building operators for some popular addons like the Kubernetes dashboard, flannel, NodeLocalDNS and so on. Please check the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons">cluster-addons&lt;/a> repository for the different addon operators. In this section, I will just highlight one that is a little different from the others.&lt;/p>
&lt;h2 id="generic-controller">Generic Controller&lt;/h2>
&lt;p>The generic controller can be shared between addons that don’t require much configuration. This minimizes resource consumption on the cluster as it reduces the number of controllers that need to be run. Also instead of building your own operator, you can just use the generic controller and whenever you feel that your needs have grown and you need a more complex operator, you can always scaffold the code with kubebuilder and continue from where the generic operator stopped. To use the generic controller, you can generate the CustomResourceDefinition(CRD) using this tool (&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">generic-addon&lt;/a>). You pass in the kind, group, and the location of your channels directory (it could be a Git repository too!). The tool generates the - CRD, RBAC manifest and two custom resources for you.&lt;/p>
&lt;p>The process is as follows:&lt;/p>
&lt;ul>
&lt;li>Create the Generic CRD&lt;/li>
&lt;li>Generate all the manifests needed with the &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/tools/generic-addon/README.md">&lt;code>generic-addon tool&lt;/code>&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>This tool creates:&lt;/p>
&lt;ol>
&lt;li>The CRD for your addon&lt;/li>
&lt;li>The RBAC rules for the CustomResourceDefinitions&lt;/li>
&lt;li>The RBAC rules for applying the manifests&lt;/li>
&lt;li>The custom resource for your addon&lt;/li>
&lt;li>A Generic custom resource&lt;/li>
&lt;/ol>
&lt;p>The Generic custom resource looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io/v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Generic&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>generic-sample&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">objectKind&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NodeLocalDNS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1alpha1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">group&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>addons.x-k8s.io&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">channel&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;../nodelocaldns/channels&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Apply these manifests but ensure to apply the CRD before the CR.
Then, run the Generic controller, either on your machine or in-cluster.&lt;/p>
&lt;p>If you are interested in building an operator, Please check out &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">this guide&lt;/a>.&lt;/p>
&lt;h1 id="relevant-links">Relevant Links&lt;/h1>
&lt;ul>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s">Detailed breakdown of work done during the internship&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cluster-lifecycle/addons/0035-20190128-addons-via-operators.md">Addon Operator (KEP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/issues/39">Original GSoC Issue&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/SomtochiAma/gsoc-2020-meta-k8s/blob/master/GSoC%202020%20PROPOSAL%20-%20PACKAGE%20ALL%20THINGS.pdf">Proposal Submitted for GSoC&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/cluster-addons/commits?author=SomtochiAma">All commits to kubernetes-sigs/cluster-addons&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes-sigs/kubebuilder-declarative-pattern/commits?author=SomtochiAma">All commits to kubernetes-sigs/kubebuidler-declarative-pattern&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="further-work">Further Work&lt;/h1>
&lt;p>A lot of work was definitely done on the cluster addons during the GSoC period. But we need more people building operators and using them in the cluster. We need wider adoption in the community. Build operators for your favourite addons and tell us how it went and if you had any issues. Check out this &lt;a href="https://github.com/kubernetes-sigs/cluster-addons/blob/master/dashboard/README.md">README.md&lt;/a> to get started.&lt;/p>
&lt;h1 id="appreciation">Appreciation&lt;/h1>
&lt;p>I really want to appreciate my mentors &lt;a href="https://github.com/justinsb">Justin Santa Barbara&lt;/a> (Google) and &lt;a href="https://github.com/stealthybox">Leigh Capili&lt;/a> (Weaveworks). My internship was awesome because they were awesome. They set a golden standard for what mentorship should be. They were accessible and always available to clear any confusion. I think what I liked best was that they didn’t just dish out tasks, instead, we had open discussions about what was wrong and what could be improved. They are really the best and I hope I get to work with them again!
Also, I want to say a huge thanks to &lt;a href="https://github.com/neolit123">Lubomir I. Ivanov&lt;/a> for reviewing this blog post!&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>So far I have learnt a lot about Go, the internals of Kubernetes, and operators. I want to conclude by encouraging people to contribute to open-source (especially Kubernetes :)) regardless of your level of experience. It has been a well-rounded experience for me and I have come to love the community. It is a great initiative and it is a great way to learn and meet awesome people. Special shoutout to Google for organizing this program.&lt;/p>
&lt;p>If you are interested in cluster addons and finding out more on addon operators, you are welcome to join our slack channel on the Kubernetes &lt;a href="https://kubernetes.slack.com/messages/cluster-addons">#cluster-addons&lt;/a>.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/SomtochiAma">Somtochi Onyekwere&lt;/a> is a software engineer that loves contributing to open-source and exploring cloud native solutions.&lt;/em>&lt;/p></description></item><item><title>Blog: Introducing Structured Logs</title><link>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</link><pubDate>Fri, 04 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/04/kubernetes-1-19-introducing-structured-logs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Marek Siarkowicz (Google), Nathan Beach (Google)&lt;/p>
&lt;p>Logs are an essential aspect of observability and a critical tool for debugging. But Kubernetes logs have traditionally been unstructured strings, making any automated parsing difficult and any downstream processing, analysis, or querying challenging to do reliably.&lt;/p>
&lt;p>In Kubernetes 1.19, we are adding support for structured logs, which natively support (key, value) pairs and object references. We have also updated many logging calls such that over 99% of logging volume in a typical deployment are now migrated to the structured format.&lt;/p>
&lt;p>To maintain backwards compatibility, structured logs will still be outputted as a string where the string contains representations of those &amp;quot;key&amp;quot;=&amp;quot;value&amp;quot; pairs. Starting in alpha in 1.19, logs can also be outputted in JSON format using the &lt;code>--logging-format=json&lt;/code> flag.&lt;/p>
&lt;h2 id="using-structured-logs">Using Structured Logs&lt;/h2>
&lt;p>We've added two new methods to the klog library: InfoS and ErrorS. For example, this invocation of InfoS:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-golang" data-lang="golang">klog.&lt;span style="color:#00a000">InfoS&lt;/span>(&lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>, &lt;span style="color:#b44">&amp;#34;pod&amp;#34;&lt;/span>, klog.&lt;span style="color:#00a000">KObj&lt;/span>(pod), &lt;span style="color:#b44">&amp;#34;status&amp;#34;&lt;/span>, status)
&lt;/code>&lt;/pre>&lt;/div>&lt;p>will result in this log:&lt;/p>
&lt;pre>&lt;code>I1025 00:15:15.525108 1 controller_utils.go:116] &amp;quot;Pod status updated&amp;quot; pod=&amp;quot;kube-system/kubedns&amp;quot; status=&amp;quot;ready&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Or, if the --logging-format=json flag is set, it will result in this output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;ts&amp;#34;&lt;/span>: &lt;span style="color:#666">1580306777.04728&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;msg&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;Pod status updated&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;pod&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;name&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;coredns&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kube-system&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;status&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ready&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This means downstream logging tools can easily ingest structured logging data and instead of using regular expressions to parse unstructured strings. This also makes processing logs easier, querying logs more robust, and analyzing logs much faster.&lt;/p>
&lt;p>With structured logs, all references to Kubernetes objects are structured the same way, so you can filter the output and only log entries referencing the particular pod. You can also find logs indicating how the scheduler was scheduling the pod, how the pod was created, the health probes of the pod, and all other changes in the lifecycle of the pod.&lt;/p>
&lt;p>Suppose you are debugging an issue with a pod. With structured logs, you can filter to only those log entries referencing the pod of interest, rather than needing to scan through potentially thousands of log lines to find the relevant ones.&lt;/p>
&lt;p>Not only are structured logs more useful when manual debugging of issues, they also enable richer features like automated pattern recognition within logs or tighter correlation of log and trace data.&lt;/p>
&lt;p>Finally, structured logs can help reduce storage costs for logs because most storage systems are more efficiently able to compress structured key=value data than unstructured strings.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>While we have updated over 99% of the log entries by log volume in a typical deployment, there are still thousands of logs to be updated. Pick a file or directory that you would like to improve and &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/devel/sig-instrumentation/migration-to-structured-logging.md">migrate existing log calls to use structured logs&lt;/a>. It's a great and easy way to make your first contribution to Kubernetes!&lt;/p></description></item><item><title>Blog: Warning: Helpful Warnings Ahead</title><link>https://kubernetes.io/blog/2020/09/03/warnings/</link><pubDate>Thu, 03 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/03/warnings/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Jordan Liggitt (Google)&lt;/p>
&lt;p>As Kubernetes maintainers, we're always looking for ways to improve usability while preserving compatibility.
As we develop features, triage bugs, and answer support questions, we accumulate information that would be helpful for Kubernetes users to know.
In the past, sharing that information was limited to out-of-band methods like release notes, announcement emails, documentation, and blog posts.
Unless someone knew to seek out that information and managed to find it, they would not benefit from it.&lt;/p>
&lt;p>In Kubernetes v1.19, we added a feature that allows the Kubernetes API server to
&lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps/sig-api-machinery/1693-warnings">send warnings to API clients&lt;/a>.
The warning is sent using a &lt;a href="https://tools.ietf.org/html/rfc7234#section-5.5">standard &lt;code>Warning&lt;/code> response header&lt;/a>,
so it does not change the status code or response body in any way.
This allows the server to send warnings easily readable by any API client, while remaining compatible with previous client versions.&lt;/p>
&lt;p>Warnings are surfaced by &lt;code>kubectl&lt;/code> v1.19+ in &lt;code>stderr&lt;/code> output, and by the &lt;code>k8s.io/client-go&lt;/code> client library v0.19.0+ in log output.
The &lt;code>k8s.io/client-go&lt;/code> behavior can be &lt;a href="#customize-client-handling">overridden per-process or per-client&lt;/a>.&lt;/p>
&lt;h2 id="deprecation-warnings">Deprecation Warnings&lt;/h2>
&lt;p>The first way we are using this new capability is to send warnings for use of deprecated APIs.&lt;/p>
&lt;p>Kubernetes is a &lt;a href="https://www.cncf.io/cncf-kubernetes-project-journey/#development-velocity">big, fast-moving project&lt;/a>.
Keeping up with the &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#changelog-since-v1180">changes&lt;/a>
in each release can be daunting, even for people who work on the project full-time. One important type of change is API deprecations.
As APIs in Kubernetes graduate to GA versions, pre-release API versions are deprecated and eventually removed.&lt;/p>
&lt;p>Even though there is an &lt;a href="https://kubernetes.io/docs/reference/using-api/deprecation-policy/">extended deprecation period&lt;/a>,
and deprecations are &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md#deprecation">included in release notes&lt;/a>,
they can still be hard to track. During the deprecation period, the pre-release API remains functional,
allowing several releases to transition to the stable API version. However, we have found that users often don't even realize
they are depending on a deprecated API version until they upgrade to the release that stops serving it.&lt;/p>
&lt;p>Starting in v1.19, whenever a request is made to a deprecated REST API, a warning is returned along with the API response.
This warning includes details about the release in which the API will no longer be available, and the replacement API version.&lt;/p>
&lt;p>Because the warning originates at the server, and is intercepted at the client level, it works for all kubectl commands,
including high-level commands like &lt;code>kubectl apply&lt;/code>, and low-level commands like &lt;code>kubectl get --raw&lt;/code>:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file, then displaying a warning message 'networking.k8s.io/v1beta1 Ingress is deprecated in v1.19+, unavailable in v1.22+; use networking.k8s.io/v1 Ingress'."
src="kubectl-warnings.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This helps people affected by the deprecation to know the request they are making is deprecated,
how long they have to address the issue, and what API they should use instead.
This is especially helpful when the user is applying a manifest they didn't create,
so they have time to reach out to the authors to ask for an updated version.&lt;/p>
&lt;p>We also realized that the person &lt;em>using&lt;/em> a deprecated API is often not the same person responsible for upgrading the cluster,
so we added two administrator-facing tools to help track use of deprecated APIs and determine when upgrades are safe.&lt;/p>
&lt;h3 id="metrics">Metrics&lt;/h3>
&lt;p>Starting in Kubernetes v1.19, when a request is made to a deprecated REST API endpoint,
an &lt;code>apiserver_requested_deprecated_apis&lt;/code> gauge metric is set to &lt;code>1&lt;/code> in the kube-apiserver process.
This metric has labels for the API &lt;code>group&lt;/code>, &lt;code>version&lt;/code>, &lt;code>resource&lt;/code>, and &lt;code>subresource&lt;/code>,
and a &lt;code>removed_version&lt;/code> label that indicates the Kubernetes release in which the API will no longer be served.&lt;/p>
&lt;p>This is an example query using &lt;code>kubectl&lt;/code>, &lt;a href="https://github.com/prometheus/prom2json">prom2json&lt;/a>,
and &lt;a href="https://stedolan.github.io/jq/">jq&lt;/a> to determine which deprecated APIs have been requested
from the current instance of the API server:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;removed_release&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1.22&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This shows the deprecated &lt;code>extensions/v1beta1&lt;/code> Ingress and &lt;code>rbac.authorization.k8s.io/v1beta1&lt;/code> ClusterRole APIs
have been requested on this server, and will be removed in v1.22.&lt;/p>
&lt;p>We can join that information with the &lt;code>apiserver_request_total&lt;/code> metrics to get more details about the requests being made to these APIs:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl get --raw /metrics | prom2json | jq &lt;span style="color:#b44">&amp;#39;
&lt;/span>&lt;span style="color:#b44"> # set $deprecated to a list of deprecated APIs
&lt;/span>&lt;span style="color:#b44"> [
&lt;/span>&lt;span style="color:#b44"> .[] |
&lt;/span>&lt;span style="color:#b44"> select(.name==&amp;#34;apiserver_requested_deprecated_apis&amp;#34;).metrics[].labels |
&lt;/span>&lt;span style="color:#b44"> {group,version,resource}
&lt;/span>&lt;span style="color:#b44"> ] as $deprecated
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> |
&lt;/span>&lt;span style="color:#b44">
&lt;/span>&lt;span style="color:#b44"> # select apiserver_request_total metrics which are deprecated
&lt;/span>&lt;span style="color:#b44"> .[] | select(.name==&amp;#34;apiserver_request_total&amp;#34;).metrics[] |
&lt;/span>&lt;span style="color:#b44"> select(.labels | {group,version,resource} as $key | $deprecated | index($key))
&lt;/span>&lt;span style="color:#b44">&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Output:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;0&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf;stream=watch&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;WATCH&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;21&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/vnd.kubernetes.protobuf&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;extensions&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;ingresses&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;labels&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;code&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;200&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;component&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;apiserver&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;contentType&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;application/json&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;dry_run&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;group&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;rbac.authorization.k8s.io&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;resource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;clusterroles&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;scope&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;cluster&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;subresource&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;verb&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;LIST&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;version&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;v1beta1&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;value&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The output shows that only read requests are being made to these APIs, and the most requests have been made to watch the deprecated Ingress API.&lt;/p>
&lt;p>You can also find that information through the following Prometheus query,
which returns information about requests made to deprecated APIs which will be removed in v1.22:&lt;/p>
&lt;pre>&lt;code class="language-promql" data-lang="promql">apiserver_requested_deprecated_apis{removed_version=&amp;quot;1.22&amp;quot;} * on(group,version,resource,subresource)
group_right() apiserver_request_total
&lt;/code>&lt;/pre>&lt;h3 id="audit-annotations">Audit Annotations&lt;/h3>
&lt;p>Metrics are a fast way to check whether deprecated APIs are being used, and at what rate,
but they don't include enough information to identify particular clients or API objects.
Starting in Kubernetes v1.19, &lt;a href="https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">audit events&lt;/a>
for requests to deprecated APIs include an audit annotation of &lt;code>&amp;quot;k8s.io/deprecated&amp;quot;:&amp;quot;true&amp;quot;&lt;/code>.
Administrators can use those audit events to identify specific clients or objects that need to be updated.&lt;/p>
&lt;h2 id="custom-resource-definitions">Custom Resource Definitions&lt;/h2>
&lt;p>Along with the API server ability to warn about deprecated API use, starting in v1.19, a CustomResourceDefinition can indicate a
&lt;a href="https://kubernetes.io/docs/tasks/extend-kubernetes/custom-resources/custom-resource-definition-versioning/#version-deprecation">particular version of the resource it defines is deprecated&lt;/a>.
When API requests to a deprecated version of a custom resource are made, a warning message is returned, matching the behavior of built-in APIs.&lt;/p>
&lt;p>The author of the CustomResourceDefinition can also customize the warning for each version if they want to.
This allows them to give a pointer to a migration guide or other information if needed.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apiextensions.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CustomResourceDefinition&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>crontabs.example.com&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">versions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1alpha1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1alpha1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This overrides the default warning returned to clients making v1alpha1 API requests.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">deprecationWarning&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/v1alpha1 CronTab is deprecated; use example.com/v1 CronTab (see http://example.com/v1alpha1-v1)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># This indicates the v1beta1 version of the custom resource is deprecated.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># API requests to this version receive a warning in the server response.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># A default warning message is returned for this version.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">deprecated&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>...&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="admission-webhooks">Admission Webhooks&lt;/h2>
&lt;p>&lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers">Admission webhooks&lt;/a>
are the primary way to integrate custom policies or validation with Kubernetes.
Starting in v1.19, admission webhooks can &lt;a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/#response">return warning messages&lt;/a>
that are passed along to the requesting API client. Warnings can be returned with allowed or rejected admission responses.&lt;/p>
&lt;p>As an example, to allow a request but warn about a configuration known not to work well, an admission webhook could send this response:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;apiVersion&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;admission.k8s.io/v1&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kind&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;AdmissionReview&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;response&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;&amp;lt;value from request.uid&amp;gt;&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;allowed&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;warnings&amp;#34;&lt;/span>: [
&lt;span style="color:#b44">&amp;#34;.spec.memory: requests &amp;gt;1GB do not work on Fridays&amp;#34;&lt;/span>
]
}
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>If you are implementing a webhook that returns a warning message, here are some tips:&lt;/p>
&lt;ul>
&lt;li>Don't include a &amp;quot;Warning:&amp;quot; prefix in the message (that is added by clients on output)&lt;/li>
&lt;li>Use warning messages to describe problems the client making the API request should correct or be aware of&lt;/li>
&lt;li>Be brief; limit warnings to 120 characters if possible&lt;/li>
&lt;/ul>
&lt;p>There are many ways admission webhooks could use this new feature, and I'm looking forward to seeing what people come up with.
Here are a couple ideas to get you started:&lt;/p>
&lt;ul>
&lt;li>webhook implementations adding a &amp;quot;complain&amp;quot; mode, where they return warnings instead of rejections,
to allow trying out a policy to verify it is working as expected before starting to enforce it&lt;/li>
&lt;li>&amp;quot;lint&amp;quot; or &amp;quot;vet&amp;quot;-style webhooks, inspecting objects and surfacing warnings when best practices are not followed&lt;/li>
&lt;/ul>
&lt;h2 id="customize-client-handling">Customize Client Handling&lt;/h2>
&lt;p>Applications that use the &lt;code>k8s.io/client-go&lt;/code> library to make API requests can customize
how warnings returned from the server are handled. By default, warnings are logged to
stderr as they are received, but this behavior can be customized
&lt;a href="https://godoc.org/k8s.io/client-go/rest#SetDefaultWarningHandler">per-process&lt;/a>
or &lt;a href="https://godoc.org/k8s.io/client-go/rest#Config">per-client&lt;/a>.&lt;/p>
&lt;p>This example shows how to make your application behave like &lt;code>kubectl&lt;/code>,
overriding message handling process-wide to deduplicate warnings
and highlighting messages using colored output where supported:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;os&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/kubectl/pkg/util/term&amp;#34;&lt;/span>
&lt;span style="color:#666">...&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">main&lt;/span>() {
rest.&lt;span style="color:#00a000">SetDefaultWarningHandler&lt;/span>(
rest.&lt;span style="color:#00a000">NewWarningWriter&lt;/span>(os.Stderr, rest.WarningWriterOptions{
&lt;span style="color:#080;font-style:italic">// only print a given warning the first time we receive it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Deduplicate: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#080;font-style:italic">// highlight the output with color when the output supports it
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> Color: term.&lt;span style="color:#00a000">AllowsColorOutput&lt;/span>(os.Stderr),
},
),
)
&lt;span style="color:#666">...&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The next example shows how to construct a client that ignores warnings.
This is useful for clients that operate on metadata for all resource types
(found dynamically at runtime using the discovery API)
and do not benefit from warnings about a particular resource being deprecated.
Suppressing deprecation warnings is not recommended for clients that require use of particular APIs.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-go" data-lang="go">&lt;span style="color:#a2f;font-weight:bold">import&lt;/span> (
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/rest&amp;#34;&lt;/span>
&lt;span style="color:#b44">&amp;#34;k8s.io/client-go/kubernetes&amp;#34;&lt;/span>
)
&lt;span style="color:#a2f;font-weight:bold">func&lt;/span> &lt;span style="color:#00a000">getClientWithoutWarnings&lt;/span>(config &lt;span style="color:#666">*&lt;/span>rest.Config) (kubernetes.Interface, &lt;span style="color:#0b0;font-weight:bold">error&lt;/span>) {
&lt;span style="color:#080;font-style:italic">// copy to avoid mutating the passed-in config
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config = rest.&lt;span style="color:#00a000">CopyConfig&lt;/span>(config)
&lt;span style="color:#080;font-style:italic">// set the warning handler for this client to ignore warnings
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> config.WarningHandler = rest.NoWarnings{}
&lt;span style="color:#080;font-style:italic">// construct and return the client
&lt;/span>&lt;span style="color:#080;font-style:italic">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">return&lt;/span> kubernetes.&lt;span style="color:#00a000">NewForConfig&lt;/span>(config)
}
&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="kubectl-strict-mode">Kubectl Strict Mode&lt;/h2>
&lt;p>If you want to be sure you notice deprecations as soon as possible and get a jump start on addressing them,
&lt;code>kubectl&lt;/code> added a &lt;code>--warnings-as-errors&lt;/code> option in v1.19. When invoked with this option,
&lt;code>kubectl&lt;/code> treats any warnings it receives from the server as errors and exits with a non-zero exit code:&lt;/p>
&lt;p>&lt;img alt="kubectl applying a manifest file with a --warnings-as-errors flag, displaying a warning message and exiting with a non-zero exit code."
src="kubectl-warnings-as-errors.png"
style="width:637px;max-width:100%;">&lt;/p>
&lt;p>This could be used in a CI job to apply manifests to a current server,
and required to pass with a zero exit code in order for the CI job to succeed.&lt;/p>
&lt;h2 id="future-possibilities">Future Possibilities&lt;/h2>
&lt;p>Now that we have a way to communicate helpful information to users in context,
we're already considering other ways we can use this to improve people's experience with Kubernetes.
A couple areas we're looking at next are warning about &lt;a href="http://issue.k8s.io/64841#issuecomment-395141013">known problematic values&lt;/a>
we cannot reject outright for compatibility reasons, and warning about use of deprecated fields or field values
(like selectors using beta os/arch node labels, &lt;a href="https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/#beta-kubernetes-io-arch-deprecated">deprecated in v1.14&lt;/a>).
I'm excited to see progress in this area, continuing to make it easier to use Kubernetes.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/liggitt">Jordan Liggitt&lt;/a> is a software engineer at Google, and helps lead Kubernetes authentication, authorization, and API efforts.&lt;/em>&lt;/p></description></item><item><title>Blog: Scaling Kubernetes Networking With EndpointSlices</title><link>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</link><pubDate>Wed, 02 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/02/scaling-kubernetes-networking-with-endpointslices/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rob Scott (Google)&lt;/p>
&lt;p>EndpointSlices are an exciting new API that provides a scalable and extensible alternative to the Endpoints API. EndpointSlices track IP addresses, ports, readiness, and topology information for Pods backing a Service.&lt;/p>
&lt;p>In Kubernetes 1.19 this feature is enabled by default with kube-proxy reading from &lt;a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">EndpointSlices&lt;/a> instead of Endpoints. Although this will mostly be an invisible change, it should result in noticeable scalability improvements in large clusters. It also enables significant new features in future Kubernetes releases like &lt;a href="https://kubernetes.io/docs/concepts/services-networking/service-topology/">Topology Aware Routing&lt;/a>.&lt;/p>
&lt;h2 id="scalability-limitations-of-the-endpoints-api">Scalability Limitations of the Endpoints API&lt;/h2>
&lt;p>With the Endpoints API, there was only one Endpoints resource for a Service. That meant that it needed to be able to store IP addresses and ports (network endpoints) for every Pod that was backing the corresponding Service. This resulted in huge API resources. To compound this problem, kube-proxy was running on every node and watching for any updates to Endpoints resources. If even a single network endpoint changed in an Endpoints resource, the whole object would have to be sent to each of those instances of kube-proxy.&lt;/p>
&lt;p>A further limitation of the Endpoints API is that it limits the number of network endpoints that can be tracked for a Service. The default size limit for an object stored in etcd is 1.5MB. In some cases that can limit an Endpoints resource to 5,000 Pod IPs. This is not an issue for most users, but it becomes a significant problem for users with Services approaching this size.&lt;/p>
&lt;p>To show just how significant these issues become at scale it helps to have a simple example. Think about a Service which has 5,000 Pods, it might end up with a 1.5MB Endpoints resource. If even a single network endpoint in that list changes, the full Endpoints resource will need to be distributed to each Node in the cluster. This becomes quite an issue in a large cluster with 3,000 Nodes. Each update would involve sending 4.5GB of data (1.5MB Endpoints * 3,000 Nodes) across the cluster. That's nearly enough to fill up a DVD, and it would happen for each Endpoints change. Imagine a rolling update that results in all 5,000 Pods being replaced - that's more than 22TB (or 5,000 DVDs) worth of data transferred.&lt;/p>
&lt;h2 id="splitting-endpoints-up-with-the-endpointslice-api">Splitting endpoints up with the EndpointSlice API&lt;/h2>
&lt;p>The EndpointSlice API was designed to address this issue with an approach similar to sharding. Instead of tracking all Pod IPs for a Service with a single Endpoints resource, we split them into multiple smaller EndpointSlices.&lt;/p>
&lt;p>Consider an example where a Service is backed by 15 pods. We'd end up with a single Endpoints resource that tracked all of them. If EndpointSlices were configured to store 5 endpoints each, we'd end up with 3 different EndpointSlices:
&lt;img src="https://kubernetes.io/images/blog/2020-09-02-scaling-kubernetes-networking-endpointslices/endpoint-slices.png" alt="EndpointSlices">&lt;/p>
&lt;p>By default, EndpointSlices store as many as 100 endpoints each, though this can be configured with the &lt;code>--max-endpoints-per-slice&lt;/code> flag on kube-controller-manager.&lt;/p>
&lt;h2 id="endpointslices-provide-10x-scalability-improvements">EndpointSlices provide 10x scalability improvements&lt;/h2>
&lt;p>This API dramatically improves networking scalability. Now when a Pod is added or removed, only 1 small EndpointSlice needs to be updated. This difference becomes quite noticeable when hundreds or thousands of Pods are backing a single Service.&lt;/p>
&lt;p>Potentially more significant, now that all Pod IPs for a Service don't need to be stored in a single resource, we don't have to worry about the size limit for objects stored in etcd. EndpointSlices have already been used to scale Services beyond 100,000 network endpoints.&lt;/p>
&lt;p>All of this is brought together with some significant performance improvements that have been made in kube-proxy. When using EndpointSlices at scale, significantly less data will be transferred for endpoints updates and kube-proxy should be faster to update iptables or ipvs rules. Beyond that, Services can now scale to at least 10 times beyond any previous limitations.&lt;/p>
&lt;h2 id="endpointslices-enable-new-functionality">EndpointSlices enable new functionality&lt;/h2>
&lt;p>Introduced as an alpha feature in Kubernetes v1.16, EndpointSlices were built to enable some exciting new functionality in future Kubernetes releases. This could include dual-stack Services, topology aware routing, and endpoint subsetting.&lt;/p>
&lt;p>Dual-Stack Services are an exciting new feature that has been in development alongside EndpointSlices. They will utilize both IPv4 and IPv6 addresses for Services and rely on the addressType field on EndpointSlices to track these addresses by IP family.&lt;/p>
&lt;p>Topology aware routing will update kube-proxy to prefer routing requests within the same zone or region. This makes use of the topology fields stored for each endpoint in an EndpointSlice. As a further refinement of that, we're exploring the potential of endpoint subsetting. This would allow kube-proxy to only watch a subset of EndpointSlices. For example, this might be combined with topology aware routing so that kube-proxy would only need to watch EndpointSlices containing endpoints within the same zone. This would provide another very significant scalability improvement.&lt;/p>
&lt;h2 id="what-does-this-mean-for-the-endpoints-api">What does this mean for the Endpoints API?&lt;/h2>
&lt;p>Although the EndpointSlice API is providing a newer and more scalable alternative to the Endpoints API, the Endpoints API will continue to be considered generally available and stable. The most significant change planned for the Endpoints API will involve beginning to truncate Endpoints that would otherwise run into scalability issues.&lt;/p>
&lt;p>The Endpoints API is not going away, but many new features will rely on the EndpointSlice API. To take advantage of the new scalability and functionality that EndpointSlices provide, applications that currently consume Endpoints will likely want to consider supporting EndpointSlices in the future.&lt;/p></description></item><item><title>Blog: Ephemeral volumes with storage capacity tracking: EmptyDir on steroids</title><link>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</link><pubDate>Tue, 01 Sep 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/09/01/ephemeral-volumes-with-storage-capacity-tracking/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>Some applications need additional storage but don't care whether that
data is stored persistently across restarts. For example, caching
services are often limited by memory size and can move infrequently
used data into storage that is slower than memory with little impact
on overall performance. Other applications expect some read-only input
data to be present in files, like configuration data or secret keys.&lt;/p>
&lt;p>Kubernetes already supports several kinds of such &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes">ephemeral
volumes&lt;/a>, but the
functionality of those is limited to what is implemented inside
Kubernetes.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/">CSI ephemeral volumes&lt;/a>
made it possible to extend Kubernetes with CSI
drivers that provide light-weight, local volumes. These &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md#motivation">&lt;em>inject
arbitrary states, such as configuration, secrets, identity, variables
or similar
information&lt;/em>&lt;/a>.
CSI drivers must be modified to support this Kubernetes feature,
i.e. normal, standard-compliant CSI drivers will not work, and
by design such volumes are supposed to be usable on whatever node
is chosen for a pod.&lt;/p>
&lt;p>This is problematic for volumes which consume significant resources on
a node or for special storage that is only available on some nodes.
Therefore, Kubernetes 1.19 introduces two new alpha features for
volumes that are conceptually more like the &lt;code>EmptyDir&lt;/code> volumes:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#generic-ephemeral-volumes">&lt;em>generic&lt;/em> ephemeral volumes&lt;/a> and&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity">CSI storage capacity tracking&lt;/a>.&lt;/li>
&lt;/ul>
&lt;p>The advantages of the new approach are:&lt;/p>
&lt;ul>
&lt;li>Storage can be local or network-attached.&lt;/li>
&lt;li>Volumes can have a fixed size that applications are never able to exceed.&lt;/li>
&lt;li>Works with any CSI driver that supports provisioning of persistent
volumes and (for capacity tracking) implements the CSI &lt;code>GetCapacity&lt;/code> call.&lt;/li>
&lt;li>Volumes may have some initial data, depending on the driver and
parameters.&lt;/li>
&lt;li>All of the typical volume operations (snapshotting,
resizing, the future storage capacity tracking, etc.)
are supported.&lt;/li>
&lt;li>The volumes are usable with any app controller that accepts
a Pod or volume specification.&lt;/li>
&lt;li>The Kubernetes scheduler itself picks suitable nodes, i.e. there is
no need anymore to implement and configure scheduler extenders and
mutating webhooks.&lt;/li>
&lt;/ul>
&lt;p>This makes generic ephemeral volumes a suitable solution for several
use cases:&lt;/p>
&lt;h1 id="use-cases">Use cases&lt;/h1>
&lt;h2 id="persistent-memory-as-dram-replacement-for-memcached">Persistent Memory as DRAM replacement for memcached&lt;/h2>
&lt;p>Recent releases of memcached added &lt;a href="https://memcached.org/blog/persistent-memory/">support for using Persistent
Memory&lt;/a> (PMEM) instead
of standard DRAM. When deploying memcached through one of the app
controllers, generic ephemeral volumes make it possible to request a PMEM volume
of a certain size from a CSI driver like
&lt;a href="https://intel.github.io/pmem-csi/">PMEM-CSI&lt;/a>.&lt;/p>
&lt;h2 id="local-lvm-storage-as-scratch-space">Local LVM storage as scratch space&lt;/h2>
&lt;p>Applications working with data sets that exceed the RAM size can
request local storage with performance characteristics or size that is
not met by the normal Kubernetes &lt;code>EmptyDir&lt;/code> volumes. For example,
&lt;a href="https://github.com/cybozu-go/topolvm">TopoLVM&lt;/a> was written for that
purpose.&lt;/p>
&lt;h2 id="read-only-access-to-volumes-with-data">Read-only access to volumes with data&lt;/h2>
&lt;p>Provisioning a volume might result in a non-empty volume:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#volume-snapshot-and-restore-volume-from-snapshot-support">restore a snapshot&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/docs/concepts/storage/volume-pvc-datasource">cloning a volume&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20200120-generic-data-populators.md">generic data populators&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Such volumes can be mounted read-only.&lt;/p>
&lt;h1 id="how-it-works">How it works&lt;/h1>
&lt;h2 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h2>
&lt;p>The key idea behind generic ephemeral volumes is that a new volume
source, the so-called
&lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/#ephemeralvolumesource-v1alpha1-core">&lt;code>EphemeralVolumeSource&lt;/code>&lt;/a>
contains all fields that are needed to created a volume claim
(historically called persistent volume claim, PVC). A new controller
in the &lt;code>kube-controller-manager&lt;/code> waits for Pods which embed such a
volume source and then creates a PVC for that pod. To a CSI driver
deployment, that PVC looks like any other, so no special support is
needed.&lt;/p>
&lt;p>As long as these PVCs exist, they can be used like any other volume claim. In
particular, they can be referenced as data source in volume cloning or
snapshotting. The PVC object also holds the current status of the
volume.&lt;/p>
&lt;p>Naming of the automatically created PVCs is deterministic: the name is
a combination of Pod name and volume name, with a hyphen (&lt;code>-&lt;/code>) in the
middle. This deterministic naming makes it easier to
interact with the PVC because one does not have to search for it once
the Pod name and volume name are known. The downside is that the name might
be in use already. This is detected by Kubernetes and then blocks Pod
startup.&lt;/p>
&lt;p>To ensure that the volume gets deleted together with the pod, the
controller makes the Pod the owner of the volume claim. When the Pod
gets deleted, the normal garbage-collection mechanism also removes the
claim and thus the volume.&lt;/p>
&lt;p>Claims select the storage driver through the normal storage class
mechanism. Although storage classes with both immediate and late
binding (aka &lt;code>WaitForFirstConsumer&lt;/code>) are supported, for ephemeral
volumes it makes more sense to use &lt;code>WaitForFirstConsumer&lt;/code>: then Pod
scheduling can take into account both node utilization and
availability of storage when choosing a node. This is where the other
new feature comes in.&lt;/p>
&lt;h2 id="storage-capacity-tracking">Storage capacity tracking&lt;/h2>
&lt;p>Normally, the Kubernetes scheduler has no information about where a
CSI driver might be able to create a volume. It also has no way of
talking directly to a CSI driver to retrieve that information. It
therefore tries different nodes until it finds one where all volumes
can be made available (late binding) or leaves it entirely to the
driver to choose a location (immediate binding).&lt;/p>
&lt;p>The new &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csistoragecapacity-v1alpha1-storage-k8s-io">&lt;code>CSIStorageCapacity&lt;/code> alpha
API&lt;/a>
allows storing the necessary information in etcd where it is available to the
scheduler. In contrast to support for generic ephemeral volumes,
storage capacity tracking must be &lt;a href="https://github.com/kubernetes-csi/external-provisioner/blob/master/README.md#capacity-support">enabled when deploying a CSI
driver&lt;/a>:
the &lt;code>external-provisioner&lt;/code> must be told to publish capacity
information that it then retrieves from the CSI driver through the normal
&lt;code>GetCapacity&lt;/code> call.&lt;/p>
&lt;!-- TODO: update the link with a revision once https://github.com/kubernetes-csi/external-provisioner/pull/450 is merged -->
&lt;p>When the Kubernetes scheduler needs to choose a node for a Pod with an
unbound volume that uses late binding and the CSI driver deployment
has opted into the feature by setting the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#csidriver-v1beta1-storage-k8s-io">&lt;code>CSIDriver.storageCapacity&lt;/code>
flag&lt;/a>
flag, the scheduler automatically filters out nodes that do not have
access to enough storage capacity. This works for generic ephemeral
and persistent volumes but &lt;em>not&lt;/em> for CSI ephemeral volumes because the
parameters of those are opaque for Kubernetes.&lt;/p>
&lt;p>As usual, volumes with immediate binding get created before scheduling
pods, with their location chosen by the storage driver. Therefore, the
external-provisioner's default configuration skips storage
classes with immediate binding as the information wouldn't be used anyway.&lt;/p>
&lt;p>Because the Kubernetes scheduler must act on potentially outdated
information, it cannot be ensured that the capacity is still available
when a volume is to be created. Still, the chances that it can be created
without retries should be higher.&lt;/p>
&lt;h1 id="security">Security&lt;/h1>
&lt;h2 id="csistoragecapacity">CSIStorageCapacity&lt;/h2>
&lt;p>CSIStorageCapacity objects are namespaced. When deploying each CSI
drivers in its own namespace and, as recommended, limiting the RBAC
permissions for CSIStorageCapacity to that namespace, it is
always obvious where the data came from. However, Kubernetes does
not check that and typically drivers get installed in the same
namespace anyway, so ultimately drivers are &lt;em>expected to behave&lt;/em> and
not publish incorrect data.&lt;/p>
&lt;h2 id="generic-ephemeral-volumes-1">Generic ephemeral volumes&lt;/h2>
&lt;p>If users have permission to create a Pod (directly or indirectly),
then they can also create generic ephemeral volumes even when they do
not have permission to create a volume claim. That's because RBAC
permission checks are applied to the controller which creates the
PVC, not the original user. This is a fundamental change that must be
&lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes#security">taken into
account&lt;/a> before
enabling the feature in clusters where untrusted users are not
supposed to have permission to create volumes.&lt;/p>
&lt;h1 id="example">Example&lt;/h1>
&lt;p>A &lt;a href="https://github.com/intel/pmem-csi/commits/kubernetes-1-19-blog-post">special branch&lt;/a>
in PMEM-CSI contains all the necessary changes to bring up a
Kubernetes 1.19 cluster inside QEMU VMs with both alpha features
enabled. The PMEM-CSI driver code is used unchanged, only the
deployment was updated.&lt;/p>
&lt;p>On a suitable machine (Linux, non-root user can use Docker - see the
&lt;a href="https://intel.github.io/pmem-csi/0.7/docs/autotest.html#qemu-and-kubernetes">QEMU and
Kubernetes&lt;/a>
section in the PMEM-CSI documentation), the following commands bring
up a cluster and install the PMEM-CSI driver:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">git clone --branch=kubernetes-1-19-blog-post https://github.com/intel/pmem-csi.git
cd pmem-csi
export TEST_KUBERNETES_VERSION=1.19 TEST_FEATURE_GATES=CSIStorageCapacity=true,GenericEphemeralVolume=true TEST_PMEM_REGISTRY=intel
make start &amp;amp;&amp;amp; echo &amp;amp;&amp;amp; test/setup-deployment.sh
&lt;/code>&lt;/pre>&lt;p>If all goes well, the output contains the following usage
instructions:&lt;/p>
&lt;pre>&lt;code>The test cluster is ready. Log in with [...]/pmem-csi/_work/pmem-govm/ssh.0, run
kubectl once logged in. Alternatively, use kubectl directly with the
following env variable:
KUBECONFIG=[...]/pmem-csi/_work/pmem-govm/kube.config
secret/pmem-csi-registry-secrets created
secret/pmem-csi-node-secrets created
serviceaccount/pmem-csi-controller created
...
To try out the pmem-csi driver ephemeral volumes:
cat deploy/kubernetes-1.19/pmem-app-ephemeral.yaml |
[...]/pmem-csi/_work/pmem-govm/ssh.0 kubectl create -f -
&lt;/code>&lt;/pre>&lt;p>The CSIStorageCapacity objects are not meant to be human-readable, so
some post-processing is needed. The following Golang template filters
all objects by the storage class that the example uses and prints the
name, topology and capacity:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get \
-o go-template='{{range .items}}{{if eq .storageClassName &amp;quot;pmem-csi-sc-late-binding&amp;quot;}}{{.metadata.name}} {{.nodeTopology.matchLabels}} {{.capacity}}
{{end}}{{end}}' \
csistoragecapacities
&lt;/code>&lt;/pre>&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 30716Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>One individual object has the following content:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl describe csistoragecapacities/csisc-6cw8j
&lt;/code>&lt;/pre>&lt;pre>&lt;code>Name: csisc-sqdnt
Namespace: default
Labels: &amp;lt;none&amp;gt;
Annotations: &amp;lt;none&amp;gt;
API Version: storage.k8s.io/v1alpha1
Capacity: 30716Mi
Kind: CSIStorageCapacity
Metadata:
Creation Timestamp: 2020-08-11T15:41:03Z
Generate Name: csisc-
Managed Fields:
...
Owner References:
API Version: apps/v1
Controller: true
Kind: StatefulSet
Name: pmem-csi-controller
UID: 590237f9-1eb4-4208-b37b-5f7eab4597d1
Resource Version: 2994
Self Link: /apis/storage.k8s.io/v1alpha1/namespaces/default/csistoragecapacities/csisc-sqdnt
UID: da36215b-3b9d-404a-a4c7-3f1c3502ab13
Node Topology:
Match Labels:
pmem-csi.intel.com/node: pmem-csi-pmem-govm-worker1
Storage Class Name: pmem-csi-sc-late-binding
Events: &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;p>Now let's create the example app with one generic ephemeral
volume. The &lt;code>pmem-app-ephemeral.yaml&lt;/code> file contains:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#080;font-style:italic"># This example Pod definition demonstrates&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># how to use generic ephemeral inline volumes&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#080;font-style:italic"># with a PMEM-CSI storage class.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-app-inline-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-frontend&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>intel/pmem-csi-driver-test:v0&lt;span style="color:#666">.7.14&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;sleep&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;100000&amp;#34;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/data&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>my-csi-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ephemeral&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeClaimTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>4Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pmem-csi-sc-late-binding&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>After creating that as shown in the usage instructions above, we have one additional Pod and PVC:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pods/my-csi-app-inline-volume -o wide
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES
my-csi-app-inline-volume 1/1 Running 0 6m58s 10.36.0.2 pmem-csi-pmem-govm-worker1 &amp;lt;none&amp;gt; &amp;lt;none&amp;gt;
&lt;/code>&lt;/pre>&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
my-csi-app-inline-volume-my-csi-volume Bound pvc-c11eb7ab-a4fa-46fe-b515-b366be908823 4Gi RWO pmem-csi-sc-late-binding 9m21s
&lt;/code>&lt;/pre>&lt;p>That PVC is owned by the Pod:&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console">kubectl get -o yaml pvc/my-csi-app-inline-volume-my-csi-volume
&lt;/code>&lt;/pre>&lt;pre>&lt;code>apiVersion: v1
kind: PersistentVolumeClaim
metadata:
annotations:
pv.kubernetes.io/bind-completed: &amp;quot;yes&amp;quot;
pv.kubernetes.io/bound-by-controller: &amp;quot;yes&amp;quot;
volume.beta.kubernetes.io/storage-provisioner: pmem-csi.intel.com
volume.kubernetes.io/selected-node: pmem-csi-pmem-govm-worker1
creationTimestamp: &amp;quot;2020-08-11T15:44:57Z&amp;quot;
finalizers:
- kubernetes.io/pvc-protection
managedFields:
...
name: my-csi-app-inline-volume-my-csi-volume
namespace: default
ownerReferences:
- apiVersion: v1
blockOwnerDeletion: true
controller: true
kind: Pod
name: my-csi-app-inline-volume
uid: 75c925bf-ca8e-441a-ac67-f190b7a2265f
...
&lt;/code>&lt;/pre>&lt;p>Eventually, the storage capacity information for &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> also gets updated:&lt;/p>
&lt;pre>&lt;code>csisc-2js6n map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker2] 30716Mi
csisc-sqdnt map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker1] 26620Mi
csisc-ws4bv map[pmem-csi.intel.com/node:pmem-csi-pmem-govm-worker3] 30716Mi
&lt;/code>&lt;/pre>&lt;p>If another app needs more than 26620Mi, the Kubernetes
scheduler will not pick &lt;code>pmem-csi-pmem-govm-worker1&lt;/code> anymore.&lt;/p>
&lt;h1 id="next-steps">Next steps&lt;/h1>
&lt;p>Both features are under development. Several open questions were
already raised during the alpha review process. The two enhancement
proposals document the work that will be needed for migration to beta and what
alternatives were already considered and rejected:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/blob/9d7a75d/keps/sig-storage/1698-generic-ephemeral-volumes/README.md">KEP-1698: generic ephemeral inline
volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/tree/9d7a75d/keps/sig-storage/1472-storage-capacity-tracking">KEP-1472: Storage Capacity
Tracking&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Your feedback is crucial for driving that development. SIG-Storage
&lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets
regularly&lt;/a>
and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and a mailing
list&lt;/a>.&lt;/p></description></item><item><title>Blog: Increasing the Kubernetes Support Window to One Year</title><link>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</link><pubDate>Mon, 31 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/31/kubernetes-1-19-feature-one-year-support/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Tim Pepper (VMware), Nick Young (VMware)&lt;/p>
&lt;p>Starting with Kubernetes 1.19, the support window for Kubernetes versions &lt;a href="https://github.com/kubernetes/enhancements/issues/1498">will increase from 9 months to one year&lt;/a>. The longer support window is intended to allow organizations to perform major upgrades at a time of the year that works the best for them.&lt;/p>
&lt;p>This is a big change. For many years, the Kubernetes project has delivered a new minor release (e.g.: 1.13 or 1.14) every 3 months. The project provides bugfix support via patch releases (e.g.: 1.13.Y) for three parallel branches of the codebase. Combined, this led to each minor release (e.g.: 1.13) having a patch release stream of support for approximately 9 months. In the end, a cluster operator had to upgrade at least every 9 months to remain supported.&lt;/p>
&lt;p>A survey conducted in early 2019 by the WG LTS showed that a significant subset of Kubernetes end-users fail to upgrade within the 9-month support period.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/versions-in-production-text-2.png" alt="Versions in Production">&lt;/p>
&lt;p>This, and other responses from the survey, suggest that a considerable portion of our community would better be able to manage their deployments on supported versions if the patch support period were extended to 12-14 months. It appears to be true regardless of whether the users are on DIY builds or commercially vendored distributions. An extension in the patch support length of time would thus lead to a larger percentage of our user base running supported versions compared to what we have now.&lt;/p>
&lt;p>A yearly support period provides the cushion end-users appear to desire, and is more aligned with familiar annual planning cycles.
There are many unknowns about changing the support windows for a project with as many moving parts as Kubernetes. Keeping the change relatively small (relatively being the important word), gives us the chance to find out what those unknowns are in detail and address them.
From Kubernetes version 1.19 on, the support window will be extended to one year. For Kubernetes versions 1.16, 1.17, and 1.18, the story is more complicated.&lt;/p>
&lt;p>All of these versions still fall under the older “three releases support” model, and will drop out of support when 1.19, 1.20 and 1.21 are respectively released. However, because the 1.19 release has been delayed due to the events of 2020, they will end up with close to a year of support (depending on their exact release dates).&lt;/p>
&lt;p>For example, 1.19 was released on the 26th of August 2020, which is 11 months since the release of 1.16. Since 1.16 is still under the old release policy, this means that it is now out of support.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-31-increase-kubernetes-support-one-year/support-timeline.png" alt="Support Timeline">&lt;/p>
&lt;p>If you’ve got thoughts or feedback, we’d love to hear them. Please contact us on &lt;a href="https://kubernetes.slack.com/messages/wg-lts/">#wg-lts&lt;/a> on the Kubernetes Slack, or to the &lt;a href="https://groups.google.com/g/kubernetes-wg-lts">kubernetes-wg-lts mailing list&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.19: Accentuate the Paw-sitive</title><link>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</link><pubDate>Wed, 26 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/26/kubernetes-release-1.19-accentuate-the-paw-sitive/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">Kubernetes 1.19 Release Team&lt;/a>&lt;/p>
&lt;p>Finally, we have arrived with Kubernetes 1.19, the second release for 2020, and by far the longest release cycle lasting 20 weeks in total. It consists of 34 enhancements: 10 enhancements are moving to stable, 15 enhancements in beta, and 9 enhancements in alpha.&lt;/p>
&lt;p>The 1.19 release was quite different from a regular release due to COVID-19, the George Floyd protests, and several other global events that we experienced as a release team. Due to these events, we made the decision to adjust our timeline and allow the SIGs, Working Groups, and contributors more time to get things done. The extra time also allowed for people to take time to focus on their lives outside of the Kubernetes project, and ensure their mental wellbeing was in a good place.&lt;/p>
&lt;p>Contributors are the heart of Kubernetes, not the other way around. The Kubernetes code of conduct asks that people be excellent to one another and despite the unrest in our world, we saw nothing but greatness and humility from the community.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="increase-kubernetes-support-window-to-one-year">Increase Kubernetes support window to one year&lt;/h3>
&lt;p>A survey conducted in early 2019 by the &lt;a href="https://github.com/kubernetes/community/tree/master/wg-lts#readme">Long Term Support (LTS) working group&lt;/a> showed that a significant subset of Kubernetes end-users fail to upgrade within the current 9-month support period.
This, and other responses from the survey, suggest that 30% of users would be able to keep their deployments on supported versions if the patch support period were extended to 12-14 months. This appears to be true regardless of whether the users are on self build or commercially vendored distributions. An extension would thus lead to more than 80% of users being on supported versions, instead of the 50-60% we have now.
A yearly support period provides the cushion end-users appear to desire, and is more in harmony with familiar annual planning cycles.
From Kubernetes version 1.19 on, the support window will be extended to one year.&lt;/p>
&lt;h3 id="storage-capacity-tracking">Storage capacity tracking&lt;/h3>
&lt;p>Traditionally, the Kubernetes scheduler was based on the assumptions that additional persistent storage is available everywhere in the cluster and has infinite capacity. Topology constraints addressed the first point, but up to now pod scheduling was still done without considering that the remaining storage capacity may not be enough to start a new pod. &lt;a href="https://kubernetes.io/docs/concepts/storage/storage-capacity/">Storage capacity tracking&lt;/a>, a new alpha feature, addresses that by adding an API for a CSI driver to report storage capacity and uses that information in the Kubernetes scheduler when choosing a node for a pod. This feature serves as a stepping stone for supporting dynamic provisioning for local volumes and other volume types that are more capacity constrained.&lt;/p>
&lt;h4 id="generic-ephemeral-volumes">Generic ephemeral volumes&lt;/h4>
&lt;p>Kubernetes provides volume plugins whose lifecycle is tied to a pod and can be used as scratch space (e.g. the builtin &lt;code>emptydir&lt;/code> volume type) or to load some data in to a pod (e.g. the builtin &lt;code>configmap&lt;/code> and &lt;code>secret&lt;/code> volume types, or “CSI inline volumes”). The new &lt;a href="https://kubernetes.io/docs/concepts/storage/ephemeral-volumes/#generic-ephemeral-volumes">generic ephemeral volumes&lt;/a> alpha feature allows any existing storage driver that supports dynamic provisioning to be used as an ephemeral volume with the volume’s lifecycle bound to the Pod.
It can be used to provide scratch storage that is different from the root disk, for example persistent memory, or a separate local disk on that node.
All StorageClass parameters for volume provisioning are supported.
All features supported with PersistentVolumeClaims are supported, such as storage capacity tracking, snapshots and restore, and volume resizing.&lt;/p>
&lt;h4 id="csi-volume-health-monitoring">CSI Volume Health Monitoring&lt;/h4>
&lt;p>The alpha version of CSI health monitoring is being released with Kubernetes 1.19. This feature enables CSI Drivers to share abnormal volume conditions from the underlying storage systems with Kubernetes so that they can be reported as events on PVCs or Pods. This feature serves as a stepping stone towards programmatic detection and resolution of individual volume health issues by Kubernetes.&lt;/p>
&lt;h3 id="ingress-graduates-to-general-availability">Ingress graduates to General Availability&lt;/h3>
&lt;p>In terms of moving the Ingress API towards GA, the API itself has been available in beta for so long that it has attained de facto GA status through usage and adoption (both by users and by load balancer / ingress controller providers). Abandoning it without a full replacement is not a viable approach. It is clearly a useful API and captures a non-trivial set of use cases. At this point, it seems more prudent to declare the current API as something the community will support as a V1, codifying its status, while working on either a V2 Ingress API or an entirely different API with a superset of features.&lt;/p>
&lt;h3 id="structured-logging">Structured logging&lt;/h3>
&lt;p>Before v1.19, logging in the Kubernetes control plane couldn't guarantee any uniform structure for log messages and references to Kubernetes objects in those logs. This makes parsing, processing, storing, querying and analyzing logs hard and forces administrators and developers to rely on ad-hoc solutions in most cases based on some regular expressions. Due to those problems any analytical solution based on those logs is hard to implement and maintain.&lt;/p>
&lt;h4 id="new-klog-methods">New klog methods&lt;/h4>
&lt;p>This Kubernetes release introduces new methods to the &lt;em>klog&lt;/em> library that provide a more structured interface for formatting log messages. Each existing formatted log method (&lt;code>Infof&lt;/code>, &lt;code>Errorf&lt;/code>) is now matched by a structured method (&lt;code>InfoS&lt;/code>, &lt;code>ErrorS&lt;/code>). The new logging methods accept log messages as a first argument and a list of key-values pairs as a variadic second argument. This approach allows incremental adoption of structured logging without converting &lt;strong>all&lt;/strong> of Kubernetes to a new API at one time.&lt;/p>
&lt;h3 id="client-tls-certificate-rotation-for-kubelet">Client TLS certificate rotation for kubelet&lt;/h3>
&lt;p>A kubelet authenticates the kubelet to the kube-apiserver using a private key and certificate. The certificate is supplied to the kubelet when it is first booted, via an out-of-cluster mechanism. Since Kubernetes v1.8, clusters have included a (beta) process for obtaining the initial cert/key pair and rotating it as expiration of the certificate approaches. In Kubernetes v1.19 this graduates to stable.&lt;/p>
&lt;p>During the kubelet start-up sequence, the filesystem is scanned for an existing cert/key pair, which is managed by the certificate manager. In the case that a cert/key is available it will be loaded. If not, the kubelet checks its config file for an encoded certificate value or a file reference in the kubeconfig. If the certificate is a bootstrap certificate, this will be used to generate a key, create a certificate signing request and request a signed certificate from the API server.&lt;/p>
&lt;p>When an expiration approaches the cert manager takes care of providing the correct certificate, generating new private keys and requesting new certificates. With the kubelet requesting certificates be signed as part of its boot sequence, and on an ongoing basis, certificate signing requests from the kubelet need to be auto approved to make cluster administration manageable.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/135">Seccomp&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/266">Kubelet client TLS certificate rotation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/279">Limit node access to API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/383">Redesign Event API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1453">Graduate Ingress to V1&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1547">Building Kubelet without Docker&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/693">Node Topology Manager&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">New Endpoint API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1498">Increase Kubernetes support window to one year&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="other-notable-features">Other Notable Features&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1451">Run multiple Scheduling Profiles&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1412">Immutable Secrets and ConfigMaps&lt;/a>&lt;/li>
&lt;/ul>
&lt;h2 id="release-notes">Release Notes&lt;/h2>
&lt;p>Check out the full details of the Kubernetes 1.19 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.19.md">release notes&lt;/a>.&lt;/p>
&lt;h2 id="availability">Availability&lt;/h2>
&lt;p>Kubernetes 1.19 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.19.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> (Kubernetes in Docker). You can also easily install 1.19 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h2 id="release-team">Release Team&lt;/h2>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.19/release_team.md">release team&lt;/a> led by Taylor Dolezal, Senior Developer Advocate at HashiCorp. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">49,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h2 id="release-logo">Release Logo&lt;/h2>
&lt;p>All of you inspired this Kubernetes 1.19 release logo! This release was a bit more of a marathon and a testament to when the world is a wild place, we can come together and do unbelievable things.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-08-26-kubernetes-1.19-release-announcement/accentuate.png" alt="Kubernetes 1.19 Release Logo">&lt;/p>
&lt;p>&amp;quot;Accentuate the Paw-sitive&amp;quot; was chosen as the release theme because it captures the positive outlook that the release team had, despite the state of the world. The characters pictured in the 1.19 logo represent everyone's personalities on our release team, from emo to peppy, and beyond!&lt;/p>
&lt;p>About the designer: Hannabeth Lagerlof is a Visual Designer based in Los Angeles, California, and she has an extensive background in Environments and Graphic Design. Hannabeth creates art and user experiences that inspire connection. You can find Hannabeth on Twitter as @emanate_design.&lt;/p>
&lt;h2 id="the-long-run">The Long Run&lt;/h2>
&lt;p>The release was also different from the enhancements side of things. Traditionally, we have had 3-4 weeks between the call for enhancements and Enhancements Freeze, which ends the phase in which contributors can acknowledge whether a particular feature will be part of the cycle. This release cycle, being unique, we had five weeks for the same milestone. The extended duration gave the contributors more time to plan and decide about the graduation of their respective features.&lt;/p>
&lt;p>The milestone until which contributors implement the features was extended from the usual five weeks to 7 weeks. Contributors were provided with 40% more time to work on their features, resulting in reduced fatigue and more to think through about the implementation. We also noticed a considerable reduction in last-minute hustles. There were also a lesser number of exception requests this cycle - 6 compared to 14 the previous release cycle.&lt;/p>
&lt;h2 id="user-highlights">User Highlights&lt;/h2>
&lt;ul>
&lt;li>The CNCF grants Zalando, Europe’s leading online platform for fashion and lifestyle, the &lt;a href="https://www.cncf.io/announcement/2020/08/20/cloud-native-computing-foundation-grants-zalando-the-top-end-user-award/">Top End User Award&lt;/a>. Zalando leverages numerous CNCF projects and open sourced multiple of their own development.&lt;/li>
&lt;/ul>
&lt;h2 id="ecosystem-updates">Ecosystem Updates&lt;/h2>
&lt;ul>
&lt;li>The CNCF just concluded its very first Virtual KubeCon. All talks are &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/">on-demand&lt;/a> for anyone registered, it's not too late!&lt;/li>
&lt;li>The &lt;a href="https://www.cncf.io/blog/2020/07/15/certified-kubernetes-security-specialist-cks-coming-in-november/">Certified Kubernetes Security Specialist&lt;/a> (CKS) coming in November! CKS focuses on cluster &amp;amp; system hardening, minimizing microservice vulnerabilities and the security of the supply chain.&lt;/li>
&lt;li>CNCF published the second &lt;a href="https://www.cncf.io/blog/2020/08/14/state-of-cloud-native-development/">State of Cloud Native Development&lt;/a>, showing the massively growing number of cloud native developer using container and serverless technology.&lt;/li>
&lt;li>&lt;a href="https://www.kubernetes.dev">Kubernetes.dev&lt;/a>, a Kubernetes contributor focused website has been launched. It brings the contributor documentation, resources and project event information into one central location.&lt;/li>
&lt;/ul>
&lt;h2 id="project-velocity">Project Velocity&lt;/h2>
&lt;p>The &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">Kubernetes DevStats dashboard&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times. If you want to gather numbers, facts and figures from Kubernetes and the CNCF community it is the best place to start.&lt;/p>
&lt;p>During this release cycle from April till August, 382 different companies and over 2,464 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All&amp;amp;from=1585692000000&amp;amp;to=1598392799000">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h2 id="upcoming-release-webinar">Upcoming release webinar&lt;/h2>
&lt;p>Join the members of the Kubernetes 1.19 release team on September 25th, 2020 to learn about the major features in this release including storage capacity tracking, structured logging, Ingress V1 GA, and many more. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-19/">https://www.cncf.io/webinars/kubernetes-1-19/&lt;/a>.&lt;/p>
&lt;h2 id="get-involved">Get Involved&lt;/h2>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our monthly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Find out more about contributing to Kubernetes at the new &lt;a href="https://www.kubernetes.dev/">Kubernetes Contributor website&lt;/a>&lt;/li>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Moving Forward From Beta</title><link>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</link><pubDate>Fri, 21 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/21/moving-forward-from-beta/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Tim Bannister, The Scale Factory&lt;/p>
&lt;p>In Kubernetes, features follow a defined
&lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/#feature-stages">lifecycle&lt;/a>.
First, as the twinkle of an eye in an interested developer. Maybe, then,
sketched in online discussions, drawn on the online equivalent of a cafe
napkin. This rough work typically becomes a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/0001-kubernetes-enhancement-proposal-process.md#kubernetes-enhancement-proposal-process">Kubernetes Enhancement Proposal&lt;/a> (KEP), and
from there it usually turns into code.&lt;/p>
&lt;p>For Kubernetes v1.20 and onwards, we're focusing on helping that code
graduate into stable features.&lt;/p>
&lt;p>That lifecycle I mentioned runs as follows:&lt;/p>
&lt;p>&lt;img src="feature_stages.svg" alt="Alpha → Beta → General Availability">&lt;/p>
&lt;p>Usually, alpha features aren't enabled by default. You turn them on by setting a feature
gate; usually, by setting a command line flag on each of the components that use the
feature.&lt;/p>
&lt;p>(If you use Kubernetes through a managed service offering such as AKS, EKS, GKE, etc then
the vendor who runs that service may have decided what feature gates are enabled for you).&lt;/p>
&lt;p>There's a defined process for graduating an existing, alpha feature into the beta phase.
This is important because &lt;strong>beta features are enabled by default&lt;/strong>, with the feature flag still
there so cluster operators can opt out if they want.&lt;/p>
&lt;p>A similar but more thorough set of graduation criteria govern the transition to general
availability (GA), also known as &amp;quot;stable&amp;quot;. GA features are part of Kubernetes, with a
commitment that they are staying in place throughout the current major version.&lt;/p>
&lt;p>Having beta features on by default lets Kubernetes and its contributors get valuable
real-world feedback. However, there's a mismatch of incentives. Once a feature is enabled
by default, people will use it. Even if there might be a few details to shake out,
the way Kubernetes' REST APIs and conventions work mean that any future stable API is going
to be compatible with the most recent beta API: your API objects won't stop working when
a beta feature graduates to GA.&lt;/p>
&lt;p>For the API and its resources in particular, there's a much less strong incentive to move
features from beta to GA than from alpha to beta. Vendors who want a particular feature
have had good reason to help get code to the point where features are enabled by default,
and beyond that the journey has been less clear.&lt;/p>
&lt;p>KEPs track more than code improvements. Essentially, anything that would need
communicating to the wider community merits a KEP. That said, most KEPs cover
Kubernetes features (and the code to implement them).&lt;/p>
&lt;p>You might know that &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Ingress&lt;/a>
has been in Kubernetes for a while, but did you realize that it actually went beta in 2015? To help
drive things forward, Kubernetes' Architecture Special Interest Group (SIG) have a new approach in
mind.&lt;/p>
&lt;h2 id="avoiding-permanent-beta">Avoiding permanent beta&lt;/h2>
&lt;p>For Kubernetes REST APIs, when a new feature's API reaches beta, that starts a countdown.
The beta-quality API now has &lt;strong>three releases&lt;/strong> (about nine calendar months) to either:&lt;/p>
&lt;ul>
&lt;li>reach GA, and deprecate the beta, or&lt;/li>
&lt;li>have a new beta version (&lt;em>and deprecate the previous beta&lt;/em>).&lt;/li>
&lt;/ul>
&lt;p>To be clear, at this point &lt;strong>only REST APIs are affected&lt;/strong>. For example, &lt;em>APIListChunking&lt;/em> is
a beta feature but isn't itself a REST API. Right now there are no plans to automatically
deprecate &lt;em>APIListChunking&lt;/em> nor any other features that aren't REST APIs.&lt;/p>
&lt;p>If a beta API has not graduated to GA after three Kubernetes releases, then the
next Kubernetes release will deprecate that API version. There's no option for
the REST API to stay at the same beta version beyond the first Kubernetes
release to come out after the release window.&lt;/p>
&lt;h3 id="what-this-means-for-you">What this means for you&lt;/h3>
&lt;p>If you're using Kubernetes, there's a good chance that you're using a beta feature. Like
I said, there are lots of them about.
As well as Ingress, you might be using &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob&lt;/a>,
or &lt;a href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">PodSecurityPolicy&lt;/a>, or others.
There's an even bigger chance that you're running on a control plane with at least one beta
feature enabled.&lt;/p>
&lt;p>If you're using or generating Kubernetes manifests that use beta APIs like Ingress, you'll
need to plan to revise those. The current APIs are going to be deprecated following a
schedule (the 9 months I mentioned earlier) and after a further 9 months those deprecated
APIs will be removed. At that point, to stay current with Kubernetes, you should already
have migrated.&lt;/p>
&lt;h3 id="what-this-means-for-kubernetes-contributors">What this means for Kubernetes contributors&lt;/h3>
&lt;p>The motivation here seems pretty clear: get features stable. Guaranteeing that beta
features will be deprecated adds a pretty big incentive so that people who want the
feature continue their effort until the code, documentation and tests are ready for this
feature to graduate to stable, backed by several Kubernetes' releases of evidence in
real-world use.&lt;/p>
&lt;h3 id="what-this-means-for-the-ecosystem">What this means for the ecosystem&lt;/h3>
&lt;p>In my opinion, these harsh-seeming measures make a lot of sense, and are going to be
good for Kubernetes. Deprecating existing APIs, through a rule that applies across all
the different Special Interest Groups (SIGs), helps avoid stagnation and encourages
fixes.&lt;/p>
&lt;p>Let's say that an API goes to beta and then real-world experience shows that it
just isn't right - that, fundamentally, the API has shortcomings. With that 9 month
countdown ticking, the people involved have the means and the justification to revise
and release an API that deals with the problem cases. Anyone who wants to live with
the deprecated API is welcome to - Kubernetes is open source - but their needs do not
have to hold up progress on the feature.&lt;/p></description></item><item><title>Blog: Introducing Hierarchical Namespaces</title><link>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</link><pubDate>Fri, 14 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/14/introducing-hierarchical-namespaces/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adrian Ludwin (Google)&lt;/p>
&lt;p>Safely hosting large numbers of users on a single Kubernetes cluster has always
been a troublesome task. One key reason for this is that different organizations
use Kubernetes in different ways, and so no one tenancy model is likely to suit
everyone. Instead, Kubernetes offers you building blocks to create your own
tenancy solution, such as Role Based Access Control (RBAC) and NetworkPolicies;
the better these building blocks, the easier it is to safely build a multitenant
cluster.&lt;/p>
&lt;h1 id="namespaces-for-tenancy">Namespaces for tenancy&lt;/h1>
&lt;p>By far the most important of these building blocks is the namespace, which forms
the backbone of almost all Kubernetes control plane security and sharing
policies. For example, RBAC, NetworkPolicies and ResourceQuotas all respect
namespaces by default, and objects such as Secrets, ServiceAccounts and
Ingresses are freely usable &lt;em>within&lt;/em> any one namespace, but fully segregated
from &lt;em>other&lt;/em> namespaces.&lt;/p>
&lt;p>Namespaces have two key properties that make them ideal for policy enforcement.
Firstly, they can be used to &lt;strong>represent ownership&lt;/strong>. Most Kubernetes objects
&lt;em>must&lt;/em> be in a namespace, so if you use namespaces to represent ownership, you
can always count on there being an owner.&lt;/p>
&lt;p>Secondly, namespaces have &lt;strong>authorized creation and use&lt;/strong>. Only
highly-privileged users can create namespaces, and other users require explicit
permission to use those namespaces - that is, create, view or modify objects in
those namespaces. This allows them to be carefully created with appropriate
policies, before unprivileged users can create “regular” objects like pods and
services.&lt;/p>
&lt;h1 id="the-limits-of-namespaces">The limits of namespaces&lt;/h1>
&lt;p>However, in practice, namespaces are not flexible enough to meet some common use
cases. For example, let’s say that one team owns several microservices with
different secrets and quotas. Ideally, they should place these services into
different namespaces in order to isolate them from each other, but this presents
two problems.&lt;/p>
&lt;p>Firstly, these namespaces have no common concept of ownership, even though
they’re both owned by the same team. This means that if the team controls
multiple namespaces, not only does Kubernetes not have any record of their
common owner, but namespaced-scoped policies cannot be applied uniformly across
them.&lt;/p>
&lt;p>Secondly, teams generally work best if they can operate autonomously, but since
namespace creation is highly privileged, it’s unlikely that any member of the
dev team is allowed to create namespaces. This means that whenever a team wants
a new namespace, they must raise a ticket to the cluster administrator. While
this is probably acceptable for small organizations, it generates unnecessary
toil as the organization grows.&lt;/p>
&lt;h1 id="introducing-hierarchical-namespaces">Introducing hierarchical namespaces&lt;/h1>
&lt;p>&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic">Hierarchical
namespaces&lt;/a>
are a new concept developed by the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">Kubernetes Working Group for Multi-Tenancy
(wg-multitenancy)&lt;/a> in order to
solve these problems. In its simplest form, a hierarchical namespace is a
regular Kubernetes namespace that contains a small custom resource that
identifies a single, optional, parent namespace. This establishes the concept of
ownership &lt;em>across&lt;/em> namespaces, not just &lt;em>within&lt;/em> them.&lt;/p>
&lt;p>This concept of ownership enables two additional types of behaviours:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Policy inheritance:&lt;/strong> if one namespace is a child of another, policy objects
such as RBAC RoleBindings are &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-propagation">copied from the parent to the
child&lt;/a>.&lt;/li>
&lt;li>&lt;strong>Delegated creation:&lt;/strong> you usually need cluster-level privileges to create a
namespace, but hierarchical namespaces adds an alternative:
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/blob/master/incubator/hnc/docs/user-guide/concepts.md#basic-subns">&lt;em>subnamespaces&lt;/em>&lt;/a>,
which can be manipulated using only limited permissions in the parent
namespace.&lt;/li>
&lt;/ul>
&lt;p>This solves both of the problems for our dev team. The cluster administrator can
create a single “root” namespace for the team, along with all necessary
policies, and then delegate permission to create subnamespaces to members of
that team. Those team members can then create subnamespaces for their own use,
without violating the policies that were imposed by the cluster administrators.&lt;/p>
&lt;h1 id="hands-on-with-hierarchical-namespaces">Hands-on with hierarchical namespaces&lt;/h1>
&lt;p>Hierarchical namespaces are provided by a Kubernetes extension known as the
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc">&lt;strong>Hierarchical Namespace
Controller&lt;/strong>&lt;/a>,
or &lt;strong>HNC&lt;/strong>. The HNC consists of two components:&lt;/p>
&lt;ul>
&lt;li>The &lt;strong>manager&lt;/strong> runs on your cluster, manages subnamespaces, propagates policy
objects, ensures that your hierarchies are legal and manages extension points.&lt;/li>
&lt;li>The &lt;strong>kubectl plugin&lt;/strong>, called &lt;code>kubectl-hns&lt;/code>, makes it easy for users to
interact with the manager.&lt;/li>
&lt;/ul>
&lt;p>Both can be easily installed from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases">releases page of our
repo&lt;/a>.&lt;/p>
&lt;p>Let’s see HNC in action. Imagine that I do not have namespace creation
privileges, but I can view the namespace &lt;code>team-a&lt;/code> and create subnamespaces
within it&lt;sup>&lt;a href="#note-1">1&lt;/a>&lt;/sup>. Using the plugin, I can now say:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns create svc1-team-a -n team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This creates a subnamespace called &lt;code>svc1-team-a&lt;/code>. Note that since subnamespaces
are just regular Kubernetes namespaces, all subnamespace names must still be
unique.&lt;/p>
&lt;p>I can view the structure of these namespaces by asking for a tree view:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl hns tree team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
team-a
└── svc1-team-a
&lt;/code>&lt;/pre>&lt;/div>&lt;p>And if there were any policies in the parent namespace, these now appear in the
child as well&lt;sup>&lt;a href="#note-2">2&lt;/a>&lt;/sup>. For example, let’s say that &lt;code>team-a&lt;/code> had
an RBAC RoleBinding called &lt;code>sres&lt;/code>. This rolebinding will also be present in the
subnamespace:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">$ kubectl describe rolebinding sres -n svc1-team-a
&lt;span style="color:#080;font-style:italic"># Output:&lt;/span>
Name: sres
Labels: hnc.x-k8s.io/inheritedFrom&lt;span style="color:#666">=&lt;/span>team-a &lt;span style="color:#080;font-style:italic"># inserted by HNC&lt;/span>
Annotations: &amp;lt;none&amp;gt;
Role:
Kind: ClusterRole
Name: admin
Subjects: ...
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Finally, HNC adds labels to these namespaces with useful information about the
hierarchy which you can use to apply other policies. For example, you can create
the following NetworkPolicy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NetworkPolicy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>networking.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>allow-team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>team-a&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ingress&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">from&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">namespaceSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchExpressions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;team-a.tree.hnc.x-k8s.io/depth&amp;#39;&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Label created by HNC&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">operator&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Exists&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>This policy will both be propagated to all descendants of &lt;code>team-a&lt;/code>, and will
&lt;em>also&lt;/em> allow ingress traffic between all of those namespaces. The “tree” label
can only be applied by HNC, and is guaranteed to reflect the latest hierarchy.&lt;/p>
&lt;p>You can learn all about the features of HNC from the &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/tree/master/incubator/hnc/docs/user-guide">user
guide&lt;/a>.&lt;/p>
&lt;h1 id="next-steps-and-getting-involved">Next steps and getting involved&lt;/h1>
&lt;p>If you think that hierarchical namespaces can work for your organization, &lt;a href="https://github.com/kubernetes-sigs/multi-tenancy/releases/tag/hnc-v0.5.1">HNC
v0.5.1 is available on
GitHub&lt;/a>.
We’d love to know what you think of it, what problems you’re using it to solve
and what features you’d most like to see added. As with all early software, you
should be cautious about using HNC in production environments, but the more
feedback we get, the sooner we’ll be able to drive to HNC 1.0.&lt;/p>
&lt;p>We’re also open to additional contributors, whether it’s to fix or report bugs,
or help prototype new features such as exceptions, improved monitoring,
hierarchical resource quotas or fine-grained configuration.&lt;/p>
&lt;p>Please get in touch with us via our
&lt;a href="https://github.com/kubernetes-sigs/multi-tenancy">repo&lt;/a>, &lt;a href="https://groups.google.com/g/kubernetes-wg-multitenancy">mailing
list&lt;/a> or on
&lt;a href="https://kubernetes.slack.com/messages/wg-multitenancy">Slack&lt;/a> - we look forward
to hearing from you!&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/aludwin">Adrian Ludwin&lt;/a> is a software engineer and the
tech lead for the Hierarchical Namespace Controller.&lt;/em>&lt;/p>
&lt;a name="note-1"/>
&lt;p>&lt;em>Note 1: technically, you create a small object called a &amp;quot;subnamespace anchor&amp;quot;
in the parent namespace, and then HNC creates the subnamespace for you.&lt;/em>&lt;/p>
&lt;a name="note-2"/>
&lt;p>&lt;em>Note 2: By default, only RBAC Roles and RoleBindings are propagated, but you
can configure HNC to propagate any namespaced Kubernetes object.&lt;/em>&lt;/p></description></item><item><title>Blog: Physics, politics and Pull Requests: the Kubernetes 1.18 release interview</title><link>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</link><pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/08/03/physics-politics-and-pull-requests-the-kubernetes-1.18-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Craig Box (Google)&lt;/p>
&lt;p>The start of the COVID-19 pandemic couldn't delay the release of Kubernetes 1.18, but unfortunately &lt;a href="https://github.com/kubernetes/utils/issues/141">a small bug&lt;/a> could — thankfully only by a day. This was the last cat that needed to be herded by 1.18 release lead &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> before the &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">release on March 25&lt;/a>.&lt;/p>
&lt;p>One of the best parts about co-hosting the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> is the conversations we have with the people who help bring Kubernetes releases together. &lt;a href="https://kubernetespodcast.com/episode/096-kubernetes-1.18/">Jorge was our guest on episode 96&lt;/a> back in March, and &lt;a href="https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/">just like last week&lt;/a> we are delighted to bring you the transcript of this interview.&lt;/p>
&lt;p>If you'd rather enjoy the &amp;quot;audiobook version&amp;quot;, including another interview when 1.19 is released later this month, &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe to the show&lt;/a> wherever you get your podcasts.&lt;/p>
&lt;p>In the last few weeks, we've talked to long-time Kubernetes contributors and SIG leads &lt;a href="https://kubernetespodcast.com/episode/114-scheduling/">David Oppenheimer&lt;/a>, &lt;a href="https://kubernetespodcast.com/episode/113-instrumentation-and-cadvisor/">David Ashpole&lt;/a> and &lt;a href="https://kubernetespodcast.com/episode/111-scalability/">Wojciech Tyczynski&lt;/a>. All are worth taking the dog for a longer walk to listen to!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You're a former physicist. I have to ask, what kind of physics did you work on?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Back in my days of math and all that, I used to work in &lt;a href="https://en.wikipedia.org/wiki/Computational_biology">computational biology&lt;/a> and a little bit of high energy physics. Computational biology was, for the most part, what I spent most of my time on. And it was essentially exploring the big idea of we have the structure of proteins. We know what they're made of. Now, based on that structure, we want to be able to predict &lt;a href="https://en.wikipedia.org/wiki/Protein_folding">how they're going to fold&lt;/a> and how they're going to behave, which essentially translates into the whole idea of designing pharmaceuticals, designing vaccines, or anything that you can possibly think of that has any connection whatsoever to a living organism.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That would seem to ladder itself well into maybe going to something like bioinformatics. Did you take a tour into that, or did you decide to go elsewhere directly?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It is related, and I worked a little bit with some people that did focus on bioinformatics on the field specifically, but I never took a detour into it. Really, my big idea with computational biology, to be honest, it wasn't even the biology. That's usually what sells it, what people are really interested in, because protein engineering, all the cool and amazing things that you can do.&lt;/p>
&lt;p>Which is definitely good, and I don't want to take away from it. But my big thing is because biology is such a real thing, it is amazingly complicated. And the math— the models that you have to design to study those systems, to be able to predict something that people can actually experiment and measure, it just captivated me. The level of complexity, the beauty, the mechanisms, all the structures that you see once you got through the math and look at things, it just kind of got to me.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How did you go from that world into the world of Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: That's both a really boring story and an interesting one.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>I did my thing with physics, and it was good. It was fun. But at some point, I wanted— working in academia— at least my feeling for it is that generally all the people that you're surrounded with are usually academics. Just another bunch of physics, a bunch of mathematicians.&lt;/p>
&lt;p>But very seldom do you actually get the opportunity to take what you're working on and give it to someone else to use. Even with the mathematicians and physicists, the things that we're working on are super specialized, and you can probably find three, four, five people that can actually understand everything that you're saying. A lot of people are going to get the gist of it, but understanding the details, it's somewhat rare.&lt;/p>
&lt;p>One of the things that I absolutely love about tech, about software engineering, coding, all that, is how open and transparent everything is. You can write your library in Python, you can publish it, and suddenly the world is going to actually use it, actually consume it. And because normally, I've seen that it has a large avenue where you can work in something really complicated, you can communicate it, and people can actually go ahead and take it and run with it in their given direction. And that is kind of what happened.&lt;/p>
&lt;p>At some point, by pure accident and chance, I came across this group of people on the internet, and they were in the stages of making up this new group that's called &lt;a href="https://datafordemocracy.org/">Data for Democracy&lt;/a>, a non-profit. And the whole idea was the internet, especially Twitter— that's how we congregated— Twitter, the internet. We have a ton of data scientists, people who work as software engineers, and the like. What if we all come together and try to solve some issues that actually affect the daily lives of people. And there were a ton of projects. Helping the ACLU gather data for something interesting that they were doing, gather data and analyze it for local governments— where do you have potholes, how much water is being consumed.&lt;/p>
&lt;p>Try to apply all the science that we knew, combined with all the code that we could write, and offer a good and digestible idea for people to say, OK, this makes sense, let's do something about it— policy, action, whatever. And I started working with this group, Data for Democracy— wonderful set of people. And the person who I believe we can blame for Data for Democracy— the one who got the idea and got it up and running, his name is Jonathan Morgan. And eventually, we got to work together. He started a startup, and I went to work with the startup. And that was essentially the thing that took me away from physics and into the world of software engineering— Data for Democracy, definitely.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Were you using Kubernetes as part of that work there?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: No, it was simple as it gets. You just try to get some data. You create a couple &lt;a href="https://ipython.org/">IPython notebooks&lt;/a>, some setting up of really simple MySQL databases, and that was it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Where did you get started using Kubernetes? And was it before you started contributing to it and being a part, or did you decide to jump right in?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: When I first started using Kubernetes, it was also on my first job. So there wasn't a lot of specific training in regards to software engineering or anything of the sort that I did before I actually started working as a software engineer. I just went from physicist to engineer. And in my days of physics, at least on the computer side, I was completely trained in the super old school system administrator, where you have your 10, 20 computers. You know physically where they are, and you have to connect the cables.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: All pets— all pets all the time.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] You have to have your huge Python, bash scripts, three, five major versions, all because doing an upgrade will break something really important and you have no idea how to work on it. And that was my training. That was the way that I learned how to do things. Those were the kind of things that I knew how to do.&lt;/p>
&lt;p>And when I got to this company— startup— we were pretty much starting from scratch. We were building a couple applications. We work testing them, we were deploying them on a couple of managed instances. But like everything, there was a lot of toil that we wanted to automate. The whole issue of, OK, after days of work, we finally managed to get this version of the application up and running in these machines.&lt;/p>
&lt;p>It's open to the internet. People can test it out. But it turns out that it is now two weeks behind the latest on all the master branches for this repo, so now we want to update. And we have to go through the process of bringing it back up, creating new machines, do that whole thing. And I had no idea what Kubernetes was, to be honest. My boss at the moment mentioned it to me like, hey, we should use Kubernetes because apparently, Kubernetes is something that might be able to help us here. And we did some— I want to call it research and development.&lt;/p>
&lt;p>It was actually just making— again, startup, small company, small team, so really me just playing around with Kubernetes trying to get it to work, trying to get it to run. I was so lost. I had no idea what I was doing— not enough. I didn't have an idea of how Kubernetes was supposed to help me. And at that point, I did the best Googling that I could manage. Didn't really find a lot of examples. Didn't find a lot of blog posts. It was early.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What time frame was this?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Three, four years ago, so definitely not 1.13. That's the best guesstimate that I can give at this point. But I wasn't able to find any good examples, any tutorials. The only book that I was able to get my hands on was the one written by Joe Beda, Kelsey Hightower, and I forget the other author. But what is it? &amp;quot;&lt;a href="%5D(http://shop.oreilly.com/product/0636920223788.do)">Kubernetes— Up and Running&lt;/a>&amp;quot;?&lt;/p>
&lt;p>And in general, right now I use it as reference— it's really good. But as a beginner, I still was lost. They give all these amazing examples, they provide the applications, but I had no idea why someone might need a Pod, why someone might need a Deployment. So my last resort was to try and find someone who actually knew Kubernetes.&lt;/p>
&lt;p>By accident, during my eternal Googling, I actually found a link to the &lt;a href="http://slack.kubernetes.io/">Kubernetes Slack&lt;/a>. I jumped into the Kubernetes Slack hoping that someone might be able to help me out. And that was my entry point into the Kubernetes community. I just kept on exploring the Slack, tried to see what people were talking about, what they were asking to try to make sense of it, and just kept on iterating. And at some point, I think I got the hang of it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What made you decide to be a release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The answer to this is my answer to why I have been contributing to Kubernetes. I really just want to be able to help out the community. Kubernetes is something that I absolutely adore.&lt;/p>
&lt;p>Comparing Kubernetes to old school system administration, a handful of years ago, it took me like a week to create a node for an application to run. It took me months to get something that vaguely looked like an Ingress resource— just setting up the Nginx, and allowing someone else to actually use my application. And the fact that I could do all of that in five minutes, it really captivated me. Plus I've got to blame it on the physics. The whole idea with physics, I really like the patterns, and I really like the design of Kubernetes.&lt;/p>
&lt;p>Once I actually got the hang of it, I loved the idea of how everything was designed, and I just wanted to learn a lot more about it. And I wanted to help the contributors. I wanted to help the people who actually build it. I wanted to help maintain it, and help provide the information for new contributors or new users. So instead of taking months for them to be up and running, let's just chat about what your issue is, and let's try to get a fix within the next hour or so.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: You work for a stealth startup right now. Is it fair to assume that they're using Kubernetes?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yes—&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>—for everything.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Are you able to say what &lt;a href="https://www.searchable.ai/">Searchable&lt;/a> does?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The thing that we are trying to build is kind of like a search engine for your documents. Usually, if people have a question, they jump on Google. And for the most part, you're going to be able to get a good answer. You can ask something really random, like 'what is the weight of an elephant?'&lt;/p>
&lt;p>Which, if you think about it, it's kind of random, but Google is going to give you an answer. And the thing that we are trying to build is something similar to that, but for files. So essentially, a search engine for your files. And most people, you have your local machine loaded up with— at least mine, I have a couple tens of gigabytes of different files.&lt;/p>
&lt;p>I have Google Drive. I have a lot of documents that live in my email and the like. So the idea is to kind of build a search engine that is going to be able to connect all of those pieces. And besides doing simple word searches— for example, 'Kubernetes interview', and bring me the documents that we're looking at with all the questions— I can also ask things like what issue did I find last week while testing Prometheus. And it's going to be able to read my files, like through natural language processing, understand it, and be able to give me an answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: It is a Google for your personal and non-public information, essentially?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Hopefully.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is the work that you do with Kubernetes as the release lead— is that part of your day job, or is that something that you're doing kind of nights and weekends separate from your day job?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Both. Strictly speaking, my day job is just keep working on the application, build the things that it needs, maintain the infrastructure, and all that. When I started working at the company— which by the way, the person who brought me into the company was also someone that I met from my days in Data for Democracy— we started talking about the work.&lt;/p>
&lt;p>I mentioned that I do a lot of work with the Kubernetes community and if it was OK that I continue doing it. And to my surprise, the answer was not only a yes, but yeah, you can do it during your day work. And at least for the time being, I just balance— I try to keep things organized.&lt;/p>
&lt;p>Some days I just focus on Kubernetes. Some mornings I do Kubernetes. And then afternoon, I do Searchable, vice-versa, or just go back and forth, and try to balance the work as much as possible. But being release lead, definitely, it is a lot, so nights and weekends.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: How much time does it take to be the release lead?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It varies, but probably, if I had to give an estimate, at the very least you have to be able to dedicate four hours most days.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Four hours a day?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, most days. It varies a lot. For example, at the beginning of the release cycle, you don't need to put in that much work because essentially, you're just waiting and helping people get set up, and people are writing their &lt;a href="https://github.com/kubernetes/enhancements/tree/master/keps">Kubernetes Enhancement Proposals&lt;/a>, they are implementing it, and you can answer some questions. It's relatively easy, but for the most part, a lot of the time the four hours go into talking with people, just making sure that, hey, are people actually writing their enhancements, do we have all the enhancements that we want. And most of those fours hours, going around, chatting with people, and making sure that things are being done. And if, for some reason, someone needs help, just directing them to the right place to get their answer.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What does Searchable get out of you doing this work?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Physically, nothing. The thing that we're striving for is to give back to the community. My manager/boss/homeslice— I told him I was going to call him my homeslice— both of us have experience working in open source. At some point, he was also working on a project that I'm probably going to mispronounce, but Mahout with Apache.&lt;/p>
&lt;p>And he also has had this experience. And both of us have this general idea and strive to build something for Searchable that's going to be useful for people, but also build knowledge, build guides, build applications that are going to be useful for the community. And at least one of the things that I was able to do right now is be the lead for the Kubernetes team. And this is a way of giving back to the community. We're using Kubernetes to run our things, so let's try to balance how things work.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Lachlan Evenson was the release lead on 1.16 as well as &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">our guest back in episode 72&lt;/a>, and he's returned on this release as the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team/role-handbooks/emeritus-adviser">emeritus advisor&lt;/a>. What did you learn from him?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Oh, everything. And it actually all started back on 1.16. So like you said, an amazing person— he's an amazing individual. And it's truly an opportunity to be able to work with him. During 1.16, I was the CI Signal lead, and Lachie is very hands on.&lt;/p>
&lt;p>He's not the kind of person to just give you a list of things and say, do them. He actually comes to you, has a conversation, and he works with you more than anything. And when we were working together on 1.16, I got to learn a lot from him in terms of CI Signal. And especially because we talked about everything just to make sure that 1.16 was ready to go, I also got to pick up a couple of things that a release lead has to know, has to be able to do, has to work on to get a release out the door.&lt;/p>
&lt;p>And now, during this release, there is a lot of information that's really useful, and there's a lot of advice and general wisdom that comes in handy. For most of the things that impact a lot of things, we are always in communication. Like, I'm doing this, you're doing that, advice. And essentially, every single thing that we do is pretty much a code review. You do it, and then you wait for someone else to give you comments. And that's been a strong part of our relationship working.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would you say the theme for this release is?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I think one of the themes is &amp;quot;fit and finish&amp;quot;. There are a lot of features that we are bumping from alpha to beta, from beta to stable. And we want to make sure that people have a good user experience. Operators and developers alike just want to get rid of as many bugs as possible, improve the flow of things.&lt;/p>
&lt;p>But the other really cool thing is we have about an equal distribution between alpha, beta, and stable. We are also bringing up a lot of new features. So besides making Kubernetes more stable for all the users that are already using it, we are working on bringing up new things that people can try out for the next release and see how it goes in the future.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did you have a release team mascot?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Kind of.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Who/what was it?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] I say kind of because I'm using the mascot in the &lt;a href="https://twitter.com/KubernetesPod/status/1242953121380392963">logo&lt;/a>, and the logo is inspired by the Large Hadron Collider.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, fantastic.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Being the release lead, I really had to take a chance on this opportunity to use the LHC as the mascot.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We've had &lt;a href="https://kubernetespodcast.com/episode/062-cern/">some of the folks from the LHC on the show&lt;/a>, and I know they listen, and they will be thrilled with that.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: [LAUGHING] Hopefully, they like the logo.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you look at this release, what part of this release, what thing that has been added to it are you personally most excited about?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Like a parent can't choose which child is his or her favorite, you really can't choose a specific thing.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: We have been following online and in the issues an enhancement that's called &lt;a href="https://github.com/kubernetes/enhancements/issues/753">sidecar containers&lt;/a>. You'd be able to mark the order of containers starting in a pod. Tim Hockin posted &lt;a href="https://github.com/kubernetes/enhancements/issues/753#issuecomment-597372056">a long comment on behalf of a number of SIG Node contributors&lt;/a> citing social, procedural, and technical concerns about what's going on with that— in particular, that it moved out of 1.18 and is now moving to 1.19. Did you have any thoughts on that?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: The sidecar enhancement has definitely been an interesting one. First off, thank you very much to Joseph Irving, the author of the KEP. And thank you very much to Tim Hockin, who voiced out the point of view of the approvers, maintainers of SIG Node. And I guess a little bit of context before we move on is, in the Kubernetes community, we have contributors, we have reviewers, and we have approvers.&lt;/p>
&lt;p>Contributors are people who write PRs, who file issues, who troubleshoot issues. Reviewers are contributors who focus on one or multiple specific areas within the project, and then approvers are maintainers for the specific area, for one or multiple specific areas, of the project. So you can think of approvers as people who have write access in a repo or someplace within a repo.&lt;/p>
&lt;p>The issue with the sidecar enhancement is that it has been deferred for multiple releases now, and that's been because there hasn't been a lot of collaboration between the KEP authors and the approvers for specific parts of the project. Something worthwhile to mention— and this was brought up during the original discussion— is this can obviously be frustrating for both contributors and for approvers. From the contributor's side of things, you are working on something. You are doing your best to make sure that it works.&lt;/p>
&lt;p>And to build something that's going to be used by people, both from the approver side of things and, I think, for the most part, every single person in the Kubernetes community, we are all really excited to see this project grow. We want to help improve it, and we love when new people come in and work on new enhancements, bug fixes, and the like.&lt;/p>
&lt;p>But one of the limitations is the day only has so many hours, and there are only so many things that we can work on at a time. So people prioritize in whatever way works best, and some things just fall behind. And a lot of the time, the things that fall behind are not because people don't want them to continue moving forward, but it's just a limited amount of resources, a limited amount of people.&lt;/p>
&lt;p>And I think this discussion around the sidecar enhancement proposal has been very useful, and it points us to the need for more standardized mentoring programs. This is something that multiple SIGs are working on. For example, SIG Contribex, SIG Cluster Lifecycle, SIG Release. The idea is to standardize some sort of mentoring experience so that we can better prepare new contributors to become reviewers and ultimately approvers.&lt;/p>
&lt;p>Because ultimately at the end of the day, if we have more people who are knowledgeable about Kubernetes, or even some specific area of Kubernetes, we can better distribute the load, and we can better collaborate on whatever new things come up. I think the sidecar enhancement has shown us mentoring is something worthwhile, and we need a lot more of it. Because as much work as we do, more things are going to continue popping in throughout the project. And the more people we have who are comfortable working in these really complicated areas of Kubernetes, the better off that we are going to be.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Was there any talk of delaying 1.18 due to the current worldwide health situation?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: We thought about it, and the plan was to just wait and see how people felt. Tried make sure that people were comfortable continuing to work and all the people were landing in new enhancements, or fixing tests, or members of the release team who were making sure that things were happening. We wanted to see that people were comfortable, that they could continue doing their job. And for a moment, I actually thought about delaying just outright— we're going to give it more time, and hopefully at some point, things are going to work out.&lt;/p>
&lt;p>But people just continue doing their amazing work. There was no delay. There was no hitch throughout the process. So at some point, I just figured we stay with the current timeline and see how we went. And at this point, things are more or less set.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Amazing power of a distributed team.&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, definitely.&lt;/p>
&lt;p>[LAUGHING]&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: &lt;a href="https://twitter.com/alejandrox135/status/1239629281766096898">Taylor Dolezal was announced as the 1.19 release lead&lt;/a>. Do you know how that choice was made, and by whom?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: I actually got to choose the lead. The practice is the current lead for the release team is going to look at people and see, first off, who's interested and out of the people interested, who can do the job, who's comfortable enough with the release team, with the Kubernetes community at large who can actually commit the amount of hours throughout the next, hopefully, three months.&lt;/p>
&lt;p>And for one, I think Taylor has been part of my team. So there is the release team. Then the release team has multiple subgroups. One of those subgroups is actually just for me and my shadows. So for this release, it was mrbobbytables and Taylor. And Taylor volunteered to take over 1.19, and I'm sure that he will do an amazing job.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I am as well. What advice will you give Taylor?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Over-communicate as much as possible. Normally, if you made it to the point that you are the lead for a release, or even the shadow for a release, you more or less are familiar with a lot of the work— CI Signal, enhancements, documentation, and the like. And a lot of people, if they know how to do their job, they might tell themselves, yeah, I could do it— no need to worry about it. I'm just going to go ahead and sign this PR, debug this test, whatever.&lt;/p>
&lt;p>But one of the interesting aspects is whenever we are actually working in a release, 50% of the work has to go into actually making the release happen. The other 50% of the work has to go into mentoring people, and making sure the newcomers, new members are able to learn everything that they need to learn to do your job, you being in the lead for a subgroup or the entire team. And whenever you actually see that things need to happen, just over-communicate.&lt;/p>
&lt;p>Try to provide the opportunity for someone else to do the work, and over-communicate with them as much as possible to make sure that they are learning whatever it is that they need to learn. If neither you or the other person knows what's going on, then I can over-communicate, so someone hopefully will see your messages and come to the rescue. That happens a lot. There's a lot of really nice and kind people who will come out and tell you how something works, help you fix it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: If you were to sum up your experience running this release, what would it be?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: It's been super fun and a little bit stressing, to be honest. Being the release lead is definitely amazing. You're kind of sitting at the center of Kubernetes.&lt;/p>
&lt;p>You not only see the people who are working on things— the things that are broken, and the users filling out issues, and saying what broke, and the like. But you also get the opportunity to work with a lot of people who do a lot of non-code related work. Docs is one of the most obvious things. There's a lot of work that goes into communications, contributor experience, public relations.&lt;/p>
&lt;p>And being connected, getting to talk with those people mostly every other day, it's really fun. It's a really good experience in terms of becoming a better contributor to the community, but also taking some of that knowledge home with you and applying it somewhere else. If you are a software engineer, if you are a project manager, whatever, it's amazing how much you can learn.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: I know the community likes to rotate around who are the release leads. But if you were given the opportunity to be a release lead for a future release of Kubernetes, would you do it again?&lt;/strong>&lt;/p>
&lt;p>JORGE ALARCÓN: Yeah, it's a fun job. To be honest, it can be really stressing. Especially, as I mentioned, at some point, most of that work is just going to be talking with people, and talking requires a lot more thought and effort than just sitting down and thinking about things sometimes. And some of that can be really stressful.&lt;/p>
&lt;p>But the job itself, it is definitely fun. And at some distant point in the future, if for some reason it was a possibility, I will think about it. But definitely, as you mentioned, one thing that we try to do is cycle out, because I can have fun in it, and that's all good and nice. And hopefully I can help another release go out the door. But providing the opportunity for other people to learn I think is a lot more important than just being the lead itself.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/alejandrox135">Jorge Alarcón&lt;/a> is a site reliability engineer with Searchable AI and served as the Kubernetes 1.18 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: Music and math: the Kubernetes 1.17 release interview</title><link>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</link><pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/07/27/music-and-math-the-kubernetes-1.17-release-interview/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: Adam Glick (Google)&lt;/p>
&lt;p>Every time the Kubernetes release train stops at the station, we like to ask the release lead to take a moment to reflect on their experience. That takes the form of an interview on the weekly &lt;a href="https://kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> that I co-host with &lt;a href="https://twitter.com/craigbox">Craig Box&lt;/a>. If you're not familiar with the show, every week we summarise the new in the Cloud Native ecosystem, and have an insightful discussion with an interesting guest from the broader Kubernetes community.&lt;/p>
&lt;p>At the time of the 1.17 release in December, we &lt;a href="https://kubernetespodcast.com/episode/083-kubernetes-1.17/">talked to release team lead Guinevere Saenger&lt;/a>. We have &lt;a href="https://kubernetes.io/blog/2018/07/16/how-the-sausage-is-made-the-kubernetes-1.11-release-interview-from-the-kubernetes-podcast/">shared&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/05/13/cat-shirts-and-groundhog-day-the-kubernetes-1.14-release-interview/">the&lt;/a> &lt;a href="https://kubernetes.io/blog/2019/12/06/when-youre-in-the-release-team-youre-family-the-kubernetes-1.16-release-interview/">transcripts&lt;/a> of previous interviews on the Kubernetes blog, and we're very happy to share another today.&lt;/p>
&lt;p>Next week we will bring you up to date with the story of Kubernetes 1.18, as we gear up for the release of 1.19 next month. &lt;a href="https://kubernetespodcast.com/subscribe/">Subscribe to the show&lt;/a> wherever you get your podcasts to make sure you don't miss that chat!&lt;/p>
&lt;hr>
&lt;p>&lt;strong>ADAM GLICK: You have a nontraditional background for someone who works as a software engineer. Can you explain that background?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: My first career was as a &lt;a href="https://en.wikipedia.org/wiki/Collaborative_piano">collaborative pianist&lt;/a>, which is an academic way of saying &amp;quot;piano accompanist&amp;quot;. I was a classically trained pianist who spends most of her time onstage, accompanying other people and making them sound great.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that the piano equivalent of pair-programming?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: No one has said it to me like that before, but all sorts of things are starting to make sense in my head right now. I think that's a really great way of putting it.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: That's a really interesting background, as someone who also has a background with music. What made you decide to get into software development?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I found myself in a life situation where I needed more stable source of income, and teaching music, and performing for various gig opportunities, was really just not cutting it anymore. And I found myself to be working really, really hard with not much to show for it. I had a lot of friends who were software engineers. I live in Seattle. That's sort of a thing that happens to you when you live in Seattle — you get to know a bunch of software engineers, one way or the other.&lt;/p>
&lt;p>The ones I met were all lovely people, and they said, hey, I'm happy to show you how to program in Python. And so I did that for a bit, and then I heard about this program called &lt;a href="https://adadevelopersacademy.org/">Ada Developers Academy&lt;/a>. That's a year long coding school, targeted at women and non-binary folks that are looking for a second career in tech. And so I applied for that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What can you tell us about that program?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's incredibly selective, for starters. It's really popular in Seattle and has gotten quite a good reputation. It took me three tries to get in. They do two classes a year, and so it was a while before I got my response saying 'congratulations, we are happy to welcome you into Cohort 6'. I think what sets Ada Developers Academy apart from other bootcamp style coding programs are three things, I think? The main important one is that if you get in, you pay no tuition. The entire program is funded by company sponsors.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The other thing that really convinced me is that five months of the 11-month program are an industry internship, which means you get both practical experience, mentorship, and potential job leads at the end of it.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: So very much like a condensed version of the University of Waterloo degree, where you do co-op terms.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Interesting. I didn't know about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Having lived in Waterloo for a while, I knew a lot of people who did that. But what would you say the advantages were of going through such a condensed schooling process in computer science?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure that the condensed process is necessarily an advantage. I think it's a necessity, though. People have to quit their jobs to go do this program. It's not an evening school type of thing.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: And your internship is basically a full-time job when you do it. One thing that Ada was really, really good at is giving us practical experience that directly relates to the workplace. We learned how to use Git. We learned how to design websites using &lt;a href="https://rubyonrails.org/">Rails&lt;/a>. And we also learned how to collaborate, how to pair-program. We had a weekly retrospective, so we sort of got a soft introduction to workflows at a real workplace. Adding to that, the internship, and I think the overall experience is a little bit more 'practical workplace oriented' and a little bit less academic.&lt;/p>
&lt;p>When you're done with it, you don't have to relearn how to be an adult in a working relationship with other people. You come with a set of previous skills. There are Ada graduates who have previously been campaign lawyers, and veterinarians, and nannies, cooks, all sorts of people. And it turns out these skills tend to translate, and they tend to matter.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: With your background in music, what do you think that that allows you to bring to software development that could be missing from, say, standard software development training that people go through?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People tend to really connect the dots when I tell them I used to be a musician. Of course, I still consider myself a musician, because you don't really ever stop being a musician. But they say, 'oh, yeah, music and math', and that's just a similar sort of brain. And that makes so much sense. And I think there's a little bit of a point to that. When you learn a piece of music, you have to start recognizing patterns incredibly quickly, almost intuitively.&lt;/p>
&lt;p>And I think that is the main skill that translates into programming— recognizing patterns, finding the things that work, finding the things that don't work. And for me, especially as a collaborative pianist, it's the communicating with people, the finding out what people really want, where something is going, how to figure out what the general direction is that we want to take, before we start writing the first line of code.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: In your experience at Ada or with other experiences you've had, have you been able to identify patterns in other backgrounds for people that you'd recommend, 'hey, you're good at music, so therefore you might want to consider doing something like a course in computer science'?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Overall, I think ultimately writing code is just giving a set of instructions to a computer. And we do that in daily life all the time. We give instructions to our kids, we give instructions to our students. We do math, we write textbooks. We give instructions to a room full of people when you're in court as a lawyer.&lt;/p>
&lt;p>Actually, the entrance exam to Ada Developers Academy used to have questions from the &lt;a href="https://en.wikipedia.org/wiki/Law_School_Admission_Test">LSAT&lt;/a> on it to see if you were qualified to join the program. They changed that when I applied, but I think that's a thing that happened at one point. So, overall, I think software engineering is a much more varied field than we give it credit for, and that there are so many ways in which you can apply your so-called other skills and bring them under the umbrella of software engineering.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I do think that programming is effectively half art and half science. There's creativity to be applied. There is perhaps one way to solve a problem most efficiently. But there are many different ways that you can choose to express how you compiled something down to that way.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah, I mean, that's definitely true. I think one way that you could probably prove that is that if you write code at work and you're working on something with other people, you can probably tell which one of your co-workers wrote which package, just by the way it's written, or how it is documented, or how it is styled, or any of those things. I really do think that the human character shines through.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What got you interested in Kubernetes and open source?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The honest answer is absolutely nothing. Going back to my programming school— and remember that I had to do a five-month internship as part of my training— the way that the internship works is that sponsor companies for the program get interns in according to how much they sponsored a specific cohort of students.&lt;/p>
&lt;p>So at the time, Samsung and SDS offered to host two interns for five months on their &lt;a href="https://samsung-cnct.github.io/">Cloud Native Computing team&lt;/a> and have that be their practical experience. So I go out of a Ruby on Rails full stack web development bootcamp and show up at my internship, and they said, &amp;quot;Welcome to Kubernetes. Try to bring up a cluster.&amp;quot; And I said, &amp;quot;Kuber what?&amp;quot;&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've all said that on occasion.&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Trial by fire, wow.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I will say that that entire team was absolutely wonderful, delightful to work with, incredibly helpful. And I will forever be grateful for all of the help and support that I got in that environment. It was a great place to learn.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You now work on GitHub's Kubernetes infrastructure. Obviously, there was GitHub before there was a Kubernetes, so a migration happened. What can you tell us about the transition that GitHub made to running on Kubernetes?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A disclaimer here— I was not at GitHub at the time that the transition to Kubernetes was made. However, to the best of my knowledge, the decision to transition to Kubernetes was made and people decided, yes, we want to try Kubernetes. We want to use Kubernetes. And mostly, the only decision left was, which one of our applications should we move over to Kubernetes?&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I thought GitHub was written on Rails, so there was only one application.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: [LAUGHING] We have a lot of supplementary stuff under the covers.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I'm sure.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But yes, GitHub is written in Rails. It is still written in Rails. And most of the supplementary things are currently running on Kubernetes. We have a fair bit of stuff that currently does not run on Kubernetes. Mainly, that is GitHub Enterprise related things. I would know less about that because I am on the platform team that helps people use the Kubernetes infrastructure. But back to your question, leadership at the time decided that it would be a good idea to start with GitHub the Rails website as the first project to move to Kubernetes.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: High stakes!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: The reason for this was that they decided if they were going to not start big, it really wasn't going to transition ever. It was really not going to happen. So they just decided to go all out, and it was successful, for which I think the lesson would probably be commit early, commit big.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Are there any other lessons that you would take away or that you've learned kind of from the transition that the company made, and might be applicable to other people who are looking at moving their companies from a traditional infrastructure to a Kubernetes infrastructure?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I'm not sure this is a lesson specifically, but I was on support recently, and it turned out that, due to unforeseen circumstances and a mix of human error, a bunch of the namespaces on one of our Kubernetes clusters got deleted.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Oh, my.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It should not have affected any customers, I should mention, at this point. But all in all, it took a few of us a few hours to almost completely recover from this event. I think that, without Kubernetes, this would not have been possible.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Generally, deleting something like that is quite catastrophic. We've seen a number of other vendors suffer large outages when someone's done something to that effect, which is why we get &lt;a href="https://twitter.com/hashtag/hugops">#hugops&lt;/a> on Twitter all the time.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: People did send me #hugops, that is a thing that happened. But overall, something like this was an interesting stress test and sort of proved that it wasn't nearly as catastrophic as a worst case scenario.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: GitHub &lt;a href="https://githubengineering.com/githubs-metal-cloud/">runs its own data centers&lt;/a>. Kubernetes was largely built for running on the cloud, but a lot of people do choose to run it on their own, bare metal. How do you manage clusters and provisioning of the machinery you run?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I started, my onboarding project was to deprovision an old cluster, make sure all the traffic got moved to somewhere where it would keep running, provision a new cluster, and then move website traffic onto the new cluster. That was a really exciting onboarding project. At the time, we provisioned bare metal machines using Puppet. We still do that to a degree, but I believe the team that now runs our computing resources actually inserts virtual machines as an extra layer between the bare metal and the Kubernetes nodes.&lt;/p>
&lt;p>Again, I was not intrinsically part of that decision, but my understanding is that it just makes for a greater reliability and reproducibility across the board. We've had some interesting hardware dependency issues come up, and the virtual machines basically avoid those.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: You've been working with Kubernetes for a couple of years now. How did you get involved in the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: When I first started in the project, I started at the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience#readme">special interest group for contributor experience&lt;/a>, namely because one of my co-workers at the time, Aaron Crickenberger, was a big Kubernetes community person. Still is.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: We've &lt;a href="https://kubernetespodcast.com/episode/046-kubernetes-1.14/">had him on the show&lt;/a> for one of these very release interviews!&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: In fact, this is true! So Aaron and I actually go way back to Samsung SDS. Anyway, Aaron suggested that I should write up a contribution to the Kubernetes project, and I said, me? And he said, yes, of course. You will be &lt;a href="https://www.youtube.com/watch?v=TkCDUFR6xqw">speaking at KubeCon&lt;/a>, so you should probably get started with a PR or something. So I tried, and it was really, really hard. And I complained about it &lt;a href="https://github.com/kubernetes/community/issues/141">in a public GitHub issue&lt;/a>, and people said, yeah. Yeah, we know it's hard. Do you want to help with that?&lt;/p>
&lt;p>And so I started getting really involved with the &lt;a href="https://github.com/kubernetes/community/tree/master/contributors/guide">process for new contributors to get started&lt;/a> and have successes, kind of getting a foothold into a project that's as large and varied as Kubernetes. From there on, I began to talk to people, get to know people. The great thing about the Kubernetes community is that there is so much mentorship to go around.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Right.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: There are so many friendly people willing to help. It's really funny when I talk to other people about it. They say, what do you mean, your coworker? And I said, well, he's really a colleague. He really works for another company.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: He's sort-of officially a competitor.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yeah.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: But we're friends.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: But he totally helped me when I didn't know how to git patch my borked pull request. So that happened. And eventually, somebody just suggested that I start following along in the release process and shadow someone on their release team role. And that, at the time, was Tim Pepper, who was bug triage lead, and I shadowed him for that role.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Another &lt;a href="https://kubernetespodcast.com/episode/010-kubernetes-1.11/">podcast guest&lt;/a> on the interview train.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This is a pattern that probably will make more sense once I explain to you about the shadow process of the release team.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Well, let's turn to the Kubernetes release and the release process. First up, what's new in this release of 1.17?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: We have only a very few new things. The one that I'm most excited about is that we have moved &lt;a href="https://github.com/kubernetes/enhancements/issues/563">IPv4 and IPv6 dual stack&lt;/a> support to alpha. That is the most major change, and it has been, I think, a year and a half in coming. So this is the very first cut of that feature, and I'm super excited about that.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The people who have been promised IPv6 for many, many years and still don't really see it, what will this mean for them?&lt;/strong>&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: And most importantly, why did we skip IPv5 support?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I don't know!&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Please see &lt;a href="https://softwareengineering.stackexchange.com/questions/185380/ipv4-to-ipv6-where-is-ipv5">the appendix to this podcast&lt;/a> for technical explanations.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Having a dual stack configuration obviously enables people to have a much more flexible infrastructure and not have to worry so much about making decisions that will become outdated or that may be over-complicated. This basically means that pods can have dual stack addresses, and nodes can have dual stack addresses. And that basically just makes communication a lot easier.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: What about features that didn't make it into the release? We had a conversation with Lachie in the &lt;a href="https://kubernetespodcast.com/episode/072-kubernetes-1.16/">1.16 interview&lt;/a>, where he mentioned &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-apps/sidecarcontainers.md">sidecar containers&lt;/a>. They unfortunately didn't make it into that release. And I see now that they haven't made this one either.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: They have not, and we are actually currently undergoing an effort of tracking features that flip multiple releases.&lt;/p>
&lt;p>As a community, we need everyone's help. There are a lot of features that people want. There is also a lot of cleanup that needs to happen. And we have started talking at previous KubeCons repeatedly about problems with maintainer burnout, reviewer burnout, have a hard time finding reviews for your particular contributions, especially if you are not an entrenched member of the community. And it has become very clear that this is an area where the entire community needs to improve.&lt;/p>
&lt;p>So the unfortunate reality is that sometimes life happens, and people are busy. This is an open source project. This is not something that has company mandated OKRs. Particularly during the fourth quarter of the year in North America, but around the world, we have a lot of holidays. It is the end of the year. Kubecon North America happened as well. This makes it often hard to find a reviewer in time or to rally the support that you need for your enhancement proposal. Unfortunately, slipping releases is fairly common and, at this point, expected. We started out with having 42 enhancements and &lt;a href="https://docs.google.com/spreadsheets/d/1ebKGsYB1TmMnkx86bR2ZDOibm5KWWCs_UjV3Ys71WIs/edit#gid=0">landed with roughly half of that&lt;/a>.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: I was going to ask about the truncated schedule due to the fourth quarter of the year, where there are holidays in large parts of the world. Do you find that the Q4 release on the whole is smaller than others, if not for the fact that it's some week shorter?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Q4 releases are shorter by necessity because we are trying to finish the final release of the year before the end of the year holidays. Often, releases are under pressure of KubeCons, during which finding reviewers or even finding the time to do work can be hard to do, if you are attending. And even if you're not attending, your reviewers might be attending.&lt;/p>
&lt;p>It has been brought up last year to make the final release more of a stability release, meaning no new alpha features. In practice, for this release, this is actually quite close to the truth. We have four features graduating to beta and most of our features are graduating to stable. I am hoping to use this as a precedent to change our process to make the final release a stability release from here on out. The timeline fits. The past experience fits this model.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: On top of all of the release work that was going on, there was also KubeCon that happened. And you were involved in the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2019/11-contributor-summit">contributor summit&lt;/a>. How was the summit?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: This was the first contributor summit where we had an organized events team with events organizing leads, and handbooks, and processes. And I have heard from multiple people— this is just word of mouth— that it was their favorite contributor summit ever.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Was someone allocated to hat production? &lt;a href="https://flickr.com/photos/143247548@N03/49093218951/">Everyone had sailor hats&lt;/a>.&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Yes, the entire event staff had sailor hats with their GitHub handle on them, and it was pretty fantastic. You can probably see me wearing one in some of the pictures from the contributor summit. That literally was something that was pulled out of a box the morning of the contributor summit, and no one had any idea. But at first, I was a little skeptical, but then I put it on and looked at myself in the mirror. And I was like, yes. Yes, this is accurate. We should all wear these.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Did getting everyone together for the contributor summit help with the release process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It did not. It did quite the opposite, really. Well, that's too strong.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: Is that just a matter of the time taken up?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It's just a completely different focus. Honestly, it helped getting to know people face-to-face that I had currently only interacted with on video. But we did have to cancel the release team meeting the day of the contributor summit because there was kind of no sense in having it happen. We moved it to the Tuesday, I believe.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The role of the release team leader has been described as servant leadership. Do you consider the position proactive or reactive?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Honestly, I think that depends on who's the release team lead, right? There are some people who are very watchful and look for trends, trying to detect problems before they happen. I tend to be in that camp, but I also know that sometimes it's not possible to predict things. There will be last minute bugs sometimes, sometimes not. If there is a last minute bug, you have to be ready to be on top of that. So for me, the approach has been I want to make sure that I have my priorities in order and also that I have backups in case I can't be available.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What was the most interesting part of the release process for you?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: A release lead has to have served in other roles on the release team prior to being release team lead. To me, it was very interesting to see what other roles were responsible for, ones that I hadn't seen from the inside before, such as docs, CI signal. I had helped out with CI signal for a bit, but I want to give a big shout out to CI signal lead, Alena Varkockova, who was able to communicate effectively and kindly with everyone who was running into broken tests, failing tests. And she was very effective in getting all of our tests up and running.&lt;/p>
&lt;p>So that was actually really cool to see. And yeah, just getting to see more of the workings of the team, for me, it was exciting. The other big exciting thing, of course, was to see all the changes that were going in and all the efforts that were being made.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: The release lead for 1.18 has just been announced as &lt;a href="https://twitter.com/alejandrox135">Jorge Alarcon&lt;/a>. What are you going to put in the proverbial envelope as advice for him?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: I would want Jorge to be really on top of making sure that every Special Interest Group that enters a change, that has an enhancement for 1.18, is on top of the timelines and is responsive. Communication tends to be a problem. And I had hinted at this earlier, but some enhancements slipped simply because there wasn't enough reviewer bandwidth.&lt;/p>
&lt;p>Greater communication of timelines and just giving people more time and space to be able to get in their changes, or at least, seemingly give them more time and space by sending early warnings, is going to be helpful. Of course, he's going to have a slightly longer release, too, than I did. This might be related to a unique Q4 challenge. Overall, I would encourage him to take more breaks, to rely more on his release shadows, and split out the work in a fashion that allows everyone to have a turn and everyone to have a break as well.&lt;/p>
&lt;p>&lt;strong>ADAM GLICK: What would your advice be to someone who is hearing your experience and is inspired to get involved with the Kubernetes release or contributer process?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: Those are two separate questions. So let me tackle the Kubernetes release question first. Kubernetes &lt;a href="https://github.com/kubernetes/sig-release/#readme">SIG Release&lt;/a> has, in my opinion, a really excellent onboarding program for new members. We have what is called the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/release-team/shadows.md">Release Team Shadow Program&lt;/a>. We also have the Release Engineering Shadow Program, or the Release Management Shadow Program. Those are two separate subprojects within SIG Release. And each subproject has a team of roles, and each role can have two to four shadows that are basically people who are part of that role team, and they are learning that role as they are doing it.&lt;/p>
&lt;p>So for example, if I am the lead for bug triage on the release team, I may have two, three or four people that I closely work with on the bug triage tasks. These people are my shadows. And once they have served one release cycle as a shadow, they are now eligible to be lead in that role. We have an application form for this process, and it should probably be going up in January. It usually happens the first week of the release once all the release leads are put together.&lt;/p>
&lt;p>&lt;strong>CRAIG BOX: Do you think being a member of the release team is something that is a good first contribution to the Kubernetes project overall?&lt;/strong>&lt;/p>
&lt;p>GUINEVERE SAENGER: It depends on what your goals are, right? I believe so. I believe, for me, personally, it has been incredibly helpful looking into corners of the project that I don't know very much about at all, like API machinery, storage. It's been really exciting to look over all the areas of code that I normally never touch.&lt;/p>
&lt;p>It depends on what you want to get out of it. In general, I think that being a release team shadow is a really, really great on-ramp to being a part of the community because it has a paved path solution to contributing. All you have to do is show up to the meetings, ask questions of your lead, who is required to answer those questions.&lt;/p>
&lt;p>And you also do real work. You really help, you really contribute. If you go across the issues and pull requests in the repo, you will see, 'Hi, my name is so-and-so. I am shadowing the CI signal lead for the current release. Can you help me out here?' And that's a valuable contribution, and it introduces people to others. And then people will recognize your name. They'll see a pull request by you, and they're like oh yeah, I know this person. They're legit.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&lt;a href="https://twitter.com/guincodes">Guinevere Saenger&lt;/a> is a software engineer for GitHub and served as the Kubernetes 1.17 release team lead.&lt;/em>&lt;/p>
&lt;p>&lt;em>You can find the &lt;a href="http://www.kubernetespodcast.com/">Kubernetes Podcast from Google&lt;/a> at &lt;a href="https://twitter.com/KubernetesPod">@KubernetesPod&lt;/a> on Twitter, and you can &lt;a href="https://kubernetespodcast.com/subscribe/">subscribe&lt;/a> so you never miss an episode.&lt;/em>&lt;/p></description></item><item><title>Blog: SIG-Windows Spotlight</title><link>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</link><pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/30/sig-windows-spotlight-2020/</guid><description>
&lt;p>&lt;em>This post tells the story of how Kubernetes contributors work together to provide a container orchestrator that works for both Linux and Windows.&lt;/em>&lt;/p>
&lt;img alt="Image of a computer with Kubernetes logo" width="30%" src="KubernetesComputer_transparent.png">
&lt;p>Most people who are familiar with Kubernetes are probably used to associating it with Linux. The connection makes sense, since Kubernetes ran on Linux from its very beginning. However, many teams and organizations working on adopting Kubernetes need the ability to orchestrate containers on Windows. Since the release of Docker and rise to popularity of containers, there have been efforts both from the community and from Microsoft itself to make container technology as accessible in Windows systems as it is in Linux systems.&lt;/p>
&lt;p>Within the Kubernetes community, those who are passionate about making Kubernetes accessible to the Windows community can find a home in the Windows Special Interest Group. To learn more about SIG-Windows and the future of Kubernetes on Windows, I spoke to co-chairs &lt;a href="https://github.com/marosset">Mark Rossetti&lt;/a> and &lt;a href="https://github.com/michmike">Michael Michael&lt;/a> about the SIG's goals and how others can contribute.&lt;/p>
&lt;h2 id="intro-to-windows-containers-kubernetes">Intro to Windows Containers &amp;amp; Kubernetes&lt;/h2>
&lt;p>Kubernetes is the most popular tool for orchestrating container workloads, so to understand the Windows Special Interest Group (SIG) within the Kubernetes project, it's important to first understand what we mean when we talk about running containers on Windows.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;When looking at Windows support in Kubernetes,&amp;quot; says SIG (Special Interest Group) Co-chairs Mark Rossetti and Michael Michael, &amp;quot;many start drawing comparisons to Linux containers. Although some of the comparisons that highlight limitations are fair, it is important to distinguish between operational limitations and differences between the Windows and Linux operating systems. Windows containers run the Windows operating system and Linux containers run Linux.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In essence, any &amp;quot;container&amp;quot; is simply a process being run on its host operating system, with some key tooling in place to isolate that process and its dependencies from the rest of the environment. The goal is to make that running process safely isolated, while taking up minimal resources from the system to perform that isolation. On Linux, the tooling used to isolate processes to create &amp;quot;containers&amp;quot; commonly boils down to cgroups and namespaces (among a few others), which are themselves tools built in to the Linux Kernel.&lt;/p>
&lt;img alt="A visual analogy using dogs to explain Linux cgroups and namespaces." width="40%" src="cgroupsNamespacesComboPic.png">
&lt;h4 id="if-dogs-were-processes-containerization-would-be-like-giving-each-dog-their-own-resources-like-toys-and-food-using-cgroups-and-isolating-troublesome-dogs-using-namespaces">&lt;em>If dogs were processes: containerization would be like giving each dog their own resources like toys and food using cgroups, and isolating troublesome dogs using namespaces.&lt;/em>&lt;/h4>
&lt;p>Native Windows processes are processes that are or must be run on a Windows operating system. This makes them fundamentally different from a process running on a Linux operating system. Since Linux containers are Linux processes being isolated by the Linux kernel tools known as cgroups and namespaces, containerizing native Windows processes meant implementing similar isolation tools within the Windows kernel itself. Thus, &amp;quot;Windows Containers&amp;quot; and &amp;quot;Linux Containers&amp;quot; are fundamentally different technologies, even though they have the same goals (isolating processes) and in some ways work similarly (using kernel level containerization).&lt;/p>
&lt;p>So when it comes to running containers on Windows, there are actually two very important concepts to consider:&lt;/p>
&lt;ul>
&lt;li>Native Windows processes running as native Windows Server style containers,&lt;/li>
&lt;li>and traditional Linux containers running on a Linux Kernel, generally hosted on a lightweight Hyper-V Virtual Machine.&lt;/li>
&lt;/ul>
&lt;p>You can learn more about Linux and Windows containers in this &lt;a href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/deploy-containers/linux-containers">tutorial&lt;/a> from Microsoft.&lt;/p>
&lt;h3 id="kubernetes-on-windows">Kubernetes on Windows&lt;/h3>
&lt;p>Kubernetes was initially designed with Linux containers in mind and was itself designed to run on Linux systems. Because of that, much of the functionality of Kubernetes involves unique Linux functionality. The Linux-specific work is intentional--we all want Kubernetes to run optimally on Linux--but there is a growing demand for similar optimization for Windows servers. For cases where users need container orchestration on Windows, the Kubernetes contributor community of SIG-Windows has incorporated functionality for Windows-specific use cases.&lt;/p>
&lt;hr>
&lt;p>&lt;em>&amp;quot;A common question we get is, will I be able to have a Windows-only cluster. The answer is NO. Kubernetes control plane components will continue to be based on Linux, while SIG-Windows is concentrating on the experience of having Windows worker nodes in a Kubernetes cluster.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>Rather than separating out the concepts of &amp;quot;Windows Kubernetes,&amp;quot; and &amp;quot;Linux Kubernetes,&amp;quot; the community of SIG-Windows works toward adding functionality to the main Kubernetes project which allows it to handle use cases for Windows. These Windows capabilities mirror, and in some cases add unique functionality to, the Linux use cases Kubernetes has served since its release in 2014 (want to learn more history? Scroll through this &lt;a href="https://github.com/kubernetes/kubernetes/blob/e2b948dbfbba62b8cb681189377157deee93bb43/DESIGN.md">original design document&lt;/a>.&lt;/p>
&lt;h2 id="what-does-sig-windows-do">What Does SIG-Windows Do?&lt;/h2>
&lt;hr>
&lt;p>&lt;em>&amp;quot;SIG-Windows is really the center for all things Windows in Kubernetes,&amp;quot;&lt;/em> SIG chairs Mark and Michael said, &lt;em>&amp;quot;We mainly focus on the compute side of things, but really anything related to running Kubernetes on Windows is in scope for SIG-Windows.&amp;quot;&lt;/em>&lt;/p>
&lt;hr>
&lt;p>In order to best serve users, SIG-Windows works to make the Kubernetes user experience as consistent as possible for users of Windows and Linux. However some use cases simply only apply to one Operating System, and as such, the SIG-Windows group also works to create functionality that is unique to Windows-only workloads.&lt;/p>
&lt;p>Many SIGs, or &amp;quot;Special Interest Groups&amp;quot; within Kubernetes have a narrow focus, allowing members to dive deep on a certain facet of the technology. While specific expertise is welcome, those interested in SIG-Windows will find it to be a great community to build broad understanding across many focus areas of Kubernetes. &amp;quot;Members from our SIG interface with storage, network, testing, cluster-lifecycle and others groups in Kubernetes.&amp;quot;&lt;/p>
&lt;h3 id="who-are-sig-windows-users">Who are SIG-Windows' Users?&lt;/h3>
&lt;p>The best way to understand the technology a group makes, is often to understand who their customers or users are.&lt;/p>
&lt;h4 id="a-majority-of-the-users-we-ve-interacted-with-have-business-critical-infrastructure-running-on-windows-developed-over-many-years-and-can-t-move-those-workloads-to-linux-for-various-reasons-cost-time-compliance-etc-the-sig-chairs-shared-by-transporting-those-workloads-into-windows-containers-and-running-them-in-kubernetes-they-are-able-to-quickly-modernize-their-infrastructure-and-help-migrate-it-to-the-cloud">&amp;quot;A majority of the users we've interacted with have business-critical infrastructure running on Windows developed over many years and can't move those workloads to Linux for various reasons (cost, time, compliance, etc),&amp;quot; the SIG chairs shared. &amp;quot;By transporting those workloads into Windows containers and running them in Kubernetes they are able to quickly modernize their infrastructure and help migrate it to the cloud.&amp;quot;&lt;/h4>
&lt;p>As anyone in the Kubernetes space can attest, companies around the world, in many different industries, see Kubernetes as their path to modernizing their infrastructure. Often this involves re-architecting or event totally re-inventing many of the ways they've been doing business. With the goal being to make their systems more scalable, more robust, and more ready for anything the future may bring. But not every application or workload can or should change the core operating system it runs on, so many teams need the ability to run containers at scale on Windows, or Linux, or both.&lt;/p>
&lt;p>&amp;quot;Sometimes the driver to Windows containers is a modernization effort and sometimes it’s because of expiring hardware warranties or end-of-support cycles for the current operating system. Our efforts in SIG-Windows enable Windows developers to take advantage of cloud native tools and Kubernetes to build and deploy distributed applications faster. That’s exciting! In essence, users can retain the benefits of application availability while decreasing costs.&amp;quot;&lt;/p>
&lt;h2 id="who-are-sig-windows">Who are SIG-Windows?&lt;/h2>
&lt;p>Who are these contributors working on enabling Windows workloads for Kubernetes? It could be you!&lt;/p>
&lt;p>Like with other Kubernetes SIGs, contributors to SIG-Windows can be anyone from independent hobbyists to professionals who work at many different companies. They come from many different parts of the world and bring to the table many different skill sets.&lt;/p>
&lt;img alt="Image of several people chatting pleasantly" width="30%" src="PeopleDoodle_transparent.png">
&lt;p>&lt;em>&amp;quot;Like most other Kubernetes SIGs, we are a very welcome and open community,&amp;quot; explained the SIG co-chairs Michael Michael and Mark Rosetti.&lt;/em>&lt;/p>
&lt;h3 id="becoming-a-contributor">Becoming a contributor&lt;/h3>
&lt;p>For anyone interested in getting started, the co-chairs added, &amp;quot;New contributors can view old community meetings on GitHub (we record every single meeting going back three years), read our documentation, attend new community meetings, ask questions in person or on Slack, and file some issues on Github. We also attend all KubeCon conferences and host 1-2 sessions, a contributor session, and meet-the-maintainer office hours.&amp;quot;&lt;/p>
&lt;p>The co-chairs also shared a glimpse into what the path looks like to becoming a member of the SIG-Windows community:&lt;/p>
&lt;p>&amp;quot;We encourage new contributors to initially just join our community and listen, then start asking some questions and get educated on Windows in Kubernetes. As they feel comfortable, they could graduate to improving our documentation, file some bugs/issues, and eventually they can be a code contributor by fixing some bugs. If they have long-term and sustained substantial contributions to Windows, they could become a technical lead or a chair of SIG-Windows. You won't know if you love this area unless you get started :) To get started, &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows">visit this getting-started page&lt;/a>. It's a one stop shop with links to everything related to SIG-Windows in Kubernetes.&amp;quot;&lt;/p>
&lt;p>When asked if there were any useful skills for new contributors, the co-chairs said,&lt;/p>
&lt;p>&amp;quot;We are always looking for expertise in Go and Networking and Storage, along with a passion for Windows. Those are huge skills to have. However, we don’t require such skills, and we welcome any and all contributors, with varying skill sets. If you don’t know something, we will help you acquire it.&amp;quot;&lt;/p>
&lt;p>You can get in touch with the folks at SIG-Windows in their &lt;a href="https://kubernetes.slack.com/archives/C0SJ4AFB7">Slack channel&lt;/a> or attend one of their regular meetings - currently 30min long on Tuesdays at 12:30PM EST! You can find links to their regular meetings as well as past meeting notes and recordings from the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-windows#readme">SIG-Windows README&lt;/a> on GitHub.&lt;/p>
&lt;p>As a closing message from SIG-Windows:&lt;/p>
&lt;hr>
&lt;h4 id="we-welcome-you-to-get-involved-and-join-our-community-to-share-feedback-and-deployment-stories-and-contribute-to-code-docs-and-improvements-of-any-kind">&lt;em>&amp;quot;We welcome you to get involved and join our community to share feedback and deployment stories, and contribute to code, docs, and improvements of any kind.&amp;quot;&lt;/em>&lt;/h4>
&lt;hr></description></item><item><title>Blog: Working with Terraform and Kubernetes</title><link>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</link><pubDate>Mon, 29 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/working-with-terraform-and-kubernetes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> &lt;a href="https://twitter.com/pst418">Philipp Strube&lt;/a>, Kubestack&lt;/p>
&lt;p>Maintaining Kubestack, an open-source &lt;a href="https://www.kubestack.com/lp/terraform-gitops-framework">Terraform GitOps Framework&lt;/a> for Kubernetes, I unsurprisingly spend a lot of time working with Terraform and Kubernetes. Kubestack provisions managed Kubernetes services like AKS, EKS and GKE using Terraform but also integrates cluster services from Kustomize bases into the GitOps workflow. Think of cluster services as everything that's required on your Kubernetes cluster, before you can deploy application workloads.&lt;/p>
&lt;p>Hashicorp recently announced &lt;a href="https://www.hashicorp.com/blog/deploy-any-resource-with-the-new-kubernetes-provider-for-hashicorp-terraform/">better integration between Terraform and Kubernetes&lt;/a>. I took this as an opportunity to give an overview of how Terraform can be used with Kubernetes today and what to be aware of.&lt;/p>
&lt;p>In this post I will however focus only on using Terraform to provision Kubernetes API resources, not Kubernetes clusters.&lt;/p>
&lt;p>&lt;a href="https://www.terraform.io/intro/index.html">Terraform&lt;/a> is a popular infrastructure as code solution, so I will only introduce it very briefly here. In a nutshell, Terraform allows declaring a desired state for resources as code, and will determine and execute a plan to take the infrastructure from its current state, to the desired state.&lt;/p>
&lt;p>To be able to support different resources, Terraform requires providers that integrate the respective API. So, to create Kubernetes resources we need a Kubernetes provider. Here are our options:&lt;/p>
&lt;h2 id="terraform-kubernetes-provider-official">Terraform &lt;code>kubernetes&lt;/code> provider (official)&lt;/h2>
&lt;p>First, the &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes">official Kubernetes provider&lt;/a>. This provider is undoubtedly the most mature of the three. However, it comes with a big caveat that's probably the main reason why using Terraform to maintain Kubernetes resources is not a popular choice.&lt;/p>
&lt;p>Terraform requires a schema for each resource and this means the maintainers have to translate the schema of each Kubernetes resource into a Terraform schema. This is a lot of effort and was the reason why for a long time the supported resources where pretty limited. While this has improved over time, still not everything is supported. And especially &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">custom resources&lt;/a> are not possible to support this way.&lt;/p>
&lt;p>This schema translation also results in some edge cases to be aware of. For example, &lt;code>metadata&lt;/code> in the Terraform schema is a list of maps. Which means you have to refer to the &lt;code>metadata.name&lt;/code> of a Kubernetes resource like this in Terraform: &lt;code>kubernetes_secret.example.metadata.0.name&lt;/code>.&lt;/p>
&lt;p>On the plus side however, having a Terraform schema means full integration between Kubernetes and other Terraform resources. Like for &lt;a href="https://github.com/kbst/terraform-kubestack/blob/e5caa6d20926d546a045144ebe79c7cc8c0b4c8a/aws/_modules/eks/ingress.tf#L37">example&lt;/a>, using Terraform to create a Kubernetes service of type &lt;code>LoadBalancer&lt;/code> and then use the returned ELB hostname in a Route53 record to configure DNS.&lt;/p>
&lt;p>The biggest benefit when using Terraform to maintain Kubernetes resources is integration into the Terraform plan/apply life-cycle. So you can review planned changes before applying them. Also, using &lt;code>kubectl&lt;/code>, purging of resources from the cluster is not trivial without manual intervention. Terraform does this reliably.&lt;/p>
&lt;h2 id="terraform-kubernetes-alpha-provider">Terraform &lt;code>kubernetes-alpha&lt;/code> provider&lt;/h2>
&lt;p>Second, the new &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha">alpha Kubernetes provider&lt;/a>. As a response to the limitations of the current Kubernetes provider the Hashicorp team recently released an alpha version of a new provider.&lt;/p>
&lt;p>This provider uses dynamic resource types and server-side-apply to support all Kubernetes resources. I personally think this provider has the potential to be a game changer - even if &lt;a href="https://github.com/hashicorp/terraform-provider-kubernetes-alpha#moving-from-yaml-to-hcl">managing Kubernetes resources in HCL&lt;/a> may still not be for everyone. Maybe the Kustomize provider below will help with that.&lt;/p>
&lt;p>The only downside really is, that it's explicitly discouraged to use it for anything but testing. But the more people test it, the sooner it should be ready for prime time. So I encourage everyone to give it a try.&lt;/p>
&lt;h2 id="terraform-kustomize-provider">Terraform &lt;code>kustomize&lt;/code> provider&lt;/h2>
&lt;p>Last, we have the &lt;a href="https://github.com/kbst/terraform-provider-kustomize">&lt;code>kustomize&lt;/code> provider&lt;/a>. Kustomize provides a way to do customizations of Kubernetes resources using inheritance instead of templating. It is designed to output the result to &lt;code>stdout&lt;/code>, from where you can apply the changes using &lt;code>kubectl&lt;/code>. This approach means that &lt;code>kubectl&lt;/code> edge cases like no purging or changes to immutable attributes still make full automation difficult.&lt;/p>
&lt;p>Kustomize is a popular way to handle customizations. But I was looking for a more reliable way to automate applying changes. Since this is exactly what Terraform is great at the Kustomize provider was born.&lt;/p>
&lt;p>Not going into too much detail here, but from Terraform's perspective, this provider treats every Kubernetes resource as a JSON string. This way it can handle any Kubernetes resource resulting from the Kustomize build. But it has the big disadvantage that Kubernetes resources can not easily be integrated with other Terraform resources. Remember the load balancer example from above.&lt;/p>
&lt;p>Under the hood, similarly to the new Kubernetes alpha provider, the Kustomize provider also uses the dynamic Kubernetes client and server-side-apply. Going forward, I plan to deprecate this part of the Kustomize provider that overlaps with the new Kubernetes provider and only keep the Kustomize integration.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>For teams that are already invested into Terraform, or teams that are looking for ways to replace &lt;code>kubectl&lt;/code> in automation, Terraform's plan/apply life-cycle has always been a promising option to automate changes to Kubernetes resources. However, the limitations of the official Kubernetes provider resulted in this not seeing significant adoption.&lt;/p>
&lt;p>The new alpha provider removes the limitations and has the potential to make Terraform a prime option to automate changes to Kubernetes resources.&lt;/p>
&lt;p>Teams that have already adopted Kustomize, may find integrating Kustomize and Terraform using the Kustomize provider beneficial over &lt;code>kubectl&lt;/code> because it avoids common edge cases. Even if in this set up, Terraform can only easily be used to plan and apply the changes, not to adapt the Kubernetes resources. In the future, this issue may be resolved by combining the Kustomize provider with the new Kubernetes provider.&lt;/p>
&lt;p>If you have any questions regarding these three options, feel free to reach out to me on the Kubernetes Slack in either the &lt;a href="https://app.slack.com/client/T09NY5SBT/CMBCT7XRQ">#kubestack&lt;/a> or the &lt;a href="https://app.slack.com/client/T09NY5SBT/C9A5ALABG">#kustomize&lt;/a> channel. If you happen to give any of the providers a try and encounter a problem, please file a GitHub issue to help the maintainers fix it.&lt;/p></description></item><item><title>Blog: A Better Docs UX With Docsy</title><link>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</link><pubDate>Mon, 15 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/better-docs-ux-with-docsy/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>I'm pleased to announce that the &lt;a href="https://kubernetes.io">Kubernetes website&lt;/a> now features the &lt;a href="https://docsy.dev">Docsy Hugo theme&lt;/a>.&lt;/p>
&lt;p>The Docsy theme improves the site's organization and navigability, and opens a path to improved API references. After over 4 years with few meaningful UX improvements, Docsy implements some best practices for technical content. The theme makes the Kubernetes site easier to read and makes individual pages easier to navigate. It gives the site a much-needed facelift.&lt;/p>
&lt;p>For example: adding a right-hand rail for navigating topics on the page. No more scrolling up to navigate!&lt;/p>
&lt;p>The theme opens a path for future improvements to the website. The Docsy functionality I'm most excited about is the theme's &lt;a href="https://www.docsy.dev/docs/adding-content/shortcodes/#swaggerui">&lt;code>swaggerui&lt;/code> shortcode&lt;/a>, which provides native support for generating API references from an OpenAPI spec. The CNCF is partnering with &lt;a href="https://developers.google.com/season-of-docs">Google Season of Docs&lt;/a> (GSoD) for staffing to make better API references a reality in Q4 this year. We're hopeful to be chosen, and we're looking forward to Google's list of announced projects on August 16th. Better API references have been a personal goal since I first started working with SIG Docs in 2017. It's exciting to see the goal within reach.&lt;/p>
&lt;p>One of SIG Docs' tech leads, &lt;a href="https://github.com/kbhawkey">Karen Bradshaw&lt;/a> did a lot of heavy lifting to fix a wide range of site compatibility issues, including a fix to the last of our &lt;a href="https://github.com/kubernetes/website/pull/21359">legacy pieces&lt;/a> when we &lt;a href="2018-05-05-hugo-migration/">migrated from Jekyll to Hugo&lt;/a> in 2018. Our other tech leads, &lt;a href="https://github.com/sftim">Tim Bannister&lt;/a> and &lt;a href="https://github.com/onlydole">Taylor Dolezal&lt;/a> provided extensive reviews.&lt;/p>
&lt;p>Thanks also to &lt;a href="https://bep.is/">Björn-Erik Pedersen&lt;/a>, who provided invaluable advice about how to navigate a Hugo upgrade beyond &lt;a href="https://gohugo.io/news/0.60.0-relnotes/">version 0.60.0&lt;/a>.&lt;/p>
&lt;p>The CNCF contracted with &lt;a href="https://gearboxbuilt.com/">Gearbox&lt;/a> in Victoria, BC to apply the theme to the site. Thanks to Aidan, Troy, and the rest of the team for all their work!&lt;/p></description></item><item><title>Blog: Supporting the Evolving Ingress Specification in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</link><pubDate>Fri, 05 Jun 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/06/05/supporting-the-evolving-ingress-specification-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Gervais (Datawire.io)&lt;/p>
&lt;p>Earlier this year, the Kubernetes team released &lt;a href="https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/">Kubernetes 1.18&lt;/a>, which extended Ingress. In this blog post, we’ll walk through what’s new in the new Ingress specification, what it means for your applications, and how to upgrade to an ingress controller that supports this new specification.&lt;/p>
&lt;h3 id="what-is-kubernetes-ingress">What is Kubernetes Ingress&lt;/h3>
&lt;p>When deploying your applications in Kubernetes, one of the first challenges many people encounter is how to get traffic into their cluster. &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">Kubernetes ingress&lt;/a> is a collection of routing rules that govern how external users access services running in a Kubernetes cluster. There are &lt;a href="https://blog.getambassador.io/kubernetes-ingress-nodeport-load-balancers-and-ingress-controllers-6e29f1c44f2d">three general approaches&lt;/a> for exposing your application:&lt;/p>
&lt;ul>
&lt;li>Using a &lt;code>NodePort&lt;/code> to expose your application on a port across each of your nodes&lt;/li>
&lt;li>Using a &lt;code>LoadBalancer&lt;/code> service to create an external load balancer that points to a Kubernetes service in your cluster&lt;/li>
&lt;li>Using a Kubernetes Ingress resource&lt;/li>
&lt;/ul>
&lt;h3 id="what-s-new-in-kubernetes-1-18-ingress">What’s new in Kubernetes 1.18 Ingress&lt;/h3>
&lt;p>There are three significant additions to the Ingress API in Kubernetes 1.18:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource&lt;/li>
&lt;li>Support for wildcards in hostnames&lt;/li>
&lt;/ul>
&lt;p>The new &lt;code>pathType&lt;/code> field allows you to specify how Ingress paths should match.
The field supports three types: &lt;code>ImplementationSpecific&lt;/code> (default), &lt;code>exact&lt;/code>, and &lt;code>prefix&lt;/code>. Explicitly defining the expected behavior of path matching will allow every ingress-controller to support a user’s needs and will increase portability between ingress-controller implementation solutions.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource specifies how Ingresses should be implemented by controllers. This was added to formalize the commonly used but never standardized &lt;code>kubernetes.io/ingress.class&lt;/code> annotation and allow for implementation-specific extensions and configuration.&lt;/p>
&lt;p>You can read more about these changes, as well as the support for wildcards in hostnames in more detail in &lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">a previous blog post&lt;/a>.&lt;/p>
&lt;h2 id="supporting-kubernetes-ingress">Supporting Kubernetes ingress&lt;/h2>
&lt;p>&lt;a href="https://www.getambassador.io">Ambassador&lt;/a> is an open-source Envoy-based ingress controller. We believe strongly in supporting common standards such as Kubernetes ingress, which we adopted and &lt;a href="https://blog.getambassador.io/ambassador-ingress-controller-better-config-reporting-updated-envoy-proxy-99dc9139e28f">announced our initial support for back in 2019&lt;/a>.&lt;/p>
&lt;p>Every Ambassador release goes through rigorous testing. Therefore, we also contributed an &lt;a href="https://github.com/kubernetes-sigs/ingress-controller-conformance">open conformance test suite&lt;/a>, supporting Kubernetes ingress. We wrote the initial bits of test code and will keep iterating over the newly added features and different versions of the Ingress specification as it evolves to a stable v1 GA release. Documentation and usage samples, is one of our top priorities. We understand how complex usage can be, especially when transitioning from a previous version of an API.&lt;/p>
&lt;p>Following a test-driven development approach, the first step we took in supporting Ingress improvements in Ambassador was to translate the revised specification -- both in terms of API and behavior -- into a comprehensible test suite. The test suite, although still under heavy development and going through multiple iterations, was rapidly added to the Ambassador CI infrastructure and acceptance criteria. This means every change to the Ambassador codebase going forward will be compliant with the Ingress API and be tested end-to-end in a lightweight &lt;a href="https://kind.sigs.k8s.io/">KIND cluster&lt;/a>. Using KIND allowed us to make rapid improvements while limiting our cloud provider infrastructure bill and testing out unreleased Kubernetes features with pre-release builds.&lt;/p>
&lt;h3 id="adopting-a-new-specification">Adopting a new specification&lt;/h3>
&lt;p>With a global comprehension of additions to Ingress introduced in Kubernetes 1.18 and a test suite on hand, we tackled the task of adapting the Ambassador code so that it would support translating the high-level Ingress API resources into Envoy configurations and constructs. Luckily Ambassador already supported previous versions of ingress functionalities so the development effort was incremental.&lt;/p>
&lt;p>We settled on a controller name of &lt;code>getambassador.io/ingress-controller&lt;/code>. This value, consistent with Ambassador's domain and CRD versions, must be used to tie in an IngressClass &lt;code>spec.controller&lt;/code> with an Ambassador deployment. The new IngressClass resource allows for extensibility by setting a &lt;code>spec.parameters&lt;/code> field. At the moment Ambassador makes no use of this field and its usage is reserved for future development.&lt;/p>
&lt;p>Paths can now define different matching behaviors using the &lt;code>pathType&lt;/code> field. The field will default to a value of &lt;code>ImplementationSpecific&lt;/code>, which uses the same matching rules as the &lt;a href="https://www.getambassador.io/docs/latest/topics/using/mappings/">Ambassador Mappings&lt;/a> prefix field and previous Ingress specification for backward compatibility reasons.&lt;/p>
&lt;h3 id="kubernetes-ingress-controllers">Kubernetes Ingress Controllers&lt;/h3>
&lt;p>A comprehensive &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">list of Kubernetes ingress controllers&lt;/a> is available in the Kubernetes documentation. Currently, Ambassador is the only ingress controller that supports these new additions to the ingress specification. Powered by the &lt;a href="https://www.envoyproxy.io">Envoy Proxy&lt;/a>, Ambassador is the fastest way for you to try out the new ingress specification today.&lt;/p>
&lt;p>Check out the following resources:&lt;/p>
&lt;ul>
&lt;li>Ambassador on &lt;a href="https://www.github.com/datawire/ambassador">GitHub&lt;/a>&lt;/li>
&lt;li>The Ambassador &lt;a href="https://www.getambassador.io/docs">documentation&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/">Improvements to the Ingress API&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Or join the community on &lt;a href="http://d6e.co/slack">Slack&lt;/a>!&lt;/p></description></item><item><title>Blog: K8s KPIs with Kuberhealthy</title><link>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</link><pubDate>Fri, 29 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/29/k8s-kpis-with-kuberhealthy/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Joshulyne Park (Comcast), Eric Greer (Comcast)&lt;/p>
&lt;h3 id="building-onward-from-kuberhealthy-v2-0-0">Building Onward from Kuberhealthy v2.0.0&lt;/h3>
&lt;p>Last November at KubeCon San Diego 2019, we announced the release of
&lt;a href="https://www.youtube.com/watch?v=aAJlWhBtzqY">Kuberhealthy 2.0.0&lt;/a> - transforming Kuberhealthy into a Kubernetes operator
for synthetic monitoring. This new ability granted developers the means to create their own Kuberhealthy check
containers to synthetically monitor their applications and clusters. The community was quick to adopt this new feature and we're grateful for everyone who implemented and tested Kuberhealthy 2.0.0 in their clusters. Thanks to all of you who reported
issues and contributed to discussions on the #kuberhealthy Slack channel. We quickly set to work to address all your feedback
with a newer version of Kuberhealthy. Additionally, we created a guide on how to easily install and use Kuberhealthy in order to capture some helpful synthetic &lt;a href="https://kpi.org/KPI-Basics">KPIs&lt;/a>.&lt;/p>
&lt;h3 id="deploying-kuberhealthy">Deploying Kuberhealthy&lt;/h3>
&lt;p>To install Kuberhealthy, make sure you have &lt;a href="https://helm.sh/docs/intro/install/">Helm 3&lt;/a> installed. If not, you can use the generated flat spec files located
in this &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/deploy">deploy folder&lt;/a>. You should use &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus.yaml">kuberhealthy-prometheus.yaml&lt;/a> if you don't use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>, and &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy-prometheus-operator.yaml">kuberhealthy-prometheus-operator.yaml&lt;/a> if you do. If you don't use Prometheus at all, you can still use Kuberhealthy with a JSON status page and/or InfluxDB integration using &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/deploy/kuberhealthy.yaml">this spec&lt;/a>.&lt;/p>
&lt;h4 id="to-install-using-helm-3">To install using Helm 3:&lt;/h4>
&lt;h5 id="1-create-namespace-kuberhealthy-in-the-desired-kubernetes-cluster-context">1. Create namespace &amp;quot;kuberhealthy&amp;quot; in the desired Kubernetes cluster/context:&lt;/h5>
&lt;pre>&lt;code>kubectl create namespace kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="2-set-your-current-namespace-to-kuberhealthy">2. Set your current namespace to &amp;quot;kuberhealthy&amp;quot;:&lt;/h5>
&lt;pre>&lt;code>kubectl config set-context --current --namespace=kuberhealthy
&lt;/code>&lt;/pre>&lt;h5 id="3-add-the-kuberhealthy-repo-to-helm">3. Add the kuberhealthy repo to Helm:&lt;/h5>
&lt;pre>&lt;code>helm repo add kuberhealthy https://comcast.github.io/kuberhealthy/helm-repos
&lt;/code>&lt;/pre>&lt;h5 id="4-depending-on-your-prometheus-implementation-install-kuberhealthy-using-the-appropriate-command-for-your-cluster">4. Depending on your Prometheus implementation, install Kuberhealthy using the appropriate command for your cluster:&lt;/h5>
&lt;ul>
&lt;li>If you use the &lt;a href="https://github.com/coreos/prometheus-operator">Prometheus Operator&lt;/a>:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true,prometheus.serviceMonitor=true
&lt;/code>&lt;/pre>&lt;ul>
&lt;li>If you use Prometheus, but NOT Prometheus Operator:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy --set prometheus.enabled=true,prometheus.enableAlerting=true,prometheus.enableScraping=true
&lt;/code>&lt;/pre>&lt;p>See additional details about configuring the appropriate scrape annotations in the section &lt;a href="#prometheus-integration-details">Prometheus Integration Details&lt;/a> below.&lt;/p>
&lt;ul>
&lt;li>Finally, if you don't use Prometheus:&lt;/li>
&lt;/ul>
&lt;pre>&lt;code>helm install kuberhealthy kuberhealthy/kuberhealthy
&lt;/code>&lt;/pre>&lt;p>Running the Helm command should automatically install the newest version of Kuberhealthy (v2.2.0) along with a few basic checks. If you run &lt;code>kubectl get pods&lt;/code>, you should see two Kuberhealthy pods. These are the pods that create, coordinate, and track test pods. These two Kuberhealthy pods also serve a JSON status page as well as a &lt;code>/metrics&lt;/code> endpoint. Every other pod you see created is a checker pod designed to execute and shut down when done.&lt;/p>
&lt;h3 id="configuring-additional-checks">Configuring Additional Checks&lt;/h3>
&lt;p>Next, you can run &lt;code>kubectl get khchecks&lt;/code>. You should see three Kuberhealthy checks installed by default:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/daemonset-check">daemonset&lt;/a>: Deploys and tears down a daemonset to ensure all nodes in the cluster are functional.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment&lt;/a>: Creates a deployment and then triggers a rolling update. Tests that the deployment is reachable via a service and then deletes everything. Any problem in this process will cause this check to report a failure.&lt;/li>
&lt;li>&lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/dns-resolution-check">dns-status-internal&lt;/a>: Validates that internal cluster DNS is functioning as expected.&lt;/li>
&lt;/ul>
&lt;p>To view other available external checks, check out the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECKS_REGISTRY.md">external checks registry&lt;/a> where you can find other yaml files you can apply to your cluster to enable various checks.&lt;/p>
&lt;p>Kuberhealthy check pods should start running shortly after Kuberhealthy starts running (1-2 minutes). Additionally, the check-reaper cronjob runs every few minutes to ensure there are no more than 5 completed checker pods left lying around at a time.&lt;/p>
&lt;p>To get status page view of these checks, you'll need to either expose the &lt;code>kuberhealthy&lt;/code> service externally by editing the service &lt;code>kuberhealthy&lt;/code> and setting &lt;code>Type: LoadBalancer&lt;/code> or use &lt;code>kubectl port-forward service/kuberhealthy 8080:80&lt;/code>. When viewed, the service endpoint will display a JSON status page that looks like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">{
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CheckDetails&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/daemonset&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;22.512278967s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;9abd3ec0-b82f-44f0-b8a7-fa6709f759cd&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/deployment&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;29.142295647s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:31.7176964Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;5f0d2765-60c9-47e8-b2c9-8bc6e61727b2&amp;#34;&lt;/span>
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;kuberhealthy/dns-status-internal&amp;#34;&lt;/span>: {
&lt;span style="color:#008000;font-weight:bold">&amp;#34;OK&amp;#34;&lt;/span>: &lt;span style="color:#a2f;font-weight:bold">true&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Errors&amp;#34;&lt;/span>: [],
&lt;span style="color:#008000;font-weight:bold">&amp;#34;RunDuration&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2.43940936s&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;Namespace&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;LastRun&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;2020-04-06T23:20:44.6294547Z&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;AuthoritativePod&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-67bf8c4686-mbl2j&amp;#34;&lt;/span>,
&lt;span style="color:#008000;font-weight:bold">&amp;#34;uuid&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;c85f95cb-87e2-4ff5-b513-e02b3d25973a&amp;#34;&lt;/span>
}
},
&lt;span style="color:#008000;font-weight:bold">&amp;#34;CurrentMaster&amp;#34;&lt;/span>: &lt;span style="color:#b44">&amp;#34;kuberhealthy-7cf79bdc86-m78qr&amp;#34;&lt;/span>
}
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This JSON page displays all Kuberhealthy checks running in your cluster. If you have Kuberhealthy checks running in different namespaces, you can filter them by adding the &lt;code>GET&lt;/code> variable &lt;code>namespace&lt;/code> parameter: &lt;code>?namespace=kuberhealthy,kube-system&lt;/code> onto the status page URL.&lt;/p>
&lt;h3 id="writing-your-own-checks">Writing Your Own Checks&lt;/h3>
&lt;p>Kuberhealthy is designed to be extended with custom check containers that can be written by anyone to check anything. These checks can be written in any language as long as they are packaged in a container. This makes Kuberhealthy an excellent platform for creating your own synthetic checks!&lt;/p>
&lt;p>Creating your own check is a great way to validate your client library, simulate real user workflow, and create a high level of confidence in your service or system uptime.&lt;/p>
&lt;p>To learn more about writing your own checks, along with simple examples, check the &lt;a href="https://github.com/Comcast/kuberhealthy/blob/master/docs/EXTERNAL_CHECK_CREATION.md">custom check creation&lt;/a> documentation.&lt;/p>
&lt;h3 id="prometheus-integration-details">Prometheus Integration Details&lt;/h3>
&lt;p>When enabling Prometheus (not the operator), the Kuberhealthy service gets the following annotations added:&lt;/p>
&lt;pre>&lt;code class="language-.env" data-lang=".env">prometheus.io/path: /metrics
prometheus.io/port: &amp;quot;80&amp;quot;
prometheus.io/scrape: &amp;quot;true&amp;quot;
&lt;/code>&lt;/pre>&lt;p>In your prometheus configuration, add the following example scrape_config that scrapes the Kuberhealthy service given the added prometheus annotation:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#a2f;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#39;kuberhealthy&amp;#39;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubernetes_sd_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">role&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespaces&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">names&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">relabel_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">source_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[__meta_kubernetes_service_annotation_prometheus_io_scrape]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">action&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>keep&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">regex&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>You can also specify the target endpoint to be scraped using this example job:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">- &lt;span style="color:#a2f;font-weight:bold">job_name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kuberhealthy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">scrape_interval&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">honor_labels&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metrics_path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/metrics&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">static_configs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">targets&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- kuberhealthy.kuberhealthy.svc.cluster.local:&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Once the appropriate prometheus configurations are applied, you should be able to see the following Kuberhealthy metrics:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kuberhealthy_check&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_check_duration_seconds&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_cluster_states&lt;/code>&lt;/li>
&lt;li>&lt;code>kuberhealthy_running&lt;/code>&lt;/li>
&lt;/ul>
&lt;h3 id="creating-key-performance-indicators">Creating Key Performance Indicators&lt;/h3>
&lt;p>Using these Kuberhealthy metrics, our team has been able to collect KPIs based on the following definitions, calculations, and PromQL queries.&lt;/p>
&lt;p>&lt;em>Availability&lt;/em>&lt;/p>
&lt;p>We define availability as the K8s cluster control plane being up and functioning as expected. This is measured by our ability to create a deployment, do a rolling update, and delete the deployment within a set period of time.&lt;/p>
&lt;p>We calculate this by measuring Kuberhealthy's &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> successes and failures.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>Availability = Uptime / (Uptime * Downtime)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Uptime = Number of Deployment Check Passes * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Downtime = Number of Deployment Check Fails * Check Run Interval&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Check Run Interval = how often the check runs (&lt;code>runInterval&lt;/code> set in your KuberhealthyCheck Spec)&lt;/p>
&lt;/li>
&lt;li>
&lt;p>PromQL Query (Availability % over the past 30 days):&lt;/p>
&lt;pre>&lt;code class="language-promql" data-lang="promql">1 - (sum(count_over_time(kuberhealthy_check{check=&amp;quot;kuberhealthy/deployment&amp;quot;, status=&amp;quot;0&amp;quot;}[30d])) OR vector(0)) / sum(count_over_time(kuberhealthy_check{check=&amp;quot;kuberhealthy/deployment&amp;quot;, status=&amp;quot;1&amp;quot;}[30d]))
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Utilization&lt;/em>&lt;/p>
&lt;p>We define utilization as user uptake of product (k8s) and its resources (pods, services, etc.). This is measured by how many nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs are being utilized by our customers.
We calculate this by counting the total number of nodes, deployments, statefulsets, persistent volumes, services, pods, and jobs.&lt;/p>
&lt;p>&lt;em>Duration (Latency)&lt;/em>&lt;/p>
&lt;p>We define duration as the control plane's capacity and utilization of throughput. We calculate this by capturing the average run duration of a Kuberhealthy &lt;a href="https://github.com/Comcast/kuberhealthy/tree/master/cmd/deployment-check">deployment check&lt;/a> run.&lt;/p>
&lt;ul>
&lt;li>PromQL Query (Deployment check average run duration):
&lt;pre>&lt;code class="language-promql" data-lang="promql">avg(kuberhealthy_check_duration_seconds{check=&amp;quot;kuberhealthy/deployment&amp;quot;})
&lt;/code>&lt;/pre>&lt;/li>
&lt;/ul>
&lt;p>&lt;em>Errors / Alerts&lt;/em>&lt;/p>
&lt;p>We define errors as all k8s cluster and Kuberhealthy related alerts. Every time one of our Kuberhealthy check fails, we are alerted of this failure.&lt;/p>
&lt;h3 id="thank-you">Thank You!&lt;/h3>
&lt;p>Thanks again to everyone in the community for all of your contributions and help! We are excited to see what you build. As always, if you find an issue, have a feature request, or need to open a pull request, please &lt;a href="https://github.com/Comcast/kuberhealthy/issues">open an issue&lt;/a> on the Github project.&lt;/p></description></item><item><title>Blog: My exciting journey into Kubernetes’ history</title><link>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</link><pubDate>Thu, 28 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/my-exciting-journey-into-kubernetes-history/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Sascha Grunert, SUSE Software Solutions&lt;/p>
&lt;p>&lt;em>Editor's note: Sascha is part of &lt;a href="https://github.com/kubernetes/sig-release">SIG Release&lt;/a> and is working on many other
different container runtime related topics. Feel free to reach him out on
Twitter &lt;a href="https://twitter.com/saschagrunert">@saschagrunert&lt;/a>.&lt;/em>&lt;/p>
&lt;hr>
&lt;blockquote>
&lt;p>A story of data science-ing 90,000 GitHub issues and pull requests by using
Kubeflow, TensorFlow, Prow and a fully automated CI/CD pipeline.&lt;/p>
&lt;/blockquote>
&lt;ul>
&lt;li>&lt;a href="#introduction">Introduction&lt;/a>&lt;/li>
&lt;li>&lt;a href="#getting-the-data">Getting the Data&lt;/a>&lt;/li>
&lt;li>&lt;a href="#exploring-the-data">Exploring the Data&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#labels-labels-labels">Labels, Labels, Labels&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#building-the-machine-learning-model">Building the Machine Learning Model&lt;/a>
&lt;ul>
&lt;li>&lt;a href="#doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/a>&lt;/li>
&lt;li>&lt;a href="#creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#training-the-model">Training the Model&lt;/a>&lt;/li>
&lt;li>&lt;a href="#a-first-prediction">A first Prediction&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;a href="#automate-everything">Automate Everything&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/a>&lt;/li>
&lt;li>&lt;a href="#summary">Summary&lt;/a>&lt;/li>
&lt;/ul>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>Choosing the right steps when working in the field of data science is truly no
silver bullet. Most data scientists might have their custom workflow, which
could be more or less automated, depending on their area of work. Using
&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> can be a tremendous enhancement when trying to automate
workflows on a large scale. In this blog post, I would like to take you on my
journey of doing data science while integrating the overall workflow into
Kubernetes.&lt;/p>
&lt;p>The target of the research I did in the past few months was to find any
useful information about all those thousands of GitHub issues and pull requests
(PRs) we have in the &lt;a href="https://github.com/kubernetes/kubernetes">Kubernetes repository&lt;/a>. What I ended up with was a
fully automated, in Kubernetes running Continuous Integration (CI) and
Deployment (CD) data science workflow powered by &lt;a href="https://www.kubeflow.org">Kubeflow&lt;/a> and &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>.
You may not know both of them, but we get to the point where I explain what
they’re doing in detail. The source code of my work can be found in the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis">kubernetes-analysis GitHub repository&lt;/a>, which contains everything source
code-related as well as the raw data. But how to retrieve this data I’m talking
about? Well, this is where the story begins.&lt;/p>
&lt;h1 id="getting-the-data">Getting the Data&lt;/h1>
&lt;p>The foundation for my experiments is the raw GitHub API data in plain &lt;a href="https://en.wikipedia.org/wiki/JSON">JSON&lt;/a>
format. The necessary data can be retrieved via the &lt;a href="https://developer.github.com/v3/issues">GitHub issues
endpoint&lt;/a>, which returns all pull requests as well as regular issues in the
&lt;a href="https://en.wikipedia.org/wiki/Representational_state_transfer">REST&lt;/a> API. I exported roughly &lt;strong>91000&lt;/strong> issues and pull requests in
the first iteration into a massive &lt;strong>650 MiB&lt;/strong> data blob. This took me about &lt;strong>8
hours&lt;/strong> of data retrieval time because for sure, the GitHub API is &lt;a href="https://developer.github.com/apps/building-github-apps/understanding-rate-limits-for-github-apps/">rate
limited&lt;/a>. To be able to put this data into a GitHub repository, I’d chosen
to compress it via &lt;a href="https://linux.die.net/man/1/xz">&lt;code>xz(1)&lt;/code>&lt;/a>. The result was a roundabout &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/data/api.tar.xz">25 MiB sized
tarball&lt;/a>, which fits well into the repository.&lt;/p>
&lt;p>I had to find a way to regularly update the dataset because the Kubernetes
issues and pull requests are updated by the users over time as well as new ones
are created. To achieve the continuous update without having to wait 8 hours
over and over again, I now fetch the delta GitHub API data between the
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/.update">last update&lt;/a> and the current time. This way, a Continuous Integration job
can update the data on a regular basis, whereas I can continue my research with
the latest available set of data.&lt;/p>
&lt;p>From a tooling perspective, I’ve written an &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/main">all-in-one Python executable&lt;/a>,
which allows us to trigger the different steps during the data science
experiments separately via dedicated subcommands. For example, to run an export
of the whole data set, we can call:&lt;/p>
&lt;pre>&lt;code>&amp;gt; export GITHUB_TOKEN=&amp;lt;MY-SECRET-TOKEN&amp;gt;
&amp;gt; ./main export
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Dumping all issues
INFO | Pulling 90929 items
INFO | 1: Unit test coverage in Kubelet is lousy. (~30%)
INFO | 2: Better error messages if go isn't installed, or if gcloud is old.
INFO | 3: Need real cluster integration tests
INFO | 4: kubelet should know which containers it is managing
… [just wait 8 hours] …
&lt;/code>&lt;/pre>&lt;p>To update the data between the last time stamp stored in the repository we can
run:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main export --update-api
INFO | Getting GITHUB_TOKEN from environment variable
INFO | Retrieving issues and PRs
INFO | Updating API
INFO | Got update timestamp: 2020-05-09T10:57:40.854151
INFO | 90786: Automated cherry pick of #90749: fix: azure disk dangling attach issue
INFO | 90674: Switch core master base images from debian to distroless
INFO | 90086: Handling error returned by request.Request.ParseForm()
INFO | 90544: configurable weight on the CPU and memory
INFO | 87746: Support compiling Kubelet w/o docker/docker
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Updating data
INFO | Updating issue 90786 (updated at 2020-05-09T10:59:43Z)
INFO | Updating issue 90674 (updated at 2020-05-09T10:58:27Z)
INFO | Updating issue 90086 (updated at 2020-05-09T10:58:26Z)
INFO | Updating issue 90544 (updated at 2020-05-09T10:57:51Z)
INFO | Updating issue 87746 (updated at 2020-05-09T11:01:51Z)
INFO | Saving data
&lt;/code>&lt;/pre>&lt;p>This gives us an idea of how fast the project is actually moving: On a Saturday
at noon (European time), 5 issues and pull requests got updated within literally 5
minutes!&lt;/p>
&lt;p>Funnily enough, &lt;a href="https://github.com/jbeda">Joe Beda&lt;/a>, one of the founders of Kubernetes, created the
first GitHub issue &lt;a href="https://github.com/kubernetes/kubernetes/issues/1">mentioning that the unit test coverage is too low&lt;/a>. The
issue has no further description than the title, and no enhanced labeling
applied, like we know from more recent issues and pull requests. But now we have
to explore the exported data more deeply to do something useful with it.&lt;/p>
&lt;h1 id="exploring-the-data">Exploring the Data&lt;/h1>
&lt;p>Before we can start creating machine learning models and train them, we have to
get an idea about how our data is structured and what we want to achieve in
general.&lt;/p>
&lt;p>To get a better feeling about the amount of data, let’s look at how many issues
and pull requests have been created over time inside the Kubernetes repository:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
&lt;/code>&lt;/pre>&lt;p>The Python &lt;a href="https://matplotlib.org">matplotlib&lt;/a> module should pop up a graph which looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-all.svg" alt="created all">&lt;/p>
&lt;p>Okay, this looks not that spectacular but gives us an impression on how the
project has grown over the past 6 years. To get a better idea about the speed of
development of the project, we can look at the &lt;em>created-vs-closed&lt;/em> metric. This
means on our timeline, we add one to the y-axis if an issue or pull request got
created and subtracts one if closed. Now the chart looks like this:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-all.svg" alt="created vs closed all">&lt;/p>
&lt;p>At the beginning of 2018, the Kubernetes projects introduced some more enhanced
life-cycle management via the glorious &lt;a href="https://github.com/fejta-bot">fejta-bot&lt;/a>. This automatically
closes issues and pull requests after they got stale over a longer period of
time. This resulted in a massive closing of issues, which does not apply to pull
requests in the same amount. For example, if we look at the &lt;em>created-vs-closed&lt;/em>
metric only for pull requests.&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --created-vs-closed --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/created-vs-closed-pull-requests.svg" alt="created vs closed pull requests">&lt;/p>
&lt;p>The overall impact is not that obvious. What we can see is that the increasing
number of peaks in the PR chart indicates that the project is moving faster over
time. Usually, a candlestick chart would be a better choice for showing this kind
of volatility-related information. I’d also like to highlight that it looks like
the development of the project slowed down a bit in the beginning of 2020.&lt;/p>
&lt;p>Parsing raw JSON in every analysis iteration is not the fastest approach to do
in Python. This means that I decided to parse the more important information,
for example the content, title and creation time into dedicated &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/issue.py">issue&lt;/a> and
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/pull_request.py">PR classes&lt;/a>. This data will be &lt;a href="https://docs.python.org/3/library/pickle.html">pickle&lt;/a> serialized into the repository
as well, which allows an overall faster startup independently of the JSON blob.&lt;/p>
&lt;p>A pull request is more or less the same as an issue in my analysis, except that
it contains a release note.&lt;/p>
&lt;p>Release notes in Kubernetes are written in the PRs description into a separate
&lt;code>release-note&lt;/code> block like this:&lt;/p>
&lt;pre>&lt;code>```release-note
I changed something extremely important and you should note that.
```
&lt;/code>&lt;/pre>&lt;p>Those release notes are parsed by &lt;a href="https://github.com/kubernetes/release#tools">dedicated Release Engineering Tools like
&lt;code>krel&lt;/code>&lt;/a> during the release creation process and will be part of the various
&lt;a href="https://github.com/kubernetes/kubernetes/tree/master/CHANGELOG">CHANGELOG.md&lt;/a> files and the &lt;a href="https://relnotes.k8s.io">Release Notes Website&lt;/a>. That seems like a
lot of magic, but in the end, the quality of the overall release notes is much
higher because they’re easy to edit, and the PR reviewers can ensure that we
only document real user-facing changes and nothing else.&lt;/p>
&lt;p>The quality of the input data is a key aspect when doing data science. I decided
to focus on the release notes because they seem to have the highest amount of
overall quality when comparing them to the plain descriptions in issues and PRs.
Besides that, they’re easy to parse, and we would not need to strip away
the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/.github/ISSUE_TEMPLATE">various issue&lt;/a> and &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/.github/PULL_REQUEST_TEMPLATE.md">PR template&lt;/a> text noise.&lt;/p>
&lt;h2 id="labels-labels-labels">Labels, Labels, Labels&lt;/h2>
&lt;p>Issues and pull requests in Kubernetes get different labels applied during its
life-cycle. They are usually grouped via a single slash (&lt;code>/&lt;/code>). For example, we
have &lt;code>kind/bug&lt;/code> and &lt;code>kind/api-change&lt;/code> or &lt;code>sig/node&lt;/code> and &lt;code>sig/network&lt;/code>. An easy
way to understand which label groups exist and how they’re distributed across
the repository is to plot them into a bar chart:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-group
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-group-all-top-25.svg" alt="labels by group all top 25">&lt;/p>
&lt;p>It looks like that &lt;code>sig/&lt;/code>, &lt;code>kind/&lt;/code> and &lt;code>area/&lt;/code> labels are pretty common.
Something like &lt;code>size/&lt;/code> can be ignored for now because these labels are
automatically applied based on the amount of the code changes for a pull
request. We said that we want to focus on release notes as input data, which
means that we have to check out the distribution of the labels for the PRs. This
means that the top 25 labels on pull requests are:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --labels-by-name --pull-requests
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/labels-by-name-pull-requests-top-25.svg" alt="labels by name pull requests top 25">&lt;/p>
&lt;p>Again, we can ignore labels like &lt;code>lgtm&lt;/code> (looks good to me), because every PR
which now should get merged has to look good. Pull requests containing release
notes automatically get the &lt;code>release-note&lt;/code> label applied, which enables further
filtering more easily. This does not mean that every PR containing that label
also contains the release notes block. The label could have been applied
manually and the parsing of the release notes block did not exist since the
beginning of the project. This means we will probably loose a decent amount of
input data on one hand. On the other hand we can focus on the highest possible
data quality, because applying labels the right way needs some enhanced maturity
of the project and its contributors.&lt;/p>
&lt;p>From a label group perspective I have chosen to focus on the &lt;code>kind/&lt;/code> labels.
Those labels are something which has to be applied manually by the author of the
PR, they are available on a good amount of pull requests and they’re related to
user-facing changes as well. Besides that, the &lt;code>kind/&lt;/code> choice has to be done for
every pull request because it is part of the PR template.&lt;/p>
&lt;p>Alright, how does the distribution of those labels look like when focusing only
on pull requests which have release notes?&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main analyze --release-notes-stats
&lt;/code>&lt;/pre>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/release-notes-stats.svg" alt="release notes stats">&lt;/p>
&lt;p>Interestingly, we have approximately 7,000 overall pull requests containing
release notes, but only ~5,000 have a &lt;code>kind/&lt;/code> label applied. The distribution of
the labels is not equal, and one-third of them are labeled as &lt;code>kind/bug&lt;/code>. This
brings me to the next decision in my data science journey: I will build a binary
classifier which, for the sake of simplicity, is only able to distinguish between
bugs (via &lt;code>kind/bug&lt;/code>) and non-bugs (where the label is not applied).&lt;/p>
&lt;p>The main target is now to be able to classify newly incoming release notes if
they are related to a bug or not, based on the historical data we already have
from the community.&lt;/p>
&lt;p>Before doing that, I recommend that you play around with the &lt;code>./main analyze -h&lt;/code>
subcommand as well to explore the latest set of data. You can also check out all
the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/assets">continuously updated assets&lt;/a> I provide within the analysis repository.
For example, those are the top 25 PR creators inside the Kubernetes repository:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/users-by-created-pull-requests-top-25.svg" alt="users by created pull request">&lt;/p>
&lt;h1 id="building-the-machine-learning-model">Building the Machine Learning Model&lt;/h1>
&lt;p>Now we have an idea what the data set is about, and we can start building a first
machine learning model. Before actually building the model, we have to
pre-process all the extracted release notes from the PRs. Otherwise, the model
would not be able to understand our input.&lt;/p>
&lt;h2 id="doing-some-first-natural-language-processing-nlp">Doing some first Natural Language Processing (NLP)&lt;/h2>
&lt;p>In the beginning, we have to define a vocabulary for which we want to train. I
decided to choose the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html">TfidfVectorizer&lt;/a> from the Python scikit-learn machine
learning library. This vectorizer is able to take our input texts and create a
single huge vocabulary out of it. This is our so-called &lt;a href="https://en.wikipedia.org/wiki/Bag-of-words_model">bag-of-words&lt;/a>,
which has a chosen n-gram range of &lt;code>(1, 2)&lt;/code> (unigrams and bigrams). Practically
this means that we always use the first word and the next one as a single
vocabulary entry (bigrams). We also use the single word as vocabulary entry
(unigram). The TfidfVectorizer is able to skip words that occur multiple times
(&lt;code>max_df&lt;/code>), and requires a minimum amount (&lt;code>min_df&lt;/code>) to add a word to the
vocabulary. I decided not to change those values in the first place, just
because I had the intuition that release notes are something unique to a
project.&lt;/p>
&lt;p>Parameters like &lt;code>min_df&lt;/code>, &lt;code>max_df&lt;/code> and the n-gram range can be seen as some of
our hyperparameters. Those parameters have to be optimized in a dedicated step
after the machine learning model has been built. This step is called
hyperparameter tuning and basically means that we train multiple times with
different parameters and compare the accuracy of the model. Afterwards, we choose
the parameters with the best accuracy.&lt;/p>
&lt;p>During the training, the vectorizer will produce a &lt;code>data/features.json&lt;/code> which
contains the whole vocabulary. This gives us a good understanding of how such a
vocabulary may look like:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-json" data-lang="json">[
&lt;span style="">…&lt;/span>
&lt;span style="color:#b44">&amp;#34;hostname&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname address&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname and&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname as&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname being&amp;#34;&lt;/span>,
&lt;span style="color:#b44">&amp;#34;hostname bug&amp;#34;&lt;/span>,
&lt;span style="">…&lt;/span>
]
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This produces round about 50,000 entries in the overall bag-of-words, which is
pretty much. Previous analyses between different data sets showed that it is
simply not necessary to take so many features into account. Some general data
sets state that an overall vocabulary of 20,000 is enough and higher amounts do
not influence the accuracy any more. To do so we can use the &lt;a href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html">SelectKBest&lt;/a>
feature selector to strip down the vocabulary to only choose the top features.
Anyway, I still decided to stick to the top 50,000 to not negatively influence
the model accuracy. We have a relatively low amount of data (appr. 7,000
samples) and a low number of words per sample (~15) which already made me wonder
if we have enough data at all.&lt;/p>
&lt;p>The vectorizer is not only able to create our bag-of-words, but it is also able to
encode the features in &lt;a href="https://en.wikipedia.org/wiki/Tf%e2%80%93idf">term frequency–inverse document frequency (tf-idf)&lt;/a>
format. That is where the vectorizer got its name, whereas the output of that
encoding is something the machine learning model can directly consume. All the
details of the vectorization process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L193-L235">source code&lt;/a>.&lt;/p>
&lt;h2 id="creating-the-multi-layer-perceptron-mlp-model">Creating the Multi-Layer Perceptron (MLP) Model&lt;/h2>
&lt;p>I decided to choose a simple MLP based model which is built with the help of the
popular &lt;a href="https://www.tensorflow.org/api_docs/python/tf/keras">TensorFlow&lt;/a> framework. Because we do not have that much input data,
we just use two hidden layers, so that the model basically looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/model.png" alt="model">&lt;/p>
&lt;p>There have to be &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L95-L100">multiple other&lt;/a> hyperparameters to be taken into account
when creating the model. I will not discuss them in detail here, but they’re
important to be optimized also in relation to the number of classes we want to
have in the model (only two in our case).&lt;/p>
&lt;h2 id="training-the-model">Training the Model&lt;/h2>
&lt;p>Before starting the actual training, we have to split up our input data into
training and validation data sets. I’ve chosen to use ~80% of the data for
training and 20% for validation purposes. We have to shuffle our input data as
well to ensure that the model is not affected by ordering issues. The technical
details of the training process can be found in the &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/nlp.py#L91-L170">GitHub sources&lt;/a>. So now
we’re ready to finally start the training:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main train
INFO | Using already extracted data from data/data.pickle
INFO | Loading pickle dataset
INFO | Parsed 34380 issues and 55832 pull requests (90212 items)
INFO | Training for label 'kind/bug'
INFO | 6980 items selected
INFO | Using 5584 training and 1395 testing texts
INFO | Number of classes: 2
INFO | Vocabulary len: 51772
INFO | Wrote features to file data/features.json
INFO | Using units: 1
INFO | Using activation function: sigmoid
INFO | Created model with 2 layers and 64 units
INFO | Compiling model
INFO | Starting training
Train on 5584 samples, validate on 1395 samples
Epoch 1/1000
5584/5584 - 3s - loss: 0.6895 - acc: 0.6789 - val_loss: 0.6856 - val_acc: 0.6860
Epoch 2/1000
5584/5584 - 2s - loss: 0.6822 - acc: 0.6827 - val_loss: 0.6782 - val_acc: 0.6860
Epoch 3/1000
…
Epoch 68/1000
5584/5584 - 2s - loss: 0.2587 - acc: 0.9257 - val_loss: 0.4847 - val_acc: 0.7728
INFO | Confusion matrix:
[[920 32]
[291 152]]
INFO | Confusion matrix normalized:
[[0.966 0.034]
[0.657 0.343]]
INFO | Saving model to file data/model.h5
INFO | Validation accuracy: 0.7727598547935486, loss: 0.48470408514836355
&lt;/code>&lt;/pre>&lt;p>The output of the &lt;a href="https://en.wikipedia.org/wiki/Confusion_matrix">Confusion Matrix&lt;/a> shows us that we’re pretty good on
training accuracy, but the validation accuracy could be a bit higher. We now
could start a hyperparameter tuning to see if we can optimize the output of the
model even further. I will leave that experiment up to you with the hint to the
&lt;code>./main train --tune&lt;/code> flag.&lt;/p>
&lt;p>We saved the model (&lt;code>data/model.h5&lt;/code>), the vectorizer (&lt;code>data/vectorizer.pickle&lt;/code>)
and the feature selector (&lt;code>data/selector.pickle&lt;/code>) to disk to be able to use them
later on for prediction purposes without having a need for additional training
steps.&lt;/p>
&lt;h2 id="a-first-prediction">A first Prediction&lt;/h2>
&lt;p>We are now able to test the model by loading it from disk and predicting some
input text:&lt;/p>
&lt;pre>&lt;code>&amp;gt; ./main predict --test
INFO | Testing positive text:
Fix concurrent map access panic
Don't watch .mount cgroups to reduce number of inotify watches
Fix NVML initialization race condition
Fix brtfs disk metrics when using a subdirectory of a subvolume
INFO | Got prediction result: 0.9940581321716309
INFO | Matched expected positive prediction result
INFO | Testing negative text:
action required
1. Currently, if users were to explicitly specify CacheSize of 0 for
KMS provider, they would end-up with a provider that caches up to
1000 keys. This PR changes this behavior.
Post this PR, when users supply 0 for CacheSize this will result in
a validation error.
2. CacheSize type was changed from int32 to *int32. This allows
defaulting logic to differentiate between cases where users
explicitly supplied 0 vs. not supplied any value.
3. KMS Provider's endpoint (path to Unix socket) is now validated when
the EncryptionConfiguration files is loaded. This used to be handled
by the GRPCService.
INFO | Got prediction result: 0.1251964420080185
INFO | Matched expected negative prediction result
&lt;/code>&lt;/pre>&lt;p>Both tests are real-world examples which already exist. We could also try
something completely different, like this random tweet I found a couple of
minutes ago:&lt;/p>
&lt;pre>&lt;code>./main predict &amp;quot;My dudes, if you can understand SYN-ACK, you can understand consent&amp;quot;
INFO | Got prediction result: 0.1251964420080185
ERROR | Result is lower than selected threshold 0.6
&lt;/code>&lt;/pre>&lt;p>Looks like it is not classified as bug for a release note, which seems to work.
Selecting a good threshold is also not that easy, but sticking to something &amp;gt;
50% should be the bare minimum.&lt;/p>
&lt;h1 id="automate-everything">Automate Everything&lt;/h1>
&lt;p>The next step is to find some way of automation to continuously update the model
with new data. If I change any source code within my repository, then I’d like
to get feedback about the test results of the model without having a need to run
the training on my own machine. I would like to utilize the GPUs in my
Kubernetes cluster to train faster and automatically update the data set if a PR
got merged.&lt;/p>
&lt;p>With the help of &lt;a href="https://www.kubeflow.org/docs/pipelines/overview/pipelines-overview">Kubeflow pipelines&lt;/a> we can fulfill most of these
requirements. The pipeline I built looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/kubeflow-pipeline.png" alt="pipeline">&lt;/p>
&lt;p>First, we check out the source code of the PR, which will be passed on as output
artifact to all other steps. Then we incrementally update the API and internal
data before we run the training on an always up-to-date data set. The prediction
test verifies after the training that we did not badly influence the model with
our changes.&lt;/p>
&lt;p>We also built a container image within our pipeline. &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/Dockerfile-deploy">This container image&lt;/a>
copies the previously built model, vectorizer, and selector into a container and
runs &lt;code>./main serve&lt;/code>. When doing this, we spin up a &lt;a href="https://www.kubeflow.org/docs/components/serving/kfserving">kfserving&lt;/a> web server,
which can be used for prediction purposes. Do you want to try it out by yourself? Simply
do a JSON POST request like this and run the prediction against the endpoint:&lt;/p>
&lt;pre>&lt;code>&amp;gt; curl https://kfserving.k8s.saschagrunert.de/v1/models/kubernetes-analysis:predict \
-d '{&amp;quot;text&amp;quot;: &amp;quot;my test text&amp;quot;}'
{&amp;quot;result&amp;quot;: 0.1251964420080185}
&lt;/code>&lt;/pre>&lt;p>The &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/master/src/kfserver.py">custom kfserving&lt;/a> implementation is pretty straightforward, whereas the
deployment utilizes &lt;a href="https://knative.dev/docs/serving">Knative Serving&lt;/a> and an &lt;a href="https://istio.io">Istio&lt;/a> ingress gateway
under the hood to correctly route the traffic into the cluster and provide the
right set of services.&lt;/p>
&lt;p>The &lt;code>commit-changes&lt;/code> and &lt;code>rollout&lt;/code> step will only run if the pipeline runs on
the &lt;code>master&lt;/code> branch. Those steps make sure that we always have the latest data
set available on the master branch as well as in the kfserving deployment. The
&lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/src/rollout.py#L30-L51">rollout step&lt;/a> creates a new canary deployment, which only accepts 50% of the
incoming traffic in the first place. After the canary got deployed successfully,
it will be promoted as the new main instance of the service. This is a great way
to ensure that the deployment works as intended and allows additional testing
after rolling out the canary.&lt;/p>
&lt;p>But how to trigger Kubeflow pipelines when creating a pull request? Kubeflow has
no feature for that right now. That’s why I decided to use &lt;a href="https://github.com/kubernetes/test-infra/tree/master/prow">Prow&lt;/a>,
Kubernetes test-infrastructure project for CI/CD purposes.&lt;/p>
&lt;p>First of all, a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/blob/f419ff4a3462bafc0cb067aa6973dc7280409699/ci/config.yaml#L45-L61">24h periodic job&lt;/a> ensures that we have at least daily
up-to-date data available within the repository. Then, if we create a pull
request, Prow will run the whole Kubeflow pipeline without committing or rolling
out any changes. If we merge the pull request via Prow, another job runs on the
master and updates the data as well as the deployment. That’s pretty neat, isn’t
it?&lt;/p>
&lt;h1 id="automatic-labeling-of-new-prs">Automatic Labeling of new PRs&lt;/h1>
&lt;p>The prediction API is nice for testing, but now we need a real-world use case.
Prow supports external plugins which can be used to take action on any GitHub
event. I wrote &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/tree/master/pkg">a plugin&lt;/a> which uses the kfserving API to make predictions
based on new pull requests. This means if we now create a new pull request in
the kubernetes-analysis repository, we will see the following:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-1.png" alt="pr 1">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-2.png" alt="pr 2">&lt;/p>
&lt;p>Okay cool, so now let’s change the release note based on a real bug from the
already existing dataset:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-3.png" alt="pr 3">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-4.png" alt="pr 4">&lt;/p>
&lt;p>The bot edits its own comment, predicts it with round about 90% as &lt;code>kind/bug&lt;/code>
and automatically adds the correct label! Now, if we change it back to some
different - obviously wrong - release note:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-5.png" alt="pr 5">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-6.png" alt="pr 6">&lt;/p>
&lt;p>The bot does the work for us, removes the label and informs us what it did!
Finally, if we change the release note to &lt;code>None&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-7.png" alt="pr 7">&lt;/p>
&lt;hr>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-28-my-exciting-journey-into-kubernetes-history/pr-8.png" alt="pr 8">&lt;/p>
&lt;p>The bot removed the comment, which is nice and reduces the text noise on the PR.
Everything I demonstrated is running inside a single Kubernetes cluster, which
would make it unnecessary at all to expose the kfserving API to the public. This
introduces an indirect API rate limiting because the only usage would be
possible via the Prow bot user.&lt;/p>
&lt;p>If you want to try it out for yourself, feel free to open a &lt;a href="https://github.com/kubernetes-analysis/kubernetes-analysis/issues/new?&amp;amp;template=release-notes-test.md">new test
issue&lt;/a> in &lt;code>kubernetes-analysis&lt;/code>. This works because I enabled the plugin
also for issues rather than only for pull requests.&lt;/p>
&lt;p>So then, we have a running CI bot which is able to classify new release notes
based on a machine learning model. If the bot would run in the official
Kubernetes repository, then we could correct wrong label predictions manually.
This way, the next training iteration would pick up the correction and result in
a continuously improved model over time. All totally automated!&lt;/p>
&lt;h1 id="summary">Summary&lt;/h1>
&lt;p>Thank you for reading down to here! This was my little data science journey
through the Kubernetes GitHub repository. There are a lot of other things to
optimize, for example introducing more classes (than just &lt;code>kind/bug&lt;/code> or nothing)
or automatic hyperparameter tuning with Kubeflows &lt;a href="https://www.kubeflow.org/docs/components/hyperparameter-tuning/hyperparameter">Katib&lt;/a>. If you have any
questions or suggestions, then feel free to get in touch with me anytime. See you
soon!&lt;/p></description></item><item><title>Blog: An Introduction to the K8s-Infrastructure Working Group</title><link>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</link><pubDate>Wed, 27 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/27/an-introduction-to-the-k8s-infrastructure-working-group/</guid><description>
&lt;p>&lt;strong>Author&lt;/strong>: &lt;a href="https://twitter.com/kiran_oliver">Kiran &amp;quot;Rin&amp;quot; Oliver&lt;/a> Storyteller, Kubernetes Upstream Marketing Team&lt;/p>
&lt;h1 id="an-introduction-to-the-k8s-infrastructure-working-group">An Introduction to the K8s-Infrastructure Working Group&lt;/h1>
&lt;p>&lt;em>Welcome to part one of a new series introducing the K8s-Infrastructure working group!&lt;/em>&lt;/p>
&lt;p>When Kubernetes was formed in 2014, Google undertook the task of building and maintaining the infrastructure necessary for keeping the project running smoothly. The tools itself were open source, but the Google Cloud Platform project used to run the infrastructure was internal-only, preventing contributors from being able to help out. In August 2018, Google granted the Cloud Native Computing Foundation &lt;a href="https://cloud.google.com/blog/products/gcp/google-cloud-grants-9m-in-credits-for-the-operation-of-the-kubernetes-project">$9M in credits for the operation of Kubernetes&lt;/a>. The sentiment behind this was that a project such as Kubernetes should be both maintained and operated by the community itself rather than by a single vendor.&lt;/p>
&lt;p>A group of community members enthusiastically undertook the task of collaborating on the path forward, realizing that there was a &lt;a href="https://github.com/kubernetes/community/issues/2715">more formal infrastructure necessary&lt;/a>. They joined together as a cross-team working group with ownership spanning across multiple Kubernetes SIGs (Architecture, Contributor Experience, Release, and Testing). &lt;a href="https://twitter.com/spiffxp">Aaron Crickenberger&lt;/a> worked with the Kubernetes Steering Committee to enable the formation of the working group, co-drafting a charter alongside long-time collaborator &lt;a href="https://twitter.com/dims">Davanum Srinivas&lt;/a>, and by 2019 the working group was official.&lt;/p>
&lt;h2 id="what-issues-does-the-k8s-infrastructure-working-group-tackle">What Issues Does the K8s-Infrastructure Working Group Tackle?&lt;/h2>
&lt;p>The team took on the complex task of managing the many moving parts of the infrastructure that sustains Kubernetes as a project.&lt;/p>
&lt;p>The need started with necessity: the first problem they took on was a complete migration of all of the project's infrastructure from Google-owned infrastructure to the Cloud Native Computing Foundation (CNCF). This is being done so that the project is self-sustainable without the need of any direct assistance from individual vendors. This breaks down in the following ways:&lt;/p>
&lt;ul>
&lt;li>Identifying what infrastructure the Kubernetes project depends on.
&lt;ul>
&lt;li>What applications are running?&lt;/li>
&lt;li>Where does it run?&lt;/li>
&lt;li>Where is its source code?&lt;/li>
&lt;li>What is custom built?&lt;/li>
&lt;li>What is off-the-shelf?&lt;/li>
&lt;li>What services depend on each other?&lt;/li>
&lt;li>How is it administered?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting guidelines and policies for how to run the infrastructure as a community.
&lt;ul>
&lt;li>What are our access policies?&lt;/li>
&lt;li>How do we keep track of billing?&lt;/li>
&lt;li>How do we ensure privacy and security?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Migrating infrastructure over to the CNCF as-is.
&lt;ul>
&lt;li>What is the path of least resistance to migration?&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Improving the state of the infrastructure for sustainability.
&lt;ul>
&lt;li>Moving from humans running scripts to a more automated GitOps model (YAML all the things!)&lt;/li>
&lt;li>Supporting community members who wish to develop new infrastructure&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>Documenting the state of our efforts, better defining goals, and completeness indicators.
&lt;ul>
&lt;li>The project and program management necessary to communicate this work to our &lt;a href="https://kubernetes.io/blog/2020/04/21/contributor-communication/">massive community of contributors&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="the-challenge-of-k8s-infrastructure-is-documentation">The challenge of K8s-Infrastructure is documentation&lt;/h2>
&lt;p>The most crucial problem the working group is trying to tackle is that the project is all volunteer-led. This leads to contributors, chairs, and others involved in the project quickly becoming overscheduled. As a result of this, certain areas such as documentation and organization often lack information, and efforts to progress are taking longer than the group would like to complete.&lt;/p>
&lt;p>Some of the infrastructure that is being migrated over hasn't been updated in a while, and its original authors or directly responsible individuals have moved on from working on Kubernetes. While this is great from the perspective of the fact that the code was able to run untouched for a long period of time, from the perspective of trying to migrate, this makes it difficult to identify how to operate these components, and how to move these infrastructure pieces where they need to be effectively.&lt;/p>
&lt;p>The lack of documentation is being addressed head-on by group member &lt;a href="https://twitter.com/bartsmykla">Bart Smykla&lt;/a>, but there is a definite need for others to support. If you're looking for a way to &lt;a href="https://github.com/kubernetes/community/labels/wg%2Fk8s-infra">get involved&lt;/a> and learn the infrastructure, you can become a new contributor to the working group!&lt;/p>
&lt;h2 id="celebrating-some-working-group-wins">Celebrating some Working Group wins&lt;/h2>
&lt;p>The team has made progress in the last few months that is well worth celebrating.&lt;/p>
&lt;ul>
&lt;li>The K8s-Infrastructure Working Group released an automated billing report that they start every meeting off by reviewing as a group.&lt;/li>
&lt;li>DNS for k8s.io and kubernetes.io are also fully &lt;a href="https://groups.google.com/g/kubernetes-dev/c/LZTYJorGh7c/m/u-ydk-yNEgAJ">community-owned&lt;/a>, with community members able to &lt;a href="https://github.com/kubernetes/k8s.io/issues/new?assignees=&amp;amp;labels=wg%2Fk8s-infra&amp;amp;template=dns-request.md&amp;amp;title=DNS+REQUEST%3A+%3Cyour-dns-record%3E">file issues&lt;/a> to manage records.&lt;/li>
&lt;li>The container registry &lt;a href="https://github.com/kubernetes/k8s.io/tree/master/k8s.gcr.io">k8s.gcr.io&lt;/a> is also fully community-owned and available for all Kubernetes subprojects to use.&lt;/li>
&lt;li>The Kubernetes &lt;a href="https://github.com/kubernetes/publishing-bot">publishing-bot&lt;/a> responsible for keeping k8s.io/kubernetes/staging repositories published to their own top-level repos (For example: &lt;a href="https://github.com/kubernetes/api">kubernetes/api&lt;/a>) runs on a community-owned cluster.&lt;/li>
&lt;li>The gcsweb.k8s.io service used to provide anonymous access to GCS buckets for kubernetes artifacts runs on a community-owned cluster.&lt;/li>
&lt;li>There is also an automated process of promoting all our container images. This includes a fully documented infrastructure, managed by the Kubernetes community, with automated processes for provisioning permissions.&lt;/li>
&lt;/ul>
&lt;p>These are just a few of the things currently happening in the K8s Infrastructure working group.&lt;/p>
&lt;p>If you're interested in getting involved, be sure to join the &lt;a href="https://app.slack.com/client/T09NY5SBT/CCK68P2Q2">#wg-K8s-infra Slack Channel&lt;/a>. Meetings are 60 minutes long, and are held every other Wednesday at 8:30 AM PT/16:30 UTC.&lt;/p>
&lt;p>Join to help with the documentation, stay to learn about the amazing infrastructure supporting the Kubernetes community.&lt;/p></description></item><item><title>Blog: WSL+Docker: Kubernetes on the Windows Desktop</title><link>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</link><pubDate>Thu, 21 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/21/wsl-docker-kubernetes-on-the-windows-desktop/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong>: &lt;a href="https://twitter.com/nunixtech">Nuno do Carmo&lt;/a> Docker Captain and WSL Corsair; &lt;a href="https://twitter.com/idvoretskyi">Ihor Dvoretskyi&lt;/a>, Developer Advocate, Cloud Native Computing Foundation&lt;/p>
&lt;h1 id="introduction">Introduction&lt;/h1>
&lt;p>New to Windows 10 and WSL2, or new to Docker and Kubernetes? Welcome to this blog post where we will install from scratch Kubernetes in Docker &lt;a href="https://kind.sigs.k8s.io/">KinD&lt;/a> and &lt;a href="https://minikube.sigs.k8s.io/docs/">Minikube&lt;/a>.&lt;/p>
&lt;h1 id="why-kubernetes-on-windows">Why Kubernetes on Windows?&lt;/h1>
&lt;p>For the last few years, Kubernetes became a de-facto standard platform for running containerized services and applications in distributed environments. While a wide variety of distributions and installers exist to deploy Kubernetes in the cloud environments (public, private or hybrid), or within the bare metal environments, there is still a need to deploy and run Kubernetes locally, for example, on the developer's workstation.&lt;/p>
&lt;p>Kubernetes has been originally designed to be deployed and used in the Linux environments. However, a good number of users (and not only application developers) use Windows OS as their daily driver. When Microsoft revealed WSL - &lt;a href="https://docs.microsoft.com/en-us/windows/wsl/">the Windows Subsystem for Linux&lt;/a>, the line between Windows and Linux environments became even less visible.&lt;/p>
&lt;p>Also, WSL brought an ability to run Kubernetes on Windows almost seamlessly!&lt;/p>
&lt;p>Below, we will cover in brief how to install and use various solutions to run Kubernetes locally.&lt;/p>
&lt;h1 id="prerequisites">Prerequisites&lt;/h1>
&lt;p>Since we will explain how to install KinD, we won't go into too much detail around the installation of KinD's dependencies.&lt;/p>
&lt;p>However, here is the list of the prerequisites needed and their version/lane:&lt;/p>
&lt;ul>
&lt;li>OS: Windows 10 version 2004, Build 19041&lt;/li>
&lt;li>&lt;a href="https://docs.microsoft.com/en-us/windows/wsl/wsl2-install">WSL2 enabled&lt;/a>
&lt;ul>
&lt;li>In order to install the distros as WSL2 by default, once WSL2 installed, run the command &lt;code>wsl.exe --set-default-version 2&lt;/code> in Powershell&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>WSL2 distro installed from the Windows Store - the distro used is Ubuntu-18.04&lt;/li>
&lt;li>&lt;a href="https://hub.docker.com/editions/community/docker-ce-desktop-windows">Docker Desktop for Windows&lt;/a>, stable channel - the version used is 2.2.0.4&lt;/li>
&lt;li>[Optional] Microsoft Terminal installed from the Windows Store
&lt;ul>
&lt;li>Open the Windows store and type &amp;quot;Terminal&amp;quot; in the search, it will be (normally) the first option&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-windows-store-terminal.png" alt="Windows Store Terminal">&lt;/p>
&lt;p>And that's actually it. For Docker Desktop for Windows, no need to configure anything yet as we will explain it in the next section.&lt;/p>
&lt;h1 id="wsl2-first-contact">WSL2: First contact&lt;/h1>
&lt;p>Once everything is installed, we can launch the WSL2 terminal from the Start menu, and type &amp;quot;Ubuntu&amp;quot; for searching the applications and documents:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-start-menu-search.png" alt="Start Menu Search">&lt;/p>
&lt;p>Once found, click on the name and it will launch the default Windows console with the Ubuntu bash shell running.&lt;/p>
&lt;p>Like for any normal Linux distro, you need to create a user and set a password:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-user-password.png" alt="User-Password">&lt;/p>
&lt;h2 id="optional-update-the-sudoers">[Optional] Update the &lt;code>sudoers&lt;/code>&lt;/h2>
&lt;p>As we are working, normally, on our local computer, it might be nice to update the &lt;code>sudoers&lt;/code> and set the group &lt;code>%sudo&lt;/code> to be password-less:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the sudoers with the visudo command&lt;/span>
sudo visudo
&lt;span style="color:#080;font-style:italic"># Change the %sudo group to be password-less&lt;/span>
%sudo &lt;span style="color:#b8860b">ALL&lt;/span>&lt;span style="color:#666">=(&lt;/span>ALL:ALL&lt;span style="color:#666">)&lt;/span> NOPASSWD: ALL
&lt;span style="color:#080;font-style:italic"># Press CTRL+X to exit&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Y to save&lt;/span>
&lt;span style="color:#080;font-style:italic"># Press Enter to confirm&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-visudo.png" alt="visudo">&lt;/p>
&lt;h2 id="update-ubuntu">Update Ubuntu&lt;/h2>
&lt;p>Before we move to the Docker Desktop settings, let's update our system and ensure we start in the best conditions:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Update the repositories and list of the packages available&lt;/span>
sudo apt update
&lt;span style="color:#080;font-style:italic"># Update the system based on the packages installed &amp;gt; the &amp;#34;-y&amp;#34; will approve the change automatically&lt;/span>
sudo apt upgrade -y
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-apt-update-upgrade.png" alt="apt-update-upgrade">&lt;/p>
&lt;h1 id="docker-desktop-faster-with-wsl2">Docker Desktop: faster with WSL2&lt;/h1>
&lt;p>Before we move into the settings, let's do a small test, it will display really how cool the new integration with Docker Desktop is:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-error.png" alt="kubectl-error">&lt;/p>
&lt;p>You got an error? Perfect! It's actually good news, so let's now move on to the settings.&lt;/p>
&lt;h2 id="docker-desktop-settings-enable-wsl2-integration">Docker Desktop settings: enable WSL2 integration&lt;/h2>
&lt;p>First let's start Docker Desktop for Windows if it's not still the case. Open the Windows start menu and type &amp;quot;docker&amp;quot;, click on the name to start the application:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-start.png" alt="docker-start">&lt;/p>
&lt;p>You should now see the Docker icon with the other taskbar icons near the clock:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-taskbar.png" alt="docker-taskbar">&lt;/p>
&lt;p>Now click on the Docker icon and choose settings. A new window will appear:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-general.png" alt="docker-settings-general">&lt;/p>
&lt;p>By default, the WSL2 integration is not active, so click the &amp;quot;Enable the experimental WSL 2 based engine&amp;quot; and click &amp;quot;Apply &amp;amp; Restart&amp;quot;:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-settings-wsl2-activated.png" alt="docker-settings-wsl2">&lt;/p>
&lt;p>What this feature did behind the scenes was to create two new distros in WSL2, containing and running all the needed backend sockets, daemons and also the CLI tools (read: docker and kubectl command).&lt;/p>
&lt;p>Still, this first setting is still not enough to run the commands inside our distro. If we try, we will have the same error as before.&lt;/p>
&lt;p>In order to fix it, and finally be able to use the commands, we need to tell the Docker Desktop to &amp;quot;attach&amp;quot; itself to our distro also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-resources-wsl-integration.png" alt="docker-resources-wsl">&lt;/p>
&lt;p>Let's now switch back to our WSL2 terminal and see if we can (finally) launch the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Try to see if the docker cli and daemon are installed&lt;/span>
docker version
&lt;span style="color:#080;font-style:italic"># Same for kubectl&lt;/span>
kubectl version
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-docker-kubectl-success.png" alt="docker-kubectl-success">&lt;/p>
&lt;blockquote>
&lt;p>Tip: if nothing happens, restart Docker Desktop and restart the WSL process in Powershell: &lt;code>Restart-Service LxssManager&lt;/code> and launch a new Ubuntu session&lt;/p>
&lt;/blockquote>
&lt;p>And success! The basic settings are now done and we move to the installation of KinD.&lt;/p>
&lt;h1 id="kind-kubernetes-made-easy-in-a-container">KinD: Kubernetes made easy in a container&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install KinD and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to on the &lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">official KinD website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of KinD&lt;/span>
curl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.7.0/kind-linux-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./kind
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./kind /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install.png" alt="kind-install">&lt;/p>
&lt;h2 id="kind-the-first-cluster">KinD: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Create the cluster and give it a name (optional)&lt;/span>
kind create cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Check if the .kube has been created and populated with files&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create.png" alt="kind-cluster-create">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>The cluster has been successfully created, and because we are using Docker Desktop, the network is all set for us to use &amp;quot;as is&amp;quot;.&lt;/p>
&lt;p>So we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-k8s-master.png" alt="kind-browser-k8s-master">&lt;/p>
&lt;p>And this is the real strength from Docker Desktop for Windows with the WSL2 backend. Docker really did an amazing integration.&lt;/p>
&lt;h2 id="kind-counting-1-2-3">KinD: counting 1 - 2 - 3&lt;/h2>
&lt;p>Our first cluster was created and it's the &amp;quot;normal&amp;quot; one node cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-nodes-services.png" alt="kind-list-nodes-services">&lt;/p>
&lt;p>While this will be enough for most people, let's leverage one of the coolest feature, multi-node clustering:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Delete the existing cluster&lt;/span>
kind delete cluster --name wslkind
&lt;span style="color:#080;font-style:italic"># Create a config file for a 3 nodes cluster&lt;/span>
cat &lt;span style="color:#b44">&amp;lt;&amp;lt; EOF &amp;gt; kind-3nodes.yaml
&lt;/span>&lt;span style="color:#b44">kind: Cluster
&lt;/span>&lt;span style="color:#b44">apiVersion: kind.x-k8s.io/v1alpha4
&lt;/span>&lt;span style="color:#b44">nodes:
&lt;/span>&lt;span style="color:#b44"> - role: control-plane
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44"> - role: worker
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a new cluster with the config file&lt;/span>
kind create cluster --name wslkindmultinodes --config ./kind-3nodes.yaml
&lt;span style="color:#080;font-style:italic"># Check how many nodes it created&lt;/span>
kubectl get nodes
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-cluster-create-multinodes.png" alt="kind-cluster-create-multinodes">&lt;/p>
&lt;blockquote>
&lt;p>Tip: depending on how fast we run the &amp;quot;get nodes&amp;quot; command, it can be that not all the nodes are ready, wait few seconds and run it again, everything should be ready&lt;/p>
&lt;/blockquote>
&lt;p>And that's it, we have created a three-node cluster, and if we look at the services one more time, we will see several that have now three replicas:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check the services for the whole cluster&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-list-services-multinodes.png" alt="wsl2-kind-list-services-multinodes">&lt;/p>
&lt;h2 id="kind-can-i-see-a-nice-dashboard">KinD: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a> project has been created. The installation and first connection test is quite fast, so let's do it:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the Dashboard application into our cluster&lt;/span>
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-rc6/aio/deploy/recommended.yaml
&lt;span style="color:#080;font-style:italic"># Check the resources it created based on the new namespace created&lt;/span>
kubectl get all -n kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-install-dashboard.png" alt="kind-install-dashboard">&lt;/p>
&lt;p>As it created a service with a ClusterIP (read: internal network address), we cannot reach it if we type the URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-error.png" alt="kind-browse-dashboard-error">&lt;/p>
&lt;p>That's because we need to create a temporary proxy:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Start a kubectl proxy&lt;/span>
kubectl proxy
&lt;span style="color:#080;font-style:italic"># Enter the URL on your browser: http://localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-success.png" alt="kind-browse-dashboard-success">&lt;/p>
&lt;p>Finally to login, we can either enter a Token, which we didn't create, or enter the &lt;code>kubeconfig&lt;/code> file from our Cluster.&lt;/p>
&lt;p>If we try to login with the &lt;code>kubeconfig&lt;/code>, we will get the error &amp;quot;Internal error (500): Not enough data to create auth info structure&amp;quot;. This is due to the lack of credentials in the &lt;code>kubeconfig&lt;/code> file.&lt;/p>
&lt;p>So to avoid you ending with the same error, let's follow the &lt;a href="https://github.com/kubernetes/dashboard/blob/master/docs/user/access-control/creating-sample-user.md">recommended RBAC approach&lt;/a>.&lt;/p>
&lt;p>Let's open a new WSL2 session:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a new ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Create a ClusterRoleBinding for the ServiceAccount&lt;/span>
kubectl apply -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: rbac.authorization.k8s.io/v1
&lt;/span>&lt;span style="color:#b44">kind: ClusterRoleBinding
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44">roleRef:
&lt;/span>&lt;span style="color:#b44"> apiGroup: rbac.authorization.k8s.io
&lt;/span>&lt;span style="color:#b44"> kind: ClusterRole
&lt;/span>&lt;span style="color:#b44"> name: cluster-admin
&lt;/span>&lt;span style="color:#b44">subjects:
&lt;/span>&lt;span style="color:#b44">- kind: ServiceAccount
&lt;/span>&lt;span style="color:#b44"> name: admin-user
&lt;/span>&lt;span style="color:#b44"> namespace: kubernetes-dashboard
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-rbac-serviceaccount.png" alt="kind-browse-dashboard-rbac-serviceaccount">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get the Token for the ServiceAccount&lt;/span>
kubectl -n kubernetes-dashboard describe secret &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>kubectl -n kubernetes-dashboard get secret | grep admin-user | awk &lt;span style="color:#b44">&amp;#39;{print $1}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#080;font-style:italic"># Copy the token and copy it into the Dashboard login and press &amp;#34;Sign in&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-login-success.png" alt="kind-browse-dashboard-login-success">&lt;/p>
&lt;p>Success! And let's see our nodes listed also:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-kind-browse-dashboard-browse-nodes.png" alt="kind-browse-dashboard-browse-nodes">&lt;/p>
&lt;p>A nice and shiny three nodes appear.&lt;/p>
&lt;h1 id="minikube-kubernetes-from-everywhere">Minikube: Kubernetes from everywhere&lt;/h1>
&lt;p>Right now, we have Docker that is installed, configured and the last test worked fine.&lt;/p>
&lt;p>However, if we look carefully at the &lt;code>kubectl&lt;/code> command, it found the &amp;quot;Client Version&amp;quot; (1.15.5), but it didn't find any server.&lt;/p>
&lt;p>This is normal as we didn't enable the Docker Kubernetes cluster. So let's install Minikube and create our first cluster.&lt;/p>
&lt;p>And as sources are always important to mention, we will follow (partially) the how-to from the &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">Kubernetes.io website&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Download the latest version of Minikube&lt;/span>
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
&lt;span style="color:#080;font-style:italic"># Make the binary executable&lt;/span>
chmod +x ./minikube
&lt;span style="color:#080;font-style:italic"># Move the binary to your executable path&lt;/span>
sudo mv ./minikube /usr/local/bin/
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install.png" alt="minikube-install">&lt;/p>
&lt;h2 id="minikube-updating-the-host">Minikube: updating the host&lt;/h2>
&lt;p>If we follow the how-to, it states that we should use the &lt;code>--driver=none&lt;/code> flag in order to run Minikube directly on the host and Docker.&lt;/p>
&lt;p>Unfortunately, we will get an error about &amp;quot;conntrack&amp;quot; being required to run Kubernetes v 1.18:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error.png" alt="minikube-start-error">&lt;/p>
&lt;blockquote>
&lt;p>Tip: as you can see, the Terminal was changed so the nice icons are all displayed&lt;/p>
&lt;/blockquote>
&lt;p>So let's fix the issue by installing the missing package:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the conntrack package&lt;/span>
sudo apt install -y conntrack
&lt;/code>&lt;/pre>&lt;/div>&lt;p>![minikube-install-conntrack](/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-install conntrack.png)&lt;/p>
&lt;p>Let's try to launch it again:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create a minikube one node cluster&lt;/span>
minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;span style="color:#080;font-style:italic"># We got a permissions error &amp;gt; try again with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-error-systemd.png" alt="minikube-start-error-systemd">&lt;/p>
&lt;p>Ok, this error cloud be problematic ... in the past. Luckily for us, there's a solution&lt;/p>
&lt;h2 id="minikube-enabling-systemd">Minikube: enabling SystemD&lt;/h2>
&lt;p>In order to enable SystemD on WSL2, we will apply the &lt;a href="https://forum.snapcraft.io/t/running-snaps-on-wsl2-insiders-only-for-now/13033">scripts&lt;/a> from &lt;a href="https://twitter.com/diddledan">Daniel Llewellyn&lt;/a>.&lt;/p>
&lt;p>I invite you to read the full blog post and how he came to the solution, and the various iterations he did to fix several issues.&lt;/p>
&lt;p>So in a nutshell, here are the commands:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Install the needed packages&lt;/span>
sudo apt install -yqq daemonize dbus-user-session fontconfig
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-packages.png" alt="minikube-systemd-packages">&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the start-systemd-namespace script&lt;/span>
sudo vi /usr/sbin/start-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">||&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#666">(&lt;/span>&lt;span style="color:#a2f">set&lt;/span> -o posix; &lt;span style="color:#a2f">set&lt;/span>&lt;span style="color:#666">)&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^BASH&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^DIRSTACK=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^EUID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^GROUPS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^HOSTTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^IFS=&amp;#39;.*&amp;#34;&lt;/span>&lt;span style="color:#b44">$&amp;#39;\n&amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#39;&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LANG=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^LOGNAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^MACHTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^NAME=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTERR=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OPTIND=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^OSTYPE=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PIPESTATUS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^POSIXLY_CORRECT=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PPID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS1=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^PS4=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHELLOPTS=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SHLVL=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^SYSTEMD_PID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^UID=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^USER=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> grep -v &lt;span style="color:#b44">&amp;#34;^_=&amp;#34;&lt;/span> | &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> cat - &amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;PATH=&amp;#39;&lt;/span>&lt;span style="color:#b8860b">$PATH&lt;/span>&lt;span style="color:#b44">&amp;#39;&amp;#34;&lt;/span> &amp;gt;&amp;gt; &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> sudo /usr/sbin/enter-systemd-namespace &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$BASH_EXECUTION_STRING&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">export&lt;/span> &lt;span style="color:#b8860b">PATH&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$PRE_NAMESPACE_PATH&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Create the enter-systemd-namespace&lt;/span>
sudo vi /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic">#!/bin/bash&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$UID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#666">0&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;You need to run &lt;/span>&lt;span style="color:#b8860b">$0&lt;/span>&lt;span style="color:#b44"> through sudo&amp;#34;&lt;/span>
&lt;span style="color:#a2f">exit&lt;/span> &lt;span style="color:#666">1&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
/usr/sbin/daemonize /usr/bin/unshare --fork --pid --mount-proc /lib/systemd/systemd --system-unit&lt;span style="color:#666">=&lt;/span>basic.target
&lt;span style="color:#a2f;font-weight:bold">while&lt;/span> &lt;span style="color:#666">[&lt;/span> -z &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">do&lt;/span>
&lt;span style="color:#b8860b">SYSTEMD_PID&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>ps -ef | grep &lt;span style="color:#b44">&amp;#39;/lib/systemd/systemd --system-unit=basic.target$&amp;#39;&lt;/span> | grep -v unshare | awk &lt;span style="color:#b44">&amp;#39;{print $2}&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">done&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;1&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">if&lt;/span> &lt;span style="color:#666">[&lt;/span> -n &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span> &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> &lt;span style="color:#666">[&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$1&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> !&lt;span style="color:#666">=&lt;/span> &lt;span style="color:#b44">&amp;#34;/bin/bash --login&amp;#34;&lt;/span> &lt;span style="color:#666">]&lt;/span>; &lt;span style="color:#a2f;font-weight:bold">then&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /usr/bin/sudo -H -u &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/bash -c &lt;span style="color:#b44">&amp;#39;set -a; source &amp;#34;$HOME/.systemd-env&amp;#34;; set +a; exec bash -c &amp;#39;&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>&lt;span style="color:#a2f">printf&lt;/span> &lt;span style="color:#b44">&amp;#34;%q&amp;#34;&lt;/span> &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$@&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">else&lt;/span>
&lt;span style="color:#a2f">exec&lt;/span> /usr/bin/nsenter -t &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SYSTEMD_PID&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> -a &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> /bin/login -p -f &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$SUDO_USER&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>/bin/cat &lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#b8860b">$HOME&lt;/span>&lt;span style="color:#b44">/.systemd-env&amp;#34;&lt;/span> | grep -v &lt;span style="color:#b44">&amp;#34;^PATH=&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;Existential crisis&amp;#34;&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">fi&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the permissions of the enter-systemd-namespace script&lt;/span>
sudo chmod +x /usr/sbin/enter-systemd-namespace
&lt;span style="color:#080;font-style:italic"># Edit the bash.bashrc file&lt;/span>
sudo sed -i 2a&lt;span style="color:#b44">&amp;#34;# Start or enter a PID namespace in WSL2\nsource /usr/sbin/start-systemd-namespace\n&amp;#34;&lt;/span> /etc/bash.bashrc
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-files.png" alt="minikube-systemd-files">&lt;/p>
&lt;p>Finally, exit and launch a new session. You &lt;strong>do not&lt;/strong> need to stop WSL2, a new session is enough:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-systemd-enabled.png" alt="minikube-systemd-enabled">&lt;/p>
&lt;h2 id="minikube-the-first-cluster">Minikube: the first cluster&lt;/h2>
&lt;p>We are ready to create our first cluster:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Check if the KUBECONFIG is not set&lt;/span>
&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b8860b">$KUBECONFIG&lt;/span>
&lt;span style="color:#080;font-style:italic"># Check if the .kube directory is created &amp;gt; if not, no need to create it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
&lt;span style="color:#080;font-style:italic"># Check if the .minikube directory is created &amp;gt; if yes, delete it&lt;/span>
ls &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Create the cluster with sudo&lt;/span>
sudo minikube start --driver&lt;span style="color:#666">=&lt;/span>none
&lt;/code>&lt;/pre>&lt;/div>&lt;p>In order to be able to use &lt;code>kubectl&lt;/code> with our user, and not &lt;code>sudo&lt;/code>, Minikube recommends running the &lt;code>chown&lt;/code> command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Change the owner of the .kube and .minikube directories&lt;/span>
sudo chown -R &lt;span style="color:#b8860b">$USER&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube &lt;span style="color:#b8860b">$HOME&lt;/span>/.minikube
&lt;span style="color:#080;font-style:italic"># Check the access and if the cluster is running&lt;/span>
kubectl cluster-info
&lt;span style="color:#080;font-style:italic"># Check the resources created&lt;/span>
kubectl get all --all-namespaces
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-start-fixed.png" alt="minikube-start-fixed">&lt;/p>
&lt;p>The cluster has been successfully created, and Minikube used the WSL2 IP, which is great for several reasons, and one of them is that we can open the &lt;code>Kubernetes master&lt;/code> URL in our Windows browser:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master.png" alt="minikube-browse-k8s-master">&lt;/p>
&lt;p>And the real strength of WSL2 integration, the port &lt;code>8443&lt;/code> once open on WSL2 distro, it actually forwards it to Windows, so instead of the need to remind the IP address, we can also reach the &lt;code>Kubernetes master&lt;/code> URL via &lt;code>localhost&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-k8s-master-localhost.png" alt="minikube-browse-k8s-master-localhost">&lt;/p>
&lt;h2 id="minikube-can-i-see-a-nice-dashboard">Minikube: can I see a nice dashboard?&lt;/h2>
&lt;p>Working on the command line is always good and very insightful. However, when dealing with Kubernetes we might want, at some point, to have a visual overview.&lt;/p>
&lt;p>For that, Minikube embeded the &lt;a href="https://github.com/kubernetes/dashboard">Kubernetes Dashboard&lt;/a>. Thanks to it, running and accessing the Dashboard is very simple:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Enable the Dashboard service&lt;/span>
sudo minikube dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard.png" alt="minikube-browse-dashboard">&lt;/p>
&lt;p>The command creates also a proxy, which means that once we end the command, by pressing &lt;code>CTRL+C&lt;/code>, the Dashboard will no more be accessible.&lt;/p>
&lt;p>Still, if we look at the namespace &lt;code>kubernetes-dashboard&lt;/code>, we will see that the service is still created:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-get-all.png" alt="minikube-dashboard-get-all">&lt;/p>
&lt;p>Let's edit the service and change it's type to &lt;code>LoadBalancer&lt;/code>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Edit the Dashoard service&lt;/span>
kubectl edit service/kubernetes-dashboard --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Go to the very end and remove the last 2 lines&lt;/span>
status:
loadBalancer: &lt;span style="color:#666">{}&lt;/span>
&lt;span style="color:#080;font-style:italic"># Change the type from ClusterIO to LoadBalancer&lt;/span>
type: LoadBalancer
&lt;span style="color:#080;font-style:italic"># Save the file&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-dashboard-type-loadbalancer.png" alt="minikube-dashboard-type-loadbalancer">&lt;/p>
&lt;p>Check again the Dashboard service and let's access the Dashboard via the LoadBalancer:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">&lt;span style="color:#080;font-style:italic"># Get all the services from the dashboard namespace&lt;/span>
kubectl get all --namespace kubernetes-dashboard
&lt;span style="color:#080;font-style:italic"># Access the Dashboard from a browser on Windows side with the URL: localhost:&amp;lt;port exposed&amp;gt;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-21-wsl2-dockerdesktop-k8s/wsl2-minikube-browse-dashboard-loadbalancer.png" alt="minikube-browse-dashboard-loadbalancer">&lt;/p>
&lt;h1 id="conclusion">Conclusion&lt;/h1>
&lt;p>It's clear that we are far from done as we could have some LoadBalancing implemented and/or other services (storage, ingress, registry, etc...).&lt;/p>
&lt;p>Concerning Minikube on WSL2, as it needed to enable SystemD, we can consider it as an intermediate level to be implemented.&lt;/p>
&lt;p>So with two solutions, what could be the &amp;quot;best for you&amp;quot;? Both bring their own advantages and inconveniences, so here an overview from our point of view solely:&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Criteria&lt;/th>
&lt;th>KinD&lt;/th>
&lt;th>Minikube&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>Installation on WSL2&lt;/td>
&lt;td>Very Easy&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Multi-node&lt;/td>
&lt;td>Yes&lt;/td>
&lt;td>No&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Plugins&lt;/td>
&lt;td>Manual install&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Persistence&lt;/td>
&lt;td>Yes, however not designed for&lt;/td>
&lt;td>Yes&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>Alternatives&lt;/td>
&lt;td>K3d&lt;/td>
&lt;td>Microk8s&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>We hope you could have a real taste of the integration between the different components: WSL2 - Docker Desktop - KinD/Minikube. And that gave you some ideas or, even better, some answers to your Kubernetes workflows with KinD and/or Minikube on Windows and WSL2.&lt;/p>
&lt;p>See you soon for other adventures in the Kubernetes ocean.&lt;/p>
&lt;p>&lt;a href="https://twitter.com/nunixtech">Nuno&lt;/a> &amp;amp; &lt;a href="https://twitter.com/idvoretskyi">Ihor&lt;/a>&lt;/p></description></item><item><title>Blog: How Docs Handle Third Party and Dual Sourced Content</title><link>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</link><pubDate>Wed, 06 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/third-party-dual-sourced-content/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen, Cloud Native Computing Foundation&lt;/p>
&lt;p>&lt;em>Editor's note: Zach is one of the chairs for the Kubernetes documentation special interest group (SIG Docs).&lt;/em>&lt;/p>
&lt;p>Late last summer, SIG Docs started a community conversation about third party content in Kubernetes docs. This conversation became a &lt;a href="https://github.com/kubernetes/enhancements/pull/1327">Kubernetes Enhancement Proposal&lt;/a> (KEP) and, after five months for review and comment, SIG Architecture approved the KEP as a &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/">content guide&lt;/a> for Kubernetes docs.&lt;/p>
&lt;p>Here's how Kubernetes docs handle third party content now:&lt;/p>
&lt;blockquote>
&lt;p>Links to active content in the Kubernetes project (projects in the kubernetes and kubernetes-sigs GitHub orgs) are always allowed.&lt;/p>
&lt;p>Kubernetes requires some third party content to function. Examples include container runtimes (containerd, CRI-O, Docker), networking policy (CNI plugins), Ingress controllers, and logging.&lt;/p>
&lt;p>Docs can link to third party open source software (OSS) outside the Kubernetes project if it’s necessary for Kubernetes to function.&lt;/p>
&lt;/blockquote>
&lt;p>These common sense guidelines make sure that Kubernetes docs document Kubernetes.&lt;/p>
&lt;h2 id="keeping-the-docs-focused">Keeping the docs focused&lt;/h2>
&lt;p>Our goal is for Kubernetes docs to be a trustworthy guide to Kubernetes features. To achieve this goal, SIG Docs is &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a> and removing any third party content that isn't both in the Kubernetes project &lt;em>and&lt;/em> required for Kubernetes to function.&lt;/p>
&lt;h3 id="re-homing-content">Re-homing content&lt;/h3>
&lt;p>Some content will be removed that readers may find helpful. To make sure readers have continous access to information, we're giving stakeholders until the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/releases/release-1.19">1.19 release deadline for docs&lt;/a>, &lt;strong>July 9th, 2020&lt;/strong> to re-home any content slated for removal.&lt;/p>
&lt;p>Over the next few months you'll see less third party content in the docs as contributors open PRs to remove content.&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Over time, SIG Docs observed increasing vendor content in the docs. Some content took the form of vendor-specific implementations that aren't required for Kubernetes to function in-project. Other content was thinly-disguised advertising with minimal to no feature content. Some vendor content was new; other content had been in the docs for years. It became clear that the docs needed clear, well-bounded guidelines for what kind of third party content is and isn't allowed. The &lt;a href="https://kubernetes.io/docs/contribute/content-guide/">content guide&lt;/a> emerged from an extensive period for review and comment from the community.&lt;/p>
&lt;p>Docs work best when they're accurate, helpful, trustworthy, and remain focused on features. In our experience, vendor content dilutes trust and accuracy.&lt;/p>
&lt;p>Put simply: feature docs aren't a place for vendors to advertise their products. Our content policy keeps the docs focused on helping developers and cluster admins, not on marketing.&lt;/p>
&lt;h2 id="dual-sourced-content">Dual sourced content&lt;/h2>
&lt;p>Less impactful but also important is how Kubernetes docs handle &lt;em>dual-sourced content&lt;/em>. Dual-sourced content is content published in more than one location, or from a non-canonical source.&lt;/p>
&lt;p>From the &lt;a href="https://kubernetes.io/docs/contribute/style/content-guide/#dual-sourced-content">Kubernetes content guide&lt;/a>:&lt;/p>
&lt;blockquote>
&lt;p>Wherever possible, Kubernetes docs link to canonical sources instead of hosting dual-sourced content.&lt;/p>
&lt;/blockquote>
&lt;p>Minimizing dual-sourced content streamlines the docs and makes content across the Web more searchable. We're working to consolidate and redirect dual-sourced content in the Kubernetes docs as well.&lt;/p>
&lt;h2 id="ways-to-contribute">Ways to contribute&lt;/h2>
&lt;p>We're tracking third-party content in an &lt;a href="https://github.com/kubernetes/website/issues/20232">issue in the Kubernetes website repository&lt;/a>. If you see third party content that's out of project and isn't required for Kubernetes to function, please comment on the tracking issue.&lt;/p>
&lt;p>Feel free to open a PR that removes non-conforming content once you've identified it!&lt;/p>
&lt;h2 id="want-to-know-more">Want to know more?&lt;/h2>
&lt;p>For more information, read the issue description for &lt;a href="https://github.com/kubernetes/website/issues/20232">tracking third party content&lt;/a>.&lt;/p></description></item><item><title>Blog: Introducing PodTopologySpread</title><link>https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/</link><pubDate>Tue, 05 May 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/05/introducing-podtopologyspread/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Wei Huang (IBM), Aldo Culquicondor (Google)&lt;/p>
&lt;p>Managing Pods distribution across a cluster is hard. The well-known Kubernetes
features for Pod affinity and anti-affinity, allow some control of Pod placement
in different topologies. However, these features only resolve part of Pods
distribution use cases: either place unlimited Pods to a single topology, or
disallow two Pods to co-locate in the same topology. In between these two
extreme cases, there is a common need to distribute the Pods evenly across the
topologies, so as to achieve better cluster utilization and high availability of
applications.&lt;/p>
&lt;p>The PodTopologySpread scheduling plugin (originally proposed as EvenPodsSpread)
was designed to fill that gap. We promoted it to beta in 1.18.&lt;/p>
&lt;h2 id="api-changes">API changes&lt;/h2>
&lt;p>A new field &lt;code>topologySpreadConstraints&lt;/code> is introduced in the Pod's spec API:&lt;/p>
&lt;pre>&lt;code>spec:
topologySpreadConstraints:
- maxSkew: &amp;lt;integer&amp;gt;
topologyKey: &amp;lt;string&amp;gt;
whenUnsatisfiable: &amp;lt;string&amp;gt;
labelSelector: &amp;lt;object&amp;gt;
&lt;/code>&lt;/pre>&lt;p>As this API is embedded in Pod's spec, you can use this feature in all the
high-level workload APIs, such as Deployment, DaemonSet, StatefulSet, etc.&lt;/p>
&lt;p>Let's see an example of a cluster to understand this API.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/api.png" alt="API">&lt;/p>
&lt;ul>
&lt;li>&lt;strong>labelSelector&lt;/strong> is used to find matching Pods. For each topology, we count
the number of Pods that match this label selector. In the above example, given
the labelSelector as &amp;quot;app: foo&amp;quot;, the matching number in &amp;quot;zone1&amp;quot; is 2; while
the number in &amp;quot;zone2&amp;quot; is 0.&lt;/li>
&lt;li>&lt;strong>topologyKey&lt;/strong> is the key that defines a topology in the Nodes' labels. In
the above example, some Nodes are grouped into &amp;quot;zone1&amp;quot; if they have the label
&amp;quot;zone=zone1&amp;quot; label; while other ones are grouped into &amp;quot;zone2&amp;quot;.&lt;/li>
&lt;li>&lt;strong>maxSkew&lt;/strong> describes the maximum degree to which Pods can be unevenly
distributed. In the above example:
&lt;ul>
&lt;li>if we put the incoming Pod to &amp;quot;zone1&amp;quot;, the skew on &amp;quot;zone1&amp;quot; will become 3 (3
Pods matched in &amp;quot;zone1&amp;quot;; global minimum of 0 Pods matched on &amp;quot;zone2&amp;quot;), which
violates the &amp;quot;maxSkew: 1&amp;quot; constraint.&lt;/li>
&lt;li>if the incoming Pod is placed to &amp;quot;zone2&amp;quot;, the skew on &amp;quot;zone2&amp;quot; is 0 (1 Pod
matched in &amp;quot;zone2&amp;quot;; global minimum of 1 Pod matched on &amp;quot;zone2&amp;quot; itself),
which satisfies the &amp;quot;maxSkew: 1&amp;quot; constraint. Note that the skew is
calculated per each qualified Node, instead of a global skew.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>whenUnsatisfiable&lt;/strong> specifies, when &amp;quot;maxSkew&amp;quot; can't be satisfied, what
action should be taken:
&lt;ul>
&lt;li>&lt;code>DoNotSchedule&lt;/code> (default) tells the scheduler not to schedule it. It's a
hard constraint.&lt;/li>
&lt;li>&lt;code>ScheduleAnyway&lt;/code> tells the scheduler to still schedule it while prioritizing
Nodes that reduce the skew. It's a soft constraint.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="advanced-usage">Advanced usage&lt;/h2>
&lt;p>As the feature name &amp;quot;PodTopologySpread&amp;quot; implies, the basic usage of this feature
is to run your workload with an absolute even manner (maxSkew=1), or relatively
even manner (maxSkew&amp;gt;=2). See the &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/">official
document&lt;/a>
for more details.&lt;/p>
&lt;p>In addition to this basic usage, there are some advanced usage examples that
enable your workloads to benefit on high availability and cluster utilization.&lt;/p>
&lt;h3 id="usage-along-with-nodeselector-nodeaffinity">Usage along with NodeSelector / NodeAffinity&lt;/h3>
&lt;p>You may have found that we didn't have a &amp;quot;topologyValues&amp;quot; field to limit which
topologies the Pods are going to be scheduled to. By default, it is going to
search all Nodes and group them by &amp;quot;topologyKey&amp;quot;. Sometimes this may not be the
ideal case. For instance, suppose there is a cluster with Nodes tagged with
&amp;quot;env=prod&amp;quot;, &amp;quot;env=staging&amp;quot; and &amp;quot;env=qa&amp;quot;, and now you want to evenly place Pods to
the &amp;quot;qa&amp;quot; environment across zones, is it possible?&lt;/p>
&lt;p>The answer is yes. You can leverage the NodeSelector or NodeAffinity API spec.
Under the hood, the PodTopologySpread feature will &lt;strong>honor&lt;/strong> that and calculate
the spread constraints among the nodes that satisfy the selectors.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-1.png" alt="Advanced-Usage-1">&lt;/p>
&lt;p>As illustrated above, you can specify &lt;code>spec.affinity.nodeAffinity&lt;/code> to limit the
&amp;quot;searching scope&amp;quot; to be &amp;quot;qa&amp;quot; environment, and within that scope, the Pod will be
scheduled to one zone which satisfies the topologySpreadConstraints. In this
case, it's &amp;quot;zone2&amp;quot;.&lt;/p>
&lt;h3 id="multiple-topologyspreadconstraints">Multiple TopologySpreadConstraints&lt;/h3>
&lt;p>It's intuitive to understand how one single TopologySpreadConstraint works.
What's the case for multiple TopologySpreadConstraints? Internally, each
TopologySpreadConstraint is calculated independently, and the result sets will
be merged to generate the eventual result set - i.e., suitable Nodes.&lt;/p>
&lt;p>In the following example, we want to schedule a Pod to a cluster with 2
requirements at the same time:&lt;/p>
&lt;ul>
&lt;li>place the Pod evenly with Pods across zones&lt;/li>
&lt;li>place the Pod evenly with Pods across nodes&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-05-05-introducing-podtopologyspread/advanced-usage-2.png" alt="Advanced-Usage-2">&lt;/p>
&lt;p>For the first constraint, there are 3 Pods in zone1 and 2 Pods in zone2, so the
incoming Pod can be only put to zone2 to satisfy the &amp;quot;maxSkew=1&amp;quot; constraint. In
other words, the result set is nodeX and nodeY.&lt;/p>
&lt;p>For the second constraint, there are too many Pods in nodeB and nodeX, so the
incoming Pod can be only put to nodeA and nodeY.&lt;/p>
&lt;p>Now we can conclude the only qualified Node is nodeY - from the intersection of
the sets {nodeX, nodeY} (from the first constraint) and {nodeA, nodeY} (from the
second constraint).&lt;/p>
&lt;p>Multiple TopologySpreadConstraints is powerful, but be sure to understand the
difference with the preceding &amp;quot;NodeSelector/NodeAffinity&amp;quot; example: one is to
calculate result set independently and then interjoined; while the other is to
calculate topologySpreadConstraints based on the filtering results of node
constraints.&lt;/p>
&lt;p>Instead of using &amp;quot;hard&amp;quot; constraints in all topologySpreadConstraints, you can
also combine using &amp;quot;hard&amp;quot; constraints and &amp;quot;soft&amp;quot; constraints to adhere to more
diverse cluster situations.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> If two TopologySpreadConstraints are being applied for the same {topologyKey,
whenUnsatisfiable} tuple, the Pod creation will be blocked returning a
validation error.&lt;/div>
&lt;/blockquote>
&lt;h2 id="podtopologyspread-defaults">PodTopologySpread defaults&lt;/h2>
&lt;p>PodTopologySpread is a Pod level API. As such, to use the feature, workload
authors need to be aware of the underlying topology of the cluster, and then
specify proper &lt;code>topologySpreadConstraints&lt;/code> in the Pod spec for every workload.
While the Pod-level API gives the most flexibility it is also possible to
specify cluster-level defaults.&lt;/p>
&lt;p>The default PodTopologySpread constraints allow you to specify spreading for all
the workloads in the cluster, tailored for its topology. The constraints can be
specified by an operator/admin as PodTopologySpread plugin arguments in the
&lt;a href="https://kubernetes.io/docs/reference/scheduling/profiles/">scheduling profile configuration
API&lt;/a> when starting
kube-scheduler.&lt;/p>
&lt;p>A sample configuration could look like this:&lt;/p>
&lt;pre>&lt;code>apiVersion: kubescheduler.config.k8s.io/v1alpha2
kind: KubeSchedulerConfiguration
profiles:
pluginConfig:
- name: PodTopologySpread
args:
defaultConstraints:
- maxSkew: 1
topologyKey: example.com/rack
whenUnsatisfiable: ScheduleAnyway
&lt;/code>&lt;/pre>&lt;p>When configuring default constraints, label selectors must be left empty.
kube-scheduler will deduce the label selectors from the membership of the Pod to
Services, ReplicationControllers, ReplicaSets or StatefulSets. Pods can
always override the default constraints by providing their own through the
PodSpec.&lt;/p>
&lt;blockquote class="note callout">
&lt;div>&lt;strong>Note:&lt;/strong> When using default PodTopologySpread constraints, it is recommended to disable
the old DefaultTopologySpread plugin.&lt;/div>
&lt;/blockquote>
&lt;h2 id="wrap-up">Wrap-up&lt;/h2>
&lt;p>PodTopologySpread allows you to define spreading constraints for your workloads
with a flexible and expressive Pod-level API. In the past, workload authors used
Pod AntiAffinity rules to force or hint the scheduler to run a single Pod per
topology domain. In contrast, the new PodTopologySpread constraints allow Pods
to specify skew levels that can be required (hard) or desired (soft). The
feature can be paired with Node selectors and Node affinity to limit the
spreading to specific domains. Pod spreading constraints can be defined for
different topologies such as hostnames, zones, regions, racks, etc.&lt;/p>
&lt;p>Lastly, cluster operators can define default constraints to be applied to all
Pods. This way, Pods don't need to be aware of the underlying topology of the
cluster.&lt;/p></description></item><item><title>Blog: Two-phased Canary Rollout with Open Source Gloo</title><link>https://kubernetes.io/blog/2020/04/two-phased-canary-rollout-with-gloo/</link><pubDate>Wed, 22 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/two-phased-canary-rollout-with-gloo/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Rick Ducott | &lt;a href="https://github.com/rickducott/">GitHub&lt;/a> | &lt;a href="https://twitter.com/ducott">Twitter&lt;/a>&lt;/p>
&lt;p>Every day, my colleagues and I are talking to platform owners, architects, and engineers who are using &lt;a href="https://github.com/solo-io/gloo">Gloo&lt;/a> as an API gateway
to expose their applications to end users. These applications may span legacy monoliths, microservices, managed cloud services, and Kubernetes
clusters. Fortunately, Gloo makes it easy to set up routes to manage, secure, and observe application traffic while
supporting a flexible deployment architecture to meet the varying production needs of our users.&lt;/p>
&lt;p>Beyond the initial set up, platform owners frequently ask us to help design the operational workflows within their organization:
How do we bring a new application online? How do we upgrade an application? How do we divide responsibilities across our
platform, ops, and development teams?&lt;/p>
&lt;p>In this post, we're going to use Gloo to design a two-phased canary rollout workflow for application upgrades:&lt;/p>
&lt;ul>
&lt;li>In the first phase, we'll do canary testing by shifting a small subset of traffic to the new version. This allows you to safely perform smoke and correctness tests.&lt;/li>
&lt;li>In the second phase, we'll progressively shift traffic to the new version, allowing us to monitor the new version under load, and eventually, decommission the old version.&lt;/li>
&lt;/ul>
&lt;p>To keep it simple, we're going to focus on designing the workflow using &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a>, and we're going to deploy the gateway and
application to Kubernetes. At the end, we'll talk about a few extensions and advanced topics that could be interesting to explore in a follow up.&lt;/p>
&lt;h2 id="initial-setup">Initial setup&lt;/h2>
&lt;p>To start, we need a Kubernetes cluster. This example doesn't take advantage of any cloud specific
features, and can be run against a local test cluster such as &lt;a href="https://kubernetes.io/docs/tasks/tools/install-minikube/">minikube&lt;/a>.
This post assumes a basic understanding of Kubernetes and how to interact with it using &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>We'll install the latest &lt;a href="https://github.com/solo-io/gloo">open source Gloo&lt;/a> to the &lt;code>gloo-system&lt;/code> namespace and deploy
version &lt;code>v1&lt;/code> of an example application to the &lt;code>echo&lt;/code> namespace. We'll expose this application outside the cluster
by creating a route in Gloo, to end up with a picture like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h3 id="deploying-gloo">Deploying Gloo&lt;/h3>
&lt;p>We'll install gloo with the &lt;code>glooctl&lt;/code> command line tool, which we can download and add to the &lt;code>PATH&lt;/code> with the following
commands:&lt;/p>
&lt;pre>&lt;code>curl -sL https://run.solo.io/gloo/install | sh
export PATH=$HOME/.gloo/bin:$PATH
&lt;/code>&lt;/pre>&lt;p>Now, you should be able to run &lt;code>glooctl version&lt;/code> to see that it is installed correctly:&lt;/p>
&lt;pre>&lt;code>➜ glooctl version
Client: {&amp;quot;version&amp;quot;:&amp;quot;1.3.15&amp;quot;}
Server: version undefined, could not find any version of gloo running
&lt;/code>&lt;/pre>&lt;p>Now we can install the gateway to our cluster with a simple command:&lt;/p>
&lt;pre>&lt;code>glooctl install gateway
&lt;/code>&lt;/pre>&lt;p>The console should indicate the install finishes successfully:&lt;/p>
&lt;pre>&lt;code>Creating namespace gloo-system... Done.
Starting Gloo installation...
Gloo was successfully installed!
&lt;/code>&lt;/pre>&lt;p>Before long, we can see all the Gloo pods running in the &lt;code>gloo-system&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get pod -n gloo-system
NAME READY STATUS RESTARTS AGE
discovery-58f8856bd7-4fftg 1/1 Running 0 13s
gateway-66f86bc8b4-n5crc 1/1 Running 0 13s
gateway-proxy-5ff99b8679-tbp65 1/1 Running 0 13s
gloo-66b8dc8868-z5c6r 1/1 Running 0 13s
&lt;/code>&lt;/pre>&lt;h3 id="deploying-the-application">Deploying the application&lt;/h3>
&lt;p>Our &lt;code>echo&lt;/code> application is a simple container (thanks to our friends at HashiCorp) that will
respond with the application version, to help demonstrate our canary workflows as we start testing and
shifting traffic to a &lt;code>v2&lt;/code> version of the application.&lt;/p>
&lt;p>Kubernetes gives us a lot of flexibility in terms of modeling this application. We'll adopt the following
conventions:&lt;/p>
&lt;ul>
&lt;li>We'll include the version in the deployment name so we can run two versions of the application
side-by-side and manage their lifecycle differently.&lt;/li>
&lt;li>We'll label pods with an app label (&lt;code>app: echo&lt;/code>) and a version label (&lt;code>version: v1&lt;/code>) to help with our canary rollout.&lt;/li>
&lt;li>We'll deploy a single Kubernetes &lt;code>Service&lt;/code> for the application to set up networking. Instead of updating
this or using multiple services to manage routing to different versions, we'll manage the rollout with Gloo configuration.&lt;/li>
&lt;/ul>
&lt;p>The following is our &lt;code>v1&lt;/code> echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Shout out to our friends at Hashi for this useful test server&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>And here is the &lt;code>echo&lt;/code> Kubernetes &lt;code>Service&lt;/code> object:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">targetPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>For convenience, we've published this yaml in a repo so we can deploy it with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/echo.yaml
&lt;/code>&lt;/pre>&lt;p>We should see the following output:&lt;/p>
&lt;pre>&lt;code>namespace/echo created
deployment.apps/echo-v1 created
service/echo created
&lt;/code>&lt;/pre>&lt;p>And we should be able to see all the resources healthy in the &lt;code>echo&lt;/code> namespace:&lt;/p>
&lt;pre>&lt;code>➜ kubectl get all -n echo
NAME READY STATUS RESTARTS AGE
pod/echo-v1-66dbfffb79-287s5 1/1 Running 0 6s
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
service/echo ClusterIP 10.55.252.216 &amp;lt;none&amp;gt; 80/TCP 6s
NAME READY UP-TO-DATE AVAILABLE AGE
deployment.apps/echo-v1 1/1 1 1 7s
NAME DESIRED CURRENT READY AGE
replicaset.apps/echo-v1-66dbfffb79 1 1 1 7s
&lt;/code>&lt;/pre>&lt;h3 id="exposing-outside-the-cluster-with-gloo">Exposing outside the cluster with Gloo&lt;/h3>
&lt;p>We can now expose this service outside the cluster with Gloo. First, we'll model the application as a Gloo
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#upstreams">Upstream&lt;/a>, which is Gloo's abstraction
for a traffic destination:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Upstream&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kube&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceNamespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subsetSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selectors&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">keys&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- version&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Here, we're setting up subsets based on the &lt;code>version&lt;/code> label. We don't have to use this in our routes, but later
we'll start to use it to support our canary workflow.&lt;/p>
&lt;p>We can now create a route to this upstream in Gloo by defining a
&lt;a href="https://docs.solo.io/gloo/latest/introduction/architecture/concepts/#virtual-services">Virtual Service&lt;/a>:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply these resources with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/upstream.yaml
kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/1-setup/vs.yaml
&lt;/code>&lt;/pre>&lt;p>Once we apply these two resources, we can start to send traffic to the application through Gloo:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>Our setup is complete, and our cluster now looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/setup.png" alt="Setup">&lt;/p>
&lt;h2 id="two-phased-rollout-strategy">Two-Phased Rollout Strategy&lt;/h2>
&lt;p>Now we have a new version &lt;code>v2&lt;/code> of the echo application that we wish to roll out. We know that when the
rollout is complete, we are going to end up with this picture:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>However, to get there, we may want to perform a few rounds of testing to ensure the new version of the application
meets certain correctness and/or performance acceptance criteria. In this post, we'll introduce a two-phased approach to
canary rollout with Gloo, that could be used to satisfy the vast majority of acceptance tests.&lt;/p>
&lt;p>In the first phase, we'll perform smoke and correctness tests by routing a small segment of the traffic to the new version
of the application. In this demo, we'll use a header &lt;code>stage: canary&lt;/code> to trigger routing to the new service, though in
practice it may be desirable to make this decision based on another part of the request, such as a claim in a verified JWT.&lt;/p>
&lt;p>In the second phase, we've already established correctness, so we are ready to shift all of the traffic over to the new
version of the application. We'll configure weighted destinations, and shift the traffic while monitoring certain business
metrics to ensure the service quality remains at acceptable levels. Once 100% of the traffic is shifted to the new version,
the old version can be decommissioned.&lt;/p>
&lt;p>In practice, it may be desirable to only use one of the phases for testing, in which case the other phase can be
skipped.&lt;/p>
&lt;h2 id="phase-1-initial-canary-rollout-of-v2">Phase 1: Initial canary rollout of v2&lt;/h2>
&lt;p>In this phase, we'll deploy &lt;code>v2&lt;/code>, and then use a header &lt;code>stage: canary&lt;/code> to start routing a small amount of specific
traffic to the new version. We'll use this header to perform some basic smoke testing and make sure &lt;code>v2&lt;/code> is working the
way we'd expect:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/subset-routing.png" alt="Subset Routing">&lt;/p>
&lt;h3 id="setting-up-subset-routing">Setting up subset routing&lt;/h3>
&lt;p>Before deploying our &lt;code>v2&lt;/code> service, we'll update our virtual service to only route to pods that have the subset label
&lt;code>version: v1&lt;/code>, using a Gloo feature called &lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/subsets/">subset routing&lt;/a>.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply them to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="deploying-echo-v2">Deploying echo v2&lt;/h3>
&lt;p>Now we can safely deploy &lt;code>v2&lt;/code> of the echo application:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Deployment&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>hashicorp/http-echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;-text=version:v2&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- -listen=:&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo-v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8080&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/echo-v2.yaml
&lt;/code>&lt;/pre>&lt;p>Since our gateway is configured to route specifically to the &lt;code>v1&lt;/code> subset, this should have no effect. However, it does enable
&lt;code>v2&lt;/code> to be routable from the gateway if the &lt;code>v2&lt;/code> subset is configured for a route.&lt;/p>
&lt;p>Make sure &lt;code>v2&lt;/code> is running before moving on:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ kubectl get pod -n &lt;span style="color:#a2f">echo&lt;/span>
NAME READY STATUS RESTARTS AGE
echo-v1-66dbfffb79-2qw86 1/1 Running &lt;span style="color:#666">0&lt;/span> 5m25s
echo-v2-86584fbbdb-slp44 1/1 Running &lt;span style="color:#666">0&lt;/span> 93s
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The application should continue to function as before:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;h3 id="adding-a-route-to-v2-for-canary-testing">Adding a route to v2 for canary testing&lt;/h3>
&lt;p>We'll route to the &lt;code>v2&lt;/code> subset when the &lt;code>stage: canary&lt;/code> header is supplied on the request. If the header isn't
provided, we'll continue to route to the &lt;code>v1&lt;/code> subset as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can deploy with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/2-initial-subset-routing-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;h3 id="canary-testing">Canary testing&lt;/h3>
&lt;p>Now that we have this route, we can do some testing. First let's ensure that the existing route is working as expected:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>And now we can start to canary test our new application version:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/ -H &amp;quot;stage: canary&amp;quot;
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="advanced-use-cases-for-subset-routing">Advanced use cases for subset routing&lt;/h3>
&lt;p>We may decide that this approach, using user-provided request headers, is too open. Instead, we may
want to restrict canary testing to a known, authorized user.&lt;/p>
&lt;p>A common implementation of this that we've seen is for the canary route to require a valid JWT that contains
a specific claim to indicate the subject is authorized for canary testing. Enterprise Gloo has out of the box
support for verifying JWTs, updating the request headers based on the JWT claims, and recomputing the
routing destination based on the updated headers. We'll save that for a future post covering more advanced use
cases in canary testing.&lt;/p>
&lt;h2 id="phase-2-shifting-all-traffic-to-v2-and-decommissioning-v1">Phase 2: Shifting all traffic to v2 and decommissioning v1&lt;/h2>
&lt;p>At this point, we've deployed &lt;code>v2&lt;/code>, and created a route for canary testing. If we are satisfied with the
results of the testing, we can move on to phase 2 and start shifting the load from &lt;code>v1&lt;/code> to &lt;code>v2&lt;/code>. We'll use
&lt;a href="https://docs.solo.io/gloo/latest/guides/traffic_management/destination_types/multi_destination/">weighted destinations&lt;/a>
in Gloo to manage the load during the migration.&lt;/p>
&lt;h3 id="setting-up-the-weighted-destinations">Setting up the weighted destinations&lt;/h3>
&lt;p>We can change the Gloo route to route to both of these destinations, with weights to decide how much of the traffic should
go to the &lt;code>v1&lt;/code> versus the &lt;code>v2&lt;/code> subset. To start, we're going to set it up so 100% of the traffic continues to get routed to the
&lt;code>v1&lt;/code> subset, unless the &lt;code>stage: canary&lt;/code> header was provided as before.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># We&amp;#39;ll keep our route from before if we want to continue testing with this header&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now we&amp;#39;ll route the rest of the traffic to the upstream, load balanced across the two subsets.&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this virtual service update to the cluster with the following commands:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-1.yaml
&lt;/code>&lt;/pre>&lt;p>Now the cluster looks like this, for any request that doesn't have the &lt;code>stage: canary&lt;/code> header:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/init-traffic-shift.png" alt="Initialize Traffic Shift">&lt;/p>
&lt;p>With the initial weights, we should see the gateway continue to serve &lt;code>v1&lt;/code> for all traffic.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-bash" data-lang="bash">➜ curl &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>glooctl proxy url&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>/
version:v1
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="commence-rollout">Commence rollout&lt;/h3>
&lt;p>To simulate a load test, let's shift half the traffic to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/load-test.png" alt="Load Test">&lt;/p>
&lt;p>This can be expressed on our virtual service by adjusting the weights:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Update the weight so 50% of the traffic hits v1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># And 50% is routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">50&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-2.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see half of the requests return &lt;code>version:v1&lt;/code> and the
other half return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v1
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v1
&lt;/code>&lt;/pre>&lt;p>In practice, during this process it's likely you'll be monitoring some performance and business metrics
to ensure the traffic shift isn't resulting in a decline in the overall quality of service. We can even
leverage operators like &lt;a href="https://github.com/weaveworks/flagger">Flagger&lt;/a> to help automate this Gloo
workflow. Gloo Enterprise integrates with your metrics backend and provides out of the box and dynamic,
upstream-based dashboards that can be used to monitor the health of the rollout.
We will save these topics for a future post on advanced canary testing use cases with Gloo.&lt;/p>
&lt;h3 id="finishing-the-rollout">Finishing the rollout&lt;/h3>
&lt;p>We will continue adjusting weights until eventually, all of the traffic is now being routed to &lt;code>v2&lt;/code>:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/final-shift.png" alt="Final Shift">&lt;/p>
&lt;p>Our virtual service will look like this:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">headers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>stage&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>canary&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">multi&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">destinations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># No traffic will be sent to v1 anymore&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">destination&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#080;font-style:italic"># Now all the traffic will be routed to v2&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">weight&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">100&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply that to the cluster with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/3-progressive-traffic-shift-to-v2/vs-3.yaml
&lt;/code>&lt;/pre>&lt;p>Now when we send traffic to the gateway, we should see all of the requests return &lt;code>version:v2&lt;/code>.&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;h3 id="decommissioning-v1">Decommissioning v1&lt;/h3>
&lt;p>At this point, we have deployed the new version of our application, conducted correctness tests using subset routing,
conducted load and performance tests by progressively shifting traffic to the new version, and finished
the rollout. The only remaining task is to clean up our &lt;code>v1&lt;/code> resources.&lt;/p>
&lt;p>First, we'll clean up our routes. We'll leave the subset specified on the route so we are all setup for future upgrades.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gateway.solo.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>VirtualService&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">virtualHost&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;*&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">matchers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">prefix&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">routeAction&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">single&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">upstream&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>echo&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>gloo-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">subset&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">values&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v2&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>We can apply this update with the following command:&lt;/p>
&lt;pre>&lt;code>kubectl apply -f https://raw.githubusercontent.com/solo-io/gloo-ref-arch/blog-30-mar-20/platform/prog-delivery/two-phased-with-os-gloo/4-decommissioning-v1/vs.yaml
&lt;/code>&lt;/pre>&lt;p>And we can delete the &lt;code>v1&lt;/code> deployment, which is no longer serving any traffic.&lt;/p>
&lt;pre>&lt;code>kubectl delete deploy -n echo echo-v1
&lt;/code>&lt;/pre>&lt;p>Now our cluster looks like this:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-04-22-two-phased-canary-rollout-with-gloo/end-state.png" alt="End State">&lt;/p>
&lt;p>And requests to the gateway return this:&lt;/p>
&lt;pre>&lt;code>➜ curl $(glooctl proxy url)/
version:v2
&lt;/code>&lt;/pre>&lt;p>We have now completed our two-phased canary rollout of an application update using Gloo!&lt;/p>
&lt;h2 id="other-advanced-topics">Other Advanced Topics&lt;/h2>
&lt;p>Over the course of this post, we collected a few topics that could be a good starting point for advanced exploration:&lt;/p>
&lt;ul>
&lt;li>Using the &lt;strong>JWT&lt;/strong> filter to verify JWTs, extract claims onto headers, and route to canary versions depending on a claim value.&lt;/li>
&lt;li>Looking at &lt;strong>Prometheus metrics&lt;/strong> and &lt;strong>Grafana dashboards&lt;/strong> created by Gloo to monitor the health of the rollout.&lt;/li>
&lt;li>Automating the rollout by integrating &lt;strong>Flagger&lt;/strong> with &lt;strong>Gloo&lt;/strong>.&lt;/li>
&lt;/ul>
&lt;p>A few other topics that warrant further exploration:&lt;/p>
&lt;ul>
&lt;li>Supporting &lt;strong>self-service&lt;/strong> upgrades by giving teams ownership over their upstream and route configuration&lt;/li>
&lt;li>Utilizing Gloo's &lt;strong>delegation&lt;/strong> feature and Kubernetes &lt;strong>RBAC&lt;/strong> to decentralize the configuration management safely&lt;/li>
&lt;li>Fully automating the continuous delivery process by applying &lt;strong>GitOps&lt;/strong> principles and using tools like &lt;strong>Flux&lt;/strong> to push config to the cluster&lt;/li>
&lt;li>Supporting &lt;strong>hybrid&lt;/strong> or &lt;strong>non-Kubernetes&lt;/strong> application use-cases by setting up Gloo with a different deployment pattern&lt;/li>
&lt;li>Utilizing &lt;strong>traffic shadowing&lt;/strong> to begin testing the new version with realistic data before shifting production traffic to it&lt;/li>
&lt;/ul>
&lt;h2 id="get-involved-in-the-gloo-community">Get Involved in the Gloo Community&lt;/h2>
&lt;p>Gloo has a large and growing community of open source users, in addition to an enterprise customer base. To learn more about
Gloo:&lt;/p>
&lt;ul>
&lt;li>Check out the &lt;a href="https://github.com/solo-io/gloo">repo&lt;/a>, where you can see the code and file issues&lt;/li>
&lt;li>Check out the &lt;a href="https://docs.solo.io/gloo/latest">docs&lt;/a>, which have an extensive collection of guides and examples&lt;/li>
&lt;li>Join the &lt;a href="http://slack.solo.io/">slack channel&lt;/a> and start chatting with the Solo engineering team and user community&lt;/li>
&lt;/ul>
&lt;p>If you'd like to get in touch with me (feedback is always appreciated!), you can find me on the
&lt;a href="http://slack.solo.io/">Solo slack&lt;/a> or email me at &lt;strong>&lt;a href="mailto:rick.ducott@solo.io">rick.ducott@solo.io&lt;/a>&lt;/strong>.&lt;/p></description></item><item><title>Blog: Cluster API v1alpha3 Delivers New Features and an Improved User Experience</title><link>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/cluster-api-v1alpha3-delivers-new-features-and-an-improved-user-experience/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Daniel Lipovetsky (D2IQ)&lt;/p>
&lt;img src="https://kubernetes.io/images/blog/2020-04-21-Cluster-API-v1alpha3-Delivers-New-Features-and-an-Improved-User-Experience/kubernetes-cluster-logos_final-02.svg" align="right" width="25%" alt="Cluster API Logo: Turtles All The Way Down">
&lt;p>The Cluster API is a Kubernetes project to bring declarative, Kubernetes-style APIs to cluster creation, configuration, and management. It provides optional, additive functionality on top of core Kubernetes to manage the lifecycle of a Kubernetes cluster.&lt;/p>
&lt;p>Following the v1alpha2 release in October 2019, many members of the Cluster API community met in San Francisco, California, to plan the next release. The project had just gone through a major transformation, delivering a new architecture that promised to make the project easier for users to adopt, and faster for the community to build. Over the course of those two days, we found our common goals: To implement the features critical to managing production clusters, to make its user experience more intuitive, and to make it a joy to develop.&lt;/p>
&lt;p>The v1alpha3 release of Cluster API brings significant features for anyone running Kubernetes in production and at scale. Among the highlights:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#declarative-control-plane-management">Declarative Control Plane Management&lt;/a>&lt;/li>
&lt;li>&lt;a href="#distributing-control-plane-nodes-to-reduce-risk">Support for Distributing Control Plane Nodes Across Failure Domains To Reduce Risk&lt;/a>&lt;/li>
&lt;li>&lt;a href="#automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/a>&lt;/li>
&lt;li>&lt;a href="#infrastructure-managed-node-groups">Support for Infrastructure-Managed Node Groups&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>For anyone who wants to understand the API, or prizes a simple, but powerful, command-line interface, the new release brings:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#clusterctl">Redesigned clusterctl, a command-line tool (and go library) for installing and managing the lifecycle of Cluster API&lt;/a>&lt;/li>
&lt;li>&lt;a href="#the-cluster-api-book">Extensive and up-to-date documentation in The Cluster API Book&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Finally, for anyone extending the Cluster API for their custom infrastructure or software needs:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="#end-to-end-test-framework">New End-to-End (e2e) Test Framework&lt;/a>&lt;/li>
&lt;li>&lt;a href="#provider-implementer-s-guide">Documentation for integrating Cluster API into your cluster lifecycle stack&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>All this was possible thanks to the hard work of many contributors.&lt;/p>
&lt;h2 id="declarative-control-plane-management">Declarative Control Plane Management&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/detiber/">Jason DeTiberus&lt;/a>, &lt;a href="https://github.com/randomvariable">Naadir Jeewa&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Kubeadm-based Control Plane (KCP) provides a declarative API to deploy and scale the Kubernetes control plane, including etcd. This is the feature many Cluster API users have been waiting for! Until now, to deploy and scale up the control plane, users had to create specially-crafted Machine resources. To scale down the control plane, they had to manually remove members from the etcd cluster. KCP automates deployment, scaling, and upgrades.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is the Kubernetes Control Plane?&lt;/strong>
The Kubernetes control plane is, at its core, kube-apiserver and etcd. If either of these are unavailable, no API requests can be handled. This impacts not only core Kubernetes APIs, but APIs implemented with CRDs. Other components, like kube-scheduler and kube-controller-manager, are also important, but do not have the same impact on availability.&lt;/p>
&lt;p>The control plane was important in the beginning because it scheduled workloads. However, some workloads could continue to run during a control plane outage. Today, workloads depend on operators, service meshes, and API gateways, which all use the control plane as a platform. Therefore, the control plane's availability is more important than ever.&lt;/p>
&lt;p>Managing the control plane is one of the most complex parts of cluster operation. Because the typical control plane includes etcd, it is stateful, and operations must be done in the correct sequence. Control plane replicas can and do fail, and maintaining control plane availability means being able to replace failed nodes.&lt;/p>
&lt;p>The control plane can suffer a complete outage (e.g. permanent loss of quorum in etcd), and recovery (along with regular backups) is sometimes the only feasible option.&lt;/p>
&lt;p>For more details, read about &lt;a href="https://kubernetes.io/docs/concepts/overview/components/">Kubernetes Components&lt;/a> in the Kubernetes documentation.&lt;/p>
&lt;/blockquote>
&lt;p>Here's an example of a 3-replica control plane for the Cluster API Docker Infrastructure, which the project maintains for testing and development. For brevity, other required resources, like Cluster, and Infrastructure Template, referenced by its name and namespace, are not shown.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>controlplane.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KubeadmControlPlane&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">infrastructureTemplate&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerMachineTemplate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>default&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeadmConfigSpec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">clusterConfiguration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">version&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1.16.3&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Deploy this control plane with kubectl:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f example-docker-control-plane.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Scale the control plane the same way you scale other Kubernetes resources:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl scale kubeadmcontrolplane example --replicas&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">5&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>kubeadmcontrolplane.controlplane.cluster.x-k8s.io/example scaled
&lt;/code>&lt;/pre>&lt;p>Upgrade the control plane to a newer patch of the Kubernetes release:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl patch kubeadmcontrolplane example --type&lt;span style="color:#666">=&lt;/span>json -p &lt;span style="color:#b44">&amp;#39;[{&amp;#34;op&amp;#34;: &amp;#34;replace&amp;#34;, &amp;#34;path&amp;#34;: &amp;#34;/spec/version&amp;#34;, &amp;#34;value&amp;#34;: &amp;#34;1.16.4&amp;#34;}]&amp;#39;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;blockquote>
&lt;p>&lt;strong>Number of Control Plane Replicas&lt;/strong>
By default, KCP is configured to manage etcd, and requires an odd number of replicas. If KCP is configured to not manage etcd, an odd number is recommended, but not required. An odd number of replicas ensures optimal etcd configuration. To learn why your etcd cluster should have an odd number of members, see the &lt;a href="https://etcd.io/docs/v3.4.0/faq/#why-an-odd-number-of-cluster-members">etcd FAQ&lt;/a>.&lt;/p>
&lt;/blockquote>
&lt;p>Because it is a core Cluster API component, KCP can be used with any v1alpha3-compatible Infrastructure Provider that provides a fixed control plane endpoint, i.e., a load balancer or virtual IP. This endpoint enables requests to reach multiple control plane replicas.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is an Infrastructure Provider?&lt;/strong>
A source of computational resources (e.g. machines, networking, etc.). The community maintains providers for AWS, Azure, Google Cloud, and VMWare. For details, see the &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">list of providers&lt;/a> in the Cluster API Book.&lt;/p>
&lt;/blockquote>
&lt;h2 id="distributing-control-plane-nodes-to-reduce-risk">Distributing Control Plane Nodes To Reduce Risk&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/vincepri/">Vince Prignano&lt;/a>, and &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>Cluster API users can now deploy nodes in different failure domains, reducing the risk of a cluster failing due to a domain outage. This is especially important for the control plane: If nodes in one domain fail, the cluster can continue to operate as long as the control plane is available to nodes in other domains.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>What is a Failure Domain?&lt;/strong>
A failure domain is a way to group the resources that would be made unavailable by some failure. For example, in many public clouds, an &amp;quot;availability zone&amp;quot; is the default failure domain. A zone corresponds to a data center. So, if a specific data center is brought down by a power outage or natural disaster, all resources in that zone become unavailable. If you run Kubernetes on your own hardware, your failure domain might be a rack, a network switch, or power distribution unit.&lt;/p>
&lt;/blockquote>
&lt;p>The Kubeadm-based ControlPlane distributes nodes across failure domains. To minimize the chance of losing multiple nodes in the event of a domain outage, it tries to distribute them evenly: it deploys a new node in the failure domain with the fewest existing nodes, and it removes an existing node in the failure domain with the most existing nodes.&lt;/p>
&lt;p>MachineDeployments and MachineSets do not distribute nodes across failure domains. To deploy your worker nodes across multiple failure domains, create a MachineDeployment or MachineSet for each failure domain.&lt;/p>
&lt;p>The Failure Domain API works on any infrastructure. That's because every Infrastructure Provider maps failure domains in its own way. The API is optional, so if your infrastructure is not complex enough to need failure domains, you do not need to support it. This example is for the Cluster API Docker Infrastructure Provider. Note that two of the domains are marked as suitable for control plane nodes, while a third is not. The Kubeadm-based ControlPlane will only deploy nodes to domains marked suitable.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>infrastructure.cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DockerCluster&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlaneEndpoint&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">172.17.0.4&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">6443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">failureDomains&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-one&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-two&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">domain-three&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controlPlane&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">false&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>The &lt;a href="https://github.com/kubernetes-sigs/cluster-api-provider-aws">AWS Infrastructure Provider&lt;/a> (CAPA), maintained by the Cluster API project, maps failure domains to AWS Availability Zones. Using CAPA, you can deploy a cluster across multiple Availability Zones. First, define subnets for multiple Availability Zones. The CAPA controller will define a failure domain for each Availability Zone. Deploy the control plane with the KubeadmControlPlane: it will distribute replicas across the failure domains. Finally, create a separate MachineDeployment for each failure domain.&lt;/p>
&lt;h2 id="automated-replacement-of-unhealthy-nodes">Automated Replacement of Unhealthy Nodes&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/enxebre">Alberto García Lamela&lt;/a>, and &lt;a href="http://github.com/joelspeed">Joel Speed&lt;/a>&lt;/em>&lt;/p>
&lt;p>There are many reasons why a node might be unhealthy. The kubelet process may stop. The container runtime might have a bug. The kernel might have a memory leak. The disk may run out of space. CPU, disk, or memory hardware may fail. A power outage may happen. Failures like these are especially common in larger clusters.&lt;/p>
&lt;p>Kubernetes is designed to tolerate them, and to help your applications tolerate them as well. Nevertheless, only a finite number of nodes can be unhealthy before the cluster runs out of resources, and Pods are evicted or not scheduled in the first place. Unhealthy nodes should be repaired or replaced at the earliest opportunity.&lt;/p>
&lt;p>The Cluster API now includes a MachineHealthCheck resource, and a controller that monitors node health. When it detects an unhealthy node, it removes it. (Another Cluster API controller detects the node has been removed and replaces it.) You can configure the controller to suit your needs. You can configure how long to wait before removing the node. You can also set a threshold for the number of unhealthy nodes. When the threshold is reached, no more nodes are removed. The wait can be used to tolerate short-lived outages, and the threshold to prevent too many nodes from being replaced at the same time.&lt;/p>
&lt;p>The controller will remove only nodes managed by a Cluster API MachineSet. The controller does not remove control plane nodes, whether managed by the Kubeadm-based Control Plane, or by the user, as in v1alpha2. For more, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html#limitations-and-caveats-of-a-machinehealthcheck">Limits and Caveats of a MachineHealthCheck&lt;/a>.&lt;/p>
&lt;p>Here is an example of a MachineHealthCheck. For more details, see &lt;a href="https://cluster-api.sigs.k8s.io/tasks/healthcheck.html">Configure a MachineHealthCheck&lt;/a> in the Cluster API book.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cluster.x-k8s.io/v1alpha3&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>MachineHealthCheck&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example-node-unhealthy-5m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">clusterName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>example&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">maxUnhealthy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">33&lt;/span>%&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeStartupTimeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>10m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodepool&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nodepool&lt;span style="color:#666">-0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">unhealthyConditions&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Unknown&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">status&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;False&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">timeout&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>300s&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h2 id="infrastructure-managed-node-groups">Infrastructure-Managed Node Groups&lt;/h2>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/juan-lee">Juan-Lee Pang&lt;/a> and &lt;a href="https://github.com/CecileRobertMichon">Cecile Robert-Michon&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you run large clusters, you need to create and destroy hundreds of nodes, sometimes in minutes. Although public clouds make it possible to work with large numbers of nodes, having to make a separate API request to create or delete every node may scale poorly. For example, API requests may have to be delayed to stay within rate limits.&lt;/p>
&lt;p>Some public clouds offer APIs to manage groups of nodes as one single entity. For example, AWS has AutoScaling Groups, Azure has Virtual Machine Scale Sets, and GCP has Managed Instance Groups. With this release of Cluster API, Infrastructure Providers can add support for these APIs, and users can deploy groups of Cluster API Machines by using the MachinePool Resource. For more information, see the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/blob/bf51a2502f9007b531f6a9a2c1a4eae1586fb8ca/docs/proposals/20190919-machinepool-api.md">proposal&lt;/a> in the Cluster API repository.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Experimental Feature&lt;/strong>
The MachinePool API is an experimental feature that is not enabled by default. Users are encouraged to try it and report on how well it meets their needs.&lt;/p>
&lt;/blockquote>
&lt;h2 id="the-cluster-api-user-experience-reimagined">The Cluster API User Experience, Reimagined&lt;/h2>
&lt;h3 id="clusterctl">clusterctl&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/fabriziopandini">Fabrizio Pandini&lt;/a>&lt;/em>&lt;/p>
&lt;p>If you are new to Cluster API, your first experience will probably be with the project's command-line tool, clusterctl. And with the new Cluster API release, it has been re-designed to be more pleasing to use than before. The tool is all you need to deploy your first &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html?highlight=pool#workload-cluster">workload cluster&lt;/a> in just a few steps.&lt;/p>
&lt;p>First, use &lt;code>clusterctl init&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/init.html">fetch the configuration&lt;/a> for your Infrastructure and Bootstrap Providers and deploy all of the components that make up the Cluster API. Second, use &lt;code>clusterctl config cluster&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/config-cluster.html">create the workload cluster manifest&lt;/a>. This manifest is just a collection of Kubernetes objects. To create the workload cluster, just &lt;code>kubectl apply&lt;/code> the manifest. Don't be surprised if this workflow looks familiar: Deploying a workload cluster with Cluster API is just like deploying an application workload with Kubernetes!&lt;/p>
&lt;p>Clusterctl also helps with the &amp;quot;day 2&amp;quot; operations. Use &lt;code>clusterctl move&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/move.html">migrate Cluster API custom resources&lt;/a>, such as Clusters, and Machines, from one &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#management-cluster">Management Cluster&lt;/a> to another. This step--also known as a &lt;a href="https://cluster-api.sigs.k8s.io/reference/glossary.html#pivot">pivot&lt;/a>--is necessary to create a workload cluster that manages itself with Cluster API. Finally, use &lt;code>clusterctl upgrade&lt;/code> to &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/commands/upgrade.html">upgrade all of the installed components&lt;/a> when a new Cluster API release becomes available.&lt;/p>
&lt;p>One more thing! Clusterctl is not only a command-line tool. It is also a Go library! Think of the library as an integration point for projects that build on top of Cluster API. All of clusterctl's command-line functionality is available in the library, making it easy to integrate into your stack. To get started with the library, please read its &lt;a href="https://pkg.go.dev/sigs.k8s.io/cluster-api@v0.3.1/cmd/clusterctl/client?tab=doc">documentation&lt;/a>.&lt;/p>
&lt;h3 id="the-cluster-api-book">The Cluster API Book&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The &lt;a href="https://cluster-api.sigs.k8s.io/">project's documentation&lt;/a> is extensive. New users should get some background on the &lt;a href="https://cluster-api.sigs.k8s.io/user/concepts.html">architecture&lt;/a>, and then create a cluster of their own with the &lt;a href="https://cluster-api.sigs.k8s.io/user/quick-start.html">Quick Start&lt;/a>. The clusterctl tool has its own &lt;a href="https://cluster-api.sigs.k8s.io/clusterctl/overview.html">reference&lt;/a>. The &lt;a href="https://cluster-api.sigs.k8s.io/developer/guide.html">Developer Guide&lt;/a> has plenty of information for anyone interested in contributing to the project.&lt;/p>
&lt;p>Above and beyond the content itself, the project's documentation site is a pleasure to use. It is searchable, has an outline, and even supports different color themes. If you think the site a lot like the documentation for a different community project, &lt;a href="https://book.kubebuilder.io/">Kubebuilder&lt;/a>, that is no coincidence! Many thanks to Kubebuilder authors for creating a great example of documentation. And many thanks to the &lt;a href="https://github.com/rust-lang/mdBook">mdBook&lt;/a> authors for creating a great tool for building documentation.&lt;/p>
&lt;h2 id="integrate-customize">Integrate &amp;amp; Customize&lt;/h2>
&lt;h3 id="end-to-end-test-framework">End-to-End Test Framework&lt;/h3>
&lt;p>&lt;em>Special thanks to &lt;a href="https://github.com/chuckha">Chuck Ha&lt;/a>&lt;/em>&lt;/p>
&lt;p>The Cluster API project is designed to be extensible. For example, anyone can develop their own Infrastructure and Bootstrap Providers. However, it's important that Providers work in a uniform way. And, because the project is still evolving, it takes work to make sure that Providers are up-to-date with new releases of the core.&lt;/p>
&lt;p>The End-to-End Test Framework provides a set of standard tests for developers to verify that their Providers integrate correctly with the current release of Cluster API, and help identify any regressions that happen after a new release of the Cluster API, or the Provider.&lt;/p>
&lt;p>For more details on the Framework, see &lt;a href="https://cluster-api.sigs.k8s.io/developer/testing.html?highlight=e2e#running-the-end-to-end-tests">Testing&lt;/a> in the Cluster API Book, and the &lt;a href="https://github.com/kubernetes-sigs/cluster-api/tree/master/test/framework">README&lt;/a> in the repository.&lt;/p>
&lt;h3 id="provider-implementer-s-guide">Provider Implementer's Guide&lt;/h3>
&lt;p>&lt;em>Thanks to many contributors!&lt;/em>&lt;/p>
&lt;p>The community maintains &lt;a href="https://cluster-api.sigs.k8s.io/reference/providers.html">Infrastructure Providers&lt;/a> for a many popular infrastructures. However, if you want to build your own Infrastructure or Bootstrap Provider, the &lt;a href="https://cluster-api.sigs.k8s.io/developer/providers/implementers-guide/overview.html">Provider Implementer's&lt;/a> guide explains the entire process, from creating a git repository, to creating CustomResourceDefinitions for your Providers, to designing, implementing, and testing the controllers.&lt;/p>
&lt;blockquote>
&lt;p>&lt;strong>Under Active Development&lt;/strong>
The Provider Implementer's Guide is actively under development, and may not yet reflect all of the changes in the v1alpha3 release.&lt;/p>
&lt;/blockquote>
&lt;h2 id="join-us">Join Us!&lt;/h2>
&lt;p>The Cluster API project is a very active project, and covers many areas of interest. If you are an infrastructure expert, you can contribute to one of the Infrastructure Providers. If you like building controllers, you will find opportunities to innovate. If you're curious about testing distributed systems, you can help develop the project's end-to-end test framework. Whatever your interests and background, you can make a real impact on the project.&lt;/p>
&lt;p>Come introduce yourself to the community at our weekly meeting, where we dedicate a block of time for a Q&amp;amp;A session. You can also find maintainers and users on the Kubernetes Slack, and in the Kubernetes forum. Please check out the links below. We look forward to seeing you!&lt;/p>
&lt;ul>
&lt;li>Chat with us on the Kubernetes &lt;a href="http://slack.k8s.io/">Slack&lt;/a>:&lt;a href="https://kubernetes.slack.com/archives/C8TSNPY4T"> #cluster-api&lt;/a>&lt;/li>
&lt;li>Join the &lt;a href="https://groups.google.com/forum/">sig-cluster-lifecycle&lt;/a> Google Group to receive calendar invites and gain access to documents&lt;/li>
&lt;li>Join our &lt;a href="https://zoom.us/j/861487554">Zoom meeting&lt;/a>, every Wednesday at 10:00 Pacific Time&lt;/li>
&lt;li>Post to the &lt;a href="https://discuss.kubernetes.io/c/contributors/cluster-api">Cluster API community forum&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: How Kubernetes contributors are building a better communication process</title><link>https://kubernetes.io/blog/2020/04/21/contributor-communication/</link><pubDate>Tue, 21 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/21/contributor-communication/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Paris Pittman&lt;/p>
&lt;blockquote>
&lt;p>&amp;quot;Perhaps we just need to use a different word. We may need to use community development or project advocacy as a word in the open source realm as opposed to marketing, and perhaps then people will realize that they need to do it.&amp;quot;
~ &lt;a href="https://todogroup.org/www.linkedin.com/in/nithyaruff/">&lt;em>Nithya Ruff&lt;/em>&lt;/a> (from &lt;a href="https://todogroup.org/guides/marketing-open-source-projects/">&lt;em>TODO Group&lt;/em>&lt;/a>)&lt;/p>
&lt;/blockquote>
&lt;p>A common way to participate in the Kubernetes contributor community is
to be everywhere.&lt;/p>
&lt;p>We have an active &lt;a href="https://slack.k8s.io">Slack&lt;/a>, many mailing lists, Twitter account(s), and
dozens of community-driven podcasts and newsletters that highlight all
end-user, contributor, and ecosystem topics. And to add on to that, we also have &lt;a href="http://github.com/kubernetes/community">repositories of amazing documentation&lt;/a>, tons of &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">meetings&lt;/a> that drive the project forward, and &lt;a href="https://www.youtube.com/watch?v=yqB_le-N6EE">recorded code deep dives&lt;/a>. All of this information is incredibly valuable,
but it can be too much.&lt;/p>
&lt;p>Keeping up with thousands of contributors can be a challenge for anyone,
but this task of consuming information straight from the firehose is
particularly challenging for new community members. It's no secret that
the project is vast for contributors and users alike.&lt;/p>
&lt;p>To paint a picture with numbers:&lt;/p>
&lt;ul>
&lt;li>43,000 contributors&lt;/li>
&lt;li>6,579 members in #kubernetes-dev slack channel&lt;/li>
&lt;li>52 mailing lists (kubernetes-dev@ has thousands of members; sig-networking@ has 1000 alone)&lt;/li>
&lt;li>40 &lt;a href="https://github.com/kubernetes/community/blob/master/governance.md#community-groups">community groups&lt;/a>&lt;/li>
&lt;li>30 &lt;a href="https://calendar.google.com/calendar/embed?src=cgnt364vd8s86hr2phapfjc6uk%40group.calendar.google.com&amp;amp;ctz=America%2FLos_Angeles">upstream meetings&lt;/a> &lt;em>this&lt;/em> week alone&lt;/li>
&lt;/ul>
&lt;p>All of these numbers are only growing in scale, and with that comes the need to simplify how contributors get the information right information front-and-center.&lt;/p>
&lt;h2 id="how-we-got-here">How we got here&lt;/h2>
&lt;p>Kubernetes (K8s for short) communication grew out of a need for people
to connect in our growing community. With the best of intentions, the
community spun up channels for people to connect. This energy was part
of what helped Kubernetes grow so fast, and it also had us in sprawling out far and wide. As adoption grew, &lt;a href="https://github.com/kubernetes/community/issues/2466">contributors knew there was a need for standardization&lt;/a>.&lt;/p>
&lt;p>This new attention to how the community communicates led to the discovery
of a complex web of options. There were so many options, and it was a
challenge for anyone to be sure they were in the right place to receive
the right information. We started taking immediate action combining communication streams and thinking about how to reach out best to serve our community. We also asked for feedback from all our
contributors directly via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-contributor-experience/surveys">&lt;strong>annual surveys&lt;/strong>&lt;/a>
to see where folks were actually reading the news that influences their
experiences here in our community.&lt;/p>
&lt;p>&lt;img src="https://user-images.githubusercontent.com/1744971/79478603-3a3a1980-7fd1-11ea-8b7a-d36aac7a097b.png" alt="Kubernetes channel access">&lt;/p>
&lt;p>With over 43,000 contributors, our contributor base is larger than many enterprise companies. You can imagine what it's like getting important messages across to make sure they are landing and folks are taking action.&lt;/p>
&lt;h2 id="contributing-to-better-communication">Contributing to better communication&lt;/h2>
&lt;p>Think about how your company/employer solves for this kind of communication challenge. Many have done so
by building internal marketing and communication focus areas in
marketing departments. So that's what we are doing. This has also been
applied &lt;a href="https://fedoraproject.org/wiki/Marketing">at Fedora&lt;/a> and at a smaller
scale in our very &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">own release&lt;/a> and &lt;a href="https://github.com/kubernetes/community/blob/d0fd6c16f7ee754b08082cc15658eb8db7afeaf8/events/events-team/marketing/README.md">contributor summit&lt;/a> planning
teams as roles.&lt;/p>
&lt;p>We have hit the accelerator on an &lt;strong>upstream marketing group&lt;/strong> under SIG
Contributor Experience and we want to tackle this challenge straight on.
We've learned in other contributor areas that creating roles for
contributors is super helpful - onboarding, breaking down work, and
ownership. &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team">Here's our team charting the course&lt;/a>.&lt;/p>
&lt;p>Journey your way through our other documents like our &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md">charter&lt;/a> if you are
interested in our mission and scope.&lt;/p>
&lt;p>Many of you close to the ecosystem might be scratching your head - isn't
this what CNCF does?&lt;/p>
&lt;p>Yes and no. The CNCF has 40+ other projects that need to be marketed to
a countless number of different types of community members in distinct
ways and they aren't responsible for the day to day operations of their
projects. They absolutely do partner with us to highlight what we need
and when we need it, and they do a fantastic job of it (one example is
the &lt;a href="https://twitter.com/kubernetesio">&lt;em>@kubernetesio Twitter account&lt;/em>&lt;/a> and its 200,000
followers).&lt;/p>
&lt;p>Where this group differs is in its scope: we are entirely
focused on elevating the hard work being done throughout the Kubernetes
community by its contributors.&lt;/p>
&lt;h2 id="what-to-expect-from-us">What to expect from us&lt;/h2>
&lt;p>You can expect to see us on the Kubernetes &lt;a href="https://github.com/kubernetes/community/tree/master/communication">communication channels&lt;/a> supporting you by:&lt;/p>
&lt;ul>
&lt;li>Finding ways of adding our human touch to potentially overwhelming
quantities of info by storytelling and other methods - we want to
highlight the work you are doing and provide useful information!&lt;/li>
&lt;li>Keeping you in the know of the comings and goings of contributor
community events, activities, mentoring initiatives, KEPs, and more.&lt;/li>
&lt;li>Creating a presence on Twitter specifically for contributors via
@k8scontributors that is all about being a contributor in all its
forms.&lt;/li>
&lt;/ul>
&lt;p>What does this look like in the wild? Our &lt;a href="https://kubernetes.io/blog/2020/03/19/join-sig-scalability/">first post&lt;/a> in a series about our 36 community groups landed recently. Did you see it?
More articles like this and additional themes of stories to flow through
&lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#purpose">our storytellers&lt;/a>.&lt;/p>
&lt;p>We will deliver this with an &lt;a href="https://github.com/kubernetes/community/blob/master/communication/marketing-team/CHARTER.md#ethosvision">ethos&lt;/a> behind us aligned to the Kubernetes project as a whole, and we're
committed to using the same tools as all the other SIGs to do so. Check out our &lt;a href="https://github.com/orgs/kubernetes/projects/39">project board&lt;/a> to view our roadmap of upcoming work.&lt;/p>
&lt;h2 id="join-us-and-be-part-of-the-story">Join us and be part of the story&lt;/h2>
&lt;p>This initiative is in an early phase and we still have important roles to fill to make it successful.&lt;/p>
&lt;p>If you are interested in open sourcing marketing functions – it's a fun
ride – join us! Specific immediate roles include storytelling through
blogs and as a designer. We also have plenty of work in progress on our project board.
Add a comment to any open issue to let us know you're interested in getting involved.&lt;/p>
&lt;p>Also, if you're reading this, you're exactly the type of person we are
here to support. We would love to hear about how to improve, feedback,
or how we can work together.&lt;/p>
&lt;p>Reach out at one of the contact methods listed on &lt;a href="https://github.com/kubernetes/community/tree/master/communication/marketing-team#contact-us">our README&lt;/a>. We would love to hear from you.&lt;/p></description></item><item><title>Blog: API Priority and Fairness Alpha</title><link>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</link><pubDate>Mon, 06 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/06/kubernetes-1-18-feature-api-priority-and-fairness-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Min Kim (Ant Financial), Mike Spreitzer (IBM), Daniel Smith (Google)&lt;/p>
&lt;p>This blog describes “API Priority And Fairness”, a new alpha feature in Kubernetes 1.18. API Priority And Fairness permits cluster administrators to divide the concurrency of the control plane into different weighted priority levels. Every request arriving at a kube-apiserver will be categorized into one of the priority levels and get its fair share of the control plane’s throughput.&lt;/p>
&lt;h2 id="what-problem-does-this-solve">What problem does this solve?&lt;/h2>
&lt;p>Today the apiserver has a simple mechanism for protecting itself against CPU and memory overloads: max-in-flight limits for mutating and for readonly requests. Apart from the distinction between mutating and readonly, no other distinctions are made among requests; consequently, there can be undesirable scenarios where one subset of the requests crowds out other requests.&lt;/p>
&lt;p>In short, it is far too easy for Kubernetes workloads to accidentally DoS the apiservers, causing other important traffic--like system controllers or leader elections---to fail intermittently. In the worst cases, a few broken nodes or controllers can push a busy cluster over the edge, turning a local problem into a control plane outage.&lt;/p>
&lt;h2 id="how-do-we-solve-the-problem">How do we solve the problem?&lt;/h2>
&lt;p>The new feature “API Priority and Fairness” is about generalizing the existing max-in-flight request handler in each apiserver, to make the behavior more intelligent and configurable. The overall approach is as follows.&lt;/p>
&lt;ol>
&lt;li>Each request is matched by a &lt;em>Flow Schema&lt;/em>. The Flow Schema states the Priority Level for requests that match it, and assigns a “flow identifier” to these requests. Flow identifiers are how the system determines whether requests are from the same source or not.&lt;/li>
&lt;li>Priority Levels may be configured to behave in several ways. Each Priority Level gets its own isolated concurrency pool. Priority levels also introduce the concept of queuing requests that cannot be serviced immediately.&lt;/li>
&lt;li>To prevent any one user or namespace from monopolizing a Priority Level, they may be configured to have multiple queues. &lt;a href="https://aws.amazon.com/builders-library/workload-isolation-using-shuffle-sharding/#What_is_shuffle_sharding.3F">“Shuffle Sharding”&lt;/a> is used to assign each flow of requests to a subset of the queues.&lt;/li>
&lt;li>Finally, when there is capacity to service a request, a &lt;a href="https://en.wikipedia.org/wiki/Fair_queuing">“Fair Queuing”&lt;/a> algorithm is used to select the next request. Within each priority level the queues compete with even fairness.&lt;/li>
&lt;/ol>
&lt;p>Early results have been very promising! Take a look at this &lt;a href="https://github.com/kubernetes/kubernetes/pull/88177#issuecomment-588945806">analysis&lt;/a>.&lt;/p>
&lt;h2 id="how-do-i-try-this-out">How do I try this out?&lt;/h2>
&lt;p>You are required to prepare the following things in order to try out the feature:&lt;/p>
&lt;ul>
&lt;li>Download and install a kubectl greater than v1.18.0 version&lt;/li>
&lt;li>Enabling the new API groups with the command line flag &lt;code>--runtime-config=&amp;quot;flowcontrol.apiserver.k8s.io/v1alpha1=true&amp;quot;&lt;/code> on the kube-apiservers&lt;/li>
&lt;li>Switch on the feature gate with the command line flag &lt;code>--feature-gates=APIPriorityAndFairness=true&lt;/code> on the kube-apiservers&lt;/li>
&lt;/ul>
&lt;p>After successfully starting your kube-apiservers, you will see a few default FlowSchema and PriorityLevelConfiguration resources in the cluster. These default configurations are designed for a general protection and traffic management for your cluster.
You can examine and customize the default configuration by running the usual tools, e.g.:&lt;/p>
&lt;ul>
&lt;li>&lt;code>kubectl get flowschemas&lt;/code>&lt;/li>
&lt;li>&lt;code>kubectl get prioritylevelconfigurations&lt;/code>&lt;/li>
&lt;/ul>
&lt;h2 id="how-does-this-work-under-the-hood">How does this work under the hood?&lt;/h2>
&lt;p>Upon arrival at the handler, a request is assigned to exactly one priority level and exactly one flow within that priority level. Hence understanding how FlowSchema and PriorityLevelConfiguration works will be helping you manage the request traffic going through your kube-apiservers.&lt;/p>
&lt;ul>
&lt;li>
&lt;p>FlowSchema: FlowSchema will identify a PriorityLevelConfiguration object and the way to compute the request’s “flow identifier”. Currently we support matching requests according to: the identity making the request, the verb, and the target object. The identity can match in terms of: a username, a user group name, or a ServiceAccount. And as for the target objects, we can match by apiGroup, resource[/subresource], and namespace.&lt;/p>
&lt;ul>
&lt;li>The flow identifier is used for shuffle sharding, so it’s important that requests have the same flow identifier if they are from the same source! We like to consider scenarios with “elephants” (which send many/heavy requests) vs “mice” (which send few/light requests): it is important to make sure the elephant’s requests all get the same flow identifier, otherwise they will look like many different mice to the system!&lt;/li>
&lt;li>See the API Documentation &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#flowschema-v1alpha1-flowcontrol-apiserver-k8s-io">here&lt;/a>!&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>
&lt;p>PriorityLevelConfiguration: Defines a priority level.&lt;/p>
&lt;ul>
&lt;li>For apiserver self requests, and any reentrant traffic (e.g., admission webhooks which themselves make API requests), a Priority Level can be marked “exempt”, which means that no queueing or limiting of any sort is done. This is to prevent priority inversions.&lt;/li>
&lt;li>Each non-exempt Priority Level is configured with a number of &amp;quot;concurrency shares&amp;quot; and gets an isolated pool of concurrency to use. Requests of that Priority Level run in that pool when it is not full, never anywhere else. Each apiserver is configured with a total concurrency limit (taken to be the sum of the old limits on mutating and readonly requests), and this is then divided among the Priority Levels in proportion to their concurrency shares.&lt;/li>
&lt;li>A non-exempt Priority Level may select a number of queues and a &amp;quot;hand size&amp;quot; to use for the shuffle sharding. Shuffle sharding maps flows to queues in a way that is better than consistent hashing. A given flow has access to a small collection of queues, and for each incoming request the shortest queue is chosen. When a Priority Level has queues, it also sets a limit on queue length. There is also a limit placed on how long a request can wait in its queue; this is a fixed fraction of the apiserver's request timeout. A request that cannot be executed and cannot be queued (any longer) is rejected.&lt;/li>
&lt;li>Alternatively, a non-exempt Priority Level may select immediate rejection instead of waiting in a queue.&lt;/li>
&lt;li>See the &lt;a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.19/#prioritylevelconfiguration-v1alpha1-flowcontrol-apiserver-k8s-io">API documentation&lt;/a> for this feature.&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;h2 id="what-s-missing-when-will-there-be-a-beta">What’s missing? When will there be a beta?&lt;/h2>
&lt;p>We’re already planning a few enhancements based on alpha and there will be more as users send feedback to our community. Here’s a list of them:&lt;/p>
&lt;ul>
&lt;li>Traffic management for WATCH and EXEC requests&lt;/li>
&lt;li>Adjusting and improving the default set of FlowSchema/PriorityLevelConfiguration&lt;/li>
&lt;li>Enhancing observability on how this feature works&lt;/li>
&lt;li>Join the discussion &lt;a href="https://github.com/kubernetes/enhancements/pull/1632">here&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Possibly treat LIST requests differently depending on an estimate of how big their result will be.&lt;/p>
&lt;h2 id="how-can-i-get-involved">How can I get involved?&lt;/h2>
&lt;p>As always! Reach us on slack &lt;a href="https://kubernetes.slack.com/messages/sig-api-machinery">#sig-api-machinery&lt;/a>, or through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-sig-api-machinery">mailing list&lt;/a>. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>Many thanks to the contributors that have gotten this feature this far: Aaron Prindle, Daniel Smith, Jonathan Tomer, Mike Spreitzer, Min Kim, Bruce Ma, Yu Liao, Mengyi Zhou!&lt;/p></description></item><item><title>Blog: Introducing Windows CSI support alpha for Kubernetes</title><link>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</link><pubDate>Fri, 03 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/03/kubernetes-1-18-feature-windows-csi-support-alpha/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Authors: Deep Debroy [Docker], Jing Xu [Google], Krishnakumar R (KK) [Microsoft]&lt;/p>
&lt;p>&lt;em>The alpha version of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSI Proxy&lt;/a> for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/em>&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>Container Storage Interface (CSI) for Kubernetes went GA in the Kubernetes 1.13 release. CSI has become the standard for exposing block and file storage to containerized workloads on Container Orchestration systems (COs) like Kubernetes. It enables third-party storage providers to write and deploy plugins without the need to alter the core Kubernetes codebase. All new storage features will utilize CSI, therefore it is important to get CSI drivers to work on Windows.&lt;/p>
&lt;p>A CSI driver in Kubernetes has two main components: a controller plugin and a node plugin. The controller plugin generally does not need direct access to the host and can perform all its operations through the Kubernetes API and external control plane services (e.g. cloud storage service). The node plugin, however, requires direct access to the host for making block devices and/or file systems available to the Kubernetes kubelet. This was previously not possible for containers on Windows. With the release of &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a>, CSI drivers can now perform storage operations on the node. This inturn enables containerized CSI Drivers to run on Windows.&lt;/p>
&lt;h2 id="csi-support-for-windows-clusters">CSI support for Windows clusters&lt;/h2>
&lt;p>CSI drivers (e.g. AzureDisk, GCE PD, etc.) are recommended to be deployed as containers. CSI driver’s node plugin typically runs on every worker node in the cluster (as a DaemonSet). Node plugin containers need to run with elevated privileges to perform storage related operations. However, Windows currently does not support privileged containers. To solve this problem, &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> makes it so that node plugins can now be deployed as unprivileged pods and then use the proxy to perform privileged storage operations on the node.&lt;/p>
&lt;h2 id="node-plugin-interactions-with-csiproxy">Node plugin interactions with CSIProxy&lt;/h2>
&lt;p>The design of the CSI proxy is captured in this &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-windows/20190714-windows-csi-support.md">KEP&lt;/a>. The following diagram depicts the interactions with the CSI node plugin and CSI proxy.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyOverview.png">
&lt;/p>
&lt;p>The CSI proxy runs as a process directly on the host on every windows node - very similar to kubelet. The CSI code in kubelet interacts with the &lt;a href="https://kubernetes-csi.github.io/docs/node-driver-registrar.html">node driver registrar&lt;/a> component and the CSI node plugin. The node driver registrar is a community maintained CSI project which handles the registration of vendor specific node plugins. The kubelet initiates CSI gRPC calls like NodeStageVolume/NodePublishVolume on the node plugin as described in the figure. Node plugins interface with the CSIProxy process to perform local host OS storage related operations such as creation/enumeration of volumes, mounting/unmounting, etc.&lt;/p>
&lt;h2 id="csi-proxy-architecture-and-implementation">CSI proxy architecture and implementation&lt;/h2>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyArchitecture.png">
&lt;/p>
&lt;p>In the alpha release, CSIProxy supports the following API groups:&lt;/p>
&lt;ol>
&lt;li>Filesystem&lt;/li>
&lt;li>Disk&lt;/li>
&lt;li>Volume&lt;/li>
&lt;li>SMB&lt;/li>
&lt;/ol>
&lt;p>CSI proxy exposes each API group via a Windows named pipe. The communication is performed using gRPC over these pipes. The client library from the CSI proxy project uses these pipes to interact with the CSI proxy APIs. For example, the filesystem APIs are exposed via a pipe like &lt;code>\.\pipe\csi-proxy-filesystem-v1alpha1&lt;/code> and volume APIs under the &lt;code>\.\pipe\csi-proxy-volume-v1alpha1&lt;/code>, and so on.&lt;/p>
&lt;p>From each API group service, the calls are routed to the host API layer. The host API calls into the host Windows OS by either Powershell or Go standard library calls. For example, when the filesystem API &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.proto">Rmdir&lt;/a> is called the API group service would decode the grpc structure &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/client/api/filesystem/v1alpha1/api.pb.go">RmdirRequest&lt;/a> and find the directory to be removed and call into the Host APIs layer. This would result in a call to &lt;a href="https://github.com/kubernetes-csi/csi-proxy/blob/master/internal/os/filesystem/api.go">os.Remove&lt;/a>, a Go standard library call, to perform the remove operation.&lt;/p>
&lt;h2 id="control-flow-details">Control flow details&lt;/h2>
&lt;p>The following figure uses CSI call NodeStageVolume as an example to explain the interaction between kubelet, CSI plugin, and CSI proxy for provisioning a fresh volume. After the node plugin receives a CSI RPC call, it makes a few calls to CSIproxy accordingly. As a result of the NodeStageVolume call, first the required disk is identified using either of the Disk API calls: ListDiskLocations (in AzureDisk driver) or GetDiskNumberByName (in GCE PD driver). If the disk is not partitioned, then the PartitionDisk (Disk API group) is called. Subsequently, Volume API calls such as ListVolumesOnDisk, FormatVolume and MountVolume are called to perform the rest of the required operations. Similar operations are performed in case of NodeUnstageVolume, NodePublishVolume, NodeUnpublishedVolume, etc.&lt;/p>
&lt;p align="center">
&lt;img src="https://kubernetes.io/images/blog/2020-04-03-Introducing-Windows-CSI-support-alpha-for-Kubernetes/CSIProxyControlFlow.png">
&lt;/p>
&lt;h2 id="current-support">Current support&lt;/h2>
&lt;p>CSI proxy is now available as alpha. You can find more details on the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository. There are currently two cloud providers that provide alpha support for CSI drivers on Windows: Azure and GCE.&lt;/p>
&lt;h2 id="future-plans">Future plans&lt;/h2>
&lt;p>One key area of focus in beta is going to be Windows based build and CI/CD setup to improve the stability and quality of the code base. Another area is using Go based calls directly instead of Powershell commandlets to improve performance. Enhancing debuggability and adding more tests are other areas which the team will be looking into.&lt;/p>
&lt;h2 id="how-to-get-involved">How to get involved?&lt;/h2>
&lt;p>This project, like all of Kubernetes, is the result of hard work by many contributors from diverse backgrounds working together. Those interested in getting involved with the design and development of CSI Proxy, or any part of the Kubernetes Storage system, may join the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage">Kubernetes Storage Special Interest Group&lt;/a> (SIG). We’re rapidly growing and always welcome new contributors.&lt;/p>
&lt;p>For those interested in more details, the &lt;a href="https://github.com/kubernetes-csi/csi-proxy">CSIProxy&lt;/a> GitHub repository is a good place to start. In addition, the &lt;a href="https://kubernetes.slack.com/messages/csi-windows">#csi-windows&lt;/a> channel on kubernetes slack is available for discussions specific to the CSI on Windows.&lt;/p>
&lt;h2 id="acknowledgments">Acknowledgments&lt;/h2>
&lt;p>We would like to thank Michelle Au for guiding us throughout this journey to alpha. We would like to thank Jean Rougé for contributions during the initial CSI proxy effort. We would like to thank Saad Ali for all the guidance with respect to the project and review/feedback on a draft of this blog. We would like to thank Patrick Lang and Mark Rossetti for helping us with Windows specific questions and details. Special thanks to Andy Zhang for reviews and guidance with respect to Azuredisk and Azurefile work. A big thank you to Paul Burt and Karen Chu for the review and suggestions on improving this blog post.&lt;/p>
&lt;p>Last but not the least, we would like to thank the broader Kubernetes community who contributed at every step of the project.&lt;/p></description></item><item><title>Blog: Improvements to the Ingress API in Kubernetes 1.18</title><link>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</link><pubDate>Thu, 02 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/02/improvements-to-the-ingress-api-in-kubernetes-1.18/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Rob Scott (Google), Christopher M Luciano (IBM)&lt;/p>
&lt;p>The Ingress API in Kubernetes has enabled a large number of controllers to provide simple and powerful ways to manage inbound network traffic to Kubernetes workloads. In Kubernetes 1.18, we've made 3 significant additions to this API:&lt;/p>
&lt;ul>
&lt;li>A new &lt;code>pathType&lt;/code> field that can specify how Ingress paths should be matched.&lt;/li>
&lt;li>A new &lt;code>IngressClass&lt;/code> resource that can specify how Ingresses should be implemented by controllers.&lt;/li>
&lt;li>Support for wildcards in hostnames.&lt;/li>
&lt;/ul>
&lt;h2 id="better-path-matching-with-path-types">Better Path Matching With Path Types&lt;/h2>
&lt;p>The new concept of a path type allows you to specify how a path should be matched. There are three supported types:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>ImplementationSpecific (default):&lt;/strong> With this path type, matching is up to the controller implementing the &lt;code>IngressClass&lt;/code>. Implementations can treat this as a separate &lt;code>pathType&lt;/code> or treat it identically to the &lt;code>Prefix&lt;/code> or &lt;code>Exact&lt;/code> path types.&lt;/li>
&lt;li>&lt;strong>Exact:&lt;/strong> Matches the URL path exactly and with case sensitivity.&lt;/li>
&lt;li>&lt;strong>Prefix:&lt;/strong> Matches based on a URL path prefix split by &lt;code>/&lt;/code>. Matching is case sensitive and done on a path element by element basis.&lt;/li>
&lt;/ul>
&lt;h2 id="extended-configuration-with-ingress-classes">Extended Configuration With Ingress Classes&lt;/h2>
&lt;p>The Ingress resource was designed with simplicity in mind, providing a simple set of fields that would be applicable in all use cases. Over time, as use cases evolved, implementations began to rely on a long list of custom annotations for further configuration. The new &lt;code>IngressClass&lt;/code> resource provides a way to replace some of those annotations.&lt;/p>
&lt;p>Each &lt;code>IngressClass&lt;/code> specifies which controller should implement Ingresses of the class and can reference a custom resource with additional parameters.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressClass&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">controller&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example.com/ingress-controller&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">parameters&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiGroup&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;k8s.example.com/v1alpha&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IngressParameters&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="specifying-the-class-of-an-ingress">Specifying the Class of an Ingress&lt;/h3>
&lt;p>A new &lt;code>ingressClassName&lt;/code> field has been added to the Ingress spec that is used to reference the &lt;code>IngressClass&lt;/code> that should be used to implement this Ingress.&lt;/p>
&lt;h3 id="deprecating-the-ingress-class-annotation">Deprecating the Ingress Class Annotation&lt;/h3>
&lt;p>Before the &lt;code>IngressClass&lt;/code> resource was added in Kubernetes 1.18, a similar concept of Ingress class was often specified with a &lt;code>kubernetes.io/ingress.class&lt;/code> annotation on the Ingress. Although this annotation was never formally defined, it was widely supported by Ingress controllers, and should now be considered formally deprecated.&lt;/p>
&lt;h3 id="setting-a-default-ingressclass">Setting a Default IngressClass&lt;/h3>
&lt;p>It’s possible to mark a specific &lt;code>IngressClass&lt;/code> as default in a cluster. Setting the
&lt;code>ingressclass.kubernetes.io/is-default-class&lt;/code> annotation to true on an
IngressClass resource will ensure that new Ingresses without an &lt;code>ingressClassName&lt;/code> specified will be assigned this default &lt;code>IngressClass&lt;/code>.&lt;/p>
&lt;h2 id="support-for-hostname-wildcards">Support for Hostname Wildcards&lt;/h2>
&lt;p>Many Ingress providers have supported wildcard hostname matching like &lt;code>*.foo.com&lt;/code> matching &lt;code>app1.foo.com&lt;/code>, but until now the spec assumed an exact FQDN match of the host. Hosts can now be precise matches (for example “&lt;code>foo.bar.com&lt;/code>”) or a wildcard (for example “&lt;code>*.foo.com&lt;/code>”). Precise matches require that the http host header matches the Host setting. Wildcard matches require the http host header is equal to the suffix of the wildcard rule.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Host&lt;/th>
&lt;th>Host header&lt;/th>
&lt;th>Match?&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>bar.foo.com&lt;/code>&lt;/td>
&lt;td>Matches based on shared suffix&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>baz.bar.foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>*.foo.com&lt;/code>&lt;/td>
&lt;td>&lt;code>foo.com&lt;/code>&lt;/td>
&lt;td>No match, wildcard only covers a single DNS label&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="putting-it-all-together">Putting it All Together&lt;/h3>
&lt;p>These new Ingress features allow for much more configurability. Here’s an example of an Ingress that makes use of pathType, &lt;code>ingressClassName&lt;/code>, and a hostname wildcard:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;networking.k8s.io/v1beta1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-ingress&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ingressClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external-lb&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">rules&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">host&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;*.example.com&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">http&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">paths&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/example&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">pathType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Prefix&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">backend&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;example-service&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">servicePort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="ingress-controller-support">Ingress Controller Support&lt;/h3>
&lt;p>Since these features are new in Kubernetes 1.18, each Ingress controller implementation will need some time to develop support for these new features. Check the documentation for your preferred Ingress controllers to see when they will support this new functionality.&lt;/p>
&lt;h2 id="the-future-of-ingress">The Future of Ingress&lt;/h2>
&lt;p>The Ingress API is on pace to graduate from beta to a stable API in Kubernetes 1.19. It will continue to provide a simple way to manage inbound network traffic for Kubernetes workloads. This API has intentionally been kept simple and lightweight, but there has been a desire for greater configurability for more advanced use cases.&lt;/p>
&lt;p>Work is currently underway on a new highly configurable set of APIs that will provide an alternative to Ingress in the future. These APIs are being referred to as the new “Service APIs”. They are not intended to replace any existing APIs, but instead provide a more configurable alternative for complex use cases. For more information, check out the &lt;a href="http://github.com/kubernetes-sigs/service-apis">Service APIs repo on GitHub&lt;/a>.&lt;/p></description></item><item><title>Blog: Kubernetes 1.18 Feature Server-side Apply Beta 2</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1.18-feature-server-side-apply-beta-2/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Antoine Pelisse (Google)&lt;/p>
&lt;h2 id="what-is-server-side-apply">What is Server-side Apply?&lt;/h2>
&lt;p>Server-side Apply is an important effort to migrate “kubectl apply” to the apiserver. It was started in 2018 by the Apply working group.&lt;/p>
&lt;p>The use of kubectl to declaratively apply resources has exposed the following challenges:&lt;/p>
&lt;ul>
&lt;li>
&lt;p>One needs to use the kubectl go code, or they have to shell out to kubectl.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Strategic merge-patch, the patch format used by kubectl, grew organically and was challenging to fix while maintaining compatibility with various api-server versions.&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Some features are hard to implement directly on the client, for example, unions.&lt;/p>
&lt;/li>
&lt;/ul>
&lt;p>Server-side Apply is a new merging algorithm, as well as tracking of field ownership, running on the Kubernetes api-server. Server-side Apply enables new features like conflict detection, so the system knows when two actors are trying to edit the same field.&lt;/p>
&lt;h2 id="how-does-it-work-what-s-managedfields">How does it work, what’s managedFields?&lt;/h2>
&lt;p>Server-side Apply works by keeping track of which actor of the system has changed each field of an object. It does so by diffing all updates to objects, and recording all the fields that have changed as well the time of the operation. All this information is stored in the managedFields in the metadata of objects. Since objects can have many fields, this field can be quite large.&lt;/p>
&lt;p>When someone applies, we can then use the information stored within managedFields to report relevant conflicts and help the merge algorithm to do the right thing.&lt;/p>
&lt;h2 id="wasn-t-it-already-beta-before-1-18">Wasn’t it already Beta before 1.18?&lt;/h2>
&lt;p>Yes, Server-side Apply has been Beta since 1.16, but it didn’t track the owner for fields associated with objects that had not been applied. This means that most objects didn’t have the managedFields metadata stored, and conflicts for these objects cannot be resolved. With Kubernetes 1.18, all new objects will have the managedFields attached to them and provide accurate information on conflicts.&lt;/p>
&lt;h2 id="how-do-i-use-it">How do I use it?&lt;/h2>
&lt;p>The most common way to use this is through kubectl: &lt;code>kubectl apply --server-side&lt;/code>. This is likely to show conflicts with other actors, including client-side apply. When that happens, conflicts can be forced by using the &lt;code>--force-conflicts&lt;/code> flag, which will grab the ownership for the fields that have changed.&lt;/p>
&lt;h2 id="current-limitations">Current limitations&lt;/h2>
&lt;p>We have two important limitations right now, especially with sub-resources. The first is that if you apply with a status, the status is going to be ignored. We are still going to try and acquire the fields, which may lead to invalid conflicts. The other is that we do not update the managedFields on some sub-resources, including scale, so you may not see information about a horizontal pod autoscaler changing the number of replicas.&lt;/p>
&lt;h2 id="what-s-next">What’s next?&lt;/h2>
&lt;p>We are working hard to improve the experience of using server-side apply with kubectl, and we are trying to make it the default. As part of that, we want to improve the migration from client-side to server-side.&lt;/p>
&lt;h2 id="can-i-help">Can I help?&lt;/h2>
&lt;p>Of course! The working-group apply is available on slack #wg-apply, through the &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-wg-apply">mailing list&lt;/a> and we also meet every other Tuesday at 9.30 PT on Zoom. We have lots of exciting features to build and can use all sorts of help.&lt;/p>
&lt;p>We would also like to use the opportunity to thank the hard work of all the contributors involved in making this new beta possible:&lt;/p>
&lt;ul>
&lt;li>Daniel Smith&lt;/li>
&lt;li>Jenny Buckley&lt;/li>
&lt;li>Joe Betz&lt;/li>
&lt;li>Julian Modesto&lt;/li>
&lt;li>Kevin Wiesmüller&lt;/li>
&lt;li>Maria Ntalla&lt;/li>
&lt;/ul></description></item><item><title>Blog: Kubernetes Topology Manager Moves to Beta - Align Up!</title><link>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</link><pubDate>Wed, 01 Apr 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/04/01/kubernetes-1-18-feature-topoloy-manager-beta/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Kevin Klues (NVIDIA), Victor Pickard (Red Hat), Conor Nolan (Intel)&lt;/p>
&lt;p>This blog post describes the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, a beta feature of Kubernetes in release 1.18. The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> feature enables NUMA alignment of CPUs and peripheral devices (such as SR-IOV VFs and GPUs), allowing your workload to run in an environment optimized for low-latency.&lt;/p>
&lt;p>Prior to the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications. With the introduction of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>, we now have a way to avoid this.&lt;/p>
&lt;p>This blog post covers:&lt;/p>
&lt;ol>
&lt;li>A brief introduction to NUMA and why it is important&lt;/li>
&lt;li>The policies available to end-users to ensure NUMA alignment of CPUs and devices&lt;/li>
&lt;li>The internal details of how the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> works&lt;/li>
&lt;li>Current limitations of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;li>Future directions of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>&lt;/li>
&lt;/ol>
&lt;h2 id="so-what-is-numa-and-why-do-i-care">So, what is NUMA and why do I care?&lt;/h2>
&lt;p>The term NUMA stands for Non-Uniform Memory Access. It is a technology available on multi-cpu systems that allows different CPUs to access different parts of memory at different speeds. Any memory directly connected to a CPU is considered &amp;quot;local&amp;quot; to that CPU and can be accessed very fast. Any memory not directly connected to a CPU is considered &amp;quot;non-local&amp;quot; and will have variable access times depending on how many interconnects must be passed through in order to reach it. On modern systems, the idea of having &amp;quot;local&amp;quot; vs. &amp;quot;non-local&amp;quot; memory can also be extended to peripheral devices such as NICs or GPUs. For high performance, CPUs and devices should be allocated such that they have access to the same local memory.&lt;/p>
&lt;p>All memory on a NUMA system is divided into a set of &amp;quot;NUMA nodes&amp;quot;, with each node representing the local memory for a set of CPUs or devices. We talk about an individual CPU as being part of a NUMA node if its local memory is associated with that NUMA node.&lt;/p>
&lt;p>We talk about a peripheral device as being part of a NUMA node based on the shortest number of interconnects that must be passed through in order to reach it.&lt;/p>
&lt;p>For example, in Figure 1, CPUs 0-3 are said to be part of NUMA node 0, whereas CPUs 4-7 are part of NUMA node 1. Likewise GPU 0 and NIC 0 are said to be part of NUMA node 0 because they are attached to Socket 0, whose CPUs are all part of NUMA node 0. The same is true for GPU 1 and NIC 1 on NUMA node 1.&lt;/p>
&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/example-numa-system.png">
&lt;/p>
&lt;p>&lt;strong>Figure 1:&lt;/strong> An example system with 2 NUMA nodes, 2 Sockets with 4 CPUs each, 2 GPUs, and 2 NICs. CPUs on Socket 0, GPU 0, and NIC 0 are all part of NUMA node 0. CPUs on Socket 1, GPU 1, and NIC 1 are all part of NUMA node 1.&lt;/p>
&lt;p>Although the example above shows a 1-1 mapping of NUMA Node to Socket, this is not necessarily true in the general case. There may be multiple sockets on a single NUMA node, or individual CPUs of a single socket may be connected to different NUMA nodes. Moreover, emerging technologies such as Sub-NUMA Clustering (&lt;a href="https://software.intel.com/en-us/articles/intel-xeon-processor-scalable-family-technical-overview">available on recent intel CPUs&lt;/a>) allow single CPUs to be associated with multiple NUMA nodes so long as their memory access times to both nodes are the same (or have a negligible difference).&lt;/p>
&lt;p>The &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> has been built to handle all of these scenarios.&lt;/p>
&lt;h2 id="align-up-it-s-a-team-effort">Align Up! It's a TeaM Effort!&lt;/h2>
&lt;p>As previously stated, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> allows users to align their CPU and peripheral device allocations by NUMA node. There are several policies available for this:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>&lt;code>none:&lt;/code>&lt;/strong> this policy will not attempt to do any alignment of resources. It will act the same as if the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> were not present at all. This is the default policy.&lt;/li>
&lt;li>&lt;strong>&lt;code>best-effort:&lt;/code>&lt;/strong> with this policy, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will attempt to align allocations on NUMA nodes as best it can, but will always allow the pod to start even if some of the allocated resources are not aligned on the same NUMA node.&lt;/li>
&lt;li>&lt;strong>&lt;code>restricted:&lt;/code>&lt;/strong> this policy is the same as the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, except it will fail pod admission if allocated resources cannot be aligned properly. Unlike with the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policy, some allocations may come from multiple NUMA nodes if it is impossible to &lt;em>ever&lt;/em> satisfy the allocation request on a single NUMA node (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes).&lt;/li>
&lt;li>&lt;strong>&lt;code>single-numa-node:&lt;/code>&lt;/strong> this policy is the most restrictive and will only allow a pod to be admitted if &lt;em>all&lt;/em> requested CPUs and devices can be allocated from exactly one NUMA node.&lt;/li>
&lt;/ul>
&lt;p>It is important to note that the selected policy is applied to each container in a pod spec individually, rather than aligning resources across all containers together.&lt;/p>
&lt;p>Moreover, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis). We hope to relax this restriction in the future.&lt;/p>
&lt;p>The &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag to set one of these policies can be seen below:&lt;/p>
&lt;pre>&lt;code>--topology-manager-policy=
[none | best-effort | restricted | single-numa-node]
&lt;/code>&lt;/pre>&lt;p>Additionally, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> is protected by a feature gate. This feature gate has been available since Kubernetes 1.16, but has only been enabled by default since 1.18.&lt;/p>
&lt;p>The feature gate can be enabled or disabled as follows (as described in more detail &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">here&lt;/a>):&lt;/p>
&lt;pre>&lt;code>--feature-gates=&amp;quot;...,TopologyManager=&amp;lt;true|false&amp;gt;&amp;quot;
&lt;/code>&lt;/pre>&lt;p>In order to trigger alignment according to the selected policy, a user must request CPUs and peripheral devices in their pod spec, according to a certain set of requirements.&lt;/p>
&lt;p>For peripheral devices, this means requesting devices from the available resources provided by a device plugin (e.g. &lt;strong>&lt;code>intel.com/sriov&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nvidia.com/gpu&lt;/code>&lt;/strong>, etc.). This will only work if the device plugin has been extended to integrate properly with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. Currently, the only plugins known to have this extension are the &lt;a href="https://github.com/NVIDIA/k8s-device-plugin/blob/5cb45d52afdf5798a40f8d0de049bce77f689865/nvidia.go#L74">Nvidia GPU device plugin&lt;/a>, and the &lt;a href="https://github.com/intel/sriov-network-device-plugin/blob/30e33f1ce2fc7b45721b6de8c8207e65dbf2d508/pkg/resources/pciNetDevice.go#L80">Intel SRIOV network device plugin&lt;/a>. Details on how to extend a device plugin to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be found &lt;a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/#device-plugin-integration-with-the-topology-manager">here&lt;/a>.&lt;/p>
&lt;p>For CPUs, this requires that the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> has been configured with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and that the pod is running in the Guaranteed QoS class (i.e. all CPU and memory &lt;strong>&lt;code>limits&lt;/code>&lt;/strong> are equal to their respective CPU and memory &lt;strong>&lt;code>requests&lt;/code>&lt;/strong>). CPUs must also be requested in whole number values (e.g. &lt;strong>&lt;code>1&lt;/code>&lt;/strong>, &lt;strong>&lt;code>2&lt;/code>&lt;/strong>, &lt;strong>&lt;code>1000m&lt;/code>&lt;/strong>, etc). Details on how to set the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> policy can be found &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/#cpu-management-policies">here&lt;/a>.&lt;/p>
&lt;p>For example, assuming the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> is running with its &lt;strong>&lt;code>--static&lt;/code>&lt;/strong> policy enabled and the device plugins for &lt;strong>&lt;code>gpu-vendor.com&lt;/code>&lt;/strong>, and &lt;strong>&lt;code>nic-vendor.com&lt;/code>&lt;/strong> have been extended to integrate with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> properly, the pod spec below is sufficient to trigger the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to run its selected policy:&lt;/p>
&lt;pre>&lt;code>spec:
containers:
- name: numa-aligned-container
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/code>&lt;/pre>&lt;p>Following Figure 1 from the previous section, this would result in one of the following aligned allocations:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
{cpu: {0, 2}, gpu: 0, nic: 0}
{cpu: {0, 3}, gpu: 0, nic: 0}
{cpu: {1, 2}, gpu: 0, nic: 0}
{cpu: {1, 3}, gpu: 0, nic: 0}
{cpu: {2, 3}, gpu: 0, nic: 0}
{cpu: {4, 5}, gpu: 1, nic: 1}
{cpu: {4, 6}, gpu: 1, nic: 1}
{cpu: {4, 7}, gpu: 1, nic: 1}
{cpu: {5, 6}, gpu: 1, nic: 1}
{cpu: {5, 7}, gpu: 1, nic: 1}
{cpu: {6, 7}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p>And that’s it! Just follow this pattern to have the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> ensure NUMA alignment across containers that request topology-aware devices and exclusive CPUs.&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> if a pod is rejected by one of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> policies, it will be placed in a &lt;strong>&lt;code>Terminated&lt;/code>&lt;/strong> state with a pod admission error and a reason of &amp;quot;&lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;/strong>&amp;quot;. Once a pod is in this state, the Kubernetes scheduler will not attempt to reschedule it. It is therefore recommended to use a &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#creating-a-deployment">&lt;strong>&lt;code>Deployment&lt;/code>&lt;/strong>&lt;/a> with replicas to trigger a redeploy of the pod on such a failure. An &lt;a href="https://kubernetes.io/docs/concepts/architecture/controller/">external control loop&lt;/a> can also be implemented to trigger a redeployment of pods that have a &lt;strong>&lt;code>TopologyAffinityError&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="this-is-great-so-how-does-it-work-under-the-hood">This is great, so how does it work under the hood?&lt;/h2>
&lt;p>Pseudocode for the primary logic carried out by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be seen below:&lt;/p>
&lt;pre>&lt;code>for container := range append(InitContainers, Containers...) {
for provider := range HintProviders {
hints += provider.GetTopologyHints(container)
}
bestHint := policy.Merge(hints)
for provider := range HintProviders {
provider.Allocate(container, bestHint)
}
}
&lt;/code>&lt;/pre>&lt;p>The following diagram summarizes the steps taken during this loop:&lt;/p>
&lt;p align="center">
&lt;img weight="200" height="200" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-steps-during-loop.png">
&lt;/p>
&lt;p>The steps themselves are:&lt;/p>
&lt;ol>
&lt;li>Loop over all containers in a pod.&lt;/li>
&lt;li>For each container, gather &amp;quot;&lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong>&amp;quot; from a set of &amp;quot;&lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>&amp;quot; for each topology-aware resource type requested by the container (e.g. &lt;strong>&lt;code>gpu-vendor.com/gpu&lt;/code>&lt;/strong>, &lt;strong>&lt;code>nic-vendor.com/nic&lt;/code>&lt;/strong>, &lt;strong>&lt;code>cpu&lt;/code>&lt;/strong>, etc.).&lt;/li>
&lt;li>Using the selected policy, merge the gathered &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> to find the &amp;quot;best&amp;quot; hint that aligns resource allocations across all resource types.&lt;/li>
&lt;li>Loop back over the set of hint providers, instructing them to allocate the resources they control using the merged hint as a guide.&lt;/li>
&lt;li>This loop runs at pod admission time and will fail to admit the pod if any of these steps fail or alignment cannot be satisfied according to the selected policy. Any resources allocated before the failure are cleaned up accordingly.&lt;/li>
&lt;/ol>
&lt;p>The following sections go into more detail on the exact structure of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> and &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>, as well as some details on the merge strategies used by each policy.&lt;/p>
&lt;h3 id="topologyhints">TopologyHints&lt;/h3>
&lt;p>A &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> encodes a set of constraints from which a given resource request can be satisfied. At present, the only constraint we consider is NUMA alignment. It is defined as follows:&lt;/p>
&lt;pre>&lt;code>type TopologyHint struct {
NUMANodeAffinity bitmask.BitMask
Preferred bool
}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> field contains a bitmask of NUMA nodes where a resource request can be satisfied. For example, the possible masks on a system with 2 NUMA nodes include:&lt;/p>
&lt;pre>&lt;code>{00}, {01}, {10}, {11}
&lt;/code>&lt;/pre>&lt;p>The &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field contains a boolean that encodes whether the given hint is &amp;quot;preferred&amp;quot; or not. With the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy, preferred hints will be given preference over non-preferred hints when generating a &amp;quot;best&amp;quot; hint. With the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, non-preferred hints will be rejected.&lt;/p>
&lt;p>In general, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> by looking at the set of currently available resources that can satisfy a resource request. More specifically, they generate one &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> for every possible mask of NUMA nodes where that resource request can be satisfied. If a mask cannot satisfy the request, it is omitted. For example, a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> might provide the following hints on a system with 2 NUMA nodes when being asked to allocate 2 resources. These hints encode that both resources could either come from a single NUMA node (either 0 or 1), or they could each come from different NUMA nodes (but we prefer for them to come from just one).&lt;/p>
&lt;pre>&lt;code>{01: True}, {10: True}, {11: False}
&lt;/code>&lt;/pre>&lt;p>At present, all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> set the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if and only if the &lt;strong>&lt;code>NUMANodeAffinity&lt;/code>&lt;/strong> encodes a &lt;em>minimal&lt;/em> set of NUMA nodes that can satisfy the resource request. Normally, this will only be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> for &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> with a single NUMA node set in their bitmask. However, it may also be &lt;strong>&lt;code>True&lt;/code>&lt;/strong> if the only way to &lt;em>ever&lt;/em> satisfy the resource request is to span multiple NUMA nodes (e.g. 2 devices are requested and the only 2 devices on the system are on different NUMA nodes):&lt;/p>
&lt;pre>&lt;code>{0011: True}, {0111: False}, {1011: False}, {1111: False}
&lt;/code>&lt;/pre>&lt;p>&lt;strong>NOTE:&lt;/strong> Setting of the &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field in this way is &lt;em>not&lt;/em> based on the set of currently available resources. It is based on the ability to physically allocate the number of requested resources on some minimal set of NUMA nodes.&lt;/p>
&lt;p>In this way, it is possible for a &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to return a list of hints with &lt;em>all&lt;/em> &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> fields set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> if an actual preferred allocation cannot be satisfied until other containers release their resources. For example, consider the following scenario from the system in Figure 1:&lt;/p>
&lt;ol>
&lt;li>All but 2 CPUs are currently allocated to containers&lt;/li>
&lt;li>The 2 remaining CPUs are on different NUMA nodes&lt;/li>
&lt;li>A new container comes along asking for 2 CPUs&lt;/li>
&lt;/ol>
&lt;p>In this case, the only generated hint would be &lt;strong>&lt;code>{11: False}&lt;/code>&lt;/strong> and not &lt;strong>&lt;code>{11: True}&lt;/code>&lt;/strong>. This happens because it &lt;em>is&lt;/em> possible to allocate 2 CPUs from the same NUMA node on this system (just not right now, given the current allocation state). The idea being that it is better to fail pod admission and retry the deployment when the minimal alignment can be satisfied than to allow a pod to be scheduled with sub-optimal alignment.&lt;/p>
&lt;h3 id="hintproviders">HintProviders&lt;/h3>
&lt;p>A &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> is a component internal to the &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> that coordinates aligned resource allocations with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>. At present, the only &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> in Kubernetes are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. We plan to add support for &lt;strong>&lt;code>HugePages&lt;/code>&lt;/strong> soon.&lt;/p>
&lt;p>As discussed previously, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> both gathers &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> as well as triggers aligned resource allocations on them using a merged &amp;quot;best&amp;quot; hint. As such, &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> implement the following interface:&lt;/p>
&lt;pre>&lt;code>type HintProvider interface {
GetTopologyHints(*v1.Pod, *v1.Container) map[string][]TopologyHint
Allocate(*v1.Pod, *v1.Container) error
}
&lt;/code>&lt;/pre>&lt;p>Notice that the call to &lt;strong>&lt;code>GetTopologyHints()&lt;/code>&lt;/strong> returns a &lt;strong>&lt;code>map[string][]TopologyHint&lt;/code>&lt;/strong>. This allows a single &lt;strong>&lt;code>HintProvider&lt;/code>&lt;/strong> to provide hints for multiple resource types instead of just one. For example, the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong> requires this in order to pass hints back for every resource type registered by its plugins.&lt;/p>
&lt;p>As &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> generate their hints, they only consider how alignment could be satisfied for &lt;em>currently&lt;/em> available resources on the system. Any resources already allocated to other containers are not considered.&lt;/p>
&lt;p>For example, consider the system in Figure 1, with the following two containers requesting resources from it:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">&lt;strong>&lt;code>Container0&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>Container1&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container0
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;td>
&lt;pre>
spec:
containers:
- name: numa-aligned-container1
image: alpine
resources:
limits:
cpu: 2
memory: 200Mi
gpu-vendor.com/gpu: 1
nic-vendor.com/nic: 1
&lt;/pre>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>If &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> is the first container considered for allocation on the system, the following set of hints will be generated for the three topology-aware resource types in the spec.&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {0, 1}, gpu: 0, nic: 0}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider1.png">
&lt;/p>
&lt;p>When considering &lt;strong>&lt;code>Container1&lt;/code>&lt;/strong> these resources are then presumed to be unavailable, and thus only the following set of hints will be generated:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{10: True}}
nic-vendor.com/nic: {{10: True}}
&lt;/code>&lt;/pre>&lt;p>With a resulting aligned allocation of:&lt;/p>
&lt;pre>&lt;code>{cpu: {4, 5}, gpu: 1, nic: 1}
&lt;/code>&lt;/pre>&lt;p align="center">
&lt;img height="300" src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/numa-hint-provider2.png">
&lt;/p>
&lt;p>&lt;strong>NOTE:&lt;/strong> Unlike the pseudocode provided at the beginning of this section, the call to &lt;strong>&lt;code>Allocate()&lt;/code>&lt;/strong> does not actually take a parameter for the merged &amp;quot;best&amp;quot; hint directly. Instead, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> implements the following &lt;strong>&lt;code>Store&lt;/code>&lt;/strong> interface that &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> can query to retrieve the hint generated for a particular container once it has been generated:&lt;/p>
&lt;pre>&lt;code>type Store interface {
GetAffinity(podUID string, containerName string) TopologyHint
}
&lt;/code>&lt;/pre>&lt;p>Separating this out into its own API call allows one to access this hint outside of the pod admission loop. This is useful for debugging as well as for reporting generated hints in tools such as &lt;strong>&lt;code>kubectl&lt;/code>&lt;/strong>(not yet available).&lt;/p>
&lt;h3 id="policy-merge">Policy.Merge&lt;/h3>
&lt;p>The merge strategy defined by a given policy dictates how it combines the set of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated by all &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> into a single &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> that can be used to inform aligned resource allocations.&lt;/p>
&lt;p>The general merge strategy for all supported policies begins the same:&lt;/p>
&lt;ol>
&lt;li>Take the cross-product of &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> generated for each resource type&lt;/li>
&lt;li>For each entry in the cross-product, &lt;strong>&lt;code>bitwise-and&lt;/code>&lt;/strong> the NUMA affinities of each &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> together. Set this as the NUMA affinity in a resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If all of the hints in an entry have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint.&lt;/li>
&lt;li>If even one of the hints in an entry has &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> , set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the resulting &amp;quot;merged&amp;quot; hint. Also set &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> in the &amp;quot;merged&amp;quot; hint if its NUMA affinity contains all 0s.&lt;/li>
&lt;/ol>
&lt;p>Following the example from the previous section with hints for &lt;strong>&lt;code>Container0&lt;/code>&lt;/strong> generated as:&lt;/p>
&lt;pre>&lt;code> cpu: {{01: True}, {10: True}, {11: False}}
gpu-vendor.com/gpu: {{01: True}, {10: True}}
nic-vendor.com/nic: {{01: True}, {10: True}}
&lt;/code>&lt;/pre>&lt;p>The above algorithm results in the following set of cross-product entries and &amp;quot;merged&amp;quot; hints:&lt;/p>
&lt;table>
&lt;tr>
&lt;td align="center">cross-product entry
&lt;p>
&lt;strong>&lt;code>{cpu, gpu-vendor.com/gpu, nic-vendor.com/nic}&lt;/code>&lt;/strong>
&lt;/p>
&lt;/td>
&lt;td align="center">"merged" hint
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{01: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td align="center">
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{10: True}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>
&lt;/td>
&lt;td>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{01: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {01: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {01: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{00: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td align="center">
&lt;strong>&lt;code>{{11: False}, {10: True}, {10: True}}&lt;/code>&lt;/strong>
&lt;/td>
&lt;td align="center">&lt;strong>&lt;code>{10: False}&lt;/code>&lt;/strong>
&lt;/td>
&lt;/tr>
&lt;/table>
&lt;p>Once this list of &amp;quot;merged&amp;quot; hints has been generated, it is the job of the specific &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> policy in use to decide which one to consider as the &amp;quot;best&amp;quot; hint.&lt;/p>
&lt;p>In general, this involves:&lt;/p>
&lt;ol>
&lt;li>Sorting merged hints by their &amp;quot;narrowness&amp;quot;. Narrowness is defined as the number of bits set in a hint’s NUMA affinity mask. The fewer bits set, the narrower the hint. For hints that have the same number of bits set in their NUMA affinity mask, the hint with the most low order bits set is considered narrower.&lt;/li>
&lt;li>Sorting merged hints by their &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> field. Hints that have &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>True&lt;/code>&lt;/strong> are considered more likely candidates than hints with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong>.&lt;/li>
&lt;li>Selecting the narrowest hint with the best possible setting for &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong>.&lt;/li>
&lt;/ol>
&lt;p>In the case of the &lt;strong>&lt;code>best-effort&lt;/code>&lt;/strong> policy this algorithm will always result in &lt;em>some&lt;/em> hint being selected as the &amp;quot;best&amp;quot; hint and the pod being admitted. This &amp;quot;best&amp;quot; hint is then made available to &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> so they can make their resource allocations based on it.&lt;/p>
&lt;p>However, in the case of the &lt;strong>&lt;code>restricted&lt;/code>&lt;/strong> and &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> policies, any selected hint with &lt;strong>&lt;code>Preferred&lt;/code>&lt;/strong> set to &lt;strong>&lt;code>False&lt;/code>&lt;/strong> will be rejected immediately, causing pod admission to fail and no resources to be allocated. Moreover, the &lt;strong>&lt;code>single-numa-node&lt;/code>&lt;/strong> will also reject a selected hint that has more than one NUMA node set in its affinity mask.&lt;/p>
&lt;p>In the example above, the pod would be admitted by all policies with a hint of &lt;strong>&lt;code>{01: True}&lt;/code>&lt;/strong>.&lt;/p>
&lt;h2 id="upcoming-enhancements">Upcoming enhancements&lt;/h2>
&lt;p>While the 1.18 release and promotion to Beta brings along some great enhancements and fixes, there are still a number of limitations, described &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/topology-manager/#known-limitations">here&lt;/a>. We are already underway working to address these limitations and more.&lt;/p>
&lt;p>This section walks through the set of enhancements we plan to implement for the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> in the near future. This list is not exhaustive, but it gives a good idea of the direction we are moving in. It is ordered by the timeframe in which we expect to see each enhancement completed.&lt;/p>
&lt;p>If you would like to get involved in helping with any of these enhancements, please &lt;a href="https://github.com/kubernetes/community/tree/master/sig-node">join the weekly Kubernetes SIG-node meetings&lt;/a> to learn more and become part of the community effort!&lt;/p>
&lt;h3 id="supporting-device-specific-constraints">Supporting device-specific constraints&lt;/h3>
&lt;p>Currently, NUMA affinity is the only constraint considered by the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> for resource alignment. Moreover, the only scalable extensions that can be made to a &lt;strong>&lt;code>TopologyHint&lt;/code>&lt;/strong> involve &lt;em>node-level&lt;/em> constraints, such as PCIe bus alignment across device types. It would be intractable to try and add any &lt;em>device-specific&lt;/em> constraints to this struct (e.g. the internal NVLINK topology among a set of GPU devices).&lt;/p>
&lt;p>As such, we propose an extension to the device plugin interface that will allow a plugin to state its topology-aware allocation preferences, without having to expose any device-specific topology information to the kubelet. In this way, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> can be restricted to only deal with common node-level topology constraints, while still having a way of incorporating device-specific topology constraints into its allocation decisions.&lt;/p>
&lt;p>Details of this proposal can be found &lt;a href="https://github.com/kubernetes/enhancements/pull/1121">here&lt;/a>, and should be available as soon as Kubernetes 1.19.&lt;/p>
&lt;h3 id="numa-alignment-for-hugepages">NUMA alignment for hugepages&lt;/h3>
&lt;p>As stated previously, the only two &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong> currently available to the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> are the &lt;strong>&lt;code>CPUManager&lt;/code>&lt;/strong> and the &lt;strong>&lt;code>DeviceManager&lt;/code>&lt;/strong>. However, work is currently underway to add support for hugepages as well. With the completion of this work, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will finally be able to allocate memory, hugepages, CPUs and PCI devices all on the same NUMA node.&lt;/p>
&lt;p>A &lt;a href="https://github.com/kubernetes/enhancements/blob/253f1e5bdd121872d2d0f7020a5ac0365b229e30/keps/sig-node/20200203-memory-manager.md">KEP&lt;/a> for this work is currently under review, and a prototype is underway to get this feature implemented very soon.&lt;/p>
&lt;h3 id="scheduler-awareness">Scheduler awareness&lt;/h3>
&lt;p>Currently, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> acts as a Pod Admission controller. It is not directly involved in the scheduling decision of where a pod will be placed. Rather, when the kubernetes scheduler (or whatever scheduler is running in the deployment), places a pod on a node to run, the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> will decide if the pod should be &amp;quot;admitted&amp;quot; or &amp;quot;rejected&amp;quot;. If the pod is rejected due to lack of available NUMA aligned resources, things can get a little interesting. This kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/issues/84869">issue&lt;/a> highlights and discusses this situation well.&lt;/p>
&lt;p>So how do we go about addressing this limitation? We have the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-scheduling/20180409-scheduling-framework.md">Kubernetes Scheduling Framework&lt;/a> to the rescue! This framework provides a new set of plugin APIs that integrate with the existing Kubernetes Scheduler and allow scheduling features, such as NUMA alignment, to be implemented without having to resort to other, perhaps less appealing alternatives, including writing your own scheduler, or even worse, creating a fork to add your own scheduler secret sauce.&lt;/p>
&lt;p>The details of how to implement these extensions for integration with the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> have not yet been worked out. We still need to answer questions like:&lt;/p>
&lt;ul>
&lt;li>Will we require duplicated logic to determine device affinity in the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and the scheduler?&lt;/li>
&lt;li>Do we need a new API to get &lt;strong>&lt;code>TopologyHints&lt;/code>&lt;/strong> from the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to the scheduler plugin?&lt;/li>
&lt;/ul>
&lt;p>Work on this feature should begin in the next couple of months, so stay tuned!&lt;/p>
&lt;h3 id="per-pod-alignment-policy">Per-pod alignment policy&lt;/h3>
&lt;p>As stated previously, a single policy is applied to &lt;em>all&lt;/em> pods on a node via a global &lt;strong>&lt;code>kubelet&lt;/code>&lt;/strong> flag, rather than allowing users to select different policies on a pod-by-pod basis (or a container-by-container basis).&lt;/p>
&lt;p>While we agree that this would be a great feature to have, there are quite a few hurdles that need to be overcome before it is achievable. The biggest hurdle being that this enhancement will require an API change to be able to express the desired alignment policy in either the Pod spec or its associated &lt;strong>&lt;code>&lt;a href="https://kubernetes.io/docs/concepts/containers/runtime-class/">RuntimeClass&lt;/a>&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We are only now starting to have serious discussions around this feature, and it is still a few releases away, at the best, from being available.&lt;/p>
&lt;h2 id="conclusion">Conclusion&lt;/h2>
&lt;p>With the promotion of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> to Beta in 1.18, we encourage everyone to give it a try and look forward to any feedback you may have. Many fixes and enhancements have been worked on in the past several releases, greatly improving the functionality and reliability of the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong> and its &lt;strong>&lt;code>HintProviders&lt;/code>&lt;/strong>. While there are still a number of limitations, we have a set of enhancements planned to address them, and look forward to providing you with a number of new features in upcoming releases.&lt;/p>
&lt;p>If you have ideas for additional enhancements or a desire for certain features, don’t hesitate to let us know. The team is always open to suggestions to enhance and improve the &lt;strong>&lt;code>TopologyManager&lt;/code>&lt;/strong>.&lt;/p>
&lt;p>We hope you have found this blog informative and useful! Let us know if you have any questions or comments. And, happy deploying…..Align Up!&lt;/p>
&lt;!-- Docs to Markdown version 1.0β20 --></description></item><item><title>Blog: Kubernetes 1.18: Fit &amp; Finish</title><link>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</link><pubDate>Wed, 25 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/25/kubernetes-1-18-release-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">Kubernetes 1.18 Release Team&lt;/a>&lt;/p>
&lt;p>We're pleased to announce the delivery of Kubernetes 1.18, our first release of 2020! Kubernetes 1.18 consists of 38 enhancements: 15 enhancements are moving to stable, 11 enhancements in beta, and 12 enhancements in alpha.&lt;/p>
&lt;p>Kubernetes 1.18 is a &amp;quot;fit and finish&amp;quot; release. Significant work has gone into improving beta and stable features to ensure users have a better experience. An equal effort has gone into adding new developments and exciting new features that promise to enhance the user experience even more.
Having almost as many enhancements in alpha, beta, and stable is a great achievement. It shows the tremendous effort made by the community on improving the reliability of Kubernetes as well as continuing to expand its existing functionality.&lt;/p>
&lt;h2 id="major-themes">Major Themes&lt;/h2>
&lt;h3 id="kubernetes-topology-manager-moves-to-beta-align-up">Kubernetes Topology Manager Moves to Beta - Align Up!&lt;/h3>
&lt;p>A beta feature of Kubernetes in release 1.18, the &lt;a href="https://github.com/nolancon/website/blob/f4200307260ea3234540ef13ed80de325e1a7267/content/en/docs/tasks/administer-cluster/topology-manager.md">Topology Manager feature&lt;/a> enables NUMA alignment of CPU and devices (such as SR-IOV VFs) that will allow your workload to run in an environment optimized for low-latency. Prior to the introduction of the Topology Manager, the CPU and Device Manager would make resource allocation decisions independent of each other. This could result in undesirable allocations on multi-socket systems, causing degraded performance on latency critical applications.&lt;/p>
&lt;h3 id="serverside-apply-introduces-beta-2">Serverside Apply Introduces Beta 2&lt;/h3>
&lt;p>Server-side Apply was promoted to Beta in 1.16, but is now introducing a second Beta in 1.18. This new version will track and manage changes to fields of all new Kubernetes objects, allowing you to know what changed your resources and when.&lt;/p>
&lt;h3 id="extending-ingress-with-and-replacing-a-deprecated-annotation-with-ingressclass">Extending Ingress with and replacing a deprecated annotation with IngressClass&lt;/h3>
&lt;p>In Kubernetes 1.18, there are two significant additions to Ingress: A new &lt;code>pathType&lt;/code> field and a new &lt;code>IngressClass&lt;/code> resource. The &lt;code>pathType&lt;/code> field allows specifying how paths should be matched. In addition to the default &lt;code>ImplementationSpecific&lt;/code> type, there are new &lt;code>Exact&lt;/code> and &lt;code>Prefix&lt;/code> path types.&lt;/p>
&lt;p>The &lt;code>IngressClass&lt;/code> resource is used to describe a type of Ingress within a Kubernetes cluster. Ingresses can specify the class they are associated with by using a new &lt;code>ingressClassName&lt;/code> field on Ingresses. This new resource and field replace the deprecated &lt;code>kubernetes.io/ingress.class&lt;/code> annotation.&lt;/p>
&lt;h3 id="sig-cli-introduces-kubectl-alpha-debug">SIG-CLI introduces kubectl alpha debug&lt;/h3>
&lt;p>SIG-CLI was debating the need for a debug utility for quite some time already. With the development of &lt;a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/">ephemeral containers&lt;/a>, it became more obvious how we can support developers with tooling built on top of &lt;code>kubectl exec&lt;/code>. The addition of the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">&lt;code>kubectl alpha debug&lt;/code> command&lt;/a> (it is alpha but your feedback is more than welcome), allows developers to easily debug their Pods inside the cluster. We think this addition is invaluable. This command allows one to create a temporary container which runs next to the Pod one is trying to examine, but also attaches to the console for interactive troubleshooting.&lt;/p>
&lt;h3 id="introducing-windows-csi-support-alpha-for-kubernetes">Introducing Windows CSI support alpha for Kubernetes&lt;/h3>
&lt;p>The alpha version of CSI Proxy for Windows is being released with Kubernetes 1.18. CSI proxy enables CSI Drivers on Windows by allowing containers in Windows to perform privileged storage operations.&lt;/p>
&lt;h2 id="other-updates">Other Updates&lt;/h2>
&lt;h3 id="graduated-to-stable">Graduated to Stable 💯&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/166">Taint Based Eviction&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/491">&lt;code>kubectl diff&lt;/code>&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/565">CSI Block storage support&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/576">API Server dry run&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/603">Pass Pod information in CSI calls&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/670">Support Out-of-Tree vSphere Cloud Provider&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/689">Support GMSA for Windows workloads&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/770">Skip attach for non-attachable CSI volumes&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/989">PVC cloning&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1043">RunAsUserName for Windows&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1507">AppProtocol for Services and Endpoints&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1024">Node-local DNS cache&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="major-changes">Major Changes&lt;/h3>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/752">EndpointSlice API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1020">Moving kubectl package code to staging&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1513">CertificateSigningRequest API&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1539">Extending Hugepage Feature&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/enhancements/issues/1601">client-go signature refactor to standardize options and context handling&lt;/a>&lt;/li>
&lt;/ul>
&lt;h3 id="release-notes">Release Notes&lt;/h3>
&lt;p>Check out the full details of the Kubernetes 1.18 release in our &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG/CHANGELOG-1.18.md">release notes&lt;/a>.&lt;/p>
&lt;h3 id="availability">Availability&lt;/h3>
&lt;p>Kubernetes 1.18 is available for download on &lt;a href="https://github.com/kubernetes/kubernetes/releases/tag/v1.18.0">GitHub&lt;/a>. To get started with Kubernetes, check out these &lt;a href="https://kubernetes.io/docs/tutorials/">interactive tutorials&lt;/a> or run local Kubernetes clusters using Docker container “nodes” with &lt;a href="https://kind.sigs.k8s.io/">kind&lt;/a>. You can also easily install 1.18 using &lt;a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/">kubeadm&lt;/a>.&lt;/p>
&lt;h3 id="release-team">Release Team&lt;/h3>
&lt;p>This release is made possible through the efforts of hundreds of individuals who contributed both technical and non-technical content. Special thanks to the &lt;a href="https://github.com/kubernetes/sig-release/blob/master/releases/release-1.18/release_team.md">release team&lt;/a> led by Jorge Alarcon Ochoa, Site Reliability Engineer at Searchable AI. The 34 release team members coordinated many aspects of the release, from documentation to testing, validation, and feature completeness.&lt;/p>
&lt;p>As the Kubernetes community has grown, our release process represents an amazing demonstration of collaboration in open source software development. Kubernetes continues to gain new users at a rapid pace. This growth creates a positive feedback cycle where more contributors commit code creating a more vibrant ecosystem. Kubernetes has had over &lt;a href="https://k8s.devstats.cncf.io/d/24/overall-project-statistics?orgId=1">40,000 individual contributors&lt;/a> to date and an active community of more than 3,000 people.&lt;/p>
&lt;h3 id="release-logo">Release Logo&lt;/h3>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-03-25-kubernetes-1.18-release-announcement/release-logo.png" alt="Kubernetes 1.18 Release Logo">&lt;/p>
&lt;h4 id="why-the-lhc">Why the LHC?&lt;/h4>
&lt;p>The LHC is the world’s largest and most powerful particle accelerator. It is the result of the collaboration of thousands of scientists from around the world, all for the advancement of science. In a similar manner, Kubernetes has been a project that has united thousands of contributors from hundreds of organizations – all to work towards the same goal of improving cloud computing in all aspects! &amp;quot;A Bit Quarky&amp;quot; as the release name is meant to remind us that unconventional ideas can bring about great change and keeping an open mind to diversity will lead help us innovate.&lt;/p>
&lt;h4 id="about-the-designer">About the designer&lt;/h4>
&lt;p>Maru Lango is a designer currently based in Mexico City. While her area of expertise is Product Design, she also enjoys branding, illustration and visual experiments using CSS + JS and contributing to diversity efforts within the tech and design communities. You may find her in most social media as @marulango or check her website: &lt;a href="https://marulango.com">https://marulango.com&lt;/a>&lt;/p>
&lt;h3 id="user-highlights">User Highlights&lt;/h3>
&lt;ul>
&lt;li>Ericsson is using Kubernetes and other cloud native technology to deliver a &lt;a href="https://www.cncf.io/case-study/ericsson/">highly demanding 5G network&lt;/a> that resulted in up to 90 percent CI/CD savings.&lt;/li>
&lt;li>Zendesk is using Kubernetes to &lt;a href="https://www.cncf.io/case-study/zendesk/">run around 70% of its existing applications&lt;/a>. It’s also building all new applications to also run on Kubernetes, which has brought time savings, greater flexibility, and increased velocity to its application development.&lt;/li>
&lt;li>LifeMiles has &lt;a href="https://www.cncf.io/case-study/lifemiles/">reduced infrastructure spending by 50%&lt;/a> because of its move to Kubernetes. It has also allowed them to double its available resource capacity.&lt;/li>
&lt;/ul>
&lt;h3 id="ecosystem-updates">Ecosystem Updates&lt;/h3>
&lt;ul>
&lt;li>The CNCF published the results of its &lt;a href="https://www.cncf.io/blog/2020/03/04/2019-cncf-survey-results-are-here-deployments-are-growing-in-size-and-speed-as-cloud-native-adoption-becomes-mainstream/">annual survey&lt;/a> showing that Kubernetes usage in production is skyrocketing. The survey found that 78% of respondents are using Kubernetes in production compared to 58% last year.&lt;/li>
&lt;li>The “Introduction to Kubernetes” course hosted by the CNCF &lt;a href="https://www.cncf.io/announcement/2020/01/28/cloud-native-computing-foundation-announces-introduction-to-kubernetes-course-surpasses-100000-registrations/">surpassed 100,000 registrations&lt;/a>.&lt;/li>
&lt;/ul>
&lt;h3 id="project-velocity">Project Velocity&lt;/h3>
&lt;p>The CNCF has continued refining DevStats, an ambitious project to visualize the myriad contributions that go into the project. &lt;a href="https://k8s.devstats.cncf.io/d/12/dashboards?orgId=1">K8s DevStats&lt;/a> illustrates the breakdown of contributions from major company contributors, as well as an impressive set of preconfigured reports on everything from individual contributors to pull request lifecycle times.&lt;/p>
&lt;p>This past quarter, 641 different companies and over 6,409 individuals contributed to Kubernetes. &lt;a href="https://k8s.devstats.cncf.io/d/11/companies-contributing-in-repository-groups?orgId=1&amp;amp;var-period=m&amp;amp;var-repogroup_name=All">Check out DevStats&lt;/a> to learn more about the overall velocity of the Kubernetes project and community.&lt;/p>
&lt;h3 id="event-update">Event Update&lt;/h3>
&lt;p>Kubecon + CloudNativeCon EU 2020 is being pushed back – for the more most up-to-date information, please check the &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">Novel Coronavirus Update page&lt;/a>.&lt;/p>
&lt;h3 id="upcoming-release-webinar">Upcoming Release Webinar&lt;/h3>
&lt;p>Join members of the Kubernetes 1.18 release team on April 23rd, 2020 to learn about the major features in this release including kubectl debug, Topography Manager, Ingress to V1 graduation, and client-go. Register here: &lt;a href="https://www.cncf.io/webinars/kubernetes-1-18/">https://www.cncf.io/webinars/kubernetes-1-18/&lt;/a>.&lt;/p>
&lt;h3 id="get-involved">Get Involved&lt;/h3>
&lt;p>The simplest way to get involved with Kubernetes is by joining one of the many &lt;a href="https://github.com/kubernetes/community/blob/master/sig-list.md">Special Interest Groups&lt;/a> (SIGs) that align with your interests. Have something you’d like to broadcast to the Kubernetes community? Share your voice at our weekly &lt;a href="https://github.com/kubernetes/community/tree/master/communication">community meeting&lt;/a>, and through the channels below. Thank you for your continued feedback and support.&lt;/p>
&lt;ul>
&lt;li>Follow us on Twitter &lt;a href="https://twitter.com/kubernetesio">@Kubernetesio&lt;/a> for latest updates&lt;/li>
&lt;li>Join the community discussion on &lt;a href="https://discuss.kubernetes.io/">Discuss&lt;/a>&lt;/li>
&lt;li>Join the community on &lt;a href="http://slack.k8s.io/">Slack&lt;/a>&lt;/li>
&lt;li>Post questions (or answer questions) on &lt;a href="http://stackoverflow.com/questions/tagged/kubernetes">Stack Overflow&lt;/a>&lt;/li>
&lt;li>Share your Kubernetes &lt;a href="https://docs.google.com/a/linuxfoundation.org/forms/d/e/1FAIpQLScuI7Ye3VQHQTwBASrgkjQDSS5TP0g3AXfFhwSM9YpHgxRKFA/viewform">story&lt;/a>&lt;/li>
&lt;li>Read more about what’s happening with Kubernetes on the &lt;a href="https://kubernetes.io/blog/">blog&lt;/a>&lt;/li>
&lt;li>Learn more about the &lt;a href="https://github.com/kubernetes/sig-release/tree/master/release-team">Kubernetes Release Team&lt;/a>&lt;/li>
&lt;/ul></description></item><item><title>Blog: Join SIG Scalability and Learn Kubernetes the Hard Way</title><link>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</link><pubDate>Thu, 19 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/19/join-sig-scalability/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Alex Handy&lt;/p>
&lt;p>Contributing to SIG Scalability is a great way to learn Kubernetes in all its depth and breadth, and the team would love to have you &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">join as a contributor&lt;/a>. I took a look at the value of learning the hard way and interviewed the current SIG chairs to give you an idea of what contribution feels like.&lt;/p>
&lt;h2 id="the-value-of-learning-the-hard-way">The value of Learning The Hard Way&lt;/h2>
&lt;p>There is a belief in the software development community that pushes for the most challenging and rigorous possible method of learning a new language or system. These tend to go by the moniker of &amp;quot;Learn __ the Hard Way.&amp;quot; Examples abound: Learn Code the Hard Way, Learn Python the Hard Way, and many others originating with Zed Shaw's courses in the topic.&lt;/p>
&lt;p>While there are folks out there who offer you a &amp;quot;Learn Kubernetes the Hard Way&amp;quot; type experience (most notably &lt;a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">Kelsey Hightower's&lt;/a>), any &amp;quot;Hard Way&amp;quot; project should attempt to cover every aspect of the core topic's principles.&lt;/p>
&lt;p>Therefore, the real way to &amp;quot;Learn Kubernetes the Hard Way,&amp;quot; is to join the CNCF and get involved in the project itself. And there is only one SIG that could genuinely offer a full-stack learning experience for Kubernetes: SIG Scalability.&lt;/p>
&lt;p>The team behind SIG Scalability is responsible for detecting and dealing with issues that arise when Kubernetes clusters are working with upwards of a thousand nodes. Said &lt;a href="https://github.com/wojtek-t">Wojiciech Tyczynski&lt;/a>, a staff software engineer at Google and a member of SIG Scalability, the standard size for a test cluster for this SIG is over 5,000 nodes.&lt;/p>
&lt;p>And yet, this SIG is not composed of Ph.D.'s in highly scalable systems designs. Many of the folks working with Tyczynski, for example, joined the SIG knowing very little about these types of issues, and often, very little about Kubernetes.&lt;/p>
&lt;p>Working on SIG Scalability is like jumping into the deep end of the pool to learn to swim, and the SIG is inherently concerned with the entire Kubernetes project. SIG Scalability focuses on how Kubernetes functions as a whole and at scale. The SIG Scalability team members have an impetus to learn about every system and to understand how all systems interact with one another.&lt;/p>
&lt;h2 id="a-complex-and-rewarding-contributor-experience">A complex and rewarding contributor experience&lt;/h2>
&lt;p>While that may sound complicated (and it is!), that doesn't mean it's outside the reach of an average developer, tester, or administrator. Google software developer Matt Matejczyk has only been on the team since the beginning of 2019, and he's been a valued member of the team since then, ferreting out bugs.&lt;/p>
&lt;p>&amp;quot;I am new here,&amp;quot; said Matejczyk. &amp;quot;I joined the team in January [2019]. Before that, I worked on AdWords at Google in New York. Why did I join? I knew some people there, so that was one of the decisions for me to move. I thought at that time that Kubernetes is a unique, cutting edge technology. I thought it'd be cool to work on that.&amp;quot;&lt;/p>
&lt;p>Matejczyk was correct about the coolness. &amp;quot;It's cool,&amp;quot; he said. &amp;quot;So actually, ramping up on scalability is not easy. There are many things you need to understand. You need to understand Kubernetes very well. It can use every part of Kubernetes. I am still ramping up after these 8 months. I think it took me maybe 3 months to get up to decent speed.&amp;quot;&lt;/p>
&lt;p>When Matejczyk spoke to what he had worked on during those 8 months, he answered, &amp;quot;An interesting example is a regression I have been working on recently. We noticed the overall slowness of Kubernetes control plane in specific scenarios, and we couldn't attribute it to any particular component. In the end, we realized that everything boiled down to the memory allocation on the golang level. It was very counterintuitive to have two completely separate pieces of code (running as a part of the same binary) affecting the performance of each other only because one of them was allocating memory too fast. But connecting all the dots and getting to the bottom of regression like this gives great satisfaction.&amp;quot;&lt;/p>
&lt;p>Tyczynski said that &amp;quot;It's not only debugging regressions, but it's also debugging and finding bottlenecks. In general, those can be regressions, but those can be things we can improve. The other significant area is extending what we want to guarantee to users. Extending SLA and SLO coverage of the system so users can rely on what they can expect from the system in terms of performance and scalability. Matt is doing much work in extending our tests to be more representative and cover more Kubernetes concepts.&amp;quot;&lt;/p>
&lt;h2 id="give-sig-scalability-a-try">Give SIG Scalability a try&lt;/h2>
&lt;p>The SIG Scalability team is always in need of new members, and if you're the sort of developer or tester who loves taking on new complex challenges, and perhaps loves learning things the hard way, consider joining this SIG. As the team points out, adding Kubernetes expertise to your resume is never a bad idea, and this is the one SIG where you can learn it all from top to bottom.&lt;/p>
&lt;p>See &lt;a href="https://github.com/kubernetes/community/tree/master/sig-scalability#scalability-special-interest-group">the SIG's documentation&lt;/a> to learn about upcoming meetings, its charter, and more. You can also join the &lt;a href="https://kubernetes.slack.com/archives/C09QZTRH7">#sig-scalability Slack channel&lt;/a> to see what it's like. We hope to see you join in to take advantage of this great opportunity to learn Kubernetes and contribute back at the same time.&lt;/p></description></item><item><title>Blog: Kong Ingress Controller and Service Mesh: Setting up Ingress to Istio on Kubernetes</title><link>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</link><pubDate>Wed, 18 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/18/kong-ingress-controller-and-istio-service-mesh/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Kevin Chen, Kong&lt;/p>
&lt;p>Kubernetes has become the de facto way to orchestrate containers and the services within services. But how do we give services outside our cluster access to what is within? Kubernetes comes with the Ingress API object that manages external access to services within a cluster.&lt;/p>
&lt;p>Ingress is a group of rules that will proxy inbound connections to endpoints defined by a backend. However, Kubernetes does not know what to do with Ingress resources without an Ingress controller, which is where an open source controller can come into play. In this post, we are going to use one option for this: the Kong Ingress Controller. The Kong Ingress Controller was open-sourced a year ago and recently reached one million downloads. In the recent 0.7 release, service mesh support was also added. Other features of this release include:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Built-In Kubernetes Admission Controller&lt;/strong>, which validates Custom Resource Definitions (CRD) as they are created or updated and rejects any invalid configurations.&lt;/li>
&lt;li>&lt;strong>In-memory Mode&lt;/strong> - Each pod’s controller actively configures the Kong container in its pod, which limits the blast radius of failure of a single container of Kong or controller container to that pod only.&lt;/li>
&lt;li>&lt;strong>Native gRPC Routing&lt;/strong> - gRPC traffic can now be routed via Kong Ingress Controller natively with support for method-based routing.&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/KIC-gRPC.png" alt="K4K-gRPC">&lt;/p>
&lt;p>If you would like a deeper dive into Kong Ingress Controller 0.7, please check out the &lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub repository&lt;/a>.&lt;/p>
&lt;p>But let’s get back to the service mesh support since that will be the main focal point of this blog post. Service mesh allows organizations to address microservices challenges related to security, reliability, and observability by abstracting inter-service communication into a mesh layer. But what if our mesh layer sits within Kubernetes and we still need to expose certain services beyond our cluster? Then you need an Ingress controller such as the Kong Ingress Controller. In this blog post, we’ll cover how to deploy Kong Ingress Controller as your Ingress layer to an Istio mesh. Let’s dive right in:&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/Kong-Ingress-Controller-and-Service-Mesh/k4k8s.png" alt="Kong Kubernetes Ingress Controller">&lt;/p>
&lt;h3 id="part-0-set-up-istio-on-kubernetes">Part 0: Set up Istio on Kubernetes&lt;/h3>
&lt;p>This blog will assume you have Istio set up on Kubernetes. If you need to catch up to this point, please check out the &lt;a href="https://istio.io/docs/setup/">Istio documentation&lt;/a>. It will walk you through setting up Istio on Kubernetes.&lt;/p>
&lt;h3 id="1-install-the-bookinfo-application">1. Install the Bookinfo Application&lt;/h3>
&lt;p>First, we need to label the namespaces that will host our application and Kong proxy. To label our default namespace where the bookinfo app sits, run this command:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace default istio-injection=enabled
namespace/default labeled
&lt;/code>&lt;/pre>&lt;p>Then create a new namespace that will be hosting our Kong gateway and the Ingress controller:&lt;/p>
&lt;pre>&lt;code>$ kubectl create namespace kong
namespace/kong created
&lt;/code>&lt;/pre>&lt;p>Because Kong will be sitting outside the default namespace, be sure you also label the Kong namespace with istio-injection enabled as well:&lt;/p>
&lt;pre>&lt;code>$ kubectl label namespace kong istio-injection=enabled
namespace/kong labeled
&lt;/code>&lt;/pre>&lt;p>Having both namespaces labeled &lt;code>istio-injection=enabled&lt;/code> is necessary. Or else the default configuration will not inject a sidecar container into the pods of your namespaces.&lt;/p>
&lt;p>Now deploy your BookInfo application with the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f http://bit.ly/bookinfoapp
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
&lt;/code>&lt;/pre>&lt;p>Let’s double-check our Services and Pods to make sure that we have it all set up correctly:&lt;/p>
&lt;pre>&lt;code>$ kubectl get services
NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE
details ClusterIP 10.97.125.254 &amp;lt;none&amp;gt; 9080/TCP 29s
kubernetes ClusterIP 10.96.0.1 &amp;lt;none&amp;gt; 443/TCP 29h
productpage ClusterIP 10.97.62.68 &amp;lt;none&amp;gt; 9080/TCP 28s
ratings ClusterIP 10.96.15.180 &amp;lt;none&amp;gt; 9080/TCP 28s
reviews ClusterIP 10.104.207.136 &amp;lt;none&amp;gt; 9080/TCP 28s
&lt;/code>&lt;/pre>&lt;p>You should see four new services: details, productpage, ratings, and reviews. None of them have an external IP so we will use the &lt;a href="https://github.com/Kong/kong">Kong gateway&lt;/a> to expose the necessary services. And to check pods, run the following command:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods
NAME READY STATUS RESTARTS AGE
details-v1-c5b5f496d-9wm29 2/2 Running 0 101s
productpage-v1-7d6cfb7dfd-5mc96 2/2 Running 0 100s
ratings-v1-f745cf57b-hmkwf 2/2 Running 0 101s
reviews-v1-85c474d9b8-kqcpt 2/2 Running 0 101s
reviews-v2-ccffdd984-9jnsj 2/2 Running 0 101s
reviews-v3-98dc67b68-nzw97 2/2 Running 0 101s
&lt;/code>&lt;/pre>&lt;p>This command outputs useful data, so let’s take a second to understand it. If you examine the READY column, each pod has two containers running: the service and an Envoy sidecar injected alongside it. Another thing to highlight is that there are three review pods but only 1 review service. The Envoy sidecar will load balance the traffic to three different review pods that contain different versions, giving us the ability to A/B test our changes. We have one step before we can access the deployed application. We need to add an additional annotation to the &lt;code>productpage&lt;/code> service. To do so, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate service productpage ingress.kubernetes.io/service-upstream=true
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Both the API gateway (Kong) and the service mesh (Istio) can handle the load-balancing. Without the additional &lt;code>ingress.kubernetes.io/service-upstream: &amp;quot;true&amp;quot;&lt;/code> annotation, Kong will try to load-balance by selecting its own endpoint/target from the productpage service. This causes Envoy to receive that pod’s IP as the upstream local address, instead of the service’s cluster IP. But we want the service's cluster IP so that Envoy can properly load balance.&lt;/p>
&lt;p>With that added, you should now be able to access your product page!&lt;/p>
&lt;pre>&lt;code>$ kubectl exec -it $(kubectl get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o &amp;quot;&amp;lt;title&amp;gt;.*&amp;lt;/title&amp;gt;&amp;quot;
&amp;lt;title&amp;gt;Simple Bookstore App&amp;lt;/title&amp;gt;
&lt;/code>&lt;/pre>&lt;h3 id="2-kong-kubernetes-ingress-controller-without-database">2. Kong Kubernetes Ingress Controller Without Database&lt;/h3>
&lt;p>To expose your services to the world, we will deploy Kong as the north-south traffic gateway. &lt;a href="https://github.com/Kong/kong/releases/tag/1.1.2">Kong 1.1&lt;/a> released with declarative configuration and DB-less mode. Declarative configuration allows you to specify the desired system state through a YAML or JSON file instead of a sequence of API calls. Using declarative config provides several key benefits to reduce complexity, increase automation and enhance system performance. And with the Kong Ingress Controller, any Ingress rules you apply to the cluster will automatically be configured on the Kong proxy. Let’s set up the Kong Ingress Controller and the actual Kong proxy first like this:&lt;/p>
&lt;pre>&lt;code>$ kubectl apply -f https://bit.ly/k4k8s
namespace/kong configured
customresourcedefinition.apiextensions.k8s.io/kongconsumers.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongcredentials.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongingresses.configuration.konghq.com created
customresourcedefinition.apiextensions.k8s.io/kongplugins.configuration.konghq.com created
serviceaccount/kong-serviceaccount created
clusterrole.rbac.authorization.k8s.io/kong-ingress-clusterrole created
clusterrolebinding.rbac.authorization.k8s.io/kong-ingress-clusterrole-nisa-binding created
configmap/kong-server-blocks created
service/kong-proxy created
service/kong-validation-webhook created
deployment.apps/ingress-kong created
&lt;/code>&lt;/pre>&lt;p>To check if the Kong pod is up and running, run:&lt;/p>
&lt;pre>&lt;code>$ kubectl get pods -n kong
NAME READY STATUS RESTARTS AGE
pod/ingress-kong-8b44c9856-9s42v 3/3 Running 0 2m26s
&lt;/code>&lt;/pre>&lt;p>There will be three containers within this pod. The first container is the Kong Gateway that will be the Ingress point to your cluster. The second container is the Ingress controller. It uses Ingress resources and updates the proxy to follow rules defined in the resource. And lastly, the third container is the Envoy proxy injected by Istio. Kong will route traffic through the Envoy sidecar proxy to the appropriate service. To send requests into the cluster via our newly deployed Kong Gateway, setup an environment variable with the a URL based on the IP address at which Kong is accessible.&lt;/p>
&lt;pre>&lt;code>$ export PROXY_URL=&amp;quot;$(minikube service -n kong kong-proxy --url | head -1)&amp;quot;
$ echo $PROXY_URL
http://192.168.99.100:32728
&lt;/code>&lt;/pre>&lt;p>Next, we need to change some configuration so that the side-car Envoy process can route the request correctly based on the host/authority header of the request. Run the following to stop the route from preserving host:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: configuration.konghq.com/v1
kind: KongIngress
metadata:
name: do-not-preserve-host
route:
preserve_host: false
upstream:
host_header: productpage.default.svc
&amp;quot; | kubectl apply -f -
kongingress.configuration.konghq.com/do-not-preserve-host created
&lt;/code>&lt;/pre>&lt;p>And annotate the existing productpage service to set service-upstream as true:&lt;/p>
&lt;pre>&lt;code>$ kubectl annotate svc productpage Ingress.kubernetes.io/service-upstream=&amp;quot;true&amp;quot;
service/productpage annotated
&lt;/code>&lt;/pre>&lt;p>Now that we have everything set up, we can look at how to use the Ingress resource to help route external traffic to the services within your Istio mesh. We’ll create an Ingress rule that routes all traffic with the path of &lt;code>/&lt;/code> to our productpage service:&lt;/p>
&lt;pre>&lt;code>$ echo &amp;quot;
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
name: productpage
annotations:
configuration.konghq.com: do-not-preserve-host
spec:
rules:
- http:
paths:
- path: /
backend:
serviceName: productpage
servicePort: 9080
&amp;quot; | kubectl apply -f -
ingress.extensions/productpage created
&lt;/code>&lt;/pre>&lt;p>And just like that, the Kong Ingress Controller is able to understand the rules you defined in the Ingress resource and routes it to the productpage service! To view the product page service’s GUI, go to &lt;code>$PROXY_URL/productpage&lt;/code> in your browser. Or to test it in your command line, try:&lt;/p>
&lt;pre>&lt;code>$ curl $PROXY_URL/productpage
&lt;/code>&lt;/pre>&lt;p>That is all I have for this walk-through. If you enjoyed the technologies used in this post, please check out their repositories since they are all open source and would love to have more contributors! Here are their links for your convenience:&lt;/p>
&lt;ul>
&lt;li>Kong: [&lt;a href="https://github.com/Kong/kubernetes-ingress-controller">GitHub&lt;/a>] [&lt;a href="https://twitter.com/thekonginc">Twitter&lt;/a>]&lt;/li>
&lt;li>Kubernetes: [&lt;a href="https://github.com/kubernetes/kubernetes">GitHub&lt;/a>] [&lt;a href="https://twitter.com/kubernetesio">Twitter&lt;/a>]&lt;/li>
&lt;li>Istio: [&lt;a href="https://github.com/istio/istio">GitHub&lt;/a>] [&lt;a href="https://twitter.com/IstioMesh">Twitter&lt;/a>]&lt;/li>
&lt;li>Envoy: [&lt;a href="https://github.com/envoyproxy/envoy">GitHub&lt;/a>] [&lt;a href="https://twitter.com/EnvoyProxy">Twitter&lt;/a>]&lt;/li>
&lt;/ul>
&lt;p>Thank you for following along!&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Postponed</title><link>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</link><pubDate>Wed, 04 Mar 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/03/04/contributor-summit-delayed/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Dawn Foster (VMware), Jorge Castro (VMware)&lt;/p>
&lt;p>The CNCF has announced that &lt;a href="https://events.linuxfoundation.org/kubecon-cloudnativecon-europe/attend/novel-coronavirus-update/">KubeCon + CloudNativeCon EU has been delayed&lt;/a> until July/August of 2020. As a result the Contributor Summit planning team is weighing options for how to proceed. Here’s the current plan:&lt;/p>
&lt;ul>
&lt;li>There will be an in-person Contributor Summit as planned when KubeCon + CloudNativeCon is rescheduled.&lt;/li>
&lt;li>We are looking at options for having additional virtual contributor activities in the meantime.&lt;/li>
&lt;/ul>
&lt;p>We will communicate via this blog and the usual communications channels on the final plan. Please bear with us as we adapt when we get more information. Thank you for being patient as the team pivots to bring you a great Contributor Summit!&lt;/p></description></item><item><title>Blog: Bring your ideas to the world with kubectl plugins</title><link>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</link><pubDate>Fri, 28 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/28/bring-your-ideas-to-the-world-with-kubectl-plugins/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Cornelius Weig (TNG Technology Consulting GmbH)&lt;/p>
&lt;p>&lt;code>kubectl&lt;/code> is the most critical tool to interact with Kubernetes and has to address multiple user personas, each with their own needs and opinions.
One way to make &lt;code>kubectl&lt;/code> do what you need is to build new functionality into &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;h2 id="challenges-with-building-commands-into-kubectl">Challenges with building commands into &lt;code>kubectl&lt;/code>&lt;/h2>
&lt;p>However, that's easier said than done. Being such an important cornerstone of
Kubernetes, any meaningful change to &lt;code>kubectl&lt;/code> needs to undergo a Kubernetes
Enhancement Proposal (KEP) where the intended change is discussed beforehand.&lt;/p>
&lt;p>When it comes to implementation, you'll find that &lt;code>kubectl&lt;/code> is an ingenious and
complex piece of engineering. It might take a long time to get used to
the processes and style of the codebase to get done what you want to achieve. Next
comes the review process which may go through several rounds until it meets all
the requirements of the Kubernetes maintainers -- after all, they need to take
over ownership of this feature and maintain it from the day it's merged.&lt;/p>
&lt;p>When everything goes well, you can finally rejoice. Your code will be shipped
with the next Kubernetes release. Well, that could mean you need to wait
another 3 months to ship your idea in &lt;code>kubectl&lt;/code> if you are unlucky.&lt;/p>
&lt;p>So this was the happy path where everything goes well. But there are good
reasons why your new functionality may never make it into &lt;code>kubectl&lt;/code>. For one,
&lt;code>kubectl&lt;/code> has a particular look and feel and violating that style will not be
acceptable by the maintainers. For example, an interactive command that
produces output with colors would be inconsistent with the rest of &lt;code>kubectl&lt;/code>.
Also, when it comes to tools or commands useful only to a minuscule proportion
of users, the maintainers may simply reject your proposal as &lt;code>kubectl&lt;/code> needs to
address common needs.&lt;/p>
&lt;p>But this doesn’t mean you can’t ship your ideas to &lt;code>kubectl&lt;/code> users.&lt;/p>
&lt;h2 id="what-if-you-didn-t-have-to-change-kubectl-to-add-functionality">What if you didn’t have to change &lt;code>kubectl&lt;/code> to add functionality?&lt;/h2>
&lt;p>This is where &lt;code>kubectl&lt;/code> &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">plugins&lt;/a> shine.
Since &lt;code>kubectl&lt;/code> v1.12, you can simply
drop executables into your &lt;code>PATH&lt;/code>, which follows the naming pattern
&lt;code>kubectl-myplugin&lt;/code>. Then you can execute this plugin as &lt;code>kubectl myplugin&lt;/code>, and
it will just feel like a normal sub-command of &lt;code>kubectl&lt;/code>.&lt;/p>
&lt;p>Plugins give you the opportunity to try out new experiences like terminal UIs,
colorful output, specialized functionality, or other innovative ideas. You can
go creative, as you’re the owner of your own plugin.&lt;/p>
&lt;p>Further, plugins offer safe experimentation space for commands you’d like to
propose to &lt;code>kubectl&lt;/code>. By pre-releasing as a plugin, you can push your
functionality faster to the end-users and quickly gather feedback. For example,
the &lt;a href="https://github.com/verb/kubectl-debug">kubectl-debug&lt;/a> plugin is proposed
to become a built-in command in &lt;code>kubectl&lt;/code> in a
&lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-cli/20190805-kubectl-debug.md">KEP&lt;/a>).
In the meanwhile, the plugin author can ship the functionality and collect
feedback using the plugin mechanism.&lt;/p>
&lt;h2 id="how-to-get-started-with-developing-plugins">How to get started with developing plugins&lt;/h2>
&lt;p>If you already have an idea for a plugin, how do you best make it happen?
First you have to ask yourself if you can implement it as a wrapper around
existing &lt;code>kubectl&lt;/code> functionality. If so, writing the plugin as a shell script
is often the best way forward, because the resulting plugin will be small,
works cross-platform, and has a high level of trust because it is not
compiled.&lt;/p>
&lt;p>On the other hand, if the plugin logic is complex, a general-purpose language
is usually better. The canonical choice here is Go, because you can use the
excellent &lt;code>client-go&lt;/code> library to interact with the Kubernetes API. The Kubernetes
maintained &lt;a href="https://github.com/kubernetes/sample-cli-plugin">sample-cli-plugin&lt;/a>
demonstrates some best practices and can be used as a template for new plugin
projects.&lt;/p>
&lt;p>When the development is done, you just need to ship your plugin to the
Kubernetes users. For the best plugin installation experience and discoverability,
you should consider doing so via the
&lt;a href="https://github.com/kubernetes-sigs/krew">krew&lt;/a> plugin manager. For an in-depth
discussion about the technical details around &lt;code>kubectl&lt;/code> plugins, refer to the
documentation on &lt;a href="https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">kubernetes.io&lt;/a>.&lt;/p></description></item><item><title>Blog: Contributor Summit Amsterdam Schedule Announced</title><link>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</link><pubDate>Tue, 18 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/18/contributor-summit-amsterdam-schedule-announced/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Jeffrey Sica (Red Hat), Amanda Katona (VMware)&lt;/p>
&lt;p>tl;dr &lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a> and the &lt;a href="https://kcseu2020.sched.com/">schedule is live&lt;/a> so register now and we’ll see you in Amsterdam!&lt;/p>
&lt;h2 id="kubernetes-contributor-summit">Kubernetes Contributor Summit&lt;/h2>
&lt;p>&lt;strong>Sunday, March 29, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>Evening Contributor Celebration:
&lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=KubeCon+Amsterdam+2020&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 22, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 18:00 - 21:00&lt;/li>
&lt;/ul>
&lt;p>&lt;strong>Monday, March 30, 2020&lt;/strong>&lt;/p>
&lt;ul>
&lt;li>All Day Contributor Summit:&lt;/li>
&lt;li>&lt;a href="https://www.rai.nl/en/">Amsterdam RAI&lt;/a>&lt;/li>
&lt;li>Address: &lt;a href="https://www.google.com/search?q=kubecon+amsterdam+2020&amp;amp;oq=kubecon+amste&amp;amp;aqs=chrome.0.35i39j69i57j0l4j69i61l2.3957j1j4&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&amp;amp;ibp=htl;events&amp;amp;rciv=evn&amp;amp;sa=X&amp;amp;ved=2ahUKEwiZoLvQ0dvnAhVST6wKHScBBZ8Q5bwDMAB6BAgSEAE#">Europaplein 24, 1078 GZ Amsterdam, Netherlands&lt;/a>&lt;/li>
&lt;li>Time: 09:00 - 17:00 (Breakfast at 08:00)&lt;/li>
&lt;/ul>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-02-18-Contributor-Summit-Amsterdam-Schedule-Announced/contribsummit.jpg" alt="Contributor Summit">&lt;/p>
&lt;p>Hello everyone and Happy 2020! It’s hard to believe that KubeCon EU 2020 is less than six weeks away, and with that another contributor summit! This year we have the pleasure of being in Amsterdam in early spring, so be sure to pack some warmer clothing. This summit looks to be exciting with a lot of fantastic community-driven content. We received &lt;strong>26&lt;/strong> submissions from the CFP. From that, the events team selected &lt;strong>12&lt;/strong> sessions. Each of the sessions falls into one of four categories:&lt;/p>
&lt;ul>
&lt;li>Community&lt;/li>
&lt;li>Contributor Improvement&lt;/li>
&lt;li>Sustainability&lt;/li>
&lt;li>In-depth Technical&lt;/li>
&lt;/ul>
&lt;p>On top of the presentations, there will be a dedicated Docs Sprint as well as the New Contributor Workshop 101 and 201 Sessions. All told, we will have five separate rooms of content throughout the day on Monday. Please &lt;strong>&lt;a href="https://kcseu2020.sched.com/">see the full schedule&lt;/a>&lt;/strong> to see what sessions you’d be interested in. We hope between the content provided and the inevitable hallway track, everyone has a fun and enriching experience.&lt;/p>
&lt;p>Speaking of fun, the social Sunday night should be a blast! We’re hosting this summit’s social close to the conference center, at &lt;a href="https://www.zuid-pool.nl/en/">ZuidPool&lt;/a>. There will be games, bingo, and unconference sign-up throughout the evening. It should be a relaxed way to kick off the week.&lt;/p>
&lt;p>&lt;a href="https://events.linuxfoundation.org/kubernetes-contributor-summit-europe/">Registration is open&lt;/a>! Space is limited so it’s always a good idea to register early.&lt;/p>
&lt;p>If you have any questions, reach out to the &lt;a href="https://github.com/kubernetes/community/tree/master/events/2020/03-contributor-summit#team">Amsterdam Team&lt;/a> on Slack in the &lt;a href="https://kubernetes.slack.com/archives/C7J893413">#contributor-summit&lt;/a> channel.&lt;/p>
&lt;p>Hope to see you there!&lt;/p></description></item><item><title>Blog: Deploying External OpenStack Cloud Provider with Kubeadm</title><link>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</link><pubDate>Fri, 07 Feb 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/02/07/deploying-external-openstack-cloud-provider-with-kubeadm/</guid><description>
&lt;p>This document describes how to install a single control-plane Kubernetes cluster v1.15 with kubeadm on CentOS, and then deploy an external OpenStack cloud provider and Cinder CSI plugin to use Cinder volumes as persistent volumes in Kubernetes.&lt;/p>
&lt;h3 id="preparation-in-openstack">Preparation in OpenStack&lt;/h3>
&lt;p>This cluster runs on OpenStack VMs, so let's create a few things in OpenStack first.&lt;/p>
&lt;ul>
&lt;li>A project/tenant for this Kubernetes cluster&lt;/li>
&lt;li>A user in this project for Kubernetes, to query node information and attach volumes etc&lt;/li>
&lt;li>A private network and subnet&lt;/li>
&lt;li>A router for this private network and connect it to a public network for floating IPs&lt;/li>
&lt;li>A security group for all Kubernetes VMs&lt;/li>
&lt;li>A VM as a control-plane node and a few VMs as worker nodes&lt;/li>
&lt;/ul>
&lt;p>The security group will have the following rules to open ports for Kubernetes.&lt;/p>
&lt;p>&lt;strong>Control-Plane Node&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>6443&lt;/td>
&lt;td>Kubernetes API Server&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>2379-2380&lt;/td>
&lt;td>etcd server client API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10251&lt;/td>
&lt;td>kube-scheduler&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10252&lt;/td>
&lt;td>kube-controller-manager&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10255&lt;/td>
&lt;td>Read-only Kubelet API&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Worker Nodes&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10250&lt;/td>
&lt;td>Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>10255&lt;/td>
&lt;td>Read-only Kubelet API&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>30000-32767&lt;/td>
&lt;td>NodePort Services&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>CNI ports on both control-plane and worker nodes&lt;/strong>&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Protocol&lt;/th>
&lt;th>Port Number&lt;/th>
&lt;th>Description&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>179&lt;/td>
&lt;td>Calico BGP network&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>9099&lt;/td>
&lt;td>Calico felix (health check)&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>8285&lt;/td>
&lt;td>Flannel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>8472&lt;/td>
&lt;td>Flannel&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>TCP&lt;/td>
&lt;td>6781-6784&lt;/td>
&lt;td>Weave Net&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>UDP&lt;/td>
&lt;td>6783-6784&lt;/td>
&lt;td>Weave Net&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>CNI specific ports are only required to be opened when that particular CNI plugin is used. In this guide, we will use Weave Net. Only the Weave Net ports (TCP 6781-6784 and UDP 6783-6784), will need to be opened in the security group.&lt;/p>
&lt;p>The control-plane node needs at least 2 cores and 4GB RAM. After the VM is launched, verify its hostname and make sure it is the same as the node name in Nova.
If the hostname is not resolvable, add it to &lt;code>/etc/hosts&lt;/code>.&lt;/p>
&lt;p>For example, if the VM is called master1, and it has an internal IP 192.168.1.4. Add that to &lt;code>/etc/hosts&lt;/code> and set hostname to master1.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#a2f">echo&lt;/span> &lt;span style="color:#b44">&amp;#34;192.168.1.4 master1&amp;#34;&lt;/span> &amp;gt;&amp;gt; /etc/hosts
hostnamectl set-hostname master1
&lt;/code>&lt;/pre>&lt;/div>&lt;h3 id="install-docker-and-kubernetes">Install Docker and Kubernetes&lt;/h3>
&lt;p>Next, we'll follow the official documents to install docker and Kubernetes using kubeadm.&lt;/p>
&lt;p>Install Docker following the steps from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/">container runtime&lt;/a> documentation.&lt;/p>
&lt;p>Note that it is a &lt;a href="https://kubernetes.io/docs/setup/production-environment/container-runtimes/#cgroup-drivers">best practice to use systemd as the cgroup driver&lt;/a> for Kubernetes.
If you use an internal container registry, add them to the docker config.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># Install Docker CE&lt;/span>
&lt;span style="color:#080;font-style:italic">## Set up the repository&lt;/span>
&lt;span style="color:#080;font-style:italic">### Install required packages.&lt;/span>
yum install yum-utils device-mapper-persistent-data lvm2
&lt;span style="color:#080;font-style:italic">### Add Docker repository.&lt;/span>
yum-config-manager &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> --add-repo &lt;span style="color:#b62;font-weight:bold">\
&lt;/span>&lt;span style="color:#b62;font-weight:bold">&lt;/span> https://download.docker.com/linux/centos/docker-ce.repo
&lt;span style="color:#080;font-style:italic">## Install Docker CE.&lt;/span>
yum update &lt;span style="color:#666">&amp;amp;&amp;amp;&lt;/span> yum install docker-ce-18.06.2.ce
&lt;span style="color:#080;font-style:italic">## Create /etc/docker directory.&lt;/span>
mkdir /etc/docker
&lt;span style="color:#080;font-style:italic"># Configure the Docker daemon&lt;/span>
cat &amp;gt; /etc/docker/daemon.json &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">{
&lt;/span>&lt;span style="color:#b44"> &amp;#34;exec-opts&amp;#34;: [&amp;#34;native.cgroupdriver=systemd&amp;#34;],
&lt;/span>&lt;span style="color:#b44"> &amp;#34;log-driver&amp;#34;: &amp;#34;json-file&amp;#34;,
&lt;/span>&lt;span style="color:#b44"> &amp;#34;log-opts&amp;#34;: {
&lt;/span>&lt;span style="color:#b44"> &amp;#34;max-size&amp;#34;: &amp;#34;100m&amp;#34;
&lt;/span>&lt;span style="color:#b44"> },
&lt;/span>&lt;span style="color:#b44"> &amp;#34;storage-driver&amp;#34;: &amp;#34;overlay2&amp;#34;,
&lt;/span>&lt;span style="color:#b44"> &amp;#34;storage-opts&amp;#34;: [
&lt;/span>&lt;span style="color:#b44"> &amp;#34;overlay2.override_kernel_check=true&amp;#34;
&lt;/span>&lt;span style="color:#b44"> ]
&lt;/span>&lt;span style="color:#b44">}
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
mkdir -p /etc/systemd/system/docker.service.d
&lt;span style="color:#080;font-style:italic"># Restart Docker&lt;/span>
systemctl daemon-reload
systemctl restart docker
systemctl &lt;span style="color:#a2f">enable&lt;/span> docker
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Install kubeadm following the steps from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/">Installing Kubeadm&lt;/a> documentation.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; /etc/yum.repos.d/kubernetes.repo
&lt;/span>&lt;span style="color:#b44">[kubernetes]
&lt;/span>&lt;span style="color:#b44">name=Kubernetes
&lt;/span>&lt;span style="color:#b44">baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64
&lt;/span>&lt;span style="color:#b44">enabled=1
&lt;/span>&lt;span style="color:#b44">gpgcheck=1
&lt;/span>&lt;span style="color:#b44">repo_gpgcheck=1
&lt;/span>&lt;span style="color:#b44">gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;span style="color:#080;font-style:italic"># Set SELinux in permissive mode (effectively disabling it)&lt;/span>
&lt;span style="color:#080;font-style:italic"># Caveat: In a production environment you may not want to disable SELinux, please refer to Kubernetes documents about SELinux&lt;/span>
setenforce &lt;span style="color:#666">0&lt;/span>
sed -i &lt;span style="color:#b44">&amp;#39;s/^SELINUX=enforcing$/SELINUX=permissive/&amp;#39;&lt;/span> /etc/selinux/config
yum install -y kubelet kubeadm kubectl --disableexcludes&lt;span style="color:#666">=&lt;/span>kubernetes
systemctl &lt;span style="color:#a2f">enable&lt;/span> --now kubelet
cat &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF &amp;gt; /etc/sysctl.d/k8s.conf
&lt;/span>&lt;span style="color:#b44">net.bridge.bridge-nf-call-ip6tables = 1
&lt;/span>&lt;span style="color:#b44">net.bridge.bridge-nf-call-iptables = 1
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
sysctl --system
&lt;span style="color:#080;font-style:italic"># check if br_netfilter module is loaded&lt;/span>
lsmod | grep br_netfilter
&lt;span style="color:#080;font-style:italic"># if not, load it explicitly with&lt;/span>
modprobe br_netfilter
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The official document about how to create a single control-plane cluster can be found from the &lt;a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/">Creating a single control-plane cluster with kubeadm&lt;/a> documentation.&lt;/p>
&lt;p>We'll largely follow that document but also add additional things for the cloud provider.
To make things more clear, we'll use a &lt;code>kubeadm-config.yml&lt;/code> for the control-plane node.
In this config we specify to use an external OpenStack cloud provider, and where to find its config.
We also enable storage API in API server's runtime config so we can use OpenStack volumes as persistent volumes in Kubernetes.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>InitConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeRegistration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeletExtraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cloud-provider&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ClusterConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubernetesVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;v1.15.1&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiServer&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">enable-admission-plugins&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NodeRestriction&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">runtime-config&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;storage.k8s.io/v1=true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">controllerManager&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">external-cloud-volume-plugin&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">extraVolumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/etc/kubernetes/cloud-config&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">pathType&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>File&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">networking&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceSubnet&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10.96.0.0/12&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">podSubnet&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;10.224.0.0/16&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">dnsDomain&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;cluster.local&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Now we'll create the cloud config, &lt;code>/etc/kubernetes/cloud-config&lt;/code>, for OpenStack.
Note that the tenant here is the one we created for all Kubernetes VMs in the beginning.
All VMs should be launched in this project/tenant.
In addition you need to create a user in this tenant for Kubernetes to do queries.
The ca-file is the CA root certificate for OpenStack's API endpoint, for example &lt;code>https://openstack.cloud:5000/v3&lt;/code>
At the time of writing the cloud provider doesn't allow insecure connections (skip CA check).&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-ini" data-lang="ini">&lt;span style="color:#a2f;font-weight:bold">[Global]&lt;/span>
&lt;span style="color:#b44">region&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">RegionOne&lt;/span>
&lt;span style="color:#b44">username&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">username&lt;/span>
&lt;span style="color:#b44">password&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">password&lt;/span>
&lt;span style="color:#b44">auth-url&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">https://openstack.cloud:5000/v3&lt;/span>
&lt;span style="color:#b44">tenant-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">14ba698c0aec4fd6b7dc8c310f664009&lt;/span>
&lt;span style="color:#b44">domain-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">default&lt;/span>
&lt;span style="color:#b44">ca-file&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">/etc/kubernetes/ca.pem&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[LoadBalancer]&lt;/span>
&lt;span style="color:#b44">subnet-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">b4a9a292-ea48-4125-9fb2-8be2628cb7a1&lt;/span>
&lt;span style="color:#b44">floating-network-id&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">bc8a590a-5d65-4525-98f3-f7ef29c727d5&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[BlockStorage]&lt;/span>
&lt;span style="color:#b44">bs-version&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">v2&lt;/span>
&lt;span style="color:#a2f;font-weight:bold">[Networking]&lt;/span>
&lt;span style="color:#b44">public-network-name&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">public&lt;/span>
&lt;span style="color:#b44">ipv6-support-disabled&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">false&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next run kubeadm to initiate the control-plane node&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubeadm init --config&lt;span style="color:#666">=&lt;/span>kubeadm-config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>With the initialization completed, copy admin config to .kube&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell"> mkdir -p &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube
sudo cp -i /etc/kubernetes/admin.conf &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube/config
sudo chown &lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>id -u&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>:&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>id -g&lt;span style="color:#a2f;font-weight:bold">)&lt;/span> &lt;span style="color:#b8860b">$HOME&lt;/span>/.kube/config
&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this stage, the control-plane node is created but not ready. All the nodes have the taint &lt;code>node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule&lt;/code> and are waiting to be initialized by the cloud-controller-manager.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl describe no master1
Name: master1
Roles: master
......
Taints: node-role.kubernetes.io/master:NoSchedule
node.cloudprovider.kubernetes.io/uninitialized=true:NoSchedule
node.kubernetes.io/not-ready:NoSchedule
......
&lt;/code>&lt;/pre>&lt;p>Now deploy the OpenStack cloud controller manager into the cluster, following &lt;a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-controller-manager-with-kubeadm.md">using controller manager with kubeadm&lt;/a>.&lt;/p>
&lt;p>Create a secret with the cloud-config for the openstack cloud provider.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create secret -n kube-system generic cloud-config --from-literal&lt;span style="color:#666">=&lt;/span>cloud.conf&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>cat /etc/kubernetes/cloud-config&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> --dry-run -o yaml &amp;gt; cloud-config-secret.yaml
kubectl apply -f cloud-config-secret.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Get the CA certificate for OpenStack API endpoints and put that into &lt;code>/etc/kubernetes/ca.pem&lt;/code>.&lt;/p>
&lt;p>Create RBAC resources.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-roles.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/cluster/addons/rbac/cloud-controller-manager-role-bindings.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>We'll run the OpenStack cloud controller manager as a DaemonSet rather than a pod.
The manager will only run on the control-plane node, so if there are multiple control-plane nodes, multiple pods will be run for high availability.
Create &lt;code>openstack-cloud-controller-manager-ds.yaml&lt;/code> containing the following manifests, then apply it.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ServiceAccount&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DaemonSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">updateStrategy&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>RollingUpdate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">k8s-app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeSelector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">node-role.kubernetes.io/master&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">runAsUser&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1001&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">tolerations&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node.cloudprovider.kubernetes.io/uninitialized&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;true&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node-role.kubernetes.io/master&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">effect&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NoSchedule&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">key&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node.kubernetes.io/not-ready&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccountName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/openstack-cloud-controller-manager:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/openstack-cloud-controller-manager&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --v=&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --cloud-config=$(CLOUD_CONFIG)&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --cloud-provider=openstack&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --use-service-account-credentials=&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- --address=&lt;span style="color:#666">127.0.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes/pki&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/ssl/certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>flexvolume-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cpu&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>200m&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostNetwork&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/usr/libexec/kubernetes/kubelet-plugins/volume/exec&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>flexvolume-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes/pki&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>k8s-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/ssl/certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-certs&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config-volume&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the controller manager is running, it will query OpenStack to get information about the nodes and remove the taint. In the node info you'll see the VM's UUID in OpenStack.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl describe no master1
Name: master1
Roles: master
......
Taints: node-role.kubernetes.io/master:NoSchedule
node.kubernetes.io/not-ready:NoSchedule
......
sage:docker: network plugin is not ready: cni config uninitialized
......
PodCIDR: 10.224.0.0/24
ProviderID: openstack:///548e3c46-2477-4ce2-968b-3de1314560a5
&lt;/code>&lt;/pre>&lt;p>Now install your favourite CNI and the control-plane node will become ready.&lt;/p>
&lt;p>For example, to install Weave Net, run this command:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f &lt;span style="color:#b44">&amp;#34;https://cloud.weave.works/k8s/net?k8s-version=&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>kubectl version | base64 | tr -d &lt;span style="color:#b44">&amp;#39;\n&amp;#39;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Next we'll set up worker nodes.&lt;/p>
&lt;p>Firstly, install docker and kubeadm in the same way as how they were installed in the control-plane node.
To join them to the cluster we need a token and ca cert hash from the output of control-plane node installation.
If it is expired or lost we can recreate it using these commands.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">&lt;span style="color:#080;font-style:italic"># check if token is expired&lt;/span>
kubeadm token list
&lt;span style="color:#080;font-style:italic"># re-create token and show join command&lt;/span>
kubeadm token create --print-join-command
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create &lt;code>kubeadm-config.yml&lt;/code> for worker nodes with the above token and ca cert hash.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubeadm.k8s.io/v1beta2&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">discovery&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">bootstrapToken&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiServerEndpoint&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">192.168.1.7&lt;/span>:&lt;span style="color:#666">6443&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">token&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>0c0z4p.dnafh6vnmouus569&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">caCertHashes&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;sha256:fcb3e956a6880c05fc9d09714424b827f57a6fdc8afc44497180905946527adf&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>JoinConfiguration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">nodeRegistration&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">kubeletExtraArgs&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">cloud-provider&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;external&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>apiServerEndpoint is the control-plane node, token and caCertHashes can be taken from the join command printed in the output of 'kubeadm token create' command.&lt;/p>
&lt;p>Run kubeadm and the worker nodes will be joined to the cluster.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubeadm join --config kubeadm-config.yml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>At this stage we'll have a working Kubernetes cluster with an external OpenStack cloud provider.
The provider tells Kubernetes about the mapping between Kubernetes nodes and OpenStack VMs.
If Kubernetes wants to attach a persistent volume to a pod, it can find out which OpenStack VM the pod is running on from the mapping, and attach the underlying OpenStack volume to the VM accordingly.&lt;/p>
&lt;h3 id="deploy-cinder-csi">Deploy Cinder CSI&lt;/h3>
&lt;p>The integration with Cinder is provided by an external Cinder CSI plugin, as described in the &lt;a href="https://github.com/kubernetes/cloud-provider-openstack/blob/master/docs/using-cinder-csi-plugin.md">Cinder CSI&lt;/a> documentation.&lt;/p>
&lt;p>We'll perform the following steps to install the Cinder CSI plugin.
Firstly, create a secret with CA certs for OpenStack's API endpoints. It is the same cert file as what we use in cloud provider above.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl create secret -n kube-system generic openstack-ca-cert --from-literal&lt;span style="color:#666">=&lt;/span>ca.pem&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span>&lt;span style="color:#a2f;font-weight:bold">$(&lt;/span>cat /etc/kubernetes/ca.pem&lt;span style="color:#a2f;font-weight:bold">)&lt;/span>&lt;span style="color:#b44">&amp;#34;&lt;/span> --dry-run -o yaml &amp;gt; openstack-ca-cert.yaml
kubectl apply -f openstack-ca-cert.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then create RBAC resources.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-shell" data-lang="shell">kubectl apply -f https://raw.githubusercontent.com/kubernetes/cloud-provider-openstack/release-1.15/manifests/cinder-csi-plugin/cinder-csi-controllerplugin-rbac.yaml
kubectl apply -f https://github.com/kubernetes/cloud-provider-openstack/raw/release-1.15/manifests/cinder-csi-plugin/cinder-csi-nodeplugin-rbac.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>The Cinder CSI plugin includes a controller plugin and a node plugin.
The controller communicates with Kubernetes APIs and Cinder APIs to create/attach/detach/delete Cinder volumes. The node plugin in-turn runs on each worker node to bind a storage device (attached volume) to a pod, and unbind it during deletion.
Create &lt;code>cinder-csi-controllerplugin.yaml&lt;/code> and apply it to create csi controller.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controller-service&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>dummy&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">port&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">12345&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>---&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StatefulSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;csi-cinder-controller-service&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">replicas&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controllerplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-controller-sa&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-attacher&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-attacher:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-provisioner&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-provisioner:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--provisioner=csi-cinderplugin&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-snapshotter&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-snapshotter:v1&lt;span style="color:#666">.0.1&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--connection-timeout=15s&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Always&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args &lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cluster=$(CLUSTER_NAME)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NODE_ID&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CSI_ENDPOINT&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>unix://csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLUSTER_NAME&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/csi/sockets/pluginproxy/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Create &lt;code>cinder-csi-nodeplugin.yaml&lt;/code> and apply it to create csi node.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DaemonSet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>apps/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">namespace&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kube-system&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">selector&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">matchLabels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">template&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">labels&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">app&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-nodeplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">serviceAccount&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinder-node-sa&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostNetwork&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>node-driver-registrar&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>quay.io/k8scsi/csi-node-driver-registrar:v1&lt;span style="color:#666">.1.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--v=5&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--csi-address=$(ADDRESS)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--kubelet-registration-path=$(DRIVER_REG_SOCK_PATH)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">lifecycle&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">preStop&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">exec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">command&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;/bin/sh&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;-c&amp;#34;&lt;/span>,&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;rm -rf /registration/cinder.csi.openstack.org /registration/cinder.csi.openstack.org-reg.sock&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ADDRESS&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DRIVER_REG_SOCK_PATH&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins/cinder.csi.openstack.org/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>KUBE_NODE_NAME&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registration-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/registration&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">securityContext&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">privileged&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">capabilities&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">add&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>[&lt;span style="color:#b44">&amp;#34;SYS_ADMIN&amp;#34;&lt;/span>]&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">allowPrivilegeEscalation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>docker.io/k8scloudprovider/cinder-csi-plugin:v1&lt;span style="color:#666">.15.0&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">args &lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- /bin/cinder-csi-plugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--nodeid=$(NODE_ID)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--endpoint=$(CSI_ENDPOINT)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#b44">&amp;#34;--cloud-config=$(CLOUD_CONFIG)&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">env&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>NODE_ID&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">valueFrom&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldRef&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">fieldPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>spec.nodeName&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CSI_ENDPOINT&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>unix://csi/csi.sock&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>CLOUD_CONFIG&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">value&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config/cloud.conf&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">imagePullPolicy&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;IfNotPresent&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/csi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-mount-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/pods&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Bidirectional&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;Bidirectional&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-cloud-data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/cloud/data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-probe-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/dev&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPropagation&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;HostToContainer&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/etc/kubernetes&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">readOnly&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">true&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>socket-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins/cinder.csi.openstack.org&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>DirectoryOrCreate&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>registration-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/plugins_registry/&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>kubelet-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-mount-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/kubelet/pods&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-cloud-data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/var/lib/cloud/data&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>pods-probe-dir&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPath&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">path&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>/dev&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">type&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Directory&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>secret-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>cloud-config&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secret&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">secretName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>openstack-ca-cert&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When they are both running, create a storage class for Cinder.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>storage.k8s.io/v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>StorageClass&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-sc-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">provisioner&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>Then we can create a PVC with this class.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>PersistentVolumeClaim&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myvol&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">accessModes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- ReadWriteOnce&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">resources&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">requests&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storage&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>1Gi&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">storageClassName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>csi-sc-cinderplugin&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the PVC is created, a Cinder volume is created correspondingly.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># kubectl get pvc
NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE
myvol Bound pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad 1Gi RWO csi-sc-cinderplugin 3s
&lt;/code>&lt;/pre>&lt;p>In OpenStack the volume name will match the Kubernetes persistent volume generated name. In this example it would be: &lt;em>pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad&lt;/em>&lt;/p>
&lt;p>Now we can create a pod with the PVC.&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-yaml" data-lang="yaml">&lt;span style="color:#a2f;font-weight:bold">apiVersion&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>v1&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">kind&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>Pod&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">metadata&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb">&lt;/span>&lt;span style="color:#a2f;font-weight:bold">spec&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containers&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">image&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>nginx&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">ports&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>web&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">containerPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">80&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">hostPort&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#666">8081&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">protocol&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>TCP&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumeMounts&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">mountPath&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#b44">&amp;#34;/usr/share/nginx/html&amp;#34;&lt;/span>&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypd&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">volumes&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>- &lt;span style="color:#a2f;font-weight:bold">name&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>mypd&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">persistentVolumeClaim&lt;/span>:&lt;span style="color:#bbb">
&lt;/span>&lt;span style="color:#bbb"> &lt;/span>&lt;span style="color:#a2f;font-weight:bold">claimName&lt;/span>:&lt;span style="color:#bbb"> &lt;/span>myvol&lt;span style="color:#bbb">
&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>When the pod is running, the volume will be attached to the pod.
If we go back to OpenStack, we can see the Cinder volume is mounted to the worker node where the pod is running on.&lt;/p>
&lt;pre>&lt;code class="language-console" data-lang="console"># openstack volume show 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Field | Value |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| attachments | [{u'server_id': u'1c5e1439-edfa-40ed-91fe-2a0e12bc7eb4', u'attachment_id': u'11a15b30-5c24-41d4-86d9-d92823983a32', u'attached_at': u'2019-07-24T05:02:34.000000', u'host_name': u'compute-6', u'volume_id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f', u'device': u'/dev/vdb', u'id': u'6b5f3296-b0eb-40cd-bd4f-2067a0d6287f'}] |
| availability_zone | nova |
| bootable | false |
| consistencygroup_id | None |
| created_at | 2019-07-24T05:02:18.000000 |
| description | Created by OpenStack Cinder CSI driver |
| encrypted | False |
| id | 6b5f3296-b0eb-40cd-bd4f-2067a0d6287f |
| migration_status | None |
| multiattach | False |
| name | pvc-14b8bc68-6c4c-4dc6-ad79-4cb29a81faad |
| os-vol-host-attr:host | rbd:volumes@rbd#rbd |
| os-vol-mig-status-attr:migstat | None |
| os-vol-mig-status-attr:name_id | None |
| os-vol-tenant-attr:tenant_id | 14ba698c0aec4fd6b7dc8c310f664009 |
| properties | attached_mode='rw', cinder.csi.openstack.org/cluster='kubernetes' |
| replication_status | None |
| size | 1 |
| snapshot_id | None |
| source_volid | None |
| status | in-use |
| type | rbd |
| updated_at | 2019-07-24T05:02:35.000000 |
| user_id | 5f6a7a06f4e3456c890130d56babf591 |
+--------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
&lt;/code>&lt;/pre>&lt;h3 id="summary">Summary&lt;/h3>
&lt;p>In this walk-through, we deployed a Kubernetes cluster on OpenStack VMs and integrated it with OpenStack using an external OpenStack cloud provider. Then on this Kubernetes cluster we deployed Cinder CSI plugin which can create Cinder volumes and expose them in Kubernetes as persistent volumes.&lt;/p></description></item><item><title>Blog: KubeInvaders - Gamified Chaos Engineering Tool for Kubernetes</title><link>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</link><pubDate>Wed, 22 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/22/kubeinvaders-gamified-chaos-engineering-tool-for-kubernetes/</guid><description>
&lt;p>&lt;strong>Authors&lt;/strong> Eugenio Marzo, Sourcesense&lt;/p>
&lt;p>Some months ago, I released my latest project called KubeInvaders. The
first time I shared it with the community was during an Openshift
Commons Briefing session. Kubenvaders is a Gamified Chaos Engineering
tool for Kubernetes and Openshift and helps test how resilient your
Kubernetes cluster is, in a fun way.&lt;/p>
&lt;p>It is like Space Invaders, but the aliens are pods.&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img1.png" alt="">&lt;/p>
&lt;p>During my presentation at Codemotion Milan 2019, I started saying &amp;quot;of
course you can do it with few lines of Bash, but it is boring.&amp;quot;&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img2.png" alt="">&lt;/p>
&lt;p>Using the code above you can kill random pods across a Kubernetes cluster, but I
think it is much more fun with the spaceship of KubeInvaders.&lt;/p>
&lt;p>I published the code at
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders">https://github.com/lucky-sideburn/KubeInvaders&lt;/a>
and there is a little community that is growing gradually. Some people
love to use it for demo sessions killing pods on a big screen.&lt;/p>
&lt;p>&lt;img src="https://github.com/lucky-sideburn/KubeInvaders-kubernetes-post/raw/master/img3.png" alt="">&lt;/p>
&lt;h2 id="how-to-install-kubeinvaders">How to install KubeInvaders&lt;/h2>
&lt;p>I defined multiples modes to install it:&lt;/p>
&lt;ol>
&lt;li>
&lt;p>Helm Chart
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders">https://github.com/lucky-sideburn/KubeInvaders/tree/master/helm-charts/kubeinvaders&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manual Installation for Openshift using a template
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift">https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-openshift&lt;/a>&lt;/p>
&lt;/li>
&lt;li>
&lt;p>Manual Installation for Kubernetes
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes">https://github.com/lucky-sideburn/KubeInvaders#install-kubeinvaders-on-kubernetes&lt;/a>&lt;/p>
&lt;/li>
&lt;/ol>
&lt;p>The preferred way, of course, is with a Helm chart:&lt;/p>
&lt;pre>&lt;code># Please set target_namespace to set your target namespace!
helm install --set-string target_namespace=&amp;quot;namespace1,namespace2&amp;quot; \
--name kubeinvaders --namespace kubeinvaders ./helm-charts/kubeinvaders
&lt;/code>&lt;/pre>&lt;h2 id="how-to-use-kubeinvaders">How to use KubeInvaders&lt;/h2>
&lt;p>Once it is installed on your cluster you can use the following
functionalities:&lt;/p>
&lt;ul>
&lt;li>Key 'a' — Switch to automatic pilot&lt;/li>
&lt;li>Key 'm' — Switch to manual pilot&lt;/li>
&lt;li>Key 'i' — Show pod's name. Move the ship towards an alien&lt;/li>
&lt;li>Key 'h' — Print help&lt;/li>
&lt;li>Key 'n' — Jump between different namespaces (my favorite feature!)&lt;/li>
&lt;/ul>
&lt;h2 id="tuning-kubeinvaders">Tuning KubeInvaders&lt;/h2>
&lt;p>At Codemotion Milan 2019, my colleagues and I organized a desk with a
game station for playing KubeInvaders. People had to fight with Kubernetes to
win a t-shirt.&lt;/p>
&lt;p>If you have pods that require a few seconds to start, you may lose. It
is possible to set the complexity of the game with these parameters as
environmment variables in the Kubernetes deployment:&lt;/p>
&lt;ul>
&lt;li>ALIENPROXIMITY — Reduce this value to increase the distance between aliens;&lt;/li>
&lt;li>HITSLIMIT — Seconds of CPU time to wait before shooting;&lt;/li>
&lt;li>UPDATETIME — Seconds to wait before updating pod status (you can set also 0.x Es: 0.5);&lt;/li>
&lt;/ul>
&lt;p>The result is a harder game experience against the machine.&lt;/p>
&lt;h2 id="use-cases">Use cases&lt;/h2>
&lt;p>Adopting chaos engineering strategies for your production environment is
really useful, because it is the only way to test if a system supports
unexpected destructive events.&lt;/p>
&lt;p>KubeInvaders is a game — so please do not take it too seriously! — but it demonstrates
some important use cases:&lt;/p>
&lt;ul>
&lt;li>Test how resilient Kubernetes clusters are on unexpected pod deletion&lt;/li>
&lt;li>Collect metrics like pod restart time&lt;/li>
&lt;li>Tune readiness probes&lt;/li>
&lt;/ul>
&lt;h2 id="next-steps">Next steps&lt;/h2>
&lt;p>I want to continue to add some cool features and integrate it into a
Kubernetes dashboard because I am planning to transform it into a
&amp;quot;Gamified Chaos Engineering and Development Tool for Kubernetes&amp;quot;, to help
developer to interact with deployments in a Kubernetes environment. For
example:&lt;/p>
&lt;ul>
&lt;li>Point to the aliens to get pod logs&lt;/li>
&lt;li>Deploy Helm charts by shooting some particular objects&lt;/li>
&lt;li>Read messages stored in a specific label present in a deployment&lt;/li>
&lt;/ul>
&lt;p>Please feel free to contribute to
&lt;a href="https://github.com/lucky-sideburn/KubeInvaders">https://github.com/lucky-sideburn/KubeInvaders&lt;/a>
and stay updated following #kubeinvaders news &lt;a href="https://twitter.com/luckysideburn">on Twitter&lt;/a>.&lt;/p></description></item><item><title>Blog: CSI Ephemeral Inline Volumes</title><link>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/21/csi-ephemeral-inline-volumes/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>Typically, volumes provided by an external storage driver in
Kubernetes are &lt;em>persistent&lt;/em>, with a lifecycle that is completely
independent of pods or (as a special case) loosely coupled to the
first pod which uses a volume (&lt;a href="https://kubernetes.io/docs/concepts/storage/storage-classes/#volume-binding-mode">late binding
mode&lt;/a>).
The mechanism for requesting and defining such volumes in Kubernetes
are &lt;a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/">Persistent Volume Claim (PVC) and Persistent Volume
(PV)&lt;/a>
objects. Originally, volumes that are backed by a Container Storage Interface
(CSI) driver could only be used via this PVC/PV mechanism.&lt;/p>
&lt;p>But there are also use cases for data volumes whose content and
lifecycle is tied to a pod. For example, a driver might populate a
volume with dynamically created secrets that are specific to the
application running in the pod. Such volumes need to be created
together with a pod and can be deleted as part of pod termination
(&lt;em>ephemeral&lt;/em>). They get defined as part of the pod spec (&lt;em>inline&lt;/em>).&lt;/p>
&lt;p>Since Kubernetes 1.15, CSI drivers can also be used for such
&lt;em>ephemeral inline&lt;/em> volumes. The &lt;a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">CSIInlineVolume feature
gate&lt;/a>
had to be set to enable it in 1.15 because support was still in alpha
state. In 1.16, the feature reached beta state, which typically means
that it is enabled in clusters by default.&lt;/p>
&lt;p>CSI drivers have to be adapted to support this because although two
existing CSI gRPC calls are used (&lt;code>NodePublishVolume&lt;/code> and &lt;code>NodeUnpublishVolume&lt;/code>),
the way how they are
used is different and not covered by the CSI spec: for ephemeral
volumes, only &lt;code>NodePublishVolume&lt;/code> is invoked by &lt;code>kubelet&lt;/code> when asking
the CSI driver for a volume. All other calls
(like &lt;code>CreateVolume&lt;/code>, &lt;code>NodeStageVolume&lt;/code>, etc.) are skipped. The volume
parameters are provided in the pod spec and from there copied into the
&lt;code>NodePublishVolumeRequest.volume_context&lt;/code> field. There are currently
no standardized parameters; even common ones like size must be
provided in a format that is defined by the CSI driver. Likewise, only
&lt;code>NodeUnpublishVolume&lt;/code> gets called after the pod has terminated and the
volume needs to be removed.&lt;/p>
&lt;p>Initially, the assumption was that CSI drivers would be specifically
written to provide either persistent or ephemeral volumes. But there
are also drivers which provide storage that is useful in both modes:
for example, &lt;a href="https://github.com/intel/pmem-csi">PMEM-CSI&lt;/a> manages
persistent memory (PMEM), a new kind of local storage that is provided
by &lt;a href="https://www.intel.com/content/www/us/en/architecture-and-technology/optane-dc-persistent-memory.html">Intel® Optane™ DC Persistent
Memory&lt;/a>. Such
memory is useful both as persistent data storage (faster than normal SSDs)
and as ephemeral scratch space (higher capacity than DRAM).&lt;/p>
&lt;p>Therefore the support in Kubernetes 1.16 was extended:&lt;/p>
&lt;ul>
&lt;li>Kubernetes and users can determine which kind of volumes a driver
supports via the &lt;code>volumeLifecycleModes&lt;/code> field in the &lt;a href="https://kubernetes-csi.github.io/docs/csi-driver-object.html#what-fields-does-the-csidriver-object-have">&lt;code>CSIDriver&lt;/code>
object&lt;/a>.&lt;/li>
&lt;li>Drivers can get information about the volume mode by enabling the
&lt;a href="https://kubernetes-csi.github.io/docs/pod-info.html">&amp;quot;pod info on
mount&amp;quot;&lt;/a> feature
which then will add the new &lt;code>csi.storage.k8s.io/ephemeral&lt;/code> entry to
the &lt;code>NodePublishRequest.volume_context&lt;/code>.&lt;/li>
&lt;/ul>
&lt;p>For more information about implementing support of ephemeral inline
volumes in a CSI driver, see the &lt;a href="https://kubernetes-csi.github.io/docs/ephemeral-local-volumes.html">Kubernetes-CSI
documentation&lt;/a>
and the &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-storage/20190122-csi-inline-volumes.md">original design
document&lt;/a>.&lt;/p>
&lt;p>What follows in this blog post are usage examples based on real drivers
and a summary at the end.&lt;/p>
&lt;h1 id="examples">Examples&lt;/h1>
&lt;h2 id="pmem-csi-https-github-com-intel-pmem-csi">&lt;a href="https://github.com/intel/pmem-csi">PMEM-CSI&lt;/a>&lt;/h2>
&lt;p>Support for ephemeral inline volumes was added in &lt;a href="https://github.com/intel/pmem-csi/releases/tag/v0.6.0">release
v0.6.0&lt;/a>. The
driver can be used on hosts with real Intel® Optane™ DC Persistent
Memory, on &lt;a href="https://github.com/intel/pmem-csi/blob/v0.6.0/examples/gce.md">special machines in
GCE&lt;/a> or
with hardware emulated by QEMU. The latter is fully &lt;a href="https://github.com/intel/pmem-csi/tree/v0.6.0#qemu-and-kubernetes">integrated into
the
makefile&lt;/a>
and only needs Go, Docker and KVM, so that approach was used for this
example:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">git clone --branch release-0.6 https://github.com/intel/pmem-csi
&lt;span style="color:#a2f">cd&lt;/span> pmem-csi
&lt;span style="color:#b8860b">TEST_DISTRO&lt;/span>&lt;span style="color:#666">=&lt;/span>clear &lt;span style="color:#b8860b">TEST_DISTRO_VERSION&lt;/span>&lt;span style="color:#666">=&lt;/span>&lt;span style="color:#666">32080&lt;/span> &lt;span style="color:#b8860b">TEST_PMEM_REGISTRY&lt;/span>&lt;span style="color:#666">=&lt;/span>intel make start
&lt;/code>&lt;/pre>&lt;/div>&lt;p>Bringing up the four-node cluster can take a while but eventually should end with:&lt;/p>
&lt;pre>&lt;code>The test cluster is ready. Log in with /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm, run kubectl once logged in.
Alternatively, KUBECONFIG=/work/pmem-csi/_work/pmem-govm/kube.config can also be used directly.
To try out the pmem-csi driver persistent volumes:
...
To try out the pmem-csi driver ephemeral volumes:
cat deploy/kubernetes-1.17/pmem-app-ephemeral.yaml | /work/pmem-csi/_work/pmem-govm/ssh-pmem-govm kubectl create -f -
&lt;/code>&lt;/pre>&lt;p>&lt;code>deploy/kubernetes-1.17/pmem-app-ephemeral.yaml&lt;/code> specifies one volume:&lt;/p>
&lt;pre>&lt;code>kind: Pod
apiVersion: v1
metadata:
name: my-csi-app-inline-volume
spec:
containers:
- name: my-frontend
image: busybox
command: [ &amp;quot;sleep&amp;quot;, &amp;quot;100000&amp;quot; ]
volumeMounts:
- mountPath: &amp;quot;/data&amp;quot;
name: my-csi-volume
volumes:
- name: my-csi-volume
csi:
driver: pmem-csi.intel.com
fsType: &amp;quot;xfs&amp;quot;
volumeAttributes:
size: &amp;quot;2Gi&amp;quot;
nsmode: &amp;quot;fsdax&amp;quot;
&lt;/code>&lt;/pre>&lt;p>Once we have created that pod, we can inspect the result:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl describe pods/my-csi-app-inline-volume
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Name: my-csi-app-inline-volume
...
Volumes:
my-csi-volume:
Type: CSI (a Container Storage Interface (CSI) volume source)
Driver: pmem-csi.intel.com
FSType: xfs
ReadOnly: false
VolumeAttributes: nsmode=fsdax
size=2Gi
&lt;/code>&lt;/pre>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl &lt;span style="color:#a2f">exec&lt;/span> my-csi-app-inline-volume -- df -h /data
&lt;/code>&lt;/pre>&lt;/div>&lt;pre>&lt;code>Filesystem Size Used Available Use% Mounted on
/dev/ndbus0region0fsdax/d7eb073f2ab1937b88531fce28e19aa385e93696
1.9G 34.2M 1.8G 2% /data
&lt;/code>&lt;/pre>&lt;h2 id="image-populator-https-github-com-kubernetes-csi-csi-driver-image-populator">&lt;a href="https://github.com/kubernetes-csi/csi-driver-image-populator">Image Populator&lt;/a>&lt;/h2>
&lt;p>The image populator automatically unpacks a container image and makes
its content available as an ephemeral volume. It's still in
development, but canary images are already available which can be
installed with:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-csidriverinfo.yaml
kubectl create -f https://github.com/kubernetes-csi/csi-driver-image-populator/raw/master/deploy/kubernetes-1.16/csi-image-daemonset.yaml
&lt;/code>&lt;/pre>&lt;/div>&lt;p>This example pod will run nginx and have it serve data that
comes from the &lt;code>kfox1111/misc:test&lt;/code> image:&lt;/p>
&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl create -f - &lt;span style="color:#b44">&amp;lt;&amp;lt;EOF
&lt;/span>&lt;span style="color:#b44">apiVersion: v1
&lt;/span>&lt;span style="color:#b44">kind: Pod
&lt;/span>&lt;span style="color:#b44">metadata:
&lt;/span>&lt;span style="color:#b44"> name: nginx
&lt;/span>&lt;span style="color:#b44">spec:
&lt;/span>&lt;span style="color:#b44"> containers:
&lt;/span>&lt;span style="color:#b44"> - name: nginx
&lt;/span>&lt;span style="color:#b44"> image: nginx:1.16-alpine
&lt;/span>&lt;span style="color:#b44"> ports:
&lt;/span>&lt;span style="color:#b44"> - containerPort: 80
&lt;/span>&lt;span style="color:#b44"> volumeMounts:
&lt;/span>&lt;span style="color:#b44"> - name: data
&lt;/span>&lt;span style="color:#b44"> mountPath: /usr/share/nginx/html
&lt;/span>&lt;span style="color:#b44"> volumes:
&lt;/span>&lt;span style="color:#b44"> - name: data
&lt;/span>&lt;span style="color:#b44"> csi:
&lt;/span>&lt;span style="color:#b44"> driver: image.csi.k8s.io
&lt;/span>&lt;span style="color:#b44"> volumeAttributes:
&lt;/span>&lt;span style="color:#b44"> image: kfox1111/misc:test
&lt;/span>&lt;span style="color:#b44">EOF&lt;/span>
&lt;/code>&lt;/pre>&lt;/div>&lt;div class="highlight">&lt;pre style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4">&lt;code class="language-sh" data-lang="sh">kubectl &lt;span style="color:#a2f">exec&lt;/span> nginx -- cat /usr/share/nginx/html/test
&lt;/code>&lt;/pre>&lt;/div>&lt;p>That &lt;code>test&lt;/code> file just contains a single word:&lt;/p>
&lt;pre>&lt;code>testing
&lt;/code>&lt;/pre>&lt;p>Such data containers can be built with Dockerfiles such as:&lt;/p>
&lt;pre>&lt;code>FROM scratch
COPY index.html /index.html
&lt;/code>&lt;/pre>&lt;h2 id="cert-manager-csi-https-github-com-jetstack-cert-manager-csi">&lt;a href="https://github.com/jetstack/cert-manager-csi">cert-manager-csi&lt;/a>&lt;/h2>
&lt;p>cert-manager-csi works together with
&lt;a href="https://github.com/jetstack/cert-manager">cert-manager&lt;/a>. The goal for
this driver is to facilitate requesting and mounting certificate key
pairs to pods seamlessly. This is useful for facilitating mTLS, or
otherwise securing connections of pods with guaranteed present
certificates whilst having all of the features that cert-manager
provides. This project is experimental.&lt;/p>
&lt;h1 id="next-steps">Next steps&lt;/h1>
&lt;p>One of the issues with ephemeral inline volumes is that pods get
scheduled by Kubernetes onto nodes without knowing anything about the
currently available storage on that node. Once the pod has been
scheduled, the CSI driver must make the volume available one that
node. If that is currently not possible, the pod cannot start. This
will be retried until eventually the volume becomes ready. The
&lt;a href="https://github.com/kubernetes/enhancements/pull/1353">storage capacity tracking
KEP&lt;/a> is an
attempt to address this problem.&lt;/p>
&lt;p>A related KEP introduces a &lt;a href="https://github.com/kubernetes/enhancements/pull/1409">standardized size
parameter&lt;/a>.&lt;/p>
&lt;p>Currently, CSI ephemeral inline volumes stay in beta while issues like
these are getting discussed. Your feedback is needed to decide how to
proceed with this feature. For the KEPs, the two PRs linked to above
is a good place to comment. The SIG Storage also &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#meetings">meets
regularly&lt;/a>
and can be reached via &lt;a href="https://github.com/kubernetes/community/tree/master/sig-storage#contact">Slack and a mailing
list&lt;/a>.&lt;/p></description></item><item><title>Blog: Reviewing 2019 in Docs</title><link>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</link><pubDate>Tue, 21 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/21/reviewing-2019-in-docs/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Zach Corleissen (Cloud Native Computing Foundation)&lt;/p>
&lt;p>Hi, folks! I'm one of the co-chairs for the Kubernetes documentation special interest group (SIG Docs). This blog post is a review of SIG Docs in 2019. Our contributors did amazing work last year, and I want to highlight their successes.&lt;/p>
&lt;p>Although I review 2019 in this post, my goal is to point forward to 2020. I observe some trends in SIG Docs–some good, others troubling. I want to raise visibility before those challenges increase in severity.&lt;/p>
&lt;h2 id="the-good">The good&lt;/h2>
&lt;p>There was much to celebrate in SIG Docs in 2019.&lt;/p>
&lt;p>Kubernetes docs started the year with three localizations in progress. By the end of the year, we ended with ten localizations available, four of which (Chinese, French, Japanese, Korean) are reasonably complete. The Korean and French teams deserve special mentions for their contributions to git best practices across all localizations (Korean team) and help bootstrapping other localizations (French team).&lt;/p>
&lt;p>Despite significant transition over the year, SIG Docs &lt;a href="https://k8s.devstats.cncf.io/d/44/pr-time-to-approve-and-merge?orgId=1&amp;amp;var-period=w&amp;amp;var-repogroup_name=SIG%20Docs&amp;amp;var-apichange=All&amp;amp;var-size_name=All&amp;amp;var-kind_name=All">improved its review velocity&lt;/a>, with a median review time from PR open to merge of just over 24 hours.&lt;/p>
&lt;p>Issue triage improved significantly in both volume and speed, largely due to the efforts of GitHub users @sftim, @tengqm, and @kbhawkey.&lt;/p>
&lt;p>Doc sprints remain valuable at KubeCon contributor days, introducing new contributors to Kubernetes documentation.&lt;/p>
&lt;p>The docs component of Kubernetes quarterly releases improved over 2019, thanks to iterative playbook improvements from release leads and their teams.&lt;/p>
&lt;p>Site traffic increased over the year. The website ended the year with ~6 million page views per month in December, up from ~5M page views in January. The kubernetes.io website had 851k site visitors in October, a new all-time high. Reader satisfaction &lt;a href="https://kubernetes.io/blog/2019/10/29/kubernetes-documentation-end-user-survey/">remains general&lt;/a>.&lt;/p>
&lt;p>We onboarded a new SIG chair: @jimangel, a Cloud Architect at General Motors. Jim was a docs contributor for a year, during which he led the 1.14 docs release, before stepping up as chair.&lt;/p>
&lt;h2 id="the-not-so-good">The not so good&lt;/h2>
&lt;p>While reader satisfaction is decent, &lt;strong>most respondents indicated dissatisfaction with stale content&lt;/strong> in every area: concepts, tasks, tutorials, and reference. Additionally, readers requested more diagrams, advanced conceptual content, and code samples—things that technical writers excel at providing.&lt;/p>
&lt;p>SIG Docs continues to solve how best to handle &lt;a href="https://github.com/kubernetes/enhancements/pull/1327">third-party content&lt;/a>. &lt;strong>There's too much vendor content on kubernetes.io&lt;/strong>, and guidelines for adding or rejecting third-party content remain unclear. The discussion so far has been powerful, including pushback demanding greater collaborative input—a powerful reminder that Kubernetes is in all ways a communal effort.&lt;/p>
&lt;p>We're in the middle of our third chair transition in 18 months. Each chair transition has been healthy and collegial, but it's still a lot of turnover in a short time. Chairing any open source project is difficult, but especially so with SIG Docs. Chairship of SIG Docs requires a steep learning curve across multiple domains: docs (both written and generated from spec), information architecture, specialized contribution paths (for example, localization), how to run a release cycle, website development, CI/CD, community management, on and on. It's a role that requires multiple people to function successfully without burning people out. Training replacements is time-intensive.&lt;/p>
&lt;p>Perhaps most pressing in the Not So Good category is that SIG Docs currently has only one technical writer dedicated full-time to Kubernetes docs. This has impacts on Kubernetes docs: some obvious, some less so.&lt;/p>
&lt;h2 id="impacts-of-understaffing-on-kubernetes-docs">Impacts of understaffing on Kubernetes docs&lt;/h2>
&lt;blockquote class="twitter-tweet">&lt;p lang="en" dir="ltr">Me today: &lt;a href="https://t.co/cDpHOWEsjf">pic.twitter.com/cDpHOWEsjf&lt;/a>&lt;/p>&amp;mdash; Benjamin Elder (@BenTheElder) &lt;a href="https://twitter.com/BenTheElder/status/1215453579651104768?ref_src=twsrc%5Etfw">January 10, 2020&lt;/a>&lt;/blockquote> &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8">&lt;/script>
&lt;p>If Kubernetes continues through 2020 without more technical writers dedicated to the docs, here's what I see as the most likely possibilities.&lt;/p>
&lt;h3 id="but-first-a-disclaimer">But first, a disclaimer&lt;/h3>
&lt;blockquote class="caution callout">
&lt;div>&lt;strong>Caution:&lt;/strong> It is very hard to predict, especially the future.
-Niels Bohr&lt;/div>
&lt;/blockquote>
&lt;p>Some of my predictions are almost certainly wrong. Any errors are mine alone.&lt;/p>
&lt;p>That said...&lt;/p>
&lt;h3 id="effects-in-2020">Effects in 2020&lt;/h3>
&lt;p>Current levels of function aren't self-sustaining. Even with a strong playbook, the release cycle still requires expert support from at least one (and usually two) chairs during every cycle. Without fail, each release breaks in new and unexpected ways, and it requires familiarity and expertise to diagnose and resolve. As chairs continue to cycle—and to be clear, regular transitions are part of a healthy project—we accrue the risks associated with a pool lacking sufficient professional depth and employer support.&lt;/p>
&lt;p>Oddly enough, one of the challenges to staffing is that the docs appear good enough. Based on site analytics and survey responses, readers are pleased with the quality of the docs. When folks visit the site, they generally find what they need and behave like satisfied visitors.&lt;/p>
&lt;p>The danger is that this will change over time: slowly with occasional losses of function, annoying at first, then increasingly critical. The more time passes without adequate staffing, the more difficult and costly fixes will become.&lt;/p>
&lt;p>I suspect this is true because the challenges we face now at decent levels of reader satisfaction are already difficult to fix. API reference generation is complex and brittle; the site's UI is outdated; and our most consistent requests are for more tutorials, advanced concepts, diagrams, and code samples, all of which require ongoing, dedicated time to create.&lt;/p>
&lt;p>&lt;strong>Release support remains strong.&lt;/strong>&lt;/p>
&lt;p>The release team continues a solid habit of leaving each successive team with better support than the previous release. This mostly takes the form of iterative improvements to the &lt;a href="https://github.com/kubernetes/community/tree/master/sig-release#docs-lead">docs release playbook&lt;/a>, producing better documentation and reducing siloed knowledge.&lt;/p>
&lt;p>&lt;strong>Staleness accelerates.&lt;/strong>&lt;/p>
&lt;p>Conceptual content becomes less accurate or relevant as features change or deprecate. Tutorial content degrades for the same reason.&lt;/p>
&lt;p>The content structure will also degrade: the categories of concepts, tasks, and tutorials are legacy categories that may not best fit the needs of current readers, let alone future ones.&lt;/p>
&lt;p>Cruft accumulates for both readers and contributors. Reference docs become increasingly brittle without intervention.&lt;/p>
&lt;p>&lt;strong>Critical knowledge vanishes.&lt;/strong>&lt;/p>
&lt;p>As I mentioned previously, SIG Docs has a wide range of functions, some with a steep learning curve. As contributors change roles or jobs, their expertise and availability will diminish or reduce to zero. Contributors with specific knowledge may not be available for consultation, exposing critical vulnerabilities in docs function. Specific examples include reference generation and chair leadership.&lt;/p>
&lt;h3 id="that-s-a-lot-to-take-in">That's a lot to take in&lt;/h3>
&lt;p>It's difficult to strike a balance between the importance of SIG Docs' work to the community and our users, the joy it brings me personally, and the fact that things can't remain as they are without significant negative impacts (eventually). SIG Docs is by no means dying; it's a vibrant community with active contributors doing cool things. It's also a community with some critical knowledge and capacity shortages that can only be remedied with trained, paid staff dedicated to documentation.&lt;/p>
&lt;h2 id="what-the-community-can-do-for-healthy-docs">What the community can do for healthy docs&lt;/h2>
&lt;p>Hire technical writers dedicated to Kubernetes docs. Support advanced content creation, not just release docs and incremental feature updates.&lt;/p>
&lt;p>Thanks, and Happy 2020.&lt;/p></description></item><item><title>Blog: Kubernetes on MIPS</title><link>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</link><pubDate>Wed, 15 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/15/kubernetes-on-mips/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> TimYin Shi, Dominic Yin, Wang Zhan, Jessica Jiang, Will Cai, Jeffrey Gao, Simon Sun (Inspur)&lt;/p>
&lt;h2 id="background">Background&lt;/h2>
&lt;p>&lt;a href="https://en.wikipedia.org/wiki/MIPS_architecture">MIPS&lt;/a> (Microprocessor without Interlocked Pipelined Stages) is a reduced instruction set computer (RISC) instruction set architecture (ISA), appeared in 1981 and developed by MIPS Technologies. Now MIPS architecture is widely used in many electronic products.&lt;/p>
&lt;p>&lt;a href="https://kubernetes.io">Kubernetes&lt;/a> has officially supported a variety of CPU architectures such as x86, arm/arm64, ppc64le, s390x. However, it's a pity that Kubernetes doesn't support MIPS. With the widespread use of cloud native technology, users under MIPS architecture also have an urgent demand for Kubernetes on MIPS.&lt;/p>
&lt;h2 id="achievements">Achievements&lt;/h2>
&lt;p>For many years, to enrich the ecology of the open-source community, we have been working on adjusting MIPS architecture for Kubernetes use cases. With the continuous iterative optimization and the performance improvement of the MIPS CPU, we have made some breakthrough progresses on the mips64el platform.&lt;/p>
&lt;p>Over the years, we have been actively participating in the Kubernetes community and have rich experience in the using and optimization of Kubernetes technology. Recently, we tried to adapt the MIPS architecture platform for Kubernetes and achieved a new a stage on that journey. The team has completed migration and adaptation of Kubernetes and related components, built not only a stable and highly available MIPS cluster but also completed the conformance test for Kubernetes v1.16.2.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/kubernetes-on-mips.png" alt="Kubernetes on MIPS">&lt;/p>
&lt;p>&lt;em>Figure 1 Kubernetes on MIPS&lt;/em>&lt;/p>
&lt;h2 id="k8s-mips-component-build">K8S-MIPS component build&lt;/h2>
&lt;p>Almost all native cloud components related to Kubernetes do not provide a MIPS version installation package or image. The prerequisite of deploying Kubernetes on the MIPS platform is to compile and build all required components on the mips64el platform. These components include:&lt;/p>
&lt;ul>
&lt;li>golang&lt;/li>
&lt;li>docker-ce&lt;/li>
&lt;li>hyperkube&lt;/li>
&lt;li>pause&lt;/li>
&lt;li>etcd&lt;/li>
&lt;li>calico&lt;/li>
&lt;li>coredns&lt;/li>
&lt;li>metrics-server&lt;/li>
&lt;/ul>
&lt;p>Thanks to the excellent design of Golang and its good support for the MIPS platform, the compilation processes of the above cloud native components are greatly simplified. First of all, we compiled Golang on the latest stable version for the mips64el platform, and then we compiled most of the above components with source code.&lt;/p>
&lt;p>During the compilation processes, we inevitably encountered many platform compatibility problems, such as a Golang system call compatibility problem (syscall), typecasting of syscall. Stat_t from uint32 to uint64, patching for EpollEvent, and so on.&lt;/p>
&lt;p>To build K8S-MIPS components, we used cross-compilation technology. Our process involved integrating a QEMU tool to translate MIPS CPU instructions and modifying the build script of Kubernetes and E2E image script of Kubernetes, Hyperkube, and E2E test images on MIPS architecture.&lt;/p>
&lt;p>After successfully building the above components, we use tools such as kubespray and kubeadm to complete kubernetes cluster construction.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th>Name&lt;/th>
&lt;th>Version&lt;/th>
&lt;th>MIPS Repository&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>golang on MIPS&lt;/td>
&lt;td>1.12.5&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>docker-ce on MIPS&lt;/td>
&lt;td>18.09.8&lt;/td>
&lt;td>-&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>metrics-server for CKE on MIPS&lt;/td>
&lt;td>0.3.2&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/metrics-server-mips64el:v0.3.2&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>etcd for CKE on MIPS&lt;/td>
&lt;td>3.2.26&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/etcd/etcd-mips64el:v3.2.26&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>pause for CKE on MIPS&lt;/td>
&lt;td>3.1&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/pause-mips64el:3.1&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>hyperkube for CKE on MIPS&lt;/td>
&lt;td>1.14.3&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/hyperkube-mips64el:v1.14.3&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>coredns for CKE on MIPS&lt;/td>
&lt;td>1.6.5&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/kubernetes/coredns-mips64el:v1.6.5&lt;/code>&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>calico for CKE on MIPS&lt;/td>
&lt;td>3.8.0&lt;/td>
&lt;td>&lt;code>registry.inspurcloud.cn/library/cke/calico/cni-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/ctl-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/node-mips64el:v3.8.0&lt;/code> &lt;code>registry.inspurcloud.cn/library/cke/calico/kube-controllers-mips64el:v3.8.0&lt;/code>&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>&lt;strong>Note&lt;/strong>: CKE is a Kubernetes-based cloud container engine launched by Inspur&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/k8s-mips-cluster-components.png" alt="K8S-MIPS Cluster Components">&lt;/p>
&lt;p>&lt;em>Figure 2 K8S-MIPS Cluster Components&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cpu-architecture.png" alt="CPU Architecture">&lt;/p>
&lt;p>&lt;em>Figure 3 CPU Architecture&lt;/em>&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/cluster-node-information.png" alt="Cluster Node Information">&lt;/p>
&lt;p>&lt;em>Figure 4 Cluster Node Information&lt;/em>&lt;/p>
&lt;h2 id="run-k8s-conformance-test">Run K8S Conformance Test&lt;/h2>
&lt;p>The most straightforward way to verify the stability and availability of the K8S-MIPS cluster is to run a Kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.2/cluster/images/conformance/README.md">conformance test&lt;/a>.&lt;/p>
&lt;p>Conformance is a standalone container to launch Kubernetes end-to-end tests for conformance testing.&lt;/p>
&lt;p>Once the test has started, it launches several pods for various end-to-end tests. The source code of those images used by these pods is mostly from &lt;code>kubernetes/test/images&lt;/code>, and the built images are at &lt;code>gcr.io/kubernetes-e2e-test-images&lt;/code>. Since there are no MIPS images in the repository, we must first build all needed images to run the test.&lt;/p>
&lt;h3 id="build-needed-images-for-test">Build needed images for test&lt;/h3>
&lt;p>The first step is to find all needed images for the test. We can run &lt;code>sonobuoy images-p e2e&lt;/code> command to list all images, or we can find those images in &lt;a href="https://github.com/kubernetes/kubernetes/blob/master/test/utils/image/manifest.go">/test/utils/image/manifest.go&lt;/a>. Although Kubernetes officially has a complete Makefile and shell-script that provides commands for building test images, there are still a number of architecture-related issues that have not been resolved, such as the incompatibilities of base images and dependencies. So we cannot directly build mips64el architecture images by executing these commands.&lt;/p>
&lt;p>Most test images are in golang, then compiled into binaries and built as Docker image based on the corresponding Dockerfile. These images are easy to build. But note that most images are using alpine as their base image, which does not officially support mips64el architecture for now. For this moment, we are unable to make mips64el version of &lt;a href="https://www.alpinelinux.org/">alpine&lt;/a>, so we have to replace the alpine to existing MIPS images, such as Debian-stretch, fedora, ubuntu. Replacing the base image also requires replacing the command to install the dependencies, even the version of these dependencies.&lt;/p>
&lt;p>Some images are not in &lt;code>kubernetes/test/images&lt;/code>, such as &lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>. There is no clear documentation explaining where these images are locaated, though we found the source code in repository &lt;a href="https://github.com/GoogleCloudPlatform/kubernetes-engine-samples">github.com/GoogleCloudPlatform/kubernetes-engine-samples&lt;/a>. We soon ran into new problems: to build these google sample images, we have to build the base image it uses, even the base image of the base images, such as &lt;code>php:5-apache&lt;/code>, &lt;code>redis&lt;/code>, and &lt;code>perl&lt;/code>.&lt;/p>
&lt;p>After a long process of building an image, we finished with about four dozen images, including the images used by the test pod, and the base images. The last step before we run the tests is to place all those images into every node in the cluster and make sure the Pod image pull policy is &lt;code>imagePullPolicy: ifNotPresent&lt;/code>.&lt;/p>
&lt;p>Here are some of the images we built：&lt;/p>
&lt;ul>
&lt;li>&lt;code>docker.io/library/busybox:1.29&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.14-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/nginx:1.15-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/perl:5.26&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/httpd:2.4.38-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>docker.io/library/redis:5.0.5-alpine&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/conformance:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-containers/hyperkube:v1.16.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/google-samples/gb-frontend:v6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/agnhost:2.6&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/apparmor-loader:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/dnsutils:1.1&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/echoserver:2.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/ipc-utils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/jessie-dnsutils:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/kitten:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/metadata-concealment:1.2&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest-user:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/mounttest:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nautilus:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonewprivs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/nonroot:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer-controller:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/resource-consumer:1.5&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/sample-apiserver:1.10&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/test-webserver:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/gluster:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/iscsi:2.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/nfs:1.0&lt;/code>&lt;/li>
&lt;li>&lt;code>gcr.io/kubernetes-e2e-test-images/volume/rbd:1.0.1&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/etcd:3.3.15&lt;/code>&lt;/li>
&lt;li>&lt;code>k8s.gcr.io/pause:3.1&lt;/code>&lt;/li>
&lt;/ul>
&lt;p>Finally, we ran the tests and got the test result, include &lt;code>e2e.log&lt;/code>, which showed that all test cases passed. Additionally, we submitted our test result to &lt;a href="https://github.com/cncf/k8s-conformance">k8s-conformance&lt;/a> as a &lt;a href="https://github.com/cncf/k8s-conformance/pull/779">pull request&lt;/a>.&lt;/p>
&lt;p>&lt;img src="https://kubernetes.io/images/blog/2020-01-15-Kubernetes-on-MIPS/pull-request-for-conformance-test-results.png" alt="Pull request for conformance test results">&lt;/p>
&lt;p>&lt;em>Figure 5 Pull request for conformance test results&lt;/em>&lt;/p>
&lt;h2 id="what-s-next">What's next&lt;/h2>
&lt;p>We built the kubernetes-MIPS component manually and finished the conformance test, which verified the feasibility of Kubernetes On the MIPS platform and greatly enhanced our confidence in promoting the support of the MIPS architecture by Kubernetes.&lt;/p>
&lt;p>In the future, we plan to actively contribute our experience and achievements to the community, submit PR, and patch for MIPS. We hope that more developers and companies in the community join us and promote Kubernetes on MIPS.&lt;/p>
&lt;p>Contribution plan：&lt;/p>
&lt;ul>
&lt;li>contribute the source of e2e test images for MIPS&lt;/li>
&lt;li>contribute the source of hyperkube for MIPS&lt;/li>
&lt;li>contribute the source of deploy tools like kubeadm for MIPS&lt;/li>
&lt;/ul>
&lt;hr></description></item><item><title>Blog: Announcing the Kubernetes bug bounty program</title><link>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</link><pubDate>Tue, 14 Jan 2020 09:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2020/01/14/kubernetes-bug-bounty-announcement/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Maya Kaczorowski and Tim Allclair, Google, on behalf of the &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Kubernetes Product Security Committee&lt;/a>&lt;/p>
&lt;p>Today, the &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Kubernetes Product Security Committee&lt;/a> is launching a &lt;a href="https://hackerone.com/kubernetes">new bug bounty program&lt;/a>, funded by the &lt;a href="https://www.cncf.io/">CNCF&lt;/a>, to reward researchers finding security vulnerabilities in Kubernetes.&lt;/p>
&lt;h2 id="setting-up-a-new-bug-bounty-program">Setting up a new bug bounty program&lt;/h2>
&lt;p>We aimed to set up this bug bounty program as transparently as possible, with &lt;a href="https://docs.google.com/document/d/1dvlQsOGODhY3blKpjTg6UXzRdPzv5y8V55RD_Pbo7ag/edit#heading=h.7t1efwpev42p">an initial proposal&lt;/a>, &lt;a href="https://github.com/kubernetes/kubernetes/issues/73079">evaluation of vendors&lt;/a>, and &lt;a href="https://github.com/kubernetes/community/blob/master/contributors/guide/bug-bounty.md">working draft of the components in scope&lt;/a>. Once we onboarded the selected bug bounty program vendor, &lt;a href="https://www.hackerone.com/">HackerOne&lt;/a>, these documents were further refined based on the feedback from HackerOne, as well as what was learned in the recent &lt;a href="https://github.com/kubernetes/community/blob/master/wg-security-audit/findings/Kubernetes%20Final%20Report.pdf">Kubernetes security audit&lt;/a>. The bug bounty program has been in a private release for several months now, with invited researchers able to submit bugs and help us test the triage process. After almost two years since the initial proposal, the program is now ready for all security researchers to contribute!&lt;/p>
&lt;p>What’s exciting is that this is rare: a bug bounty for an open-source infrastructure tool. Some open-source bug bounty programs exist, such as the &lt;a href="https://internetbugbounty.org/">Internet Bug Bounty&lt;/a>, this mostly covers core components that are consistently deployed across environments; but most bug bounties are still for hosted web apps. In fact, with more than&lt;a href="https://www.cncf.io/certification/kcsp/"> 100 certified distributions of Kubernetes&lt;/a>, the bug bounty program needs to apply to the Kubernetes code that powers all of them. By far, the most time-consuming challenge here has been ensuring that the program provider (HackerOne) and their researchers who do the first line triage have the awareness of Kubernetes and the ability to easily test the validity of a reported bug. As part of the bootstrapping process, HackerOne had their team pass the &lt;a href="https://www.cncf.io/certification/cka/">Certified Kubernetes Administrator&lt;/a> (CKA) exam.&lt;/p>
&lt;h2 id="what-s-in-scope">What’s in scope&lt;/h2>
&lt;p>The bug bounty scope covers code from the main Kubernetes organizations on GitHub, as well as continuous integration, release, and documentation artifacts. Basically, most content you’d think of as ‘core’ Kubernetes, included at &lt;a href="https://github.com/kubernetes">https://github.com/kubernetes&lt;/a>, is in scope. We’re particularly interested in cluster attacks, such as privilege escalations, authentication bugs, and remote code execution in the kubelet or API server. Any information leak about a workload, or unexpected permission changes is also of interest. Stepping back from the cluster admin’s view of the world, you’re also encouraged to look at the Kubernetes supply chain, including the build and release processes, which would allow any unauthorized access to commits, or the ability to publish unauthorized artifacts.&lt;/p>
&lt;p>Notably out of scope is the community management tooling, e.g., the Kubernetes mailing lists or Slack channel. Container escapes, attacks on the Linux kernel, or other dependencies, such as etcd, are also out of scope and should be reported to the appropriate party. We would still appreciate that any Kubernetes vulnerability, even if not in scope for the bug bounty, be &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability">disclosed privately&lt;/a> to the Kubernetes Product Security Committee. See the full scope on the &lt;a href="https://hackerone.com/kubernetes">program reporting page&lt;/a>.&lt;/p>
&lt;h2 id="how-kubernetes-handles-vulnerabilities-and-disclosures">How Kubernetes handles vulnerabilities and disclosures&lt;/h2>
&lt;p>Kubernetes’ &lt;a href="https://github.com/kubernetes/community/tree/master/committee-product-security">Product Security Committee&lt;/a> is a group of security-focused maintainers who are responsible for receiving and responding to reports of security issues in Kubernetes. This follows the documented &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/">security vulnerability response process&lt;/a>, which includes initial triage, assessing impact, generating and rolling out a fix.&lt;/p>
&lt;p>With our bug bounty program, initial triage and initial assessment are handled by the bug bounty provider, in this case, HackerOne, enabling us better scale our limited Kubernetes security experts to handle only valid reports. Nothing else in this process is changing - the Product Security Committee will continue to develop fixes, build private patches, and coordinate special security releases. New releases with security patches will be announced at &lt;a href="https://groups.google.com/forum/#!forum/kubernetes-security-announce">kubernetes-security-announce@googlegroups.com&lt;/a>.&lt;/p>
&lt;p>If you want to report a bug, you don’t need to use the bug bounty - you can still follow the &lt;a href="https://kubernetes.io/docs/reference/issues-security/security/#report-a-vulnerability">existing process&lt;/a> and report what you’ve found at &lt;a href="mailto:security@kubernetes.io">security@kubernetes.io&lt;/a>.&lt;/p>
&lt;h2 id="get-started">Get started&lt;/h2>
&lt;p>Just as many organizations support open source by hiring developers, paying bug bounties directly supports security researchers. This bug bounty is a critical step for Kubernetes to build up its community of security researchers and reward their hard work.&lt;/p>
&lt;p>If you’re a security researcher, and new to Kubernetes, check out these resources to learn more and get started bug hunting:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>Hardening guides&lt;/strong>
&lt;ul>
&lt;li>Kubernetes.io: &lt;a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/">https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Frameworks&lt;/strong>
&lt;ul>
&lt;li>CIS benchmarks: &lt;a href="https://www.cisecurity.org/benchmark/kubernetes/">https://www.cisecurity.org/benchmark/kubernetes/&lt;/a>&lt;/li>
&lt;li>NIST 800-190: &lt;a href="https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf">https://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800-190.pdf&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;li>&lt;strong>Talks&lt;/strong>
&lt;ul>
&lt;li>The Devil in the Details: Kubernetes’ First Security Assessment (KubeCon NA 2019): &lt;a href="https://www.youtube.com/watch?v=vknE5XEa_Do">https://www.youtube.com/watch?v=vknE5XEa_Do&lt;/a>&lt;/li>
&lt;li>Crafty Requests: Deep Dive into Kubernetes CVE-2018-1002105 (KubeCon EU 2019): &lt;a href="https://www.youtube.com/watch?v=VjSJqc13PNk">https://www.youtube.com/watch?v=VjSJqc13PNk&lt;/a>&lt;/li>
&lt;li>A Hacker’s Guide to Kubernetes and the Cloud (KubeCon EU 2018): &lt;a href="https://www.youtube.com/watch?v=dxKpCO2dAy8">https://www.youtube.com/watch?v=dxKpCO2dAy8&lt;/a>&lt;/li>
&lt;li>Shipping in pirate-infested waters (KubeCon NA 2017): &lt;a href="https://www.youtube.com/watch?v=ohTq0no0ZVU">https://www.youtube.com/watch?v=ohTq0no0ZVU&lt;/a>&lt;/li>
&lt;li>Hacking and Hardening Kubernetes clusters by example (KubeCon NA 2017): &lt;a href="https://www.youtube.com/watch?v=vTgQLzeBfRU">https://www.youtube.com/watch?v=vTgQLzeBfRU&lt;/a>&lt;/li>
&lt;/ul>
&lt;/li>
&lt;/ul>
&lt;p>If you find something, please report a security bug to the Kubernetes bug bounty at &lt;a href="https://hackerone.com/kubernetes">https://hackerone.com/kubernetes&lt;/a>.&lt;/p>
&lt;!-- Docs to Markdown version 1.0β17 --></description></item><item><title>Blog: Remembering Brad Childs</title><link>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</link><pubDate>Fri, 10 Jan 2020 10:00:00 -0800</pubDate><guid>https://kubernetes.io/blog/2020/01/10/remembering-brad-childs/</guid><description>
&lt;p>&lt;strong>Authors:&lt;/strong> Paul Morie, Red Hat&lt;/p>
&lt;p>Last year, the Kubernetes family lost one of its own. Brad Childs was a
SIG Storage chair and long time contributor to the project. Brad worked on a
number of features in storage and was known as much for his friendliness and
sense of humor as for his technical contributions and leadership.&lt;/p>
&lt;p>We recently spent time remembering Brad at Kubecon NA:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://youtu.be/4eI2PTAJ-sE">A Tribute to Bradley Childs&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/cncf/memorials/blob/master/bradley-childs.md">CNCF Memorial&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Our hearts go out to Brad’s friends and family and others whose lives he touched
inside and outside the Kubernetes community.&lt;/p>
&lt;p>Thank you for everything, Brad. We’ll miss you.&lt;/p></description></item><item><title>Blog: Testing of CSI drivers</title><link>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</link><pubDate>Wed, 08 Jan 2020 00:00:00 +0000</pubDate><guid>https://kubernetes.io/blog/2020/01/08/testing-of-csi-drivers/</guid><description>
&lt;p>&lt;strong>Author:&lt;/strong> Patrick Ohly (Intel)&lt;/p>
&lt;p>When developing a &lt;a href="https://kubernetes-csi.github.io/docs/">Container Storage Interface (CSI)
driver&lt;/a>, it is useful to leverage
as much prior work as possible. This includes source code (like the
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/">sample CSI hostpath
driver&lt;/a>) but
also existing tests. Besides saving time, using tests written by
someone else has the advantage that it can point out aspects of the
specification that might have been overlooked otherwise.&lt;/p>
&lt;p>An earlier blog post about &lt;a href="https://kubernetes.io/blog/2019/03/22/kubernetes-end-to-end-testing-for-everyone/">end-to-end
testing&lt;/a>
already showed how to use the &lt;a href="https://github.com/kubernetes/kubernetes/tree/master/test/e2e/storage/testsuites">Kubernetes storage
tests&lt;/a>
for testing of a third-party CSI driver. That
approach makes sense when the goal to also add custom E2E tests, but
depends on quite a bit of effort for setting up and maintaining a test
suite.&lt;/p>
&lt;p>When the goal is to merely run the existing tests, then there are
simpler approaches. This blog post introduces those.&lt;/p>
&lt;h2 id="sanity-testing">Sanity testing&lt;/h2>
&lt;p>&lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity">csi-test
sanity&lt;/a>
ensures that a CSI driver conforms to the CSI specification by calling
the gRPC methods in various ways and checking that the outcome is as
required. Despite
its current hosting under the Kubernetes-CSI organization, it is
completely independent of Kubernetes. Tests connect to a running CSI
driver through its Unix domain socket, so although the tests are
written in Go, the driver itself can be implemented in any language.&lt;/p>
&lt;p>The main
&lt;a href="https://github.com/kubernetes-csi/csi-test/blob/master/pkg/sanity/README.md">README&lt;/a>
explains how to include those tests into an existing Go test
suite. The simpler alternative is to just invoke the &lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/cmd/csi-sanity">csi-sanity&lt;/a>
command.&lt;/p>
&lt;h3 id="installation">Installation&lt;/h3>
&lt;p>Starting with csi-test v3.0.0, you can build the &lt;code>csi-sanity&lt;/code> command
with &lt;code>go get github.com/kubernetes-csi/csi-test/cmd/csi-sanity&lt;/code> and
you'll find the compiled binary in &lt;code>$GOPATH/bin/csi-sanity&lt;/code>.&lt;/p>
&lt;p>&lt;code>go get&lt;/code> always builds the latest revision from the master branch. To
build a certain release, &lt;a href="https://github.com/kubernetes-csi/csi-test/releases">get the source
code&lt;/a> and run
&lt;code>make -C cmd/csi-sanity&lt;/code>. This produces &lt;code>cmd/csi-sanity/csi-sanity&lt;/code>.&lt;/p>
&lt;h3 id="usage">Usage&lt;/h3>
&lt;p>The &lt;code>csi-sanity&lt;/code> binary is a full &lt;a href="http://onsi.github.io/ginkgo/">Ginkgo test
suite&lt;/a> and thus has the usual &lt;code>-gingko&lt;/code>
command line flags. In particular, &lt;code>-ginkgo.focus&lt;/code> and
&lt;code>-ginkgo.skip&lt;/code> can be used to select which tests are run resp. not
run.&lt;/p>
&lt;p>During a test run, &lt;code>csi-sanity&lt;/code> simulates the behavior of a container
orchestrator (CO) by creating staging and target directories as required by the CSI spec
and calling a CSI driver via gRPC. The driver must be started before
invoking &lt;code>csi-sanity&lt;/code>. Although the tests currently only check the gRPC
return codes, that might change and so the driver really should make
the changes requested by a call, like mounting a filesystem. That may
mean that it has to run as root.&lt;/p>
&lt;p>At least one &lt;a href="https://github.com/grpc/grpc/blob/master/doc/naming.md">gRPC
endpoint&lt;/a> must
be specified via the &lt;code>-csi.endpoint&lt;/code> parameter when invoking
&lt;code>csi-sanity&lt;/code>, either as absolute path (&lt;code>unix:/tmp/csi.sock&lt;/code>) for a Unix
domain socket or as host name plus port (&lt;code>dns:///my-machine:9000&lt;/code>) for
TCP. &lt;code>csi-sanity&lt;/code> then uses that endpoint for both node and controller
operations. A separate endpoint for controller operations can be
specified with &lt;code>-csi.controllerendpoint&lt;/code>. Directories are created in
&lt;code>/tmp&lt;/code> by default. This can be changed via &lt;code>-csi.mountdir&lt;/code> and
&lt;code>-csi.stagingdir&lt;/code>.&lt;/p>
&lt;p>Some drivers cannot be deployed such that everything is guaranteed to
run on the same host. In such a case, custom scripts have to be used
to handle directories: they log into the host where the CSI node
controller runs and create or remove the directories there.&lt;/p>
&lt;p>For example, during CI testing the &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">CSI hostpath example
driver&lt;/a> gets
deployed on a real Kubernetes cluster before invoking &lt;code>csi-sanity&lt;/code> and then
&lt;code>csi-sanity&lt;/code> connects to it through port forwarding provided by
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/deploy/kubernetes-1.16/hostpath/csi-hostpath-testing.yaml">&lt;code>socat&lt;/code>&lt;/a>.
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/v1.2.0/release-tools/prow.sh#L808-L859">Scripts&lt;/a>
are used to create and remove the directories.&lt;/p>
&lt;p>Here's how one can replicate that, using the v1.2.0 release of the CSI hostpath driver:&lt;/p>
&lt;pre>&lt;code>$ cd csi-driver-host-path
$ git describe --tags HEAD
v1.2.0
$ kubectl get nodes
NAME STATUS ROLES AGE VERSION
127.0.0.1 Ready &amp;lt;none&amp;gt; 42m v1.16.0
$ deploy/kubernetes-1.16/deploy-hostpath.sh
applying RBAC rules
kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-provisioner/v1.4.0/deploy/kubernetes/rbac.yaml
...
deploying hostpath components
deploy/kubernetes-1.16/hostpath/csi-hostpath-attacher.yaml
using image: quay.io/k8scsi/csi-attacher:v2.0.0
service/csi-hostpath-attacher created
statefulset.apps/csi-hostpath-attacher created
deploy/kubernetes-1.16/hostpath/csi-hostpath-driverinfo.yaml
csidriver.storage.k8s.io/hostpath.csi.k8s.io created
deploy/kubernetes-1.16/hostpath/csi-hostpath-plugin.yaml
using image: quay.io/k8scsi/csi-node-driver-registrar:v1.2.0
using image: quay.io/k8scsi/hostpathplugin:v1.2.0
using image: quay.io/k8scsi/livenessprobe:v1.1.0
...
service/hostpath-service created
statefulset.apps/csi-hostpath-socat created
07:38:46 waiting for hostpath deployment to complete, attempt #0
deploying snapshotclass
volumesnapshotclass.snapshot.storage.k8s.io/csi-hostpath-snapclass created
$ cat &amp;gt;mkdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- mktemp -d /tmp/csi-sanity.XXXXXX
EOF
$ cat &amp;gt;rmdir_in_pod.sh &amp;lt;&amp;lt;EOF
#!/bin/sh
kubectl exec csi-hostpathplugin-0 -c hostpath -- rmdir &amp;quot;\$@&amp;quot;
EOF
$ chmod u+x *_in_pod.sh
$ csi-sanity -ginkgo.v \
-csi.endpoint dns:///127.0.0.1:$(kubectl get &amp;quot;services/hostpath-service&amp;quot; -o &amp;quot;jsonpath={..nodePort}&amp;quot;) \
-csi.createstagingpathcmd ./mkdir_in_pod.sh \
-csi.createmountpathcmd ./mkdir_in_pod.sh \
-csi.removestagingpathcmd ./rmdir_in_pod.sh \
-csi.removemountpathcmd ./rmdir_in_pod.sh
Running Suite: CSI Driver Test Suite
====================================
Random Seed: 1570540138
Will run 72 of 72 specs
...
Controller Service [Controller Server] ControllerGetCapabilities
should return appropriate capabilities
/nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:111
STEP: connecting to CSI driver
STEP: creating mount and staging directories
STEP: checking successful response
•
------------------------------
Controller Service [Controller Server] GetCapacity
should return capacity (no optional values added)
/nvme/gopath/src/github.com/kubernetes-csi/csi-test/pkg/sanity/controller.go:149
STEP: reusing connection to CSI driver at dns:///127.0.0.1:30056
STEP: creating mount and staging directories
...
Ran 53 of 72 Specs in 148.206 seconds
SUCCESS! -- 53 Passed | 0 Failed | 0 Pending | 19 Skipped
PASS
&lt;/code>&lt;/pre>&lt;p>Some comments:&lt;/p>
&lt;ul>
&lt;li>The source code of these tests is in the &lt;a href="https://github.com/kubernetes-csi/csi-test/tree/master/pkg/sanity">&lt;code>pkg/sanity&lt;/code>
package&lt;/a>.&lt;/li>
&lt;li>How to determine the external IP address of the node depends on the
cluster. In this example, the cluster was brought up with
&lt;code>hack/local-up-cluster.sh&lt;/code> and thus runs on the local host (&lt;code>127.0.0.1&lt;/code>).
It uses a port allocated by Kubernetes, obtained above with &lt;code>kubectl get &amp;quot;services/hostpath-service&amp;quot;&lt;/code>.
The Kubernetes-CSI CI uses
&lt;a href="https://kind.sigs.k8s.io/docs/user/quick-start/">kind&lt;/a> and there &lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path/blob/3488dc7f994e33485629b86b69a6f34ebb7ef2d9/release-tools/prow.sh#L850">a
Docker
command&lt;/a>
can be used.&lt;/li>
&lt;li>The create script must print the final directory. Using a
unique directory for each test case has the advantage that if
something goes wrong in one test case, others still start with a
clean slate.&lt;/li>
&lt;li>The &amp;quot;staging directory&amp;quot;, aka &lt;code>NodePublishVolumeRequest.target_path&lt;/code>
in the CSI spec, must be created and deleted by the CSI driver while
the CO is responsible for the parent directory. &lt;code>csi-sanity&lt;/code> handles
that by creating a directory and then giving the CSI driver that
directory path with &lt;code>/target&lt;/code> appended at the end. Kubernetes &lt;a href="https://github.com/kubernetes/kubernetes/issues/75535">got
this wrong&lt;/a>
and creates the actual &lt;code>target_path&lt;/code> directory, so CSI drivers which
want to work with Kubernetes currently have to be lenient and must
not fail when that directory already exists.&lt;/li>
&lt;li>The &amp;quot;mount directory&amp;quot; corresponds to
&lt;code>NodeStageVolumeRequest.staging_target_path&lt;/code> and really gets created
by the CO, i.e. &lt;code>csi-sanity&lt;/code>.&lt;/li>
&lt;/ul>
&lt;h2 id="end-to-end-testing">End-to-end testing&lt;/h2>
&lt;p>In contrast to &lt;code>csi-sanity&lt;/code>, end-to-end testing interacts with the CSI
driver through the Kubernetes API, i.e. it simulates operations from a
normal user, like creating a PersistentVolumeClaim. Support for testing external CSI
drivers was
&lt;a href="https://github.com/kubernetes/kubernetes/commit/6644db9914379a4a7b3d3487b41b2010f226e4dc#diff-5b2d9461c960bc9b146c4ab3d77bcaa5">added&lt;/a>
in Kubernetes 1.14.0.&lt;/p>
&lt;h3 id="installation-1">Installation&lt;/h3>
&lt;p>For each Kubernetes release, a test tar archive is published. It's not
listed in the release notes (for example, the ones for
&lt;a href="https://github.com/kubernetes/kubernetes/blob/master/CHANGELOG-1.16.md#downloads-for-v1161">1.16&lt;/a>),
so one has to know that the full URL is
&lt;code>https://dl.k8s.io/&amp;lt;version&amp;gt;/kubernetes-test-linux-amd64.tar.gz&lt;/code> (like
for
&lt;a href="https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz">v1.16.0&lt;/a>).&lt;/p>
&lt;p>These include a &lt;code>e2e.test&lt;/code> binary for Linux on x86-64. Archives for
other platforms are also available, see &lt;a href="https://github.com/kubernetes/enhancements/blob/master/keps/sig-testing/20190118-breaking-apart-the-kubernetes-test-tarball.md#proposal">this
KEP&lt;/a>. The
&lt;code>e2e.test&lt;/code> binary is completely self-contained, so one can &amp;quot;install&amp;quot;
it and the &lt;a href="https://onsi.github.io/ginkgo/">&lt;code>ginkgo&lt;/code> test runner&lt;/a> with:&lt;/p>
&lt;pre>&lt;code>curl --location https://dl.k8s.io/v1.16.0/kubernetes-test-linux-amd64.tar.gz | \
tar --strip-components=3 -zxf - kubernetes/test/bin/e2e.test kubernetes/test/bin/ginkgo
&lt;/code>&lt;/pre>&lt;p>Each &lt;code>e2e.test&lt;/code> binary contains tests that match the features
available in the corresponding release. In particular, the &lt;code>[Feature: xyz]&lt;/code> tags change between releases: they separate tests of alpha
features from tests of non-alpha features. Also, the tests from an
older release might rely on APIs that were removed in more recent
Kubernetes releases. To avoid problems, it's best to simply use the
&lt;code>e2e.test&lt;/code> binary that matches the Kubernetes release that is used for
testing.&lt;/p>
&lt;h3 id="usage-1">Usage&lt;/h3>
&lt;p>Not all features of a CSI driver can be discovered through the
Kubernetes API. Therefore a configuration file in YAML or JSON format
is needed which describes the driver that is to be tested. That file
is used to populate &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/external/external.go#L142-L211">the driverDefinition
struct&lt;/a>
and &lt;a href="https://github.com/kubernetes/kubernetes/blob/v1.16.0/test/e2e/storage/testsuites/testdriver.go#L139-L185">the DriverInfo
struct&lt;/a>
that is embedded inside it. For detailed usage instructions of
individual fields refer to these structs.&lt;/p>
&lt;p>A word of warning: tests are often only run when setting some fields and the
file parser does not warn about unknown fields, so always check that
the file really matches those structs.&lt;/p>
&lt;p>Here is an example that tests the
&lt;a href="https://github.com/kubernetes-csi/csi-driver-host-path">&lt;code>csi-driver-host-path&lt;/code>&lt;/a>:&lt;/p>
&lt;pre>&lt;code>$ cat &amp;gt;test-driver.yaml &amp;lt;&amp;lt;EOF
StorageClass:
FromName: true
SnapshotClass:
FromName: true
DriverInfo:
Name: hostpath.csi.k8s.io
Capabilities:
block: true
controllerExpansion: true
exec: true
multipods: true
persistence: true
pvcDataSource: true
snapshotDataSource: true
InlineVolumes:
- Attributes: {}
EOF
&lt;/code>&lt;/pre>&lt;p>At a minimum, you need to define the storage class you want to use in
the test, the name of your driver, and what capabilities you want to
test.
As with &lt;code>csi-sanity&lt;/code>, the driver has to be running in the cluster
before testing it.
The actual &lt;code>e2e.test&lt;/code> invocation then enables tests for this driver
with &lt;code>-storage.testdriver&lt;/code> and selects the storage tests for it with
&lt;code>-ginkgo.focus&lt;/code>:&lt;/p>
&lt;pre>&lt;code>$ ./e2e.test -ginkgo.v \
-ginkgo.focus='External.Storage' \
-storage.testdriver=test-driver.yaml
Oct 8 17:17:42.230: INFO: The --provider flag is not set. Continuing as if --provider=skeleton had been used.
I1008 17:17:42.230210 648569 e2e.go:92] Starting e2e run &amp;quot;90b9adb0-a3a2-435f-80e0-640742d56104&amp;quot; on Ginkgo node 1
Running Suite: Kubernetes e2e suite
===================================
Random Seed: 1570547861 - Will randomize all specs
Will run 163 of 5060 specs
Oct 8 17:17:42.237: INFO: &amp;gt;&amp;gt;&amp;gt; kubeConfig: /var/run/kubernetes/admin.kubeconfig
Oct 8 17:17:42.241: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
...
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
External Storage [Driver: hostpath.csi.k8s.io] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
should access to two volumes with different volume mode and retain data across pod recreation on the same node
/workspace/anago-v1.16.0-rc.2.1+2bd9643cee5b3b/src/k8s.io/kubernetes/_output/dockerized/go/src/k8s.io/kubernetes/test/e2e/storage/testsuites/multivolume.go:191
[BeforeEach] [Testpattern: Dynamic PV (filesystem volmode)] multiVolume [Slow]
...
&lt;/code>&lt;/pre>&lt;p>You can use &lt;code>ginkgo&lt;/code> to run some kinds of test in parallel.
Alpha feature tests or those that by design have to be run
sequentially then need to be run separately:&lt;/p>
&lt;pre>&lt;code>$ ./ginkgo -p -v \
-focus='External.Storage' \
-skip='\[Feature:|\[Disruptive\]|\[Serial\]' \
./e2e.test \
-- \
-storage.testdriver=test-driver.yaml
$ ./ginkgo -v \
-focus='External.Storage.*(\[Feature:|\[Disruptive\]|\[Serial\])' \
./e2e.test \
-- \
-storage.testdriver=test-driver.yaml
&lt;/code>&lt;/pre>&lt;h2 id="getting-involved">Getting involved&lt;/h2>
&lt;p>Both the Kubernetes storage tests and the sanity tests are meant to be
applicable to arbitrary CSI drivers. But perhaps tests are based on
additional assumptions and your driver does not pass the testing
although it complies with the CSI specification. If that happens then
please file issues (links below).&lt;/p>
&lt;p>These are open source projects which depend on the help of those
using them, so once a problem has been acknowledged, a pull request
addressing it will be highly welcome.&lt;/p>
&lt;p>The same applies to writing new tests. The following searches in the
issue trackers select issues that have been marked specifically as
something that needs someone's help:&lt;/p>
&lt;ul>
&lt;li>&lt;a href="https://github.com/kubernetes-csi/csi-test/issues?q=is%3Aissue+is%3Aopen+label%3A%22help+wanted%22">csi-test&lt;/a>&lt;/li>
&lt;li>&lt;a href="https://github.com/kubernetes/kubernetes/issues?utf8=%E2%9C%93&amp;amp;q=is%3Aopen+is%3Aissue+label%3A%22help+wanted%22+label%3Asig%2Fstorage+">Kubernetes&lt;/a>&lt;/li>
&lt;/ul>
&lt;p>Happy testing! May the issues it finds be few and easy to fix.&lt;/p></description></item></channel></rss>